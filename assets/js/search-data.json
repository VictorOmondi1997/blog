{
  
    
        "post0": {
            "title": "Dr. Semmelweis and the Discovery of Handwashing",
            "content": "1. Meet Dr. Ignaz Semmelweis . This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it&#39;s probably because he&#39;s thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It&#39;s the contaminated hands of the doctors delivering the babies. And they won&#39;t listen to him and wash their hands! . In this notebook, we&#39;re going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let&#39;s start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital. . # importing modules import pandas as pd import matplotlib.pyplot as plt # Read datasets/yearly_deaths_by_clinic.csv into yearly yearly = pd.read_csv(&quot;datasets/yearly_deaths_by_clinic.csv&quot;) # Print out yearly yearly . year births deaths clinic . 0 1841 | 3036 | 237 | clinic 1 | . 1 1842 | 3287 | 518 | clinic 1 | . 2 1843 | 3060 | 274 | clinic 1 | . 3 1844 | 3157 | 260 | clinic 1 | . 4 1845 | 3492 | 241 | clinic 1 | . 5 1846 | 4010 | 459 | clinic 1 | . 6 1841 | 2442 | 86 | clinic 2 | . 7 1842 | 2659 | 202 | clinic 2 | . 8 1843 | 2739 | 164 | clinic 2 | . 9 1844 | 2956 | 68 | clinic 2 | . 10 1845 | 3241 | 66 | clinic 2 | . 11 1846 | 3754 | 105 | clinic 2 | . 2. The alarming number of deaths . The table above shows the number of women giving birth at the two clinics at the Vienna General Hospital for the years 1841 to 1846. You&#39;ll notice that giving birth was very dangerous; an alarming number of women died as the result of childbirth, most of them from childbed fever. . We see this more clearly if we look at the proportion of deaths out of the number of women giving birth. Let&#39;s zoom in on the proportion of deaths at Clinic 1. . # Calculate proportion of deaths per no. births yearly[&quot;proportion_deaths&quot;] = yearly.deaths/yearly.births # Extract clinic 1 data into yearly1 and clinic 2 data into yearly2 yearly1 = yearly[yearly.clinic==&quot;clinic 1&quot;] yearly2 = yearly[yearly.clinic==&quot;clinic 2&quot;] # Print out yearly1 yearly1 . year births deaths clinic proportion_deaths . 0 1841 | 3036 | 237 | clinic 1 | 0.078063 | . 1 1842 | 3287 | 518 | clinic 1 | 0.157591 | . 2 1843 | 3060 | 274 | clinic 1 | 0.089542 | . 3 1844 | 3157 | 260 | clinic 1 | 0.082357 | . 4 1845 | 3492 | 241 | clinic 1 | 0.069015 | . 5 1846 | 4010 | 459 | clinic 1 | 0.114464 | . 3. Death at the clinics . If we now plot the proportion of deaths at both clinic 1 and clinic 2 we&#39;ll see a curious pattern... . # This makes plots appear in the notebook %matplotlib inline # Plot yearly proportion of deaths at the two clinics ax = yearly1.plot(x=&quot;year&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Clinic 1&quot;) yearly2.plot(x=&quot;year&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Clinic 2&quot;, ax=ax) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7fab04d06d30&gt; . 4. The handwashing begins . Why is the proportion of deaths constantly so much higher in Clinic 1? Semmelweis saw the same pattern and was puzzled and distressed. The only difference between the clinics was that many medical students served at Clinic 1, while mostly midwife students served at Clinic 2. While the midwives only tended to the women giving birth, the medical students also spent time in the autopsy rooms examining corpses. . Semmelweis started to suspect that something on the corpses, spread from the hands of the medical students, caused childbed fever. So in a desperate attempt to stop the high mortality rates, he decreed: Wash your hands! This was an unorthodox and controversial request, nobody in Vienna knew about bacteria at this point in time. . Let&#39;s load in monthly data from Clinic 1 to see if the handwashing had any effect. . # Read datasets/monthly_deaths.csv into monthly monthly = pd.read_csv(&quot;datasets/monthly_deaths.csv&quot;, parse_dates=[&quot;date&quot;]) # Calculate proportion of deaths per no. births monthly[&quot;proportion_deaths&quot;] = monthly.deaths/monthly.births # Print out the first rows in monthly monthly.head() . date births deaths proportion_deaths . 0 1841-01-01 | 254 | 37 | 0.145669 | . 1 1841-02-01 | 239 | 18 | 0.075314 | . 2 1841-03-01 | 277 | 12 | 0.043321 | . 3 1841-04-01 | 255 | 4 | 0.015686 | . 4 1841-05-01 | 255 | 2 | 0.007843 | . 5. The effect of handwashing . With the data loaded we can now look at the proportion of deaths over time. In the plot below we haven&#39;t marked where obligatory handwashing started, but it reduced the proportion of deaths to such a degree that you should be able to spot it! . # Plot monthly proportion of deaths ax = monthly.plot(x=&quot;date&quot;, y=&quot;proportion_deaths&quot;) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7faae5d53b38&gt; . 6. The effect of handwashing highlighted . Starting from the summer of 1847 the proportion of deaths is drastically reduced and, yes, this was when Semmelweis made handwashing obligatory. . The effect of handwashing is made even more clear if we highlight this in the graph. . # Date when handwashing was made mandatory import pandas as pd handwashing_start = pd.to_datetime(&#39;1847-06-01&#39;) # Split monthly into before and after handwashing_start before_washing = monthly[monthly.date&lt;handwashing_start] after_washing = monthly[monthly.date&gt;=handwashing_start] # Plot monthly proportion of deaths before and after handwashing ax = before_washing.plot(x=&quot;date&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Before Washing&quot;) after_washing.plot(ax=ax, x=&quot;date&quot;, y=&quot;proportion_deaths&quot;, label=&quot;After Washing&quot;) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7faae5c44940&gt; . 7. More handwashing, fewer deaths? . Again, the graph shows that handwashing had a huge effect. How much did it reduce the monthly proportion of deaths on average? . # Difference in mean monthly proportion of deaths due to handwashing before_proportion = before_washing.proportion_deaths after_proportion = after_washing.proportion_deaths mean_diff = after_proportion.mean() - before_proportion.mean() mean_diff . -0.08395660751183336 . 8. A Bootstrap analysis of Semmelweis handwashing data . It reduced the proportion of deaths by around 8 percentage points! From 10% on average to just 2% (which is still a high number by modern standards). . To get a feeling for the uncertainty around how much handwashing reduces mortalities we could look at a confidence interval (here calculated using the bootstrap method). . # A bootstrap analysis of the reduction of deaths due to handwashing boot_mean_diff = [] for i in range(3000): boot_before = before_proportion.sample(frac=1, replace=True) boot_after = after_proportion.sample(frac=1, replace=True) boot_mean_diff.append( boot_after.mean() - boot_before.mean() ) # Calculating a 95% confidence interval from boot_mean_diff confidence_interval = pd.Series(boot_mean_diff).quantile([.025, .975]) confidence_interval . 0.025 -0.102262 0.975 -0.067096 dtype: float64 . 9. The fate of Dr. Semmelweis . So handwashing reduced the proportion of deaths by between 6.7 and 10 percentage points, according to a 95% confidence interval. All in all, it would seem that Semmelweis had solid evidence that handwashing was a simple but highly effective procedure that could save many lives. . The tragedy is that, despite the evidence, Semmelweis&#39; theory — that childbed fever was caused by some &quot;substance&quot; (what we today know as bacteria) from autopsy room corpses — was ridiculed by contemporary scientists. The medical community largely rejected his discovery and in 1849 he was forced to leave the Vienna General Hospital for good. . One reason for this was that statistics and statistical arguments were uncommon in medical science in the 1800s. Semmelweis only published his data as long tables of raw data, but he didn&#39;t show any graphs nor confidence intervals. If he would have had access to the analysis we&#39;ve just put together he might have been more successful in getting the Viennese doctors to wash their hands. . # The data Semmelweis collected points to that: doctors_should_wash_their_hands = True .",
            "url": "https://victoromondi1997.github.io/blog/ignaz/data-analysis/handwashing/2020/07/01/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "relUrl": "/ignaz/data-analysis/handwashing/2020/07/01/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Exploring Boston Weather Data",
            "content": "weather = readRDS(gzcon(url(&#39;https://assets.datacamp.com/production/repositories/34/datasets/b3c1036d9a60a9dfe0f99051d2474a54f76055ea/weather.rds&#39;))) . Libraries . library(readr) library(dplyr) library(lubridate) library(stringr) library(installr) library(tidyr) . Warning message: &#34;package &#39;tidyr&#39; was built under R version 3.6.3&#34; . # Verify that weather is a data.frame class(weather) # Check the dimensions dim(weather) # View the column names names(weather) . &#39;data.frame&#39; &lt;ol class=list-inline&gt; 286 | 35 | &lt;/ol&gt; &lt;ol class=list-inline&gt; &#39;X&#39; | &#39;year&#39; | &#39;month&#39; | &#39;measure&#39; | &#39;X1&#39; | &#39;X2&#39; | &#39;X3&#39; | &#39;X4&#39; | &#39;X5&#39; | &#39;X6&#39; | &#39;X7&#39; | &#39;X8&#39; | &#39;X9&#39; | &#39;X10&#39; | &#39;X11&#39; | &#39;X12&#39; | &#39;X13&#39; | &#39;X14&#39; | &#39;X15&#39; | &#39;X16&#39; | &#39;X17&#39; | &#39;X18&#39; | &#39;X19&#39; | &#39;X20&#39; | &#39;X21&#39; | &#39;X22&#39; | &#39;X23&#39; | &#39;X24&#39; | &#39;X25&#39; | &#39;X26&#39; | &#39;X27&#39; | &#39;X28&#39; | &#39;X29&#39; | &#39;X30&#39; | &#39;X31&#39; | &lt;/ol&gt; We&#39;ve confirmed that the object is a data frame with 286 rows and 35 columns. . Summarize the data . Next up is to look at some summaries of the data. This is where functions like str(), glimpse() from dplyr, and summary() come in handy. . # View the structure of the data str(weather) # Look at the structure using dplyr&#39;s glimpse() glimpse(weather) # View a summary of the data summary(weather) . &#39;data.frame&#39;: 286 obs. of 35 variables: $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... $ month : int 12 12 12 12 12 12 12 12 12 12 ... $ measure: chr &#34;Max.TemperatureF&#34; &#34;Mean.TemperatureF&#34; &#34;Min.TemperatureF&#34; &#34;Max.Dew.PointF&#34; ... $ X1 : chr &#34;64&#34; &#34;52&#34; &#34;39&#34; &#34;46&#34; ... $ X2 : chr &#34;42&#34; &#34;38&#34; &#34;33&#34; &#34;40&#34; ... $ X3 : chr &#34;51&#34; &#34;44&#34; &#34;37&#34; &#34;49&#34; ... $ X4 : chr &#34;43&#34; &#34;37&#34; &#34;30&#34; &#34;24&#34; ... $ X5 : chr &#34;42&#34; &#34;34&#34; &#34;26&#34; &#34;37&#34; ... $ X6 : chr &#34;45&#34; &#34;42&#34; &#34;38&#34; &#34;45&#34; ... $ X7 : chr &#34;38&#34; &#34;30&#34; &#34;21&#34; &#34;36&#34; ... $ X8 : chr &#34;29&#34; &#34;24&#34; &#34;18&#34; &#34;28&#34; ... $ X9 : chr &#34;49&#34; &#34;39&#34; &#34;29&#34; &#34;49&#34; ... $ X10 : chr &#34;48&#34; &#34;43&#34; &#34;38&#34; &#34;45&#34; ... $ X11 : chr &#34;39&#34; &#34;36&#34; &#34;32&#34; &#34;37&#34; ... $ X12 : chr &#34;39&#34; &#34;35&#34; &#34;31&#34; &#34;28&#34; ... $ X13 : chr &#34;42&#34; &#34;37&#34; &#34;32&#34; &#34;28&#34; ... $ X14 : chr &#34;45&#34; &#34;39&#34; &#34;33&#34; &#34;29&#34; ... $ X15 : chr &#34;42&#34; &#34;37&#34; &#34;32&#34; &#34;33&#34; ... $ X16 : chr &#34;44&#34; &#34;40&#34; &#34;35&#34; &#34;42&#34; ... $ X17 : chr &#34;49&#34; &#34;45&#34; &#34;41&#34; &#34;46&#34; ... $ X18 : chr &#34;44&#34; &#34;40&#34; &#34;36&#34; &#34;34&#34; ... $ X19 : chr &#34;37&#34; &#34;33&#34; &#34;29&#34; &#34;25&#34; ... $ X20 : chr &#34;36&#34; &#34;32&#34; &#34;27&#34; &#34;30&#34; ... $ X21 : chr &#34;36&#34; &#34;33&#34; &#34;30&#34; &#34;30&#34; ... $ X22 : chr &#34;44&#34; &#34;39&#34; &#34;33&#34; &#34;39&#34; ... $ X23 : chr &#34;47&#34; &#34;45&#34; &#34;42&#34; &#34;45&#34; ... $ X24 : chr &#34;46&#34; &#34;44&#34; &#34;41&#34; &#34;46&#34; ... $ X25 : chr &#34;59&#34; &#34;52&#34; &#34;44&#34; &#34;58&#34; ... $ X26 : chr &#34;50&#34; &#34;44&#34; &#34;37&#34; &#34;31&#34; ... $ X27 : chr &#34;52&#34; &#34;45&#34; &#34;38&#34; &#34;34&#34; ... $ X28 : chr &#34;52&#34; &#34;46&#34; &#34;40&#34; &#34;42&#34; ... $ X29 : chr &#34;41&#34; &#34;36&#34; &#34;30&#34; &#34;26&#34; ... $ X30 : chr &#34;30&#34; &#34;26&#34; &#34;22&#34; &#34;10&#34; ... $ X31 : chr &#34;30&#34; &#34;25&#34; &#34;20&#34; &#34;8&#34; ... Rows: 286 Columns: 35 $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ... $ year &lt;int&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,... $ month &lt;int&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,... $ measure &lt;chr&gt; &#34;Max.TemperatureF&#34;, &#34;Mean.TemperatureF&#34;, &#34;Min.TemperatureF&#34;... $ X1 &lt;chr&gt; &#34;64&#34;, &#34;52&#34;, &#34;39&#34;, &#34;46&#34;, &#34;40&#34;, &#34;26&#34;, &#34;74&#34;, &#34;63&#34;, &#34;52&#34;, &#34;30.4... $ X2 &lt;chr&gt; &#34;42&#34;, &#34;38&#34;, &#34;33&#34;, &#34;40&#34;, &#34;27&#34;, &#34;17&#34;, &#34;92&#34;, &#34;72&#34;, &#34;51&#34;, &#34;30.7... $ X3 &lt;chr&gt; &#34;51&#34;, &#34;44&#34;, &#34;37&#34;, &#34;49&#34;, &#34;42&#34;, &#34;24&#34;, &#34;100&#34;, &#34;79&#34;, &#34;57&#34;, &#34;30.... $ X4 &lt;chr&gt; &#34;43&#34;, &#34;37&#34;, &#34;30&#34;, &#34;24&#34;, &#34;21&#34;, &#34;13&#34;, &#34;69&#34;, &#34;54&#34;, &#34;39&#34;, &#34;30.5... $ X5 &lt;chr&gt; &#34;42&#34;, &#34;34&#34;, &#34;26&#34;, &#34;37&#34;, &#34;25&#34;, &#34;12&#34;, &#34;85&#34;, &#34;66&#34;, &#34;47&#34;, &#34;30.6... $ X6 &lt;chr&gt; &#34;45&#34;, &#34;42&#34;, &#34;38&#34;, &#34;45&#34;, &#34;40&#34;, &#34;36&#34;, &#34;100&#34;, &#34;93&#34;, &#34;85&#34;, &#34;30.... $ X7 &lt;chr&gt; &#34;38&#34;, &#34;30&#34;, &#34;21&#34;, &#34;36&#34;, &#34;20&#34;, &#34;-3&#34;, &#34;92&#34;, &#34;61&#34;, &#34;29&#34;, &#34;30.6... $ X8 &lt;chr&gt; &#34;29&#34;, &#34;24&#34;, &#34;18&#34;, &#34;28&#34;, &#34;16&#34;, &#34;3&#34;, &#34;92&#34;, &#34;70&#34;, &#34;47&#34;, &#34;30.77... $ X9 &lt;chr&gt; &#34;49&#34;, &#34;39&#34;, &#34;29&#34;, &#34;49&#34;, &#34;41&#34;, &#34;28&#34;, &#34;100&#34;, &#34;93&#34;, &#34;86&#34;, &#34;30.... $ X10 &lt;chr&gt; &#34;48&#34;, &#34;43&#34;, &#34;38&#34;, &#34;45&#34;, &#34;39&#34;, &#34;37&#34;, &#34;100&#34;, &#34;95&#34;, &#34;89&#34;, &#34;29.... $ X11 &lt;chr&gt; &#34;39&#34;, &#34;36&#34;, &#34;32&#34;, &#34;37&#34;, &#34;31&#34;, &#34;27&#34;, &#34;92&#34;, &#34;87&#34;, &#34;82&#34;, &#34;29.8... $ X12 &lt;chr&gt; &#34;39&#34;, &#34;35&#34;, &#34;31&#34;, &#34;28&#34;, &#34;27&#34;, &#34;25&#34;, &#34;85&#34;, &#34;75&#34;, &#34;64&#34;, &#34;29.8... $ X13 &lt;chr&gt; &#34;42&#34;, &#34;37&#34;, &#34;32&#34;, &#34;28&#34;, &#34;26&#34;, &#34;24&#34;, &#34;75&#34;, &#34;65&#34;, &#34;55&#34;, &#34;29.8... $ X14 &lt;chr&gt; &#34;45&#34;, &#34;39&#34;, &#34;33&#34;, &#34;29&#34;, &#34;27&#34;, &#34;25&#34;, &#34;82&#34;, &#34;68&#34;, &#34;53&#34;, &#34;29.9... $ X15 &lt;chr&gt; &#34;42&#34;, &#34;37&#34;, &#34;32&#34;, &#34;33&#34;, &#34;29&#34;, &#34;27&#34;, &#34;89&#34;, &#34;75&#34;, &#34;60&#34;, &#34;30.1... $ X16 &lt;chr&gt; &#34;44&#34;, &#34;40&#34;, &#34;35&#34;, &#34;42&#34;, &#34;36&#34;, &#34;30&#34;, &#34;96&#34;, &#34;85&#34;, &#34;73&#34;, &#34;30.1... $ X17 &lt;chr&gt; &#34;49&#34;, &#34;45&#34;, &#34;41&#34;, &#34;46&#34;, &#34;41&#34;, &#34;32&#34;, &#34;100&#34;, &#34;85&#34;, &#34;70&#34;, &#34;29.... $ X18 &lt;chr&gt; &#34;44&#34;, &#34;40&#34;, &#34;36&#34;, &#34;34&#34;, &#34;30&#34;, &#34;26&#34;, &#34;89&#34;, &#34;73&#34;, &#34;57&#34;, &#34;29.8... $ X19 &lt;chr&gt; &#34;37&#34;, &#34;33&#34;, &#34;29&#34;, &#34;25&#34;, &#34;22&#34;, &#34;20&#34;, &#34;69&#34;, &#34;63&#34;, &#34;56&#34;, &#34;30.1... $ X20 &lt;chr&gt; &#34;36&#34;, &#34;32&#34;, &#34;27&#34;, &#34;30&#34;, &#34;24&#34;, &#34;20&#34;, &#34;89&#34;, &#34;79&#34;, &#34;69&#34;, &#34;30.3... $ X21 &lt;chr&gt; &#34;36&#34;, &#34;33&#34;, &#34;30&#34;, &#34;30&#34;, &#34;27&#34;, &#34;25&#34;, &#34;85&#34;, &#34;77&#34;, &#34;69&#34;, &#34;30.3... $ X22 &lt;chr&gt; &#34;44&#34;, &#34;39&#34;, &#34;33&#34;, &#34;39&#34;, &#34;34&#34;, &#34;25&#34;, &#34;89&#34;, &#34;79&#34;, &#34;69&#34;, &#34;30.4... $ X23 &lt;chr&gt; &#34;47&#34;, &#34;45&#34;, &#34;42&#34;, &#34;45&#34;, &#34;42&#34;, &#34;37&#34;, &#34;100&#34;, &#34;91&#34;, &#34;82&#34;, &#34;30.... $ X24 &lt;chr&gt; &#34;46&#34;, &#34;44&#34;, &#34;41&#34;, &#34;46&#34;, &#34;44&#34;, &#34;41&#34;, &#34;100&#34;, &#34;98&#34;, &#34;96&#34;, &#34;30.... $ X25 &lt;chr&gt; &#34;59&#34;, &#34;52&#34;, &#34;44&#34;, &#34;58&#34;, &#34;43&#34;, &#34;29&#34;, &#34;100&#34;, &#34;75&#34;, &#34;49&#34;, &#34;29.... $ X26 &lt;chr&gt; &#34;50&#34;, &#34;44&#34;, &#34;37&#34;, &#34;31&#34;, &#34;29&#34;, &#34;28&#34;, &#34;70&#34;, &#34;60&#34;, &#34;49&#34;, &#34;30.1... $ X27 &lt;chr&gt; &#34;52&#34;, &#34;45&#34;, &#34;38&#34;, &#34;34&#34;, &#34;31&#34;, &#34;29&#34;, &#34;70&#34;, &#34;60&#34;, &#34;50&#34;, &#34;30.2... $ X28 &lt;chr&gt; &#34;52&#34;, &#34;46&#34;, &#34;40&#34;, &#34;42&#34;, &#34;35&#34;, &#34;27&#34;, &#34;76&#34;, &#34;65&#34;, &#34;53&#34;, &#34;29.9... $ X29 &lt;chr&gt; &#34;41&#34;, &#34;36&#34;, &#34;30&#34;, &#34;26&#34;, &#34;20&#34;, &#34;10&#34;, &#34;64&#34;, &#34;51&#34;, &#34;37&#34;, &#34;30.2... $ X30 &lt;chr&gt; &#34;30&#34;, &#34;26&#34;, &#34;22&#34;, &#34;10&#34;, &#34;4&#34;, &#34;-6&#34;, &#34;50&#34;, &#34;38&#34;, &#34;26&#34;, &#34;30.36... $ X31 &lt;chr&gt; &#34;30&#34;, &#34;25&#34;, &#34;20&#34;, &#34;8&#34;, &#34;5&#34;, &#34;1&#34;, &#34;57&#34;, &#34;44&#34;, &#34;31&#34;, &#34;30.32&#34;,... . X year month measure Min. : 1.00 Min. :2014 Min. : 1.000 Length:286 1st Qu.: 72.25 1st Qu.:2015 1st Qu.: 4.000 Class :character Median :143.50 Median :2015 Median : 7.000 Mode :character Mean :143.50 Mean :2015 Mean : 6.923 3rd Qu.:214.75 3rd Qu.:2015 3rd Qu.:10.000 Max. :286.00 Max. :2015 Max. :12.000 X1 X2 X3 X4 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X5 X6 X7 X8 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X9 X10 X11 X12 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X13 X14 X15 X16 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X17 X18 X19 X20 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X21 X22 X23 X24 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X25 X26 X27 X28 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X29 X30 X31 Length:286 Length:286 Length:286 Class :character Class :character Class :character Mode :character Mode :character Mode :character . Now that we have a pretty good feel for how the table is structured, we&#39;ll take a look at some real observations! . Take a closer look . After understanding the structure of the data and looking at some brief summaries, it often helps to preview the actual data. The functions head() and tail() allow us to view the top and bottom rows of the data, respectively. . # View first 6 rows head(weather) # View first 15 rows head(weather, n=15) # View the last 6 rows tail(weather) # View the last 10 rows tail(weather, n=10) . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 1 | 2014 | 12 | Max.TemperatureF | 64 | 42 | 51 | 43 | 42 | 45 | ... | 44 | 47 | 46 | 59 | 50 | 52 | 52 | 41 | 30 | 30 | . 2 | 2014 | 12 | Mean.TemperatureF | 52 | 38 | 44 | 37 | 34 | 42 | ... | 39 | 45 | 44 | 52 | 44 | 45 | 46 | 36 | 26 | 25 | . 3 | 2014 | 12 | Min.TemperatureF | 39 | 33 | 37 | 30 | 26 | 38 | ... | 33 | 42 | 41 | 44 | 37 | 38 | 40 | 30 | 22 | 20 | . 4 | 2014 | 12 | Max.Dew.PointF | 46 | 40 | 49 | 24 | 37 | 45 | ... | 39 | 45 | 46 | 58 | 31 | 34 | 42 | 26 | 10 | 8 | . 5 | 2014 | 12 | MeanDew.PointF | 40 | 27 | 42 | 21 | 25 | 40 | ... | 34 | 42 | 44 | 43 | 29 | 31 | 35 | 20 | 4 | 5 | . 6 | 2014 | 12 | Min.DewpointF | 26 | 17 | 24 | 13 | 12 | 36 | ... | 25 | 37 | 41 | 29 | 28 | 29 | 27 | 10 | -6 | 1 | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 1 | 2014 | 12 | Max.TemperatureF | 64 | 42 | 51 | 43 | 42 | 45 | ... | 44 | 47 | 46 | 59 | 50 | 52 | 52 | 41 | 30 | 30 | . 2 | 2014 | 12 | Mean.TemperatureF | 52 | 38 | 44 | 37 | 34 | 42 | ... | 39 | 45 | 44 | 52 | 44 | 45 | 46 | 36 | 26 | 25 | . 3 | 2014 | 12 | Min.TemperatureF | 39 | 33 | 37 | 30 | 26 | 38 | ... | 33 | 42 | 41 | 44 | 37 | 38 | 40 | 30 | 22 | 20 | . 4 | 2014 | 12 | Max.Dew.PointF | 46 | 40 | 49 | 24 | 37 | 45 | ... | 39 | 45 | 46 | 58 | 31 | 34 | 42 | 26 | 10 | 8 | . 5 | 2014 | 12 | MeanDew.PointF | 40 | 27 | 42 | 21 | 25 | 40 | ... | 34 | 42 | 44 | 43 | 29 | 31 | 35 | 20 | 4 | 5 | . 6 | 2014 | 12 | Min.DewpointF | 26 | 17 | 24 | 13 | 12 | 36 | ... | 25 | 37 | 41 | 29 | 28 | 29 | 27 | 10 | -6 | 1 | . 7 | 2014 | 12 | Max.Humidity | 74 | 92 | 100 | 69 | 85 | 100 | ... | 89 | 100 | 100 | 100 | 70 | 70 | 76 | 64 | 50 | 57 | . 8 | 2014 | 12 | Mean.Humidity | 63 | 72 | 79 | 54 | 66 | 93 | ... | 79 | 91 | 98 | 75 | 60 | 60 | 65 | 51 | 38 | 44 | . 9 | 2014 | 12 | Min.Humidity | 52 | 51 | 57 | 39 | 47 | 85 | ... | 69 | 82 | 96 | 49 | 49 | 50 | 53 | 37 | 26 | 31 | . 10 | 2014 | 12 | Max.Sea.Level.PressureIn | 30.45 | 30.71 | 30.4 | 30.56 | 30.68 | 30.42 | ... | 30.4 | 30.31 | 30.13 | 29.96 | 30.16 | 30.22 | 29.99 | 30.22 | 30.36 | 30.32 | . 11 | 2014 | 12 | Mean.Sea.Level.PressureIn | 30.13 | 30.59 | 30.07 | 30.33 | 30.59 | 30.24 | ... | 30.35 | 30.23 | 29.9 | 29.63 | 30.11 | 30.14 | 29.87 | 30.12 | 30.32 | 30.25 | . 12 | 2014 | 12 | Min.Sea.Level.PressureIn | 30.01 | 30.4 | 29.87 | 30.09 | 30.45 | 30.16 | ... | 30.3 | 30.16 | 29.55 | 29.47 | 29.99 | 30.03 | 29.77 | 30 | 30.23 | 30.13 | . 13 | 2014 | 12 | Max.VisibilityMiles | 10 | 10 | 10 | 10 | 10 | 10 | ... | 10 | 10 | 2 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | . 14 | 2014 | 12 | Mean.VisibilityMiles | 10 | 8 | 5 | 10 | 10 | 4 | ... | 10 | 5 | 1 | 8 | 10 | 10 | 10 | 10 | 10 | 10 | . 15 | 2014 | 12 | Min.VisibilityMiles | 10 | 2 | 1 | 10 | 5 | 0 | ... | 4 | 1 | 0 | 1 | 10 | 10 | 10 | 10 | 10 | 10 | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 281281 | 2015 | 12 | Mean.Wind.SpeedMPH | 6 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 282282 | 2015 | 12 | Max.Gust.SpeedMPH | 17 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 283283 | 2015 | 12 | PrecipitationIn | 0.14 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 284284 | 2015 | 12 | CloudCover | 7 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 285285 | 2015 | 12 | Events | Rain | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 286286 | 2015 | 12 | WindDirDegrees | 109 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 277277 | 2015 | 12 | Max.VisibilityMiles | 10 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 278278 | 2015 | 12 | Mean.VisibilityMiles | 8 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 279279 | 2015 | 12 | Min.VisibilityMiles | 1 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 280280 | 2015 | 12 | Max.Wind.SpeedMPH | 15 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 281281 | 2015 | 12 | Mean.Wind.SpeedMPH | 6 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 282282 | 2015 | 12 | Max.Gust.SpeedMPH | 17 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 283283 | 2015 | 12 | PrecipitationIn | 0.14 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 284284 | 2015 | 12 | CloudCover | 7 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 285285 | 2015 | 12 | Events | Rain | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 286286 | 2015 | 12 | WindDirDegrees | 109 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . Let&#39;s tidy the data . Column names are values . The weather dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. . The tidyr package provides the gather() function for exactly this scenario. . gather(df, time, val, t1:t3) . gather() allows us to select multiple columns to be gathered by using the : operator. . # Gather the columns weather2 &lt;- gather(weather, day, value, X1:X31, na.rm = TRUE) # View the head head(weather2) . Xyearmonthmeasuredayvalue . 1 | 2014 | 12 | Max.TemperatureF | X1 | 64 | . 2 | 2014 | 12 | Mean.TemperatureF | X1 | 52 | . 3 | 2014 | 12 | Min.TemperatureF | X1 | 39 | . 4 | 2014 | 12 | Max.Dew.PointF | X1 | 46 | . 5 | 2014 | 12 | MeanDew.PointF | X1 | 40 | . 6 | 2014 | 12 | Min.DewpointF | X1 | 26 | . Values are variable names . Our data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. . The spread() function from tidyr is designed to help with this. . spread(df2, time, val) . # First remove column of row names without_x &lt;- weather2[, -1] # Spread the data weather3 &lt;- spread(without_x, measure, value) # View the head head(weather3) . yearmonthdayCloudCoverEventsMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureF...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014 | 12 | X1 | 6 | Rain | 46 | 29 | 74 | 30.45 | 64 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014 | 12 | X10 | 8 | Rain | 45 | 29 | 100 | 29.58 | 48 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014 | 12 | X11 | 8 | Rain-Snow | 37 | 28 | 92 | 29.81 | 39 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014 | 12 | X12 | 7 | Snow | 28 | 21 | 85 | 29.88 | 39 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014 | 12 | X13 | 5 | | 28 | 23 | 75 | 29.86 | 42 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014 | 12 | X14 | 4 | | 29 | 20 | 82 | 29.91 | 45 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . This dataset is looking much better already! . Prepare the data for analysis . Clean up dates . Now that the weather dataset adheres to tidy data principles, the next step is to prepare it for analysis. We&#39;ll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task. . # Remove X&#39;s from day column weather3$day &lt;- str_replace(weather3$day, &#39;X&#39;, &#39;&#39;) # Unite the year, month, and day columns weather4 &lt;- unite(weather3, date, year, month, day, sep = &quot;-&quot;) # Convert date column to proper date format using lubridates&#39;s ymd() weather4$date &lt;- ymd(weather4$date) # Rearrange columns using dplyr&#39;s select() weather5 &lt;- select(weather4, date, Events, CloudCover:WindDirDegrees) # View the head of weather5 head(weather5) . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014-12-13 | | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014-12-14 | | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . A closer look at column types . It&#39;s important for analysis that variables are coded appropriately. This is not yet the case with our weather data. . # View the structure of weather5 str(weather5) # Examine the first 20 rows of weather5. Are most of the characters numeric? head(weather5, 20) # See what happens if we try to convert PrecipitationIn to numeric as.numeric(weather5$PrecipitationIn) . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ Events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ CloudCover : chr &#34;6&#34; &#34;8&#34; &#34;8&#34; &#34;7&#34; ... $ Max.Dew.PointF : chr &#34;46&#34; &#34;45&#34; &#34;37&#34; &#34;28&#34; ... $ Max.Gust.SpeedMPH : chr &#34;29&#34; &#34;29&#34; &#34;28&#34; &#34;21&#34; ... $ Max.Humidity : chr &#34;74&#34; &#34;100&#34; &#34;92&#34; &#34;85&#34; ... $ Max.Sea.Level.PressureIn : chr &#34;30.45&#34; &#34;29.58&#34; &#34;29.81&#34; &#34;29.88&#34; ... $ Max.TemperatureF : chr &#34;64&#34; &#34;48&#34; &#34;39&#34; &#34;39&#34; ... $ Max.VisibilityMiles : chr &#34;10&#34; &#34;10&#34; &#34;10&#34; &#34;10&#34; ... $ Max.Wind.SpeedMPH : chr &#34;22&#34; &#34;23&#34; &#34;21&#34; &#34;16&#34; ... $ Mean.Humidity : chr &#34;63&#34; &#34;95&#34; &#34;87&#34; &#34;75&#34; ... $ Mean.Sea.Level.PressureIn: chr &#34;30.13&#34; &#34;29.5&#34; &#34;29.61&#34; &#34;29.85&#34; ... $ Mean.TemperatureF : chr &#34;52&#34; &#34;43&#34; &#34;36&#34; &#34;35&#34; ... $ Mean.VisibilityMiles : chr &#34;10&#34; &#34;3&#34; &#34;7&#34; &#34;10&#34; ... $ Mean.Wind.SpeedMPH : chr &#34;13&#34; &#34;13&#34; &#34;13&#34; &#34;11&#34; ... $ MeanDew.PointF : chr &#34;40&#34; &#34;39&#34; &#34;31&#34; &#34;27&#34; ... $ Min.DewpointF : chr &#34;26&#34; &#34;37&#34; &#34;27&#34; &#34;25&#34; ... $ Min.Humidity : chr &#34;52&#34; &#34;89&#34; &#34;82&#34; &#34;64&#34; ... $ Min.Sea.Level.PressureIn : chr &#34;30.01&#34; &#34;29.43&#34; &#34;29.44&#34; &#34;29.81&#34; ... $ Min.TemperatureF : chr &#34;39&#34; &#34;38&#34; &#34;32&#34; &#34;31&#34; ... $ Min.VisibilityMiles : chr &#34;10&#34; &#34;1&#34; &#34;1&#34; &#34;7&#34; ... $ PrecipitationIn : chr &#34;0.01&#34; &#34;0.28&#34; &#34;0.02&#34; &#34;T&#34; ... $ WindDirDegrees : chr &#34;268&#34; &#34;357&#34; &#34;230&#34; &#34;286&#34; ... . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014-12-13 | | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014-12-14 | | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . 2014-12-15 | | 2 | 33 | 21 | 89 | 30.15 | 42 | 10 | 15 | ... | 10 | 6 | 29 | 27 | 60 | 29.91 | 32 | 10 | 0.00 | 324 | . 2014-12-16 | Rain | 8 | 42 | 10 | 96 | 30.17 | 44 | 10 | 8 | ... | 9 | 4 | 36 | 30 | 73 | 29.92 | 35 | 5 | T | 79 | . 2014-12-17 | Rain | 8 | 46 | 26 | 100 | 29.91 | 49 | 10 | 20 | ... | 6 | 11 | 41 | 32 | 70 | 29.69 | 41 | 1 | 0.43 | 311 | . 2014-12-18 | Rain | 7 | 34 | 30 | 89 | 29.87 | 44 | 10 | 23 | ... | 10 | 14 | 30 | 26 | 57 | 29.71 | 36 | 10 | 0.01 | 281 | . 2014-12-19 | | 4 | 25 | 23 | 69 | 30.15 | 37 | 10 | 17 | ... | 10 | 11 | 22 | 20 | 56 | 29.86 | 29 | 10 | 0.00 | 305 | . 2014-12-02 | Rain-Snow | 7 | 40 | 29 | 92 | 30.71 | 42 | 10 | 24 | ... | 8 | 15 | 27 | 17 | 51 | 30.4 | 33 | 2 | 0.10 | 62 | . 2014-12-20 | Snow | 6 | 30 | 26 | 89 | 30.31 | 36 | 10 | 21 | ... | 10 | 10 | 24 | 20 | 69 | 30.17 | 27 | 7 | T | 350 | . 2014-12-21 | Snow | 8 | 30 | 20 | 85 | 30.37 | 36 | 10 | 16 | ... | 9 | 9 | 27 | 25 | 69 | 30.28 | 30 | 6 | T | 2 | . 2014-12-22 | Rain | 7 | 39 | 22 | 89 | 30.4 | 44 | 10 | 18 | ... | 10 | 8 | 34 | 25 | 69 | 30.3 | 33 | 4 | 0.05 | 24 | . 2014-12-23 | Rain | 8 | 45 | 25 | 100 | 30.31 | 47 | 10 | 20 | ... | 5 | 13 | 42 | 37 | 82 | 30.16 | 42 | 1 | 0.25 | 63 | . 2014-12-24 | Fog-Rain | 8 | 46 | 15 | 100 | 30.13 | 46 | 2 | 13 | ... | 1 | 6 | 44 | 41 | 96 | 29.55 | 41 | 0 | 0.56 | 12 | . 2014-12-25 | Rain | 6 | 58 | 40 | 100 | 29.96 | 59 | 10 | 28 | ... | 8 | 14 | 43 | 29 | 49 | 29.47 | 44 | 1 | 0.14 | 250 | . 2014-12-26 | | 1 | 31 | 25 | 70 | 30.16 | 50 | 10 | 18 | ... | 10 | 11 | 29 | 28 | 49 | 29.99 | 37 | 10 | 0.00 | 255 | . 2014-12-27 | | 3 | 34 | 21 | 70 | 30.22 | 52 | 10 | 17 | ... | 10 | 9 | 31 | 29 | 50 | 30.03 | 38 | 10 | 0.00 | 251 | . Warning message in eval(expr, envir, enclos): &#34;NAs introduced by coercion&#34; . &lt;ol class=list-inline&gt; 0.01 | 0.28 | 0.02 | &lt;NA&gt; | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0.43 | 0.01 | 0 | 0.1 | &lt;NA&gt; | &lt;NA&gt; | 0.05 | 0.25 | 0.56 | 0.14 | 0 | 0 | 0.01 | 0 | 0.44 | 0 | 0 | 0 | 0.11 | 1.09 | 0.13 | 0.03 | 2.9 | 0 | 0 | 0 | 0.2 | 0 | &lt;NA&gt; | 0.12 | 0 | 0 | 0.15 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.71 | 0 | 0.1 | 0.95 | 0.01 | &lt;NA&gt; | 0.62 | 0.06 | 0.05 | 0.57 | 0 | 0.02 | &lt;NA&gt; | 0 | 0.01 | 0 | 0.05 | 0.01 | 0.03 | 0 | 0.23 | 0.39 | 0 | 0.02 | 0.01 | 0.06 | 0.78 | 0 | 0.17 | 0.11 | 0 | &lt;NA&gt; | 0.07 | 0.02 | 0 | 0 | 0 | 0 | 0.09 | &lt;NA&gt; | 0.07 | 0.37 | 0.88 | 0.17 | 0.06 | 0.01 | 0 | 0 | 0.8 | 0.27 | 0 | 0.14 | 0 | 0 | 0.01 | 0.05 | 0.09 | 0 | 0 | 0 | 0.04 | 0.8 | 0.21 | 0.12 | 0 | 0.26 | &lt;NA&gt; | 0 | 0.02 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0.09 | 0 | 0 | 0 | 0.01 | 0 | 0 | 0.06 | 0 | 0 | 0 | 0.61 | 0.54 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0 | 0 | 0.1 | 0.07 | 0 | 0.03 | 0 | 0.39 | 0 | 0 | 0.03 | 0.26 | 0.09 | 0 | 0 | 0 | 0.02 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0.27 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0.91 | 0 | 0.02 | 0 | 0 | 0 | 0 | 0.38 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.4 | &lt;NA&gt; | 0 | 0 | 0 | 0.74 | 0.04 | 1.72 | 0 | 0.01 | 0 | 0 | &lt;NA&gt; | 0.2 | 1.43 | &lt;NA&gt; | 0 | 0 | 0 | &lt;NA&gt; | 0.09 | 0 | &lt;NA&gt; | &lt;NA&gt; | 0.5 | 1.12 | 0 | 0 | 0 | 0.03 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0.14 | &lt;NA&gt; | 0 | &lt;NA&gt; | &lt;NA&gt; | 0 | 0 | 0.01 | 0 | &lt;NA&gt; | 0.06 | 0 | 0 | 0 | 0.02 | 0 | &lt;NA&gt; | 0 | 0 | 0.02 | &lt;NA&gt; | 0.15 | &lt;NA&gt; | 0 | 0.83 | 0 | 0 | 0 | 0.08 | 0 | 0 | 0.14 | 0 | 0 | 0 | 0.63 | &lt;NA&gt; | 0.02 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0 | 0 | 0 | 0.49 | 0 | 0 | 0 | 0 | 0 | 0 | 0.17 | 0.66 | 0.01 | 0.38 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.04 | 0.01 | 2.46 | &lt;NA&gt; | 0 | 0 | 0 | 0.2 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0.12 | 0 | 0 | &lt;NA&gt; | &lt;NA&gt; | &lt;NA&gt; | 0 | 0.08 | &lt;NA&gt; | 0.07 | &lt;NA&gt; | 0 | 0 | 0.03 | 0 | 0 | 0.36 | 0.73 | 0.01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.34 | &lt;NA&gt; | 0.07 | 0.54 | 0.04 | 0.01 | 0 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.86 | 0 | 0.3 | 0.04 | 0 | 0 | 0 | 0 | 0.21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.14 | &lt;/ol&gt; Column type conversions . &quot;T&quot; was used to denote a trace amount (i.e. too small to be accurately measured) of precipitation in the PrecipitationIn column. In order to coerce this column to numeric, wwe&#39;ll need to deal with this somehow. To keep things simple, we will just replace &quot;T&quot; with zero, as a string (&quot;0&quot;). . # Replace &quot;T&quot; with &quot;0&quot; (T = trace) weather5$PrecipitationIn &lt;- str_replace(weather5$PrecipitationIn, &quot;T&quot;, &quot;0&quot;) # Convert characters to numerics weather6 &lt;- mutate_at(weather5, vars(CloudCover:WindDirDegrees), funs(as.numeric)) # Look at result str(weather6) . Warning message: &#34;`funs()` is deprecated as of dplyr 0.8.0. Please use a list of either functions or lambdas: # Simple named list: list(mean = mean, median = median) # Auto named with `tibble::lst()`: tibble::lst(mean, median) # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) This warning is displayed once every 8 hours. Call `lifecycle::last_warnings()` to see where this warning was generated.&#34; . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ Events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ CloudCover : num 6 8 8 7 5 4 2 8 8 7 ... $ Max.Dew.PointF : num 46 45 37 28 28 29 33 42 46 34 ... $ Max.Gust.SpeedMPH : num 29 29 28 21 23 20 21 10 26 30 ... $ Max.Humidity : num 74 100 92 85 75 82 89 96 100 89 ... $ Max.Sea.Level.PressureIn : num 30.4 29.6 29.8 29.9 29.9 ... $ Max.TemperatureF : num 64 48 39 39 42 45 42 44 49 44 ... $ Max.VisibilityMiles : num 10 10 10 10 10 10 10 10 10 10 ... $ Max.Wind.SpeedMPH : num 22 23 21 16 17 15 15 8 20 23 ... $ Mean.Humidity : num 63 95 87 75 65 68 75 85 85 73 ... $ Mean.Sea.Level.PressureIn: num 30.1 29.5 29.6 29.9 29.8 ... $ Mean.TemperatureF : num 52 43 36 35 37 39 37 40 45 40 ... $ Mean.VisibilityMiles : num 10 3 7 10 10 10 10 9 6 10 ... $ Mean.Wind.SpeedMPH : num 13 13 13 11 12 10 6 4 11 14 ... $ MeanDew.PointF : num 40 39 31 27 26 27 29 36 41 30 ... $ Min.DewpointF : num 26 37 27 25 24 25 27 30 32 26 ... $ Min.Humidity : num 52 89 82 64 55 53 60 73 70 57 ... $ Min.Sea.Level.PressureIn : num 30 29.4 29.4 29.8 29.8 ... $ Min.TemperatureF : num 39 38 32 31 32 33 32 35 41 36 ... $ Min.VisibilityMiles : num 10 1 1 7 10 10 10 5 1 10 ... $ PrecipitationIn : num 0.01 0.28 0.02 0 0 0 0 0 0.43 0.01 ... $ WindDirDegrees : num 268 357 230 286 298 306 324 79 311 281 ... . It looks like our data are finally in the correct formats and organized in a logical manner! Now that our data are in the right form, we can begin the analysis. . Missing, extreme, and unexpected values . Find missing values . Before dealing with missing values in the data, it&#39;s important to find them and figure out why they exist in the first place. . If the dataset is too big to look at all at once, like it is here, we will use sum() and is.na() to quickly size up the situation by counting the number of NA values. . The summary() function also come in handy for identifying which variables contain the missing values. Finally, the which() function is useful for locating the missing values within a particular column. . # Count missing values sum(is.na(weather6)) # Find missing values summary(weather6) # Find indices of NAs in Max.Gust.SpeedMPH ind &lt;- which(is.na(weather6$Max.Gust.SpeedMPH)) # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind, ] . 6 date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 85.69 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :1000.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. :-1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.861 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1612015-05-18 | Fog | 6 | 52 | NA | 100 | 30.30 | 58 | 10 | 16 | ... | 8 | 10 | 48 | 43 | 57 | 30.12 | 49 | 0 | 0 | 72 | . 2052015-06-03 | | 7 | 48 | NA | 93 | 30.31 | 56 | 10 | 14 | ... | 10 | 7 | 45 | 43 | 71 | 30.19 | 47 | 10 | 0 | 90 | . 2732015-08-08 | | 4 | 61 | NA | 87 | 30.02 | 76 | 10 | 14 | ... | 10 | 6 | 57 | 54 | 49 | 29.95 | 61 | 10 | 0 | 45 | . 2752015-09-01 | | 1 | 63 | NA | 78 | 30.06 | 79 | 10 | 15 | ... | 10 | 9 | 62 | 59 | 52 | 29.96 | 69 | 10 | 0 | 54 | . 3082015-10-12 | | 0 | 56 | NA | 89 | 29.86 | 76 | 10 | 15 | ... | 10 | 8 | 51 | 48 | 41 | 29.74 | 51 | 10 | 0 | 199 | . 3582015-11-03 | | 1 | 44 | NA | 82 | 30.25 | 73 | 10 | 16 | ... | 10 | 8 | 42 | 40 | 31 | 30.06 | 47 | 10 | 0 | 281 | . In this situation it&#39;s unclear why these values are missing and there doesn&#39;t appear to be any obvious pattern to their missingness, so we&#39;ll leave them alone for now. . An obvious error . Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with summary(). . Once implausible values are identified, they must be dealt with in an intelligent and informed way. . Sometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data. . # Review distributions for all variables summary(weather6) # Find row with Max.Humidity of 1000 ind &lt;- which(weather6$Max.Humidity==1000) # Look at the data for that day weather6[ind, ] # Change 1000 to 100 weather6$Max.Humidity[ind] &lt;- 100 . date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 85.69 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :1000.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. :-1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.861 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1352015-04-21 | Fog-Rain-Thunderstorm | 6 | 57 | 94 | 1000 | 29.75 | 65 | 10 | 20 | ... | 5 | 10 | 49 | 36 | 42 | 29.53 | 46 | 0 | 0.54 | 184 | . Once you find obvious errors, it&#39;s not too hard to fix them if you know which values they should take. . Another obvious error . We&#39;ve discovered and repaired one obvious error in the data, but it appears that there&#39;s another. Sometimes we get lucky and can infer the correct or intended value from the other data. For example, if you know the minimum and maximum values of a particular metric on a given day... . # Look at summary of Mean.VisibilityMiles summary(weather6$Mean.VisibilityMiles) # Get index of row with -1 value ind &lt;- which(weather6$Mean.VisibilityMiles == -1) # Look at full row weather6[ind,] # Set Mean.VisibilityMiles to the appropriate value weather6$Mean.VisibilityMiles[ind] &lt;- 10 . Min. 1st Qu. Median Mean 3rd Qu. Max. -1.000 8.000 10.000 8.861 10.000 10.000 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1922015-06-18 | | 5 | 54 | 23 | 72 | 30.14 | 76 | 10 | 17 | ... | -1 | 10 | 49 | 45 | 46 | 29.93 | 57 | 10 | 0 | 189 | . Our data are looking tidy. Just a quick sanity check left! . Check other extreme values . In addition to dealing with obvious errors in the data, we want to see if there are other extreme values. In addition to the trusty summary() function, hist() is useful for quickly getting a feel for how different variables are distributed. . # Review summary of full data once more summary(weather6) # Look at histogram for MeanDew.PointF hist(weather6$MeanDew.PointF) # Look at histogram for Min.TemperatureF hist(weather6$Min.TemperatureF) # Compare to histogram for Mean.TemperatureF hist(weather6$Mean.TemperatureF) . date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 83.23 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :100.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. : 1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.891 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . It looks like you have sufficiently tidied your data! . Finishing touches . Before officially calling our weather data clean, we want to put a couple of finishing touches on the data. These are a bit more subjective and may not be necessary for analysis, but they will make the data easier for others to interpret, which is generally a good thing. . There are a number of stylistic conventions in the R language. Depending on who you ask, these conventions may vary. Because the period (.) has special meaning in certain situations, we will be using underscores (_) to separate words in variable names. We also prefer all lowercase letters so that no one has to remember which letters are uppercase or lowercase. . Finally, the events column (renamed to be all lowercase in the first instruction) contains an empty string (&quot;&quot;) for any day on which there was no significant weather event such as rain, fog, a thunderstorm, etc. However, if it&#39;s the first time you&#39;re seeing these data, it may not be obvious that this is the case, so it&#39;s best for us to be explicit and replace the empty strings with something more meaningful. . new_colnames = c(&quot;date&quot;, &quot;events&quot;, &quot;cloud_cover&quot;, &quot;max_dew_point_f&quot;, &quot;max_gust_speed_mph&quot;, &quot;max_humidity&quot;, &quot;max_sea_level_pressure_in&quot;, &quot;max_temperature_f&quot;, &quot;max_visibility_miles&quot;, &quot;max_wind_speed_mph&quot;, &quot;mean_humidity&quot;, &quot;mean_sea_level_pressure_in&quot;, &quot;mean_temperature_f&quot;, &quot;mean_visibility_miles&quot;, &quot;mean_wind_speed_mph&quot;, &quot;mean_dew_point_f&quot;, &quot;min_dew_point_f&quot;, &quot;min_humidity&quot;, &quot;min_sea_level_pressure_in&quot;, &quot;min_temperature_f&quot;, &quot;min_visibility_miles&quot;, &quot;precipitation_in&quot;,&quot;wind_dir_degrees&quot;) . # Clean up column names names(weather6) &lt;- new_colnames # Replace empty cells in events column weather6$events[weather6$events == &quot;&quot;] &lt;- &quot;None&quot; # Print the first 6 rows of weather6 head(weather6) . dateeventscloud_covermax_dew_point_fmax_gust_speed_mphmax_humiditymax_sea_level_pressure_inmax_temperature_fmax_visibility_milesmax_wind_speed_mph...mean_visibility_milesmean_wind_speed_mphmean_dew_point_fmin_dew_point_fmin_humiditymin_sea_level_pressure_inmin_temperature_fmin_visibility_milesprecipitation_inwind_dir_degrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | 0.00 | 286 | . 2014-12-13 | None | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | 0.00 | 298 | . 2014-12-14 | None | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . tail(weather6) . dateeventscloud_covermax_dew_point_fmax_gust_speed_mphmax_humiditymax_sea_level_pressure_inmax_temperature_fmax_visibility_milesmax_wind_speed_mph...mean_visibility_milesmean_wind_speed_mphmean_dew_point_fmin_dew_point_fmin_humiditymin_sea_level_pressure_inmin_temperature_fmin_visibility_milesprecipitation_inwind_dir_degrees . 3612015-11-05 | None | 4 | 61 | 31 | 100 | 30.30 | 76 | 10 | 22 | ... | 9 | 12 | 55 | 48 | 53 | 30.09 | 50 | 5 | 0.00 | 224 | . 3622015-11-06 | None | 4 | 62 | 32 | 93 | 30.07 | 73 | 10 | 26 | ... | 10 | 15 | 61 | 54 | 64 | 29.71 | 62 | 10 | 0.00 | 222 | . 3632015-11-07 | None | 6 | 45 | 33 | 57 | 30.02 | 69 | 10 | 25 | ... | 10 | 13 | 38 | 33 | 39 | 29.83 | 50 | 10 | 0.00 | 280 | . 3642015-11-08 | None | 0 | 34 | 25 | 65 | 30.38 | 56 | 10 | 18 | ... | 10 | 12 | 30 | 24 | 30 | 30.04 | 44 | 10 | 0.00 | 283 | . 3652015-11-09 | None | 2 | 36 | 20 | 70 | 30.43 | 60 | 10 | 16 | ... | 10 | 9 | 32 | 30 | 33 | 30.32 | 41 | 10 | 0.00 | 237 | . 3662015-12-01 | Rain | 7 | 43 | 17 | 96 | 30.40 | 45 | 10 | 15 | ... | 8 | 6 | 35 | 25 | 69 | 30.01 | 32 | 1 | 0.14 | 109 | . str(weather6) . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ cloud_cover : num 6 8 8 7 5 4 2 8 8 7 ... $ max_dew_point_f : num 46 45 37 28 28 29 33 42 46 34 ... $ max_gust_speed_mph : num 29 29 28 21 23 20 21 10 26 30 ... $ max_humidity : num 74 100 92 85 75 82 89 96 100 89 ... $ max_sea_level_pressure_in : num 30.4 29.6 29.8 29.9 29.9 ... $ max_temperature_f : num 64 48 39 39 42 45 42 44 49 44 ... $ max_visibility_miles : num 10 10 10 10 10 10 10 10 10 10 ... $ max_wind_speed_mph : num 22 23 21 16 17 15 15 8 20 23 ... $ mean_humidity : num 63 95 87 75 65 68 75 85 85 73 ... $ mean_sea_level_pressure_in: num 30.1 29.5 29.6 29.9 29.8 ... $ mean_temperature_f : num 52 43 36 35 37 39 37 40 45 40 ... $ mean_visibility_miles : num 10 3 7 10 10 10 10 9 6 10 ... $ mean_wind_speed_mph : num 13 13 13 11 12 10 6 4 11 14 ... $ mean_dew_point_f : num 40 39 31 27 26 27 29 36 41 30 ... $ min_dew_point_f : num 26 37 27 25 24 25 27 30 32 26 ... $ min_humidity : num 52 89 82 64 55 53 60 73 70 57 ... $ min_sea_level_pressure_in : num 30 29.4 29.4 29.8 29.8 ... $ min_temperature_f : num 39 38 32 31 32 33 32 35 41 36 ... $ min_visibility_miles : num 10 1 1 7 10 10 10 5 1 10 ... $ precipitation_in : num 0.01 0.28 0.02 0 0 0 0 0 0.43 0.01 ... $ wind_dir_degrees : num 268 357 230 286 298 306 324 79 311 281 ... . glimpse(weather6) . Rows: 366 Columns: 23 $ date &lt;date&gt; 2014-12-01, 2014-12-10, 2014-12-11, 201... $ events &lt;chr&gt; &#34;Rain&#34;, &#34;Rain&#34;, &#34;Rain-Snow&#34;, &#34;Snow&#34;, &#34;No... $ cloud_cover &lt;dbl&gt; 6, 8, 8, 7, 5, 4, 2, 8, 8, 7, 4, 7, 6, 8... $ max_dew_point_f &lt;dbl&gt; 46, 45, 37, 28, 28, 29, 33, 42, 46, 34, ... $ max_gust_speed_mph &lt;dbl&gt; 29, 29, 28, 21, 23, 20, 21, 10, 26, 30, ... $ max_humidity &lt;dbl&gt; 74, 100, 92, 85, 75, 82, 89, 96, 100, 89... $ max_sea_level_pressure_in &lt;dbl&gt; 30.45, 29.58, 29.81, 29.88, 29.86, 29.91... $ max_temperature_f &lt;dbl&gt; 64, 48, 39, 39, 42, 45, 42, 44, 49, 44, ... $ max_visibility_miles &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, ... $ max_wind_speed_mph &lt;dbl&gt; 22, 23, 21, 16, 17, 15, 15, 8, 20, 23, 1... $ mean_humidity &lt;dbl&gt; 63, 95, 87, 75, 65, 68, 75, 85, 85, 73, ... $ mean_sea_level_pressure_in &lt;dbl&gt; 30.13, 29.50, 29.61, 29.85, 29.82, 29.83... $ mean_temperature_f &lt;dbl&gt; 52, 43, 36, 35, 37, 39, 37, 40, 45, 40, ... $ mean_visibility_miles &lt;dbl&gt; 10, 3, 7, 10, 10, 10, 10, 9, 6, 10, 10, ... $ mean_wind_speed_mph &lt;dbl&gt; 13, 13, 13, 11, 12, 10, 6, 4, 11, 14, 11... $ mean_dew_point_f &lt;dbl&gt; 40, 39, 31, 27, 26, 27, 29, 36, 41, 30, ... $ min_dew_point_f &lt;dbl&gt; 26, 37, 27, 25, 24, 25, 27, 30, 32, 26, ... $ min_humidity &lt;dbl&gt; 52, 89, 82, 64, 55, 53, 60, 73, 70, 57, ... $ min_sea_level_pressure_in &lt;dbl&gt; 30.01, 29.43, 29.44, 29.81, 29.78, 29.78... $ min_temperature_f &lt;dbl&gt; 39, 38, 32, 31, 32, 33, 32, 35, 41, 36, ... $ min_visibility_miles &lt;dbl&gt; 10, 1, 1, 7, 10, 10, 10, 5, 1, 10, 10, 2... $ precipitation_in &lt;dbl&gt; 0.01, 0.28, 0.02, 0.00, 0.00, 0.00, 0.00... $ wind_dir_degrees &lt;dbl&gt; 268, 357, 230, 286, 298, 306, 324, 79, 3... . summary(weather6) . date events cloud_cover max_dew_point_f Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 max_gust_speed_mph max_humidity max_sea_level_pressure_in Min. : 0.00 Min. : 39.00 Min. :29.58 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 Median :25.50 Median : 86.00 Median :30.14 Mean :26.99 Mean : 83.23 Mean :30.16 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 Max. :94.00 Max. :100.00 Max. :30.88 NA&#39;s :6 max_temperature_f max_visibility_miles max_wind_speed_mph mean_humidity Min. :18.00 Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:42.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :60.00 Median :10.000 Median :20.00 Median :66.00 Mean :58.93 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:76.00 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :96.00 Max. :10.000 Max. :38.00 Max. :98.00 mean_sea_level_pressure_in mean_temperature_f mean_visibility_miles Min. :29.49 Min. : 8.00 Min. : 1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.891 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 mean_wind_speed_mph mean_dew_point_f min_dew_point_f min_humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 min_sea_level_pressure_in min_temperature_f min_visibility_miles Min. :29.16 Min. :-3.00 Min. : 0.000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 Median :29.94 Median :46.00 Median :10.000 Mean :29.93 Mean :43.33 Mean : 6.716 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 Max. :30.64 Max. :74.00 Max. :10.000 precipitation_in wind_dir_degrees Min. :0.0000 Min. : 1.0 1st Qu.:0.0000 1st Qu.:113.0 Median :0.0000 Median :222.0 Mean :0.1016 Mean :200.1 3rd Qu.:0.0400 3rd Qu.:275.0 Max. :2.9000 Max. :360.0 .",
            "url": "https://victoromondi1997.github.io/blog/2020/06/30/Exploring-Boston-Weather-Data.html",
            "relUrl": "/2020/06/30/Exploring-Boston-Weather-Data.html",
            "date": " • Jun 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Analyzing Police Activity with Pandas",
            "content": "Preparing the data for analysis . Before beginning our analysis, it is critical that we first examine and clean the dataset, to make working with it a more efficient process. We will fixing data types, handle missing values, and dropping columns and rows while exploring the Stanford Open Policing Project dataset. . Stanford Open Policing Project dataset . Examining the dataset . We&#39;ll be analyzing a dataset of traffic stops in Rhode Island that was collected by the Stanford Open Policing Project. . Before beginning our analysis, it&#39;s important that we familiarize yourself with the dataset. We read the dataset into pandas, examine the first few rows, and then count the number of missing values. . Libraries . import pandas as pd import matplotlib.pyplot as plt from pandas.api.types import CategoricalDtype . # Read &#39;police.csv&#39; into a DataFrame named ri ri = pd.read_csv(&quot;../datasets/police.csv&quot;) # Examine the head of the DataFrame display(ri.head()) # Count the number of missing values in each column ri.isnull().sum() . state stop_date stop_time county_name driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district . 0 RI | 2005-01-04 | 12:55 | NaN | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 1 RI | 2005-01-23 | 23:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | . 2 RI | 2005-02-17 | 04:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 3 RI | 2005-02-20 | 17:15 | NaN | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | . 4 RI | 2005-02-24 | 01:20 | NaN | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | . state 0 stop_date 0 stop_time 0 county_name 91741 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . It looks like most of the columns have at least some missing values. We&#39;ll figure out how to handle these values in the next. . Dropping columns . We&#39;ll drop the county_name column because it only contains missing values, and we&#39;ll drop the state column because all of the traffic stops took place in one state (Rhode Island). . # Examine the shape of the DataFrame print(ri.shape) # Drop the &#39;county_name&#39; and &#39;state&#39; columns ri.drop([&quot;county_name&quot;, &quot;state&quot;], axis=&#39;columns&#39;, inplace=True) # Examine the shape of the DataFrame (again) print(ri.shape) . (91741, 15) (91741, 13) . We&#39;ll continue to remove unnecessary data from the DataFrame . Dropping rows . the driver_gender column will be critical to many of our analyses. Because only a small fraction of rows are missing driver_gender, we&#39;ll drop those rows from the dataset. . # Count the number of missing values in each column display(ri.isnull().sum()) # Drop all rows that are missing &#39;driver_gender&#39; ri.dropna(subset=[&quot;driver_gender&quot;], inplace=True) # Count the number of missing values in each column (again) display(ri.isnull().sum()) # Examine the shape of the DataFrame ri.shape . stop_date 0 stop_time 0 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . stop_date 0 stop_time 0 driver_gender 0 driver_race 0 violation_raw 0 violation 0 search_conducted 0 search_type 83229 stop_outcome 0 is_arrested 0 stop_duration 0 drugs_related_stop 0 district 0 dtype: int64 . (86536, 13) . We dropped around 5,000 rows, which is a small fraction of the dataset, and now only one column remains with any missing values. . Using proper data types . Finding an incorrect data type . ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested object stop_duration object drugs_related_stop bool district object dtype: object . stop_date: should be datetime | stop_time: should be datetime | driver_gender: should be category | driver_race: should be category | violation_raw: should be category | violation: should be category | district: should be category | is_arrested: should be bool | . We&#39;ll fix the data type of the is_arrested column . # Examine the head of the &#39;is_arrested&#39; column display(ri.is_arrested.head()) # Change the data type of &#39;is_arrested&#39; to &#39;bool&#39; ri[&#39;is_arrested&#39;] = ri.is_arrested.astype(&#39;bool&#39;) # Check the data type of &#39;is_arrested&#39; ri.is_arrested.dtype . 0 False 1 False 2 False 3 True 4 False Name: is_arrested, dtype: object . dtype(&#39;bool&#39;) . Creating a DatetimeIndex . Combining object columns . Currently, the date and time of each traffic stop are stored in separate object columns: stop_date and stop_time. We&#39;ll combine these two columns into a single column, and then convert it to datetime format. . ri[&#39;stop_date_time&#39;] = pd.to_datetime(ri.stop_date.str.replace(&quot;/&quot;, &quot;-&quot;).str.cat(ri.stop_time, sep=&quot; &quot;)) ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested bool stop_duration object drugs_related_stop bool district object stop_date_time datetime64[ns] dtype: object . Setting the index . # Set &#39;stop_datetime&#39; as the index ri.set_index(&quot;stop_date_time&quot;, inplace=True) # Examine the index display(ri.index) # Examine the columns ri.columns . DatetimeIndex([&#39;2005-01-04 12:55:00&#39;, &#39;2005-01-23 23:15:00&#39;, &#39;2005-02-17 04:15:00&#39;, &#39;2005-02-20 17:15:00&#39;, &#39;2005-02-24 01:20:00&#39;, &#39;2005-03-14 10:00:00&#39;, &#39;2005-03-29 21:55:00&#39;, &#39;2005-04-04 21:25:00&#39;, &#39;2005-07-14 11:20:00&#39;, &#39;2005-07-14 19:55:00&#39;, ... &#39;2015-12-31 13:23:00&#39;, &#39;2015-12-31 18:59:00&#39;, &#39;2015-12-31 19:13:00&#39;, &#39;2015-12-31 20:20:00&#39;, &#39;2015-12-31 20:50:00&#39;, &#39;2015-12-31 21:21:00&#39;, &#39;2015-12-31 21:59:00&#39;, &#39;2015-12-31 22:04:00&#39;, &#39;2015-12-31 22:09:00&#39;, &#39;2015-12-31 22:47:00&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;stop_date_time&#39;, length=86536, freq=None) . Index([&#39;stop_date&#39;, &#39;stop_time&#39;, &#39;driver_gender&#39;, &#39;driver_race&#39;, &#39;violation_raw&#39;, &#39;violation&#39;, &#39;search_conducted&#39;, &#39;search_type&#39;, &#39;stop_outcome&#39;, &#39;is_arrested&#39;, &#39;stop_duration&#39;, &#39;drugs_related_stop&#39;, &#39;district&#39;], dtype=&#39;object&#39;) . Exploring the relationship between gender and policing . Does the gender of a driver have an impact on police behavior during a traffic stop? We will explore that question while doing filtering, grouping, method chaining, Boolean math, string methods, and more! . Do the genders commit different violations? . Examining traffic violations . Before comparing the violations being committed by each gender, we should examine the violations committed by all drivers to get a baseline understanding of the data. . We&#39;ll count the unique values in the violation column, and then separately express those counts as proportions. . # Count the unique values in &#39;violation&#39; display(ri.violation.value_counts()) # Express the counts as proportions ri.violation.value_counts(normalize=True) . Speeding 48423 Moving violation 16224 Equipment 10921 Other 4409 Registration/plates 3703 Seat belt 2856 Name: violation, dtype: int64 . Speeding 0.559571 Moving violation 0.187483 Equipment 0.126202 Other 0.050950 Registration/plates 0.042791 Seat belt 0.033004 Name: violation, dtype: float64 . More than half of all violations are for speeding, followed by other moving violations and equipment violations. . Comparing violations by gender . The question we&#39;re trying to answer is whether male and female drivers tend to commit different types of traffic violations. . We&#39;ll first create a DataFrame for each gender, and then analyze the violations in each DataFrame separately. . # Create a DataFrame of female drivers female = ri[ri.driver_gender==&quot;F&quot;] # Create a DataFrame of male drivers male = ri[ri.driver_gender==&quot;M&quot;] # Compute the violations by female drivers (as proportions) display(female.violation.value_counts(normalize=True)) # Compute the violations by male drivers (as proportions) male.violation.value_counts(normalize=True) . Speeding 0.658114 Moving violation 0.138218 Equipment 0.105199 Registration/plates 0.044418 Other 0.029738 Seat belt 0.024312 Name: violation, dtype: float64 . Speeding 0.522243 Moving violation 0.206144 Equipment 0.134158 Other 0.058985 Registration/plates 0.042175 Seat belt 0.036296 Name: violation, dtype: float64 . About two-thirds of female traffic stops are for speeding, whereas stops of males are more balanced among the six categories. This doesn&#39;t mean that females speed more often than males, however, since we didn&#39;t take into account the number of stops or drivers. . Does gender affect who gets a ticket for speeding? . Comparing speeding outcomes by gender . When a driver is pulled over for speeding, many people believe that gender has an impact on whether the driver will receive a ticket or a warning. Can we find evidence of this in the dataset? . First, we&#39;ll create two DataFrames of drivers who were stopped for speeding: one containing females and the other containing males. . Then, for each gender, we&#39;ll use the stop_outcome column to calculate what percentage of stops resulted in a &quot;Citation&quot; (meaning a ticket) versus a &quot;Warning&quot;. . # Create a DataFrame of female drivers stopped for speeding female_and_speeding = ri[(ri.driver_gender==&quot;F&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Create a DataFrame of male drivers stopped for speeding male_and_speeding = ri[(ri.driver_gender==&quot;M&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Compute the stop outcomes for female drivers (as proportions) display(female_and_speeding.stop_outcome.value_counts(normalize=True)) # Compute the stop outcomes for male drivers (as proportions) male_and_speeding.stop_outcome.value_counts(normalize=True) . Citation 0.952192 Warning 0.040074 Arrest Driver 0.005752 N/D 0.000959 Arrest Passenger 0.000639 No Action 0.000383 Name: stop_outcome, dtype: float64 . Citation 0.944595 Warning 0.036184 Arrest Driver 0.015895 Arrest Passenger 0.001281 No Action 0.001068 N/D 0.000976 Name: stop_outcome, dtype: float64 . The numbers are similar for males and females: about 95% of stops for speeding result in a ticket. Thus, the data fails to show that gender has an impact on who gets a ticket for speeding. . Does gender affect whose vehicle is searched? . Calculating the search rate . During a traffic stop, the police officer sometimes conducts a search of the vehicle. We&#39;ll calculate the percentage of all stops in the ri DataFrame that result in a vehicle search, also known as the search rate. . # Check the data type of &#39;search_conducted&#39; print(ri.search_conducted.dtype) # Calculate the search rate by counting the values display(ri.search_conducted.value_counts(normalize=True)) # Calculate the search rate by taking the mean ri.search_conducted.mean() . bool . False 0.961785 True 0.038215 Name: search_conducted, dtype: float64 . 0.0382153092354627 . It looks like the search rate is about 3.8%. Next, we&#39;ll examine whether the search rate varies by driver gender. . Comparing search rates by gender . We&#39;ll compare the rates at which female and male drivers are searched during a traffic stop. Remember that the vehicle search rate across all stops is about 3.8%. . First, we&#39;ll filter the DataFrame by gender and calculate the search rate for each group separately. Then, we&#39;ll perform the same calculation for both genders at once using a .groupby(). . ri[ri.driver_gender==&quot;F&quot;].search_conducted.mean() . 0.019180617481282074 . ri[ri.driver_gender==&quot;M&quot;].search_conducted.mean() . 0.04542557598546892 . ri.groupby(&quot;driver_gender&quot;).search_conducted.mean() . driver_gender F 0.019181 M 0.045426 Name: search_conducted, dtype: float64 . Male drivers are searched more than twice as often as female drivers. Why might this be? . Adding a second factor to the analysis . Even though the search rate for males is much higher than for females, it&#39;s possible that the difference is mostly due to a second factor. . For example, we might hypothesize that the search rate varies by violation type, and the difference in search rate between males and females is because they tend to commit different violations. . we can test this hypothesis by examining the search rate for each combination of gender and violation. If the hypothesis was true, out would find that males and females are searched at about the same rate for each violation. Let&#39;s find out below if that&#39;s the case! . # Calculate the search rate for each combination of gender and violation ri.groupby([&quot;driver_gender&quot;, &quot;violation&quot;]).search_conducted.mean() . driver_gender violation F Equipment 0.039984 Moving violation 0.039257 Other 0.041018 Registration/plates 0.054924 Seat belt 0.017301 Speeding 0.008309 M Equipment 0.071496 Moving violation 0.061524 Other 0.046191 Registration/plates 0.108802 Seat belt 0.035119 Speeding 0.027885 Name: search_conducted, dtype: float64 . ri.groupby([&quot;violation&quot;, &quot;driver_gender&quot;]).search_conducted.mean() . violation driver_gender Equipment F 0.039984 M 0.071496 Moving violation F 0.039257 M 0.061524 Other F 0.041018 M 0.046191 Registration/plates F 0.054924 M 0.108802 Seat belt F 0.017301 M 0.035119 Speeding F 0.008309 M 0.027885 Name: search_conducted, dtype: float64 . For all types of violations, the search rate is higher for males than for females, disproving our hypothesis. . Does gender affect who is frisked during a search? . Counting protective frisks . During a vehicle search, the police officer may pat down the driver to check if they have a weapon. This is known as a &quot;protective frisk.&quot; . We&#39;ll first check to see how many times &quot;Protective Frisk&quot; was the only search type. Then, we&#39;ll use a string method to locate all instances in which the driver was frisked. . # Count the &#39;search_type&#39; values display(ri.search_type.value_counts()) # Check if &#39;search_type&#39; contains the string &#39;Protective Frisk&#39; ri[&#39;frisk&#39;] = ri.search_type.str.contains(&#39;Protective Frisk&#39;, na=False) # Check the data type of &#39;frisk&#39; print(ri.frisk.dtype) # Take the sum of &#39;frisk&#39; print(ri.frisk.sum()) . Incident to Arrest 1290 Probable Cause 924 Inventory 219 Reasonable Suspicion 214 Protective Frisk 164 Incident to Arrest,Inventory 123 Incident to Arrest,Probable Cause 100 Probable Cause,Reasonable Suspicion 54 Probable Cause,Protective Frisk 35 Incident to Arrest,Inventory,Probable Cause 35 Incident to Arrest,Protective Frisk 33 Inventory,Probable Cause 25 Protective Frisk,Reasonable Suspicion 19 Incident to Arrest,Inventory,Protective Frisk 18 Incident to Arrest,Probable Cause,Protective Frisk 13 Inventory,Protective Frisk 12 Incident to Arrest,Reasonable Suspicion 8 Probable Cause,Protective Frisk,Reasonable Suspicion 5 Incident to Arrest,Probable Cause,Reasonable Suspicion 5 Incident to Arrest,Inventory,Reasonable Suspicion 4 Incident to Arrest,Protective Frisk,Reasonable Suspicion 2 Inventory,Reasonable Suspicion 2 Inventory,Probable Cause,Protective Frisk 1 Inventory,Probable Cause,Reasonable Suspicion 1 Inventory,Protective Frisk,Reasonable Suspicion 1 Name: search_type, dtype: int64 . bool 303 . It looks like there were 303 drivers who were frisked. Next, we&#39;ll examine whether gender affects who is frisked. . Comparing frisk rates by gender . We&#39;ll compare the rates at which female and male drivers are frisked during a search. Are males frisked more often than females, perhaps because police officers consider them to be higher risk? . Before doing any calculations, it&#39;s important to filter the DataFrame to only include the relevant subset of data, namely stops in which a search was conducted. . # Create a DataFrame of stops in which a search was conducted searched = ri[ri.search_conducted == True] # Calculate the overall frisk rate by taking the mean of &#39;frisk&#39; print(searched.frisk.mean()) # Calculate the frisk rate for each gender searched.groupby(&quot;driver_gender&quot;).frisk.mean() . 0.09162382824312065 . driver_gender F 0.074561 M 0.094353 Name: frisk, dtype: float64 . The frisk rate is higher for males than for females, though we can&#39;t conclude that this difference is caused by the driver&#39;s gender. . Visual exploratory data analysis . Are you more likely to get arrested at a certain time of day? Are drug-related stops on the rise? We will answer these and other questions by analyzing the dataset visually, since plots can help us to understand trends in a way that examining the raw data cannot. . Does time of the day affect arrest rate? . Calculating the hourly arrest rate . When a police officer stops a driver, a small percentage of those stops ends in an arrest. This is known as the arrest rate. We&#39;ll find out whether the arrest rate varies by time of day. . First, we&#39;ll calculate the arrest rate across all stops in the ri DataFrame. Then, we&#39;ll calculate the hourly arrest rate by using the hour attribute of the index. The hour ranges from 0 to 23, in which: . 0 = midnight | 12 = noon | 23 = 11 PM | . # Calculate the overall arrest rate print(ri.is_arrested.mean()) # Calculate the hourly arrest rate # Save the hourly arrest rate hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean() hourly_arrest_rate . 0.0355690117407784 . stop_date_time 0 0.051431 1 0.064932 2 0.060798 3 0.060549 4 0.048000 5 0.042781 6 0.013813 7 0.013032 8 0.021854 9 0.025206 10 0.028213 11 0.028897 12 0.037399 13 0.030776 14 0.030605 15 0.030679 16 0.035281 17 0.040619 18 0.038204 19 0.032245 20 0.038107 21 0.064541 22 0.048666 23 0.047592 Name: is_arrested, dtype: float64 . Next we&#39;ll plot the data so that you can visually examine the arrest rate trends. . Plotting the hourly arrest rate . We&#39;ll create a line plot from the hourly_arrest_rate object. . Important: A line plot is appropriate in this case because you&#8217;re showing how a quantity changes over time. This plot should help us to spot some trends that may not have been obvious when examining the raw numbers! . # Create a line plot of &#39;hourly_arrest_rate&#39; hourly_arrest_rate.plot() # Add the xlabel, ylabel, and title plt.xlabel(&quot;Hour&quot;) plt.ylabel(&quot;Arrest Rate&quot;) plt.title(&quot;Arrest Rate by Time of Day&quot;) # Display the plot plt.show() . The arrest rate has a significant spike overnight, and then dips in the early morning hours. . Are drug-related stops on the rise? . Plotting drug-related stops . In a small portion of traffic stops, drugs are found in the vehicle during a search. In this exercise, you&#39;ll assess whether these drug-related stops are becoming more common over time. . The Boolean column drugs_related_stop indicates whether drugs were found during a given stop. We&#39;ll calculate the annual drug rate by resampling this column, and then we&#39;ll use a line plot to visualize how the rate has changed over time. . # Calculate the annual rate of drug-related stops # Save the annual rate of drug-related stops annual_drug_rate = ri.drugs_related_stop.resample(&quot;A&quot;).mean() display(annual_drug_rate) # Create a line plot of &#39;annual_drug_rate&#39; annual_drug_rate.plot() # Display the plot plt.show() . stop_date_time 2005-12-31 0.006501 2006-12-31 0.007258 2007-12-31 0.007970 2008-12-31 0.007505 2009-12-31 0.009889 2010-12-31 0.010081 2011-12-31 0.009731 2012-12-31 0.009921 2013-12-31 0.013094 2014-12-31 0.013826 2015-12-31 0.012266 Freq: A-DEC, Name: drugs_related_stop, dtype: float64 . The rate of drug-related stops nearly doubled over the course of 10 years. Why might that be the case? . Comparing drug and search rates . The rate of drug-related stops increased significantly between 2005 and 2015. We might hypothesize that the rate of vehicle searches was also increasing, which would have led to an increase in drug-related stops even if more drivers were not carrying drugs. . We can test this hypothesis by calculating the annual search rate, and then plotting it against the annual drug rate. If the hypothesis is true, then we&#39;ll see both rates increasing over time. . # Calculate and save the annual search rate annual_search_rate = ri.search_conducted.resample(&quot;A&quot;).mean() # Concatenate &#39;annual_drug_rate&#39; and &#39;annual_search_rate&#39; annual = pd.concat([annual_drug_rate, annual_search_rate], axis=&quot;columns&quot;) # Create subplots from &#39;annual&#39; annual.plot(subplots=True) # Display the subplots plt.show() . The rate of drug-related stops increased even though the search rate decreased, disproving our hypothesis. . What violations are caught in each district? . Tallying violations by district . The state of Rhode Island is broken into six police districts, also known as zones. How do the zones compare in terms of what violations are caught by police? . We&#39;ll create a frequency table to determine how many violations of each type took place in each of the six zones. Then, we&#39;ll filter the table to focus on the &quot;K&quot; zones, which we&#39;ll examine further. . # Create a frequency table of districts and violations # Save the frequency table as &#39;all_zones&#39; all_zones = pd.crosstab(ri.district, ri.violation) display(all_zones) # Select rows &#39;Zone K1&#39; through &#39;Zone K3&#39; # Save the smaller table as &#39;k_zones&#39; k_zones = all_zones.loc[&quot;Zone K1&quot;:&quot;Zone K3&quot;] k_zones . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . Zone X1 296 | 671 | 143 | 38 | 74 | 1119 | . Zone X3 2049 | 3086 | 769 | 671 | 820 | 8779 | . Zone X4 3541 | 5353 | 1560 | 1411 | 843 | 9795 | . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . We&#39;ll plot the violations so that you can compare these districts. . Plotting violations by district . Now that we&#39;ve created a frequency table focused on the &quot;K&quot; zones, we&#39;ll visualize the data to help us compare what violations are being caught in each zone. . First we&#39;ll create a bar plot, which is an appropriate plot type since we&#39;re comparing categorical data. Then we&#39;ll create a stacked bar plot in order to get a slightly different look at the data. . # Create a bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;) # Display the plot plt.show() . # Create a stacked bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;, stacked=True) # Display the plot plt.show() . The vast majority of traffic stops in Zone K1 are for speeding, and Zones K2 and K3 are remarkably similar to one another in terms of violations. . How long might you be stopped for a violation? . Converting stop durations to numbers . In the traffic stops dataset, the stop_duration column tells us approximately how long the driver was detained by the officer. Unfortunately, the durations are stored as strings, such as &#39;0-15 Min&#39;. How can we make this data easier to analyze? . We&#39;ll convert the stop durations to integers. Because the precise durations are not available, we&#39;ll have to estimate the numbers using reasonable values: . Convert &#39;0-15 Min&#39; to 8 | Convert &#39;16-30 Min&#39; to 23 | Convert &#39;30+ Min&#39; to 45 | . # Create a dictionary that maps strings to integers mapping = {&quot;0-15 Min&quot;:8, &#39;16-30 Min&#39;:23, &#39;30+ Min&#39;:45} # Convert the &#39;stop_duration&#39; strings to integers using the &#39;mapping&#39; ri[&#39;stop_minutes&#39;] = ri.stop_duration.map(mapping) # Print the unique values in &#39;stop_minutes&#39; ri.stop_minutes.unique() . array([ 8, 23, 45], dtype=int64) . Next we&#39;ll analyze the stop length for each type of violation. . Plotting stop length . If you were stopped for a particular violation, how long might you expect to be detained? . We&#39;ll visualize the average length of time drivers are stopped for each type of violation. Rather than using the violation column we&#39;ll use violation_raw since it contains more detailed descriptions of the violations. . # Calculate the mean &#39;stop_minutes&#39; for each value in &#39;violation_raw&#39; # Save the resulting Series as &#39;stop_length&#39; stop_length = ri.groupby(&quot;violation_raw&quot;).stop_minutes.mean() display(stop_length) # Sort &#39;stop_length&#39; by its values and create a horizontal bar plot stop_length.sort_values().plot(kind=&quot;barh&quot;) # Display the plot plt.show() . violation_raw APB 17.967033 Call for Service 22.124371 Equipment/Inspection Violation 11.445655 Motorist Assist/Courtesy 17.741463 Other Traffic Violation 13.844490 Registration Violation 13.736970 Seatbelt Violation 9.662815 Special Detail/Directed Patrol 15.123632 Speeding 10.581562 Suspicious Person 14.910714 Violation of City/Town Ordinance 13.254144 Warrant 24.055556 Name: stop_minutes, dtype: float64 . Analyzing the effect of weather on policing . We will use a second dataset to explore the impact of weather conditions on police behavior during traffic stops. We will be merging and reshaping datasets, assessing whether a data source is trustworthy, working with categorical data, and other advanced skills. . Exploring the weather dataset . Plotting the temperature . We&#39;ll examine the temperature columns from the weather dataset to assess whether the data seems trustworthy. First we&#39;ll print the summary statistics, and then you&#39;ll visualize the data using a box plot. . # Read &#39;weather.csv&#39; into a DataFrame named &#39;weather&#39; weather = pd.read_csv(&quot;../datasets/weather.csv&quot;) display(weather.head()) # Describe the temperature columns display(weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].describe().T) # Create a box plot of the temperature columns weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].plot(kind=&#39;box&#39;) # Display the plot plt.show() . STATION DATE TAVG TMIN TMAX AWND WSF2 WT01 WT02 WT03 ... WT11 WT13 WT14 WT15 WT16 WT17 WT18 WT19 WT21 WT22 . 0 USW00014765 | 2005-01-01 | 44.0 | 35 | 53 | 8.95 | 25.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 USW00014765 | 2005-01-02 | 36.0 | 28 | 44 | 9.40 | 14.1 | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 2 USW00014765 | 2005-01-03 | 49.0 | 44 | 53 | 6.93 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 3 USW00014765 | 2005-01-04 | 42.0 | 39 | 45 | 6.93 | 16.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | 1.0 | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 4 USW00014765 | 2005-01-05 | 36.0 | 28 | 43 | 7.83 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 5 rows × 27 columns . count mean std min 25% 50% 75% max . TMIN 4017.0 | 43.484441 | 17.020298 | -5.0 | 30.0 | 44.0 | 58.0 | 77.0 | . TAVG 1217.0 | 52.493016 | 17.830714 | 6.0 | 39.0 | 54.0 | 68.0 | 86.0 | . TMAX 4017.0 | 61.268608 | 18.199517 | 15.0 | 47.0 | 62.0 | 77.0 | 102.0 | . The temperature data looks good so far: the TAVG values are in between TMIN and TMAX, and the measurements and ranges seem reasonable. . Plotting the temperature difference . We&#39;ll continue to assess whether the dataset seems trustworthy by plotting the difference between the maximum and minimum temperatures. . # Create a &#39;TDIFF&#39; column that represents temperature difference weather[&quot;TDIFF&quot;] = weather.TMAX - weather.TMIN # Describe the &#39;TDIFF&#39; column display(weather.TDIFF.describe()) # Create a histogram with 20 bins to visualize &#39;TDIFF&#39; weather.TDIFF.plot(kind=&quot;hist&quot;, bins=20) # Display the plot plt.show() . count 4017.000000 mean 17.784167 std 6.350720 min 2.000000 25% 14.000000 50% 18.000000 75% 22.000000 max 43.000000 Name: TDIFF, dtype: float64 . The TDIFF column has no negative values and its distribution is approximately normal, both of which are signs that the data is trustworthy. . Categorizing the weather . Counting bad weather conditions . The weather DataFrame contains 20 columns that start with &#39;WT&#39;, each of which represents a bad weather condition. For example: . WT05 indicates &quot;Hail&quot; | WT11 indicates &quot;High or damaging winds&quot; | WT17 indicates &quot;Freezing rain&quot; | . For every row in the dataset, each WT column contains either a 1 (meaning the condition was present that day) or NaN (meaning the condition was not present). . We&#39;ll quantify &quot;how bad&quot; the weather was each day by counting the number of 1 values in each row. . # Copy &#39;WT01&#39; through &#39;WT22&#39; to a new DataFrame WT = weather.loc[:, &quot;WT01&quot;:&quot;WT22&quot;] # Calculate the sum of each row in &#39;WT&#39; weather[&#39;bad_conditions&#39;] = WT.sum(axis=&quot;columns&quot;) # Replace missing values in &#39;bad_conditions&#39; with &#39;0&#39; weather[&#39;bad_conditions&#39;] = weather.bad_conditions.fillna(0).astype(&#39;int&#39;) # Create a histogram to visualize &#39;bad_conditions&#39; weather.bad_conditions.plot(kind=&quot;hist&quot;) # Display the plot plt.show() . It looks like many days didn&#39;t have any bad weather conditions, and only a small portion of days had more than four bad weather conditions. . Rating the weather conditions . We counted the number of bad weather conditions each day. We&#39;ll use the counts to create a rating system for the weather. . The counts range from 0 to 9, and should be converted to ratings as follows: . Convert 0 to &#39;good&#39; | Convert 1 through 4 to &#39;bad&#39; | Convert 5 through 9 to &#39;worse&#39; | . # Count the unique values in &#39;bad_conditions&#39; and sort the index display(weather.bad_conditions.value_counts().sort_index()) # Create a dictionary that maps integers to strings mapping = {0:&#39;good&#39;, 1:&#39;bad&#39;, 2:&#39;bad&#39;, 3:&#39;bad&#39;, 4:&#39;bad&#39;, 5:&#39;worse&#39;, 6:&#39;worse&#39;, 7:&#39;worse&#39;, 8:&#39;worse&#39;, 9:&#39;worse&#39;} # Convert the &#39;bad_conditions&#39; integers to strings using the &#39;mapping&#39; weather[&#39;rating&#39;] = weather.bad_conditions.map(mapping) # Count the unique values in &#39;rating&#39; weather.rating.value_counts() . 0 1749 1 613 2 367 3 380 4 476 5 282 6 101 7 41 8 4 9 4 Name: bad_conditions, dtype: int64 . bad 1836 good 1749 worse 432 Name: rating, dtype: int64 . Changing the data type to category . Since the rating column only has a few possible values, we&#39;ll change its data type to category in order to store the data more efficiently. we&#39;ll also specify a logical order for the categories, which will be useful for future work. . # Create a list of weather ratings in logical order cats = [&#39;good&#39;, &#39;bad&#39;, &#39;worse&#39;] # Change the data type of &#39;rating&#39; to category weather[&#39;rating&#39;] = weather.rating.astype(CategoricalDtype(ordered=True, categories=cats)) # Examine the head of &#39;rating&#39; weather.rating.head() . 0 bad 1 bad 2 bad 3 bad 4 bad Name: rating, dtype: category Categories (3, object): [good &lt; bad &lt; worse] . We&#39;ll use the rating column in future exercises to analyze the effects of weather on police behavior. . Merging datasets . Preparing the DataFrames . We&#39;ll prepare the traffic stop and weather rating DataFrames so that they&#39;re ready to be merged: . With the ri DataFrame, we&#39;ll move the stop_datetime index to a column since the index will be lost during the merge. | With the weather DataFrame, we&#39;ll select the DATE and rating columns and put them in a new DataFrame. | . # Reset the index of &#39;ri&#39; ri.reset_index(inplace=True) # Examine the head of &#39;ri&#39; display(ri.head()) # Create a DataFrame from the &#39;DATE&#39; and &#39;rating&#39; columns weather_rating = weather[[&quot;DATE&quot;, &quot;rating&quot;]] # Examine the head of &#39;weather_rating&#39; weather_rating.head() . stop_date_time stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes . 0 2005-01-04 12:55:00 | 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 1 2005-01-23 23:15:00 | 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | . 2 2005-02-17 04:15:00 | 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 3 2005-02-20 17:15:00 | 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | . 4 2005-02-24 01:20:00 | 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | . DATE rating . 0 2005-01-01 | bad | . 1 2005-01-02 | bad | . 2 2005-01-03 | bad | . 3 2005-01-04 | bad | . 4 2005-01-05 | bad | . The ri and weather_rating DataFrames are now ready to be merged. . Merging the DataFrames . We&#39;ll merge the ri and weather_rating DataFrames into a new DataFrame, ri_weather. . The DataFrames will be joined using the stop_date column from ri and the DATE column from weather_rating. Thankfully the date formatting matches exactly, which is not always the case! . Once the merge is complete, we&#39;ll set stop_datetime as the index . # Examine the shape of &#39;ri&#39; print(ri.shape) # Merge &#39;ri&#39; and &#39;weather_rating&#39; using a left join ri_weather = pd.merge(left=ri, right=weather_rating, left_on=&#39;stop_date&#39;, right_on=&#39;DATE&#39;, how=&#39;left&#39;) # Examine the shape of &#39;ri_weather&#39; print(ri_weather.shape) # Set &#39;stop_datetime&#39; as the index of &#39;ri_weather&#39; ri_weather.set_index(&#39;stop_date_time&#39;, inplace=True) ri_weather.head() . (86536, 16) (86536, 18) . stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes DATE rating . stop_date_time . 2005-01-04 12:55:00 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-01-04 | bad | . 2005-01-23 23:15:00 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | 2005-01-23 | worse | . 2005-02-17 04:15:00 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-02-17 | good | . 2005-02-20 17:15:00 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | 2005-02-20 | bad | . 2005-02-24 01:20:00 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | 2005-02-24 | bad | . We&#39;ll use ri_weather to analyze the relationship between weather conditions and police behavior. . Does weather affect the arrest rate? . Comparing arrest rates by weather rating . Do police officers arrest drivers more often when the weather is bad? Let&#39;s find out below! . First, we&#39;ll calculate the overall arrest rate. | Then, we&#39;ll calculate the arrest rate for each of the weather ratings we previously assigned. | Finally, we&#39;ll add violation type as a second factor in the analysis, to see if that accounts for any differences in the arrest rate. | . Since we previously defined a logical order for the weather categories, good &lt; bad &lt; worse, they will be sorted that way in the results. . # Calculate the overall arrest rate print(ri_weather.is_arrested.mean()) . 0.0355690117407784 . # Calculate the arrest rate for each &#39;rating&#39; ri_weather.groupby(&quot;rating&quot;).is_arrested.mean() . rating good 0.033715 bad 0.036261 worse 0.041667 Name: is_arrested, dtype: float64 . # Calculate the arrest rate for each &#39;violation&#39; and &#39;rating&#39; ri_weather.groupby([&quot;violation&quot;, &#39;rating&#39;]).is_arrested.mean() . violation rating Equipment good 0.059007 bad 0.066311 worse 0.097357 Moving violation good 0.056227 bad 0.058050 worse 0.065860 Other good 0.076966 bad 0.087443 worse 0.062893 Registration/plates good 0.081574 bad 0.098160 worse 0.115625 Seat belt good 0.028587 bad 0.022493 worse 0.000000 Speeding good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . The arrest rate increases as the weather gets worse, and that trend persists across many of the violation types. This doesn&#39;t prove a causal link, but it&#39;s quite an interesting result! . Selecting from a multi-indexed Series . The output of a single .groupby() operation on multiple columns is a Series with a MultiIndex. Working with this type of object is similar to working with a DataFrame: . The outer index level is like the DataFrame rows. | The inner index level is like the DataFrame columns. | . # Save the output of the groupby operation from the last exercise arrest_rate = ri_weather.groupby([&#39;violation&#39;, &#39;rating&#39;]).is_arrested.mean() # Print the arrest rate for moving violations in bad weather display(arrest_rate.loc[&quot;Moving violation&quot;, &quot;bad&quot;]) # Print the arrest rates for speeding violations in all three weather conditions arrest_rate.loc[&quot;Speeding&quot;] . 0.05804964058049641 . rating good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . Reshaping the arrest rate data . We&#39;ll start by reshaping the arrest_rate Series into a DataFrame. This is a useful step when working with any multi-indexed Series, since it enables you to access the full range of DataFrame methods. . Then, we&#39;ll create the exact same DataFrame using a pivot table. This is a great example of how pandas often gives you more than one way to reach the same result! . # Unstack the &#39;arrest_rate&#39; Series into a DataFrame display(arrest_rate.unstack()) # Create the same DataFrame using a pivot table ri_weather.pivot_table(index=&#39;violation&#39;, columns=&#39;rating&#39;, values=&#39;is_arrested&#39;) . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | .",
            "url": "https://victoromondi1997.github.io/blog/pandas/eda/python/data-science/data-analysis/2020/06/28/Analyzing-Police-Activity-with-pandas.html",
            "relUrl": "/pandas/eda/python/data-science/data-analysis/2020/06/28/Analyzing-Police-Activity-with-pandas.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://victoromondi1997.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://victoromondi1997.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This page is under construction by Victor Omondi . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://victoromondi1997.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://victoromondi1997.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}