{
  
    
        "post0": {
            "title": "Analyzing Police Activity with Pandas",
            "content": "Preparing the data for analysis . Before beginning our analysis, it is critical that we first examine and clean the dataset, to make working with it a more efficient process. We will fixing data types, handle missing values, and dropping columns and rows while exploring the Stanford Open Policing Project dataset. . Stanford Open Policing Project dataset . Examining the dataset . We&#39;ll be analyzing a dataset of traffic stops in Rhode Island that was collected by the Stanford Open Policing Project. . Before beginning our analysis, it&#39;s important that we familiarize yourself with the dataset. We read the dataset into pandas, examine the first few rows, and then count the number of missing values. . Libraries . import pandas as pd import matplotlib.pyplot as plt from pandas.api.types import CategoricalDtype . # Read &#39;police.csv&#39; into a DataFrame named ri ri = pd.read_csv(&quot;../datasets/police.csv&quot;) # Examine the head of the DataFrame display(ri.head()) # Count the number of missing values in each column ri.isnull().sum() . state stop_date stop_time county_name driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district . 0 RI | 2005-01-04 | 12:55 | NaN | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 1 RI | 2005-01-23 | 23:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | . 2 RI | 2005-02-17 | 04:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 3 RI | 2005-02-20 | 17:15 | NaN | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | . 4 RI | 2005-02-24 | 01:20 | NaN | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | . state 0 stop_date 0 stop_time 0 county_name 91741 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . It looks like most of the columns have at least some missing values. We&#39;ll figure out how to handle these values in the next. . Dropping columns . We&#39;ll drop the county_name column because it only contains missing values, and we&#39;ll drop the state column because all of the traffic stops took place in one state (Rhode Island). . # Examine the shape of the DataFrame print(ri.shape) # Drop the &#39;county_name&#39; and &#39;state&#39; columns ri.drop([&quot;county_name&quot;, &quot;state&quot;], axis=&#39;columns&#39;, inplace=True) # Examine the shape of the DataFrame (again) print(ri.shape) . (91741, 15) (91741, 13) . We&#39;ll continue to remove unnecessary data from the DataFrame . Dropping rows . the driver_gender column will be critical to many of our analyses. Because only a small fraction of rows are missing driver_gender, we&#39;ll drop those rows from the dataset. . # Count the number of missing values in each column display(ri.isnull().sum()) # Drop all rows that are missing &#39;driver_gender&#39; ri.dropna(subset=[&quot;driver_gender&quot;], inplace=True) # Count the number of missing values in each column (again) display(ri.isnull().sum()) # Examine the shape of the DataFrame ri.shape . stop_date 0 stop_time 0 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . stop_date 0 stop_time 0 driver_gender 0 driver_race 0 violation_raw 0 violation 0 search_conducted 0 search_type 83229 stop_outcome 0 is_arrested 0 stop_duration 0 drugs_related_stop 0 district 0 dtype: int64 . (86536, 13) . We dropped around 5,000 rows, which is a small fraction of the dataset, and now only one column remains with any missing values. . Using proper data types . Finding an incorrect data type . ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested object stop_duration object drugs_related_stop bool district object dtype: object . stop_date: should be datetime | stop_time: should be datetime | driver_gender: should be category | driver_race: should be category | violation_raw: should be category | violation: should be category | district: should be category | is_arrested: should be bool | . We&#39;ll fix the data type of the is_arrested column . # Examine the head of the &#39;is_arrested&#39; column display(ri.is_arrested.head()) # Change the data type of &#39;is_arrested&#39; to &#39;bool&#39; ri[&#39;is_arrested&#39;] = ri.is_arrested.astype(&#39;bool&#39;) # Check the data type of &#39;is_arrested&#39; ri.is_arrested.dtype . 0 False 1 False 2 False 3 True 4 False Name: is_arrested, dtype: object . dtype(&#39;bool&#39;) . Creating a DatetimeIndex . Combining object columns . Currently, the date and time of each traffic stop are stored in separate object columns: stop_date and stop_time. We&#39;ll combine these two columns into a single column, and then convert it to datetime format. . ri[&#39;stop_date_time&#39;] = pd.to_datetime(ri.stop_date.str.replace(&quot;/&quot;, &quot;-&quot;).str.cat(ri.stop_time, sep=&quot; &quot;)) ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested bool stop_duration object drugs_related_stop bool district object stop_date_time datetime64[ns] dtype: object . Setting the index . # Set &#39;stop_datetime&#39; as the index ri.set_index(&quot;stop_date_time&quot;, inplace=True) # Examine the index display(ri.index) # Examine the columns ri.columns . DatetimeIndex([&#39;2005-01-04 12:55:00&#39;, &#39;2005-01-23 23:15:00&#39;, &#39;2005-02-17 04:15:00&#39;, &#39;2005-02-20 17:15:00&#39;, &#39;2005-02-24 01:20:00&#39;, &#39;2005-03-14 10:00:00&#39;, &#39;2005-03-29 21:55:00&#39;, &#39;2005-04-04 21:25:00&#39;, &#39;2005-07-14 11:20:00&#39;, &#39;2005-07-14 19:55:00&#39;, ... &#39;2015-12-31 13:23:00&#39;, &#39;2015-12-31 18:59:00&#39;, &#39;2015-12-31 19:13:00&#39;, &#39;2015-12-31 20:20:00&#39;, &#39;2015-12-31 20:50:00&#39;, &#39;2015-12-31 21:21:00&#39;, &#39;2015-12-31 21:59:00&#39;, &#39;2015-12-31 22:04:00&#39;, &#39;2015-12-31 22:09:00&#39;, &#39;2015-12-31 22:47:00&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;stop_date_time&#39;, length=86536, freq=None) . Index([&#39;stop_date&#39;, &#39;stop_time&#39;, &#39;driver_gender&#39;, &#39;driver_race&#39;, &#39;violation_raw&#39;, &#39;violation&#39;, &#39;search_conducted&#39;, &#39;search_type&#39;, &#39;stop_outcome&#39;, &#39;is_arrested&#39;, &#39;stop_duration&#39;, &#39;drugs_related_stop&#39;, &#39;district&#39;], dtype=&#39;object&#39;) . Exploring the relationship between gender and policing . Does the gender of a driver have an impact on police behavior during a traffic stop? We will explore that question while doing filtering, grouping, method chaining, Boolean math, string methods, and more! . Do the genders commit different violations? . Examining traffic violations . Before comparing the violations being committed by each gender, we should examine the violations committed by all drivers to get a baseline understanding of the data. . We&#39;ll count the unique values in the violation column, and then separately express those counts as proportions. . # Count the unique values in &#39;violation&#39; display(ri.violation.value_counts()) # Express the counts as proportions ri.violation.value_counts(normalize=True) . Speeding 48423 Moving violation 16224 Equipment 10921 Other 4409 Registration/plates 3703 Seat belt 2856 Name: violation, dtype: int64 . Speeding 0.559571 Moving violation 0.187483 Equipment 0.126202 Other 0.050950 Registration/plates 0.042791 Seat belt 0.033004 Name: violation, dtype: float64 . More than half of all violations are for speeding, followed by other moving violations and equipment violations. . Comparing violations by gender . The question we&#39;re trying to answer is whether male and female drivers tend to commit different types of traffic violations. . We&#39;ll first create a DataFrame for each gender, and then analyze the violations in each DataFrame separately. . # Create a DataFrame of female drivers female = ri[ri.driver_gender==&quot;F&quot;] # Create a DataFrame of male drivers male = ri[ri.driver_gender==&quot;M&quot;] # Compute the violations by female drivers (as proportions) display(female.violation.value_counts(normalize=True)) # Compute the violations by male drivers (as proportions) male.violation.value_counts(normalize=True) . Speeding 0.658114 Moving violation 0.138218 Equipment 0.105199 Registration/plates 0.044418 Other 0.029738 Seat belt 0.024312 Name: violation, dtype: float64 . Speeding 0.522243 Moving violation 0.206144 Equipment 0.134158 Other 0.058985 Registration/plates 0.042175 Seat belt 0.036296 Name: violation, dtype: float64 . About two-thirds of female traffic stops are for speeding, whereas stops of males are more balanced among the six categories. This doesn&#39;t mean that females speed more often than males, however, since we didn&#39;t take into account the number of stops or drivers. . Does gender affect who gets a ticket for speeding? . Comparing speeding outcomes by gender . When a driver is pulled over for speeding, many people believe that gender has an impact on whether the driver will receive a ticket or a warning. Can we find evidence of this in the dataset? . First, we&#39;ll create two DataFrames of drivers who were stopped for speeding: one containing females and the other containing males. . Then, for each gender, we&#39;ll use the stop_outcome column to calculate what percentage of stops resulted in a &quot;Citation&quot; (meaning a ticket) versus a &quot;Warning&quot;. . # Create a DataFrame of female drivers stopped for speeding female_and_speeding = ri[(ri.driver_gender==&quot;F&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Create a DataFrame of male drivers stopped for speeding male_and_speeding = ri[(ri.driver_gender==&quot;M&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Compute the stop outcomes for female drivers (as proportions) display(female_and_speeding.stop_outcome.value_counts(normalize=True)) # Compute the stop outcomes for male drivers (as proportions) male_and_speeding.stop_outcome.value_counts(normalize=True) . Citation 0.952192 Warning 0.040074 Arrest Driver 0.005752 N/D 0.000959 Arrest Passenger 0.000639 No Action 0.000383 Name: stop_outcome, dtype: float64 . Citation 0.944595 Warning 0.036184 Arrest Driver 0.015895 Arrest Passenger 0.001281 No Action 0.001068 N/D 0.000976 Name: stop_outcome, dtype: float64 . The numbers are similar for males and females: about 95% of stops for speeding result in a ticket. Thus, the data fails to show that gender has an impact on who gets a ticket for speeding. . Does gender affect whose vehicle is searched? . Calculating the search rate . During a traffic stop, the police officer sometimes conducts a search of the vehicle. We&#39;ll calculate the percentage of all stops in the ri DataFrame that result in a vehicle search, also known as the search rate. . # Check the data type of &#39;search_conducted&#39; print(ri.search_conducted.dtype) # Calculate the search rate by counting the values display(ri.search_conducted.value_counts(normalize=True)) # Calculate the search rate by taking the mean ri.search_conducted.mean() . bool . False 0.961785 True 0.038215 Name: search_conducted, dtype: float64 . 0.0382153092354627 . It looks like the search rate is about 3.8%. Next, we&#39;ll examine whether the search rate varies by driver gender. . Comparing search rates by gender . We&#39;ll compare the rates at which female and male drivers are searched during a traffic stop. Remember that the vehicle search rate across all stops is about 3.8%. . First, we&#39;ll filter the DataFrame by gender and calculate the search rate for each group separately. Then, we&#39;ll perform the same calculation for both genders at once using a .groupby(). . ri[ri.driver_gender==&quot;F&quot;].search_conducted.mean() . 0.019180617481282074 . ri[ri.driver_gender==&quot;M&quot;].search_conducted.mean() . 0.04542557598546892 . ri.groupby(&quot;driver_gender&quot;).search_conducted.mean() . driver_gender F 0.019181 M 0.045426 Name: search_conducted, dtype: float64 . Male drivers are searched more than twice as often as female drivers. Why might this be? . Adding a second factor to the analysis . Even though the search rate for males is much higher than for females, it&#39;s possible that the difference is mostly due to a second factor. . For example, we might hypothesize that the search rate varies by violation type, and the difference in search rate between males and females is because they tend to commit different violations. . we can test this hypothesis by examining the search rate for each combination of gender and violation. If the hypothesis was true, out would find that males and females are searched at about the same rate for each violation. Let&#39;s find out below if that&#39;s the case! . # Calculate the search rate for each combination of gender and violation ri.groupby([&quot;driver_gender&quot;, &quot;violation&quot;]).search_conducted.mean() . driver_gender violation F Equipment 0.039984 Moving violation 0.039257 Other 0.041018 Registration/plates 0.054924 Seat belt 0.017301 Speeding 0.008309 M Equipment 0.071496 Moving violation 0.061524 Other 0.046191 Registration/plates 0.108802 Seat belt 0.035119 Speeding 0.027885 Name: search_conducted, dtype: float64 . ri.groupby([&quot;violation&quot;, &quot;driver_gender&quot;]).search_conducted.mean() . violation driver_gender Equipment F 0.039984 M 0.071496 Moving violation F 0.039257 M 0.061524 Other F 0.041018 M 0.046191 Registration/plates F 0.054924 M 0.108802 Seat belt F 0.017301 M 0.035119 Speeding F 0.008309 M 0.027885 Name: search_conducted, dtype: float64 . For all types of violations, the search rate is higher for males than for females, disproving our hypothesis. . Does gender affect who is frisked during a search? . Counting protective frisks . During a vehicle search, the police officer may pat down the driver to check if they have a weapon. This is known as a &quot;protective frisk.&quot; . We&#39;ll first check to see how many times &quot;Protective Frisk&quot; was the only search type. Then, we&#39;ll use a string method to locate all instances in which the driver was frisked. . # Count the &#39;search_type&#39; values display(ri.search_type.value_counts()) # Check if &#39;search_type&#39; contains the string &#39;Protective Frisk&#39; ri[&#39;frisk&#39;] = ri.search_type.str.contains(&#39;Protective Frisk&#39;, na=False) # Check the data type of &#39;frisk&#39; print(ri.frisk.dtype) # Take the sum of &#39;frisk&#39; print(ri.frisk.sum()) . Incident to Arrest 1290 Probable Cause 924 Inventory 219 Reasonable Suspicion 214 Protective Frisk 164 Incident to Arrest,Inventory 123 Incident to Arrest,Probable Cause 100 Probable Cause,Reasonable Suspicion 54 Probable Cause,Protective Frisk 35 Incident to Arrest,Inventory,Probable Cause 35 Incident to Arrest,Protective Frisk 33 Inventory,Probable Cause 25 Protective Frisk,Reasonable Suspicion 19 Incident to Arrest,Inventory,Protective Frisk 18 Incident to Arrest,Probable Cause,Protective Frisk 13 Inventory,Protective Frisk 12 Incident to Arrest,Reasonable Suspicion 8 Probable Cause,Protective Frisk,Reasonable Suspicion 5 Incident to Arrest,Probable Cause,Reasonable Suspicion 5 Incident to Arrest,Inventory,Reasonable Suspicion 4 Incident to Arrest,Protective Frisk,Reasonable Suspicion 2 Inventory,Reasonable Suspicion 2 Inventory,Probable Cause,Protective Frisk 1 Inventory,Probable Cause,Reasonable Suspicion 1 Inventory,Protective Frisk,Reasonable Suspicion 1 Name: search_type, dtype: int64 . bool 303 . It looks like there were 303 drivers who were frisked. Next, we&#39;ll examine whether gender affects who is frisked. . Comparing frisk rates by gender . We&#39;ll compare the rates at which female and male drivers are frisked during a search. Are males frisked more often than females, perhaps because police officers consider them to be higher risk? . Before doing any calculations, it&#39;s important to filter the DataFrame to only include the relevant subset of data, namely stops in which a search was conducted. . # Create a DataFrame of stops in which a search was conducted searched = ri[ri.search_conducted == True] # Calculate the overall frisk rate by taking the mean of &#39;frisk&#39; print(searched.frisk.mean()) # Calculate the frisk rate for each gender searched.groupby(&quot;driver_gender&quot;).frisk.mean() . 0.09162382824312065 . driver_gender F 0.074561 M 0.094353 Name: frisk, dtype: float64 . The frisk rate is higher for males than for females, though we can&#39;t conclude that this difference is caused by the driver&#39;s gender. . Visual exploratory data analysis . Are you more likely to get arrested at a certain time of day? Are drug-related stops on the rise? We will answer these and other questions by analyzing the dataset visually, since plots can help us to understand trends in a way that examining the raw data cannot. . Does time of the day affect arrest rate? . Calculating the hourly arrest rate . When a police officer stops a driver, a small percentage of those stops ends in an arrest. This is known as the arrest rate. We&#39;ll find out whether the arrest rate varies by time of day. . First, we&#39;ll calculate the arrest rate across all stops in the ri DataFrame. Then, we&#39;ll calculate the hourly arrest rate by using the hour attribute of the index. The hour ranges from 0 to 23, in which: . 0 = midnight | 12 = noon | 23 = 11 PM | . # Calculate the overall arrest rate print(ri.is_arrested.mean()) # Calculate the hourly arrest rate # Save the hourly arrest rate hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean() hourly_arrest_rate . 0.0355690117407784 . stop_date_time 0 0.051431 1 0.064932 2 0.060798 3 0.060549 4 0.048000 5 0.042781 6 0.013813 7 0.013032 8 0.021854 9 0.025206 10 0.028213 11 0.028897 12 0.037399 13 0.030776 14 0.030605 15 0.030679 16 0.035281 17 0.040619 18 0.038204 19 0.032245 20 0.038107 21 0.064541 22 0.048666 23 0.047592 Name: is_arrested, dtype: float64 . Next we&#39;ll plot the data so that you can visually examine the arrest rate trends. . Plotting the hourly arrest rate . We&#39;ll create a line plot from the hourly_arrest_rate object. . Important: A line plot is appropriate in this case because you&#8217;re showing how a quantity changes over time. This plot should help us to spot some trends that may not have been obvious when examining the raw numbers! . # Create a line plot of &#39;hourly_arrest_rate&#39; hourly_arrest_rate.plot() # Add the xlabel, ylabel, and title plt.xlabel(&quot;Hour&quot;) plt.ylabel(&quot;Arrest Rate&quot;) plt.title(&quot;Arrest Rate by Time of Day&quot;) # Display the plot plt.show() . The arrest rate has a significant spike overnight, and then dips in the early morning hours. . Are drug-related stops on the rise? . Plotting drug-related stops . In a small portion of traffic stops, drugs are found in the vehicle during a search. In this exercise, you&#39;ll assess whether these drug-related stops are becoming more common over time. . The Boolean column drugs_related_stop indicates whether drugs were found during a given stop. We&#39;ll calculate the annual drug rate by resampling this column, and then we&#39;ll use a line plot to visualize how the rate has changed over time. . # Calculate the annual rate of drug-related stops # Save the annual rate of drug-related stops annual_drug_rate = ri.drugs_related_stop.resample(&quot;A&quot;).mean() display(annual_drug_rate) # Create a line plot of &#39;annual_drug_rate&#39; annual_drug_rate.plot() # Display the plot plt.show() . stop_date_time 2005-12-31 0.006501 2006-12-31 0.007258 2007-12-31 0.007970 2008-12-31 0.007505 2009-12-31 0.009889 2010-12-31 0.010081 2011-12-31 0.009731 2012-12-31 0.009921 2013-12-31 0.013094 2014-12-31 0.013826 2015-12-31 0.012266 Freq: A-DEC, Name: drugs_related_stop, dtype: float64 . The rate of drug-related stops nearly doubled over the course of 10 years. Why might that be the case? . Comparing drug and search rates . The rate of drug-related stops increased significantly between 2005 and 2015. We might hypothesize that the rate of vehicle searches was also increasing, which would have led to an increase in drug-related stops even if more drivers were not carrying drugs. . We can test this hypothesis by calculating the annual search rate, and then plotting it against the annual drug rate. If the hypothesis is true, then we&#39;ll see both rates increasing over time. . # Calculate and save the annual search rate annual_search_rate = ri.search_conducted.resample(&quot;A&quot;).mean() # Concatenate &#39;annual_drug_rate&#39; and &#39;annual_search_rate&#39; annual = pd.concat([annual_drug_rate, annual_search_rate], axis=&quot;columns&quot;) # Create subplots from &#39;annual&#39; annual.plot(subplots=True) # Display the subplots plt.show() . The rate of drug-related stops increased even though the search rate decreased, disproving our hypothesis. . What violations are caught in each district? . Tallying violations by district . The state of Rhode Island is broken into six police districts, also known as zones. How do the zones compare in terms of what violations are caught by police? . We&#39;ll create a frequency table to determine how many violations of each type took place in each of the six zones. Then, we&#39;ll filter the table to focus on the &quot;K&quot; zones, which we&#39;ll examine further. . # Create a frequency table of districts and violations # Save the frequency table as &#39;all_zones&#39; all_zones = pd.crosstab(ri.district, ri.violation) display(all_zones) # Select rows &#39;Zone K1&#39; through &#39;Zone K3&#39; # Save the smaller table as &#39;k_zones&#39; k_zones = all_zones.loc[&quot;Zone K1&quot;:&quot;Zone K3&quot;] k_zones . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . Zone X1 296 | 671 | 143 | 38 | 74 | 1119 | . Zone X3 2049 | 3086 | 769 | 671 | 820 | 8779 | . Zone X4 3541 | 5353 | 1560 | 1411 | 843 | 9795 | . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . We&#39;ll plot the violations so that you can compare these districts. . Plotting violations by district . Now that we&#39;ve created a frequency table focused on the &quot;K&quot; zones, we&#39;ll visualize the data to help us compare what violations are being caught in each zone. . First we&#39;ll create a bar plot, which is an appropriate plot type since we&#39;re comparing categorical data. Then we&#39;ll create a stacked bar plot in order to get a slightly different look at the data. . # Create a bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;) # Display the plot plt.show() . # Create a stacked bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;, stacked=True) # Display the plot plt.show() . The vast majority of traffic stops in Zone K1 are for speeding, and Zones K2 and K3 are remarkably similar to one another in terms of violations. . How long might you be stopped for a violation? . Converting stop durations to numbers . In the traffic stops dataset, the stop_duration column tells us approximately how long the driver was detained by the officer. Unfortunately, the durations are stored as strings, such as &#39;0-15 Min&#39;. How can we make this data easier to analyze? . We&#39;ll convert the stop durations to integers. Because the precise durations are not available, we&#39;ll have to estimate the numbers using reasonable values: . Convert &#39;0-15 Min&#39; to 8 | Convert &#39;16-30 Min&#39; to 23 | Convert &#39;30+ Min&#39; to 45 | . # Create a dictionary that maps strings to integers mapping = {&quot;0-15 Min&quot;:8, &#39;16-30 Min&#39;:23, &#39;30+ Min&#39;:45} # Convert the &#39;stop_duration&#39; strings to integers using the &#39;mapping&#39; ri[&#39;stop_minutes&#39;] = ri.stop_duration.map(mapping) # Print the unique values in &#39;stop_minutes&#39; ri.stop_minutes.unique() . array([ 8, 23, 45], dtype=int64) . Next we&#39;ll analyze the stop length for each type of violation. . Plotting stop length . If you were stopped for a particular violation, how long might you expect to be detained? . We&#39;ll visualize the average length of time drivers are stopped for each type of violation. Rather than using the violation column we&#39;ll use violation_raw since it contains more detailed descriptions of the violations. . # Calculate the mean &#39;stop_minutes&#39; for each value in &#39;violation_raw&#39; # Save the resulting Series as &#39;stop_length&#39; stop_length = ri.groupby(&quot;violation_raw&quot;).stop_minutes.mean() display(stop_length) # Sort &#39;stop_length&#39; by its values and create a horizontal bar plot stop_length.sort_values().plot(kind=&quot;barh&quot;) # Display the plot plt.show() . violation_raw APB 17.967033 Call for Service 22.124371 Equipment/Inspection Violation 11.445655 Motorist Assist/Courtesy 17.741463 Other Traffic Violation 13.844490 Registration Violation 13.736970 Seatbelt Violation 9.662815 Special Detail/Directed Patrol 15.123632 Speeding 10.581562 Suspicious Person 14.910714 Violation of City/Town Ordinance 13.254144 Warrant 24.055556 Name: stop_minutes, dtype: float64 . Analyzing the effect of weather on policing . We will use a second dataset to explore the impact of weather conditions on police behavior during traffic stops. We will be merging and reshaping datasets, assessing whether a data source is trustworthy, working with categorical data, and other advanced skills. . Exploring the weather dataset . Plotting the temperature . We&#39;ll examine the temperature columns from the weather dataset to assess whether the data seems trustworthy. First we&#39;ll print the summary statistics, and then you&#39;ll visualize the data using a box plot. . # Read &#39;weather.csv&#39; into a DataFrame named &#39;weather&#39; weather = pd.read_csv(&quot;../datasets/weather.csv&quot;) display(weather.head()) # Describe the temperature columns display(weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].describe().T) # Create a box plot of the temperature columns weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].plot(kind=&#39;box&#39;) # Display the plot plt.show() . STATION DATE TAVG TMIN TMAX AWND WSF2 WT01 WT02 WT03 ... WT11 WT13 WT14 WT15 WT16 WT17 WT18 WT19 WT21 WT22 . 0 USW00014765 | 2005-01-01 | 44.0 | 35 | 53 | 8.95 | 25.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 USW00014765 | 2005-01-02 | 36.0 | 28 | 44 | 9.40 | 14.1 | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 2 USW00014765 | 2005-01-03 | 49.0 | 44 | 53 | 6.93 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 3 USW00014765 | 2005-01-04 | 42.0 | 39 | 45 | 6.93 | 16.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | 1.0 | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 4 USW00014765 | 2005-01-05 | 36.0 | 28 | 43 | 7.83 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 5 rows × 27 columns . count mean std min 25% 50% 75% max . TMIN 4017.0 | 43.484441 | 17.020298 | -5.0 | 30.0 | 44.0 | 58.0 | 77.0 | . TAVG 1217.0 | 52.493016 | 17.830714 | 6.0 | 39.0 | 54.0 | 68.0 | 86.0 | . TMAX 4017.0 | 61.268608 | 18.199517 | 15.0 | 47.0 | 62.0 | 77.0 | 102.0 | . The temperature data looks good so far: the TAVG values are in between TMIN and TMAX, and the measurements and ranges seem reasonable. . Plotting the temperature difference . We&#39;ll continue to assess whether the dataset seems trustworthy by plotting the difference between the maximum and minimum temperatures. . # Create a &#39;TDIFF&#39; column that represents temperature difference weather[&quot;TDIFF&quot;] = weather.TMAX - weather.TMIN # Describe the &#39;TDIFF&#39; column display(weather.TDIFF.describe()) # Create a histogram with 20 bins to visualize &#39;TDIFF&#39; weather.TDIFF.plot(kind=&quot;hist&quot;, bins=20) # Display the plot plt.show() . count 4017.000000 mean 17.784167 std 6.350720 min 2.000000 25% 14.000000 50% 18.000000 75% 22.000000 max 43.000000 Name: TDIFF, dtype: float64 . The TDIFF column has no negative values and its distribution is approximately normal, both of which are signs that the data is trustworthy. . Categorizing the weather . Counting bad weather conditions . The weather DataFrame contains 20 columns that start with &#39;WT&#39;, each of which represents a bad weather condition. For example: . WT05 indicates &quot;Hail&quot; | WT11 indicates &quot;High or damaging winds&quot; | WT17 indicates &quot;Freezing rain&quot; | . For every row in the dataset, each WT column contains either a 1 (meaning the condition was present that day) or NaN (meaning the condition was not present). . We&#39;ll quantify &quot;how bad&quot; the weather was each day by counting the number of 1 values in each row. . # Copy &#39;WT01&#39; through &#39;WT22&#39; to a new DataFrame WT = weather.loc[:, &quot;WT01&quot;:&quot;WT22&quot;] # Calculate the sum of each row in &#39;WT&#39; weather[&#39;bad_conditions&#39;] = WT.sum(axis=&quot;columns&quot;) # Replace missing values in &#39;bad_conditions&#39; with &#39;0&#39; weather[&#39;bad_conditions&#39;] = weather.bad_conditions.fillna(0).astype(&#39;int&#39;) # Create a histogram to visualize &#39;bad_conditions&#39; weather.bad_conditions.plot(kind=&quot;hist&quot;) # Display the plot plt.show() . It looks like many days didn&#39;t have any bad weather conditions, and only a small portion of days had more than four bad weather conditions. . Rating the weather conditions . We counted the number of bad weather conditions each day. We&#39;ll use the counts to create a rating system for the weather. . The counts range from 0 to 9, and should be converted to ratings as follows: . Convert 0 to &#39;good&#39; | Convert 1 through 4 to &#39;bad&#39; | Convert 5 through 9 to &#39;worse&#39; | . # Count the unique values in &#39;bad_conditions&#39; and sort the index display(weather.bad_conditions.value_counts().sort_index()) # Create a dictionary that maps integers to strings mapping = {0:&#39;good&#39;, 1:&#39;bad&#39;, 2:&#39;bad&#39;, 3:&#39;bad&#39;, 4:&#39;bad&#39;, 5:&#39;worse&#39;, 6:&#39;worse&#39;, 7:&#39;worse&#39;, 8:&#39;worse&#39;, 9:&#39;worse&#39;} # Convert the &#39;bad_conditions&#39; integers to strings using the &#39;mapping&#39; weather[&#39;rating&#39;] = weather.bad_conditions.map(mapping) # Count the unique values in &#39;rating&#39; weather.rating.value_counts() . 0 1749 1 613 2 367 3 380 4 476 5 282 6 101 7 41 8 4 9 4 Name: bad_conditions, dtype: int64 . bad 1836 good 1749 worse 432 Name: rating, dtype: int64 . Changing the data type to category . Since the rating column only has a few possible values, we&#39;ll change its data type to category in order to store the data more efficiently. we&#39;ll also specify a logical order for the categories, which will be useful for future work. . # Create a list of weather ratings in logical order cats = [&#39;good&#39;, &#39;bad&#39;, &#39;worse&#39;] # Change the data type of &#39;rating&#39; to category weather[&#39;rating&#39;] = weather.rating.astype(CategoricalDtype(ordered=True, categories=cats)) # Examine the head of &#39;rating&#39; weather.rating.head() . 0 bad 1 bad 2 bad 3 bad 4 bad Name: rating, dtype: category Categories (3, object): [good &lt; bad &lt; worse] . We&#39;ll use the rating column in future exercises to analyze the effects of weather on police behavior. . Merging datasets . Preparing the DataFrames . We&#39;ll prepare the traffic stop and weather rating DataFrames so that they&#39;re ready to be merged: . With the ri DataFrame, we&#39;ll move the stop_datetime index to a column since the index will be lost during the merge. | With the weather DataFrame, we&#39;ll select the DATE and rating columns and put them in a new DataFrame. | . # Reset the index of &#39;ri&#39; ri.reset_index(inplace=True) # Examine the head of &#39;ri&#39; display(ri.head()) # Create a DataFrame from the &#39;DATE&#39; and &#39;rating&#39; columns weather_rating = weather[[&quot;DATE&quot;, &quot;rating&quot;]] # Examine the head of &#39;weather_rating&#39; weather_rating.head() . stop_date_time stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes . 0 2005-01-04 12:55:00 | 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 1 2005-01-23 23:15:00 | 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | . 2 2005-02-17 04:15:00 | 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 3 2005-02-20 17:15:00 | 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | . 4 2005-02-24 01:20:00 | 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | . DATE rating . 0 2005-01-01 | bad | . 1 2005-01-02 | bad | . 2 2005-01-03 | bad | . 3 2005-01-04 | bad | . 4 2005-01-05 | bad | . The ri and weather_rating DataFrames are now ready to be merged. . Merging the DataFrames . We&#39;ll merge the ri and weather_rating DataFrames into a new DataFrame, ri_weather. . The DataFrames will be joined using the stop_date column from ri and the DATE column from weather_rating. Thankfully the date formatting matches exactly, which is not always the case! . Once the merge is complete, we&#39;ll set stop_datetime as the index . # Examine the shape of &#39;ri&#39; print(ri.shape) # Merge &#39;ri&#39; and &#39;weather_rating&#39; using a left join ri_weather = pd.merge(left=ri, right=weather_rating, left_on=&#39;stop_date&#39;, right_on=&#39;DATE&#39;, how=&#39;left&#39;) # Examine the shape of &#39;ri_weather&#39; print(ri_weather.shape) # Set &#39;stop_datetime&#39; as the index of &#39;ri_weather&#39; ri_weather.set_index(&#39;stop_date_time&#39;, inplace=True) ri_weather.head() . (86536, 16) (86536, 18) . stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes DATE rating . stop_date_time . 2005-01-04 12:55:00 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-01-04 | bad | . 2005-01-23 23:15:00 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | 2005-01-23 | worse | . 2005-02-17 04:15:00 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-02-17 | good | . 2005-02-20 17:15:00 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | 2005-02-20 | bad | . 2005-02-24 01:20:00 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | 2005-02-24 | bad | . We&#39;ll use ri_weather to analyze the relationship between weather conditions and police behavior. . Does weather affect the arrest rate? . Comparing arrest rates by weather rating . Do police officers arrest drivers more often when the weather is bad? Let&#39;s find out below! . First, we&#39;ll calculate the overall arrest rate. | Then, we&#39;ll calculate the arrest rate for each of the weather ratings we previously assigned. | Finally, we&#39;ll add violation type as a second factor in the analysis, to see if that accounts for any differences in the arrest rate. | . Since we previously defined a logical order for the weather categories, good &lt; bad &lt; worse, they will be sorted that way in the results. . # Calculate the overall arrest rate print(ri_weather.is_arrested.mean()) . 0.0355690117407784 . # Calculate the arrest rate for each &#39;rating&#39; ri_weather.groupby(&quot;rating&quot;).is_arrested.mean() . rating good 0.033715 bad 0.036261 worse 0.041667 Name: is_arrested, dtype: float64 . # Calculate the arrest rate for each &#39;violation&#39; and &#39;rating&#39; ri_weather.groupby([&quot;violation&quot;, &#39;rating&#39;]).is_arrested.mean() . violation rating Equipment good 0.059007 bad 0.066311 worse 0.097357 Moving violation good 0.056227 bad 0.058050 worse 0.065860 Other good 0.076966 bad 0.087443 worse 0.062893 Registration/plates good 0.081574 bad 0.098160 worse 0.115625 Seat belt good 0.028587 bad 0.022493 worse 0.000000 Speeding good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . The arrest rate increases as the weather gets worse, and that trend persists across many of the violation types. This doesn&#39;t prove a causal link, but it&#39;s quite an interesting result! . Selecting from a multi-indexed Series . The output of a single .groupby() operation on multiple columns is a Series with a MultiIndex. Working with this type of object is similar to working with a DataFrame: . The outer index level is like the DataFrame rows. | The inner index level is like the DataFrame columns. | . # Save the output of the groupby operation from the last exercise arrest_rate = ri_weather.groupby([&#39;violation&#39;, &#39;rating&#39;]).is_arrested.mean() # Print the arrest rate for moving violations in bad weather display(arrest_rate.loc[&quot;Moving violation&quot;, &quot;bad&quot;]) # Print the arrest rates for speeding violations in all three weather conditions arrest_rate.loc[&quot;Speeding&quot;] . 0.05804964058049641 . rating good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . Reshaping the arrest rate data . We&#39;ll start by reshaping the arrest_rate Series into a DataFrame. This is a useful step when working with any multi-indexed Series, since it enables you to access the full range of DataFrame methods. . Then, we&#39;ll create the exact same DataFrame using a pivot table. This is a great example of how pandas often gives you more than one way to reach the same result! . # Unstack the &#39;arrest_rate&#39; Series into a DataFrame display(arrest_rate.unstack()) # Create the same DataFrame using a pivot table ri_weather.pivot_table(index=&#39;violation&#39;, columns=&#39;rating&#39;, values=&#39;is_arrested&#39;) . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | .",
            "url": "https://victoromondi1997.github.io/blog/pandas/eda/python/data-science/data-analysis/2020/09/28/Analyzing-Police-Activity-with-pandas.html",
            "relUrl": "/pandas/eda/python/data-science/data-analysis/2020/09/28/Analyzing-Police-Activity-with-pandas.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Weather Visualization",
            "content": "import matplotlib.pyplot as plt import pandas as pd import numpy as np %matplotlib inline . austin_weather = pd.read_csv(&#39;../data/austin_weather.csv&#39;) seattle_weather = pd.read_csv(&#39;../data/seattle_weather.csv&#39;) . austin_weather.head() . DATE STATION NAME MLY-CLDD-BASE45 MLY-CLDD-BASE50 MLY-CLDD-BASE55 MLY-CLDD-BASE57 MLY-CLDD-BASE60 MLY-CLDD-BASE70 MLY-CLDD-BASE72 ... MLY-TMIN-AVGNDS-LSTH070 MLY-TMIN-NORMAL MLY-TMIN-PRBOCC-LSTH016 MLY-TMIN-PRBOCC-LSTH020 MLY-TMIN-PRBOCC-LSTH024 MLY-TMIN-PRBOCC-LSTH028 MLY-TMIN-PRBOCC-LSTH032 MLY-TMIN-PRBOCC-LSTH036 MLY-TMIN-STDDEV MONTH . 0 1 | USW00013904 | AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US | 190 | 103 | 50 | 35 | 18 | 1 | -7777 | ... | 310 | 36.3 | 298 | 570 | 839 | 967 | 997 | 1000 | 2.9 | Jan | . 1 2 | USW00013904 | AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US | 228 | 132 | 68 | 49 | 29 | 3 | 1 | ... | 280 | 39.4 | 103 | 327 | 614 | 867 | 973 | 999 | 3.2 | Feb | . 2 3 | USW00013904 | AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US | 446 | 306 | 185 | 146 | 98 | 13 | 6 | ... | 308 | 46.6 | 10 | 73 | 242 | 494 | 761 | 928 | 3.6 | Mar | . 3 4 | USW00013904 | AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US | 668 | 519 | 373 | 318 | 240 | 53 | 32 | ... | 287 | 54.7 | 0 | 0 | 0 | 48 | 189 | 453 | 4.1 | Apr | . 4 5 | USW00013904 | AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US | 936 | 781 | 626 | 564 | 471 | 181 | 134 | ... | 250 | 63.7 | 0 | 0 | 0 | 0 | 0 | 0 | 2.5 | May | . 5 rows × 68 columns . seattle_weather.head() . DATE STATION NAME MLY-CLDD-BASE45 MLY-CLDD-BASE50 MLY-CLDD-BASE55 MLY-CLDD-BASE57 MLY-CLDD-BASE60 MLY-CLDD-BASE70 MLY-CLDD-BASE72 ... MLY-TMIN-AVGNDS-LSTH070 MLY-TMIN-NORMAL MLY-TMIN-PRBOCC-LSTH016 MLY-TMIN-PRBOCC-LSTH020 MLY-TMIN-PRBOCC-LSTH024 MLY-TMIN-PRBOCC-LSTH028 MLY-TMIN-PRBOCC-LSTH032 MLY-TMIN-PRBOCC-LSTH036 MLY-TMIN-STDDEV MONTH . 0 1 | USW00094290 | SEATTLE SAND PT WSFO, WA US | 27.0 | 3.0 | -7777.0 | -7777.0 | 0.0 | 0.0 | 0.0 | ... | 310.0 | 37.0 | 64.0 | 129.0 | 317.0 | 709.0 | 959.0 | 1000.0 | 2.3 | Jan | . 1 2 | USW00094290 | SEATTLE SAND PT WSFO, WA US | 31.0 | 3.0 | -7777.0 | -7777.0 | 0.0 | 0.0 | 0.0 | ... | 280.0 | 36.9 | 15.0 | 76.0 | 273.0 | 616.0 | 917.0 | 1000.0 | 2.6 | Feb | . 2 3 | USW00094290 | SEATTLE SAND PT WSFO, WA US | 81.0 | 16.0 | 2.0 | -7777.0 | -7777.0 | 0.0 | 0.0 | ... | 310.0 | 39.3 | 0.0 | 20.0 | 41.0 | 152.0 | 670.0 | 986.0 | 1.8 | Mar | . 3 4 | USW00094290 | SEATTLE SAND PT WSFO, WA US | 169.0 | 58.0 | 12.0 | 6.0 | 1.0 | 0.0 | 0.0 | ... | 300.0 | 42.5 | 0.0 | 0.0 | 0.0 | 0.0 | 114.0 | 711.0 | 1.5 | Apr | . 4 5 | USW00094290 | SEATTLE SAND PT WSFO, WA US | 343.0 | 193.0 | 78.0 | 49.0 | 21.0 | -7777.0 | -7777.0 | ... | 310.0 | 47.8 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 58.0 | 1.9 | May | . 5 rows × 81 columns . Weather Patterns in Seattle and Austin . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(14,6)) # Plot MLY-PRCP-NORMAL from seattle_weather against the MONTH ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&#39;MLY-PRCP-NORMAL&#39;], c=&#39;blue&#39;, marker=&#39;o&#39;, linestyle=&#39;dashdot&#39;) # Plot MLY-PRCP-NORMAL from austin_weather against MONTH ax.plot(austin_weather[&#39;MONTH&#39;], austin_weather[&#39;MLY-PRCP-NORMAL&#39;], c=&#39;red&#39;, marker=&#39;v&#39;, linestyle=&#39;--&#39;) ax.set_xlabel(&#39;Time (months)&#39;) ax.set_ylabel(&#39;Precipitation (inches)&#39;) ax.set_title(&#39;Weather patterns in Seattle and Austin&#39;) plt.show() . Monthly Precipitation and Temperature in Seattle and Austin . fig,ax=plt.subplots(2,2, sharey=True, figsize=(15,4)) ax[0,0].plot(seattle_weather[&#39;MONTH&#39;], seattle_weather[&#39;MLY-PRCP-NORMAL&#39;]) ax[0,1].plot(seattle_weather[&#39;MONTH&#39;], seattle_weather[&#39;MLY-TAVG-NORMAL&#39;]) ax[1,0].plot(austin_weather[&#39;MONTH&#39;], austin_weather[&#39;MLY-PRCP-NORMAL&#39;]) ax[1,1].plot(austin_weather[&#39;MONTH&#39;], austin_weather[&#39;MLY-TAVG-NORMAL&#39;]) plt.show() . Precipitation data in Seattle and Austin . # Create a figure and an array of axes: 2 rows, 1 column with shared y axis fig, ax = plt.subplots(2, 1, sharey=True) # Plot Seattle precipitation data in the top axes ax[0].plot(seattle_weather[&#39;MONTH&#39;], seattle_weather[&#39;MLY-PRCP-NORMAL&#39;], color = &#39;b&#39;) ax[0].plot(seattle_weather[&#39;MONTH&#39;], seattle_weather[&#39;MLY-PRCP-25PCTL&#39;], color = &#39;b&#39;, linestyle = &#39;--&#39;) ax[0].plot(seattle_weather[&#39;MONTH&#39;], seattle_weather[&#39;MLY-PRCP-75PCTL&#39;], color = &#39;b&#39;, linestyle = &#39;--&#39;) # Plot Austin precipitation data in the bottom axes ax[1].plot(austin_weather[&#39;MONTH&#39;], austin_weather[&#39;MLY-PRCP-NORMAL&#39;], color = &#39;r&#39;) ax[1].plot(austin_weather[&#39;MONTH&#39;], austin_weather[&#39;MLY-PRCP-25PCTL&#39;], color = &#39;r&#39;, linestyle = &#39;--&#39;) ax[1].plot(austin_weather[&#39;MONTH&#39;], austin_weather[&#39;MLY-PRCP-75PCTL&#39;], color = &#39;r&#39;, linestyle = &#39;--&#39;) plt.show() . Adding error-bars . plt.style.use(&#39;seaborn&#39;) fig, ax = plt.subplots(figsize=(14,6)) ax.errorbar(seattle_weather[&#39;MONTH&#39;], seattle_weather[&#39;MLY-TAVG-NORMAL&#39;],yerr=seattle_weather[&#39;MLY-TAVG-STDDEV&#39;]) ax.errorbar(austin_weather[&#39;MONTH&#39;], austin_weather[&#39;MLY-TAVG-NORMAL&#39;],yerr=austin_weather[&#39;MLY-TAVG-STDDEV&#39;]) ax.set_ylabel(&#39;Temperature (Fahrenheit)&#39;) plt.show() .",
            "url": "https://victoromondi1997.github.io/blog/data-visualization/weather/2020/08/30/Weather-Visualization.html",
            "relUrl": "/data-visualization/weather/2020/08/30/Weather-Visualization.html",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Analyzing Hacker News Dataset",
            "content": "import pandas as pd import re . Read the Dataset . hn = pd.read_csv(&#39;../hacker_news.csv&#39;) hn.head() . id title url num_points num_comments author created_at . 0 12224879 | Interactive Dynamic Video | http://www.interactivedynamicvideo.com/ | 386 | 52 | ne0phyte | 8/4/2016 11:52 | . 1 11964716 | Florida DJs May Face Felony for April Fools&#39; W... | http://www.thewire.com/entertainment/2013/04/f... | 2 | 1 | vezycash | 6/23/2016 22:20 | . 2 11919867 | Technology ventures: From Idea to Enterprise | https://www.amazon.com/Technology-Ventures-Ent... | 3 | 1 | hswarna | 6/17/2016 0:01 | . 3 10301696 | Note by Note: The Making of Steinway L1037 (2007) | http://www.nytimes.com/2007/11/07/movies/07ste... | 8 | 2 | walterbell | 9/30/2015 4:12 | . 4 10482257 | Title II kills investment? Comcast and other I... | http://arstechnica.com/business/2015/10/comcas... | 53 | 22 | Deinos | 10/31/2015 9:48 | . Dataset Shape . hn.shape . (20099, 7) . hn.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20099 entries, 0 to 20098 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 id 20099 non-null int64 1 title 20099 non-null object 2 url 17659 non-null object 3 num_points 20099 non-null int64 4 num_comments 20099 non-null int64 5 author 20099 non-null object 6 created_at 20099 non-null object dtypes: int64(3), object(4) memory usage: 1.1+ MB . hn.isnull().sum() . id 0 title 0 url 2440 num_points 0 num_comments 0 author 0 created_at 0 dtype: int64 . hn.describe().T . count mean std min 25% 50% 75% max . id 20099.0 | 1.131755e+07 | 696453.087424 | 10176908.0 | 10701720.0 | 11284523.0 | 11926127.0 | 12578975.0 | . num_points 20099.0 | 5.029663e+01 | 107.110322 | 1.0 | 3.0 | 9.0 | 54.0 | 2553.0 | . num_comments 20099.0 | 2.480303e+01 | 56.108639 | 1.0 | 1.0 | 3.0 | 21.0 | 1733.0 | . how many times is Python mentioned in the title of stories in our Hacker News dataset. . len([title for title in hn.title.to_list() if re.search(&#39;[Pp]ython&#39;, title)]) . 160 . hn.title.str.contains(&#39;[Pp]ython&#39;).sum() . 160 . Titles that mention the programming language Ruby . hn.title[hn.title.str.contains(&#39;[Rr]uby&#39;)] . 190 Ruby on Google AppEngine Goes Beta 484 Related: Pure Ruby Relational Algebra Engine 1388 Show HN: HTTPalooza Ruby&#39;s greatest HTTP clie... 1949 Rewriting a Ruby C Extension in Rust: How a Na... 2022 Show HN: CrashBreak Reproduce exceptions as f... 2163 Ruby 2.3 Is Only 4% Faster than 2.2 2306 Websocket Shootout: Clojure, C++, Elixir, Go, ... 2620 Why Startups Use Ruby on Rails? 2645 Ask HN: Should I continue working a Ruby gem f... 3290 Ruby on Rails and the importance of being stup... 3749 Telegram.org Bot Platform Webhooks Server, for... 3874 Warp Directory (wd) unix command line tool for... 4026 OS X 10.11 Ruby / Rails users can install ther... 4163 Charles Nutter of JRuby Banned by Rubinius for... 4602 Quiz: Ruby or Rails? Matz and DHH were not abl... 5832 Show HN: An experimental Python to C#/Go/Ruby/... 6180 Shrine A new solution for handling file uploa... 7171 JRuby+Truffle: Why its important to optimise t... 7235 Ruby or Rails? 7671 How I hunted the most odd ruby bug 7776 Elixir obsoletes Ruby, Erlang and Clojure in o... 7870 Elixir and Ruby Comparison 8502 Show HN: Di-ary a math note-taking app built ... 10212 Ruby has been fast enough for 13 years 11060 Show HN: VeryAnts: Probabilistic Integer Arith... 11534 The Ruby Code of Conduct 11622 FasterPath: Faster Pathname Handling for Ruby ... 12061 Ask HN: What&#39;s your favorite ruby HTTP client? 12091 Show HN: Automated Bundle Update with Descript... 12114 Awesome Ruby 12543 Ruby Bug: SecureRandom should try /dev/urandom... 12987 Show HN: Klipse code evaluator pluggable on a... 13550 Matz: I cannot accept the CoC for the Ruby com... 13650 Programs that rewrite Ruby programs 14798 Ruby Wrapper for Telegram&#39;s Bot API 14980 A Ruby gem for genetic algorithms 16093 Master Ruby Web APIs Is Out 16149 Ruru: native Ruby extensions written in Rust 16327 Make Ruby Great Again [transcript] 16422 Object Oriented Ruby 16536 Ruby Deoptimization Engine 16875 Video: Make Ruby Great Again 17072 A coupon/deals site built using Roda gem for Ruby 17510 Table Flip on Ruby Exceptions 18877 Using Rust with Ruby, a Deep Dive with Yehuda ... 19077 Python is Better than Ruby 19224 Modern concurrency tools for Ruby 19743 Using a Neural Network to Train a Ruby Twitter... Name: title, dtype: object . how many titles in our dataset mention email or e-mail . hn.title[hn.title.str.contains(&#39;e-?mail&#39;)] . 119 Show HN: Send an email from your shell to your... 313 Disposable emails for safe spam free shopping 1361 Ask HN: Doing cold emails? helps us prove this... 1750 Protect yourself from spam, bots and phishing ... 2421 Ashley Madison hack treating email ... 18098 House panel looking into Reddit post about Cli... 18583 Mailgen Generates clean, responsive HTML for ... 18847 Show HN: Crisp iOS keyboard for email and text... 19303 Ask HN: Why big email providers don&#39;t sign the... 19446 Tell HN: Secure email provider Riseup will run... Name: title, Length: 86, dtype: object . how many titles in our dataset have tags? . hn.title[hn.title.str.contains(&#39; [ w+ ]&#39;)] . 66 Analysis of 114 propaganda sources from ISIS, ... 100 Munich Gunman Got Weapon from the Darknet [Ger... 159 File indexing and searching for Plan 9 [pdf] 162 Attack on Kunduz Trauma Centre, Afghanistan I... 195 [Beta] Speedtest.net HTML5 Speed Test ... 19763 TSA can now force you to go through body scann... 19867 Using Pony for Fintech [video] 19947 Swift Reversing [pdf] 19979 WSJ/Dowjones Announce Unauthorized Access Betw... 20089 Users Really Do Plug in USB Drives They Find [... Name: title, Length: 444, dtype: object . we were able to calculate that 444 of the 20,100 Hacker News stories in our dataset contain tags. What if we wanted to find out what the text of these tags were, and how many of each are in the dataset? In order to do this, we&#39;ll need to use capture groups. . extract all of the tags from the Hacker News titles and build a frequency table of those tags. . hn[&#39;title&#39;].str.extract(r&#39; [( w+) ]&#39;)[0].value_counts().head() . pdf 276 video 111 2015 3 audio 3 slides 2 Name: 0, dtype: int64 . def first_10_matches(pattern): &quot;&quot;&quot; Return the story titles that match the provided regular expression &quot;&quot;&quot; return titles[titles.str.contains(pattern)] . Titles that contain Java . hn.title[hn.title.str.contains(r&#39;[Jj]ava[^Ss]&#39;)] . 436 Unikernel Power Comes to Java, Node.js, Go, an... 811 Ask HN: Are there any projects or compilers wh... 1840 Adopting RxJava on the Airbnb App 1972 Node.js vs. Java: Which Is Faster for APIs? 2093 Java EE and Microservices in 2016 2367 Code that is valid in both PHP and Java, and p... 2493 Ask HN: I&#39;ve been a java dev for a couple of y... 2751 Eventsourcing for Java 0.4.0 released 2910 2016 JavaOne Intel Keynote 32mn Talk 3452 What are the Differences Between Java Platform... 4273 Ask HN: Is Bloch&#39;s Effective Java Still Current? 4624 Oracle Discloses Critical Java Vulnerability i... 5461 Lambdas (in Java 8) Screencast 5847 IntelliJ IDEA and the whole IntelliJ platform ... 5947 JavaFX is dead 6268 Oracle deprecating Java applets in Java 9 7436 Forget Guava: 5 Google Libraries Java Develope... 7481 Ask HN: Beside Java what languages have a stro... 8100 Advantages of Functional Programming in Java 8 8135 Show HN: Rogue AI Dungeon, javacript bot scrip... 8447 Show HN: Java multicore intelligence 8487 Why IntelliJ IDEA is hailed as the most friend... 8984 Ask HN: Should Learn/switch to JavaScript Prog... 8987 Last-khajiit/vkb: Java bot for vk.com competit... 10529 Angular 2 coming to Java, Python and PHP 11454 Ask HN: Java or .NET for a new big enterprise ... 11902 The Java Deserialization Bug 12382 Ask HN: Why does Java continue to dominate? 12582 Java Memory Model Examples: Good, Bad and Ugly... 12711 Oracle seeks $9.3B for Googles use of Java in ... 13048 A high performance caching library for Java 8 13105 Show HN: Backblaze-b2 is a simple java library... 13150 Java Tops TIOBE&#39;s Popular-Languages List 13170 Show HN: Tablesaw: A Java data-frame for 500M-... 13272 Java StringBuffer and StringBuilder performance 13620 1M Java questions have now been asked on Stack... 13839 Ask HN: Hosting a Java Spring web application 13843 Var and val in Java? 13844 Answerz.com Java and J2ee Programming 13930 Java 8s new Optional type doesn&#39;t solve anything 13934 Java 6 vs. Java 7 vs. Java 8 between 2013 201... 15257 Oracle and the fall of Java EE 15868 Java generics never cease to impress 16023 Will you use ReactJS with a REST service inste... 16932 Swift versus Java: the bitset performance test 16948 Show HN: Bt 0-hassle BitTorrent for Java 8 17579 Java Lazy Streamed Zip Implementation 18407 Show HN: Scala idioms in Java: cases, patterns... 19481 Show HN: Adding List Comprehension in Java - E... 19735 Java Named Top Programming Language of 2015 Name: title, dtype: object . hn.title[hn.title.str.contains(r&#39; b[Jj]ava b&#39;)] . 436 Unikernel Power Comes to Java, Node.js, Go, an... 811 Ask HN: Are there any projects or compilers wh... 1023 Pippo Web framework in Java 1972 Node.js vs. Java: Which Is Faster for APIs? 2093 Java EE and Microservices in 2016 2367 Code that is valid in both PHP and Java, and p... 2493 Ask HN: I&#39;ve been a java dev for a couple of y... 2751 Eventsourcing for Java 0.4.0 released 3228 Comparing Rust and Java 3452 What are the Differences Between Java Platform... 3627 Friends don&#39;t let friends do Java 4273 Ask HN: Is Bloch&#39;s Effective Java Still Current? 4624 Oracle Discloses Critical Java Vulnerability i... 5461 Lambdas (in Java 8) Screencast 5847 IntelliJ IDEA and the whole IntelliJ platform ... 6268 Oracle deprecating Java applets in Java 9 7436 Forget Guava: 5 Google Libraries Java Develope... 7481 Ask HN: Beside Java what languages have a stro... 7686 Insider: Oracle has lost interest in Java 8100 Advantages of Functional Programming in Java 8 8447 Show HN: Java multicore intelligence 8487 Why IntelliJ IDEA is hailed as the most friend... 8984 Ask HN: Should Learn/switch to JavaScript Prog... 8987 Last-khajiit/vkb: Java bot for vk.com competit... 10529 Angular 2 coming to Java, Python and PHP 11454 Ask HN: Java or .NET for a new big enterprise ... 11902 The Java Deserialization Bug 12382 Ask HN: Why does Java continue to dominate? 12582 Java Memory Model Examples: Good, Bad and Ugly... 12711 Oracle seeks $9.3B for Googles use of Java in ... 12730 Show HN: Shazam in Java 13048 A high performance caching library for Java 8 13105 Show HN: Backblaze-b2 is a simple java library... 13150 Java Tops TIOBE&#39;s Popular-Languages List 13170 Show HN: Tablesaw: A Java data-frame for 500M-... 13272 Java StringBuffer and StringBuilder performance 13620 1M Java questions have now been asked on Stack... 13839 Ask HN: Hosting a Java Spring web application 13843 Var and val in Java? 13844 Answerz.com Java and J2ee Programming 13930 Java 8s new Optional type doesn&#39;t solve anything 13934 Java 6 vs. Java 7 vs. Java 8 between 2013 201... 14393 JavaScript is immature compared to Java 14847 Show HN: TurboRLE: Bringing Turbo Run Length E... 15257 Oracle and the fall of Java EE 15868 Java generics never cease to impress 16023 Will you use ReactJS with a REST service inste... 16932 Swift versus Java: the bitset performance test 16948 Show HN: Bt 0-hassle BitTorrent for Java 8 17458 Super Mario clone in Java 17579 Java Lazy Streamed Zip Implementation 18407 Show HN: Scala idioms in Java: cases, patterns... 19481 Show HN: Adding List Comprehension in Java - E... 19735 Java Named Top Programming Language of 2015 Name: title, dtype: object . how many titles have tags at the start versus the end of the story title in our Hacker News dataset. . hn.title.str.contains(r&#39;^ [ w+ ]&#39;).sum() . 15 . hn.title.str.contains(r&#39; [ w+ ]$&#39;).sum() . 417 . count the number of times that email is mentioned in story titles. . hn.title.str.contains(r&#39; be -? s?mails? b&#39;, flags=re.I).sum() . 141 . We&#39;ll continue to analyze and count mentions of different programming languages in the dataset, and then we&#39;ll finish by extracting the different components of the URLs submitted to Hacker News. . count the number of times that sql is mentioned in story titles. . hn.title.str.contains(r&#39;sql&#39;, flags=re.I).sum() . 108 . hn_sql = hn[hn.title.str.contains(r&#39; w+sql&#39;, flags=re.I)].copy() hn_sql[&#39;flavor&#39;] = hn[&#39;title&#39;].str.extract(r&#39;( w+sql)&#39;, flags=re.I)[0].str.lower() sql_pivot = hn_sql.pivot_table(index=&#39;flavor&#39;, values=&#39;num_comments&#39;) sql_pivot . num_comments . flavor . cloudsql 5.000000 | . memsql 14.000000 | . mysql 12.230769 | . nosql 14.529412 | . postgresql 25.962963 | . sparksql 1.000000 | . version of Python that is mentioned most often in our dataset . hn.title.str.extract(r&#39;python ([ d .]+)&#39;, flags=re.I)[0].value_counts().to_dict() . {&#39;3&#39;: 10, &#39;2&#39;: 3, &#39;3.5&#39;: 3, &#39;3.6&#39;: 2, &#39;2.7&#39;: 1, &#39;8&#39;: 1, &#39;1.5&#39;: 1, &#39;3.5.0&#39;: 1, &#39;4&#39;: 1} . C programming titles . hn.title[hn.title.str.contains(r&#39;(?!&lt;series) bc b(?![ . +])&#39;, flags=re.I)] . 221 MemSQL (YC W11) Raises $36M Series C 365 The new C standards are worth it 444 Moz raises $10m Series C from Foundry Group 521 Fuchsia: Micro kernel written in C by Google 1307 Show HN: Yupp, yet another C preprocessor ... 18549 Show HN: An awesome C library for Windows 18649 Python vs. C/C++ in embedded systems 18689 Philz Coffee raises $45M Series C 19151 Ask HN: How to learn C in 2016? 19933 Lightweight C library to parse NMEA 0183 sente... Name: title, Length: 105, dtype: object . make all the different variations of &quot;email&quot; in the dataset uniform. . hn[&#39;title&#39;] = hn.title.str.replace(r&#39;e[ - s]?mail&#39;,&#39;email&#39;, flags=re.I) hn.title[hn.title.str.contains(&#39;email&#39;)] . 119 Show HN: Send an email from your shell to your... 161 Computer Specialist Who Deleted Clinton emails... 174 email Apps Suck 261 emails Show Unqualified Clinton Foundation Don... 313 Disposable emails for safe spam free shopping ... 19303 Ask HN: Why big email providers don&#39;t sign the... 19395 I used HTML email when applying for jobs, here... 19446 Tell HN: Secure email provider Riseup will run... 19838 Petition to Open Sourcemailbox 19905 Gmail Will Soon Warn Users When emails Arrive ... Name: title, Length: 151, dtype: object . extract components of URLs from our dataset. . most stories on Hacker News contain a link to an external resource. Once we have extracted the domains, we will be building a frequency table so we can determine the most popular domains. There are over 7,000 unique domains in our dataset, so to make the frequency table easier to analyze, we&#39;ll look at only the top 20 domains . hn.url.str.extract(r&#39;https?://([ w - .]+)&#39;, flags=re.I)[0].value_counts() . github.com 1008 medium.com 825 www.nytimes.com 525 www.theguardian.com 248 techcrunch.com 245 ... pss-camera.appspot.com 1 www.mrrrgn.com 1 ams-ix.net 1 www.codeshare.co.uk 1 lambdaschool.com 1 Name: 0, Length: 7251, dtype: int64 . hn_urls=hn.url.str.extract(r&#39;(?P&lt;protocol&gt; w+://(?P&lt;domain&gt;[ w . -]+)/?(?P&lt;path&gt;.*))&#39;, flags=re.I) hn_urls.head() . protocol domain path . 0 http://www.interactivedynamicvideo.com/ | www.interactivedynamicvideo.com | | . 1 http://www.thewire.com/entertainment/2013/04/f... | www.thewire.com | entertainment/2013/04/florida-djs-april-fools-... | . 2 https://www.amazon.com/Technology-Ventures-Ent... | www.amazon.com | Technology-Ventures-Enterprise-Thomas-Byers/dp... | . 3 http://www.nytimes.com/2007/11/07/movies/07ste... | www.nytimes.com | 2007/11/07/movies/07stein.html?_r=0 | . 4 http://arstechnica.com/business/2015/10/comcas... | arstechnica.com | business/2015/10/comcast-and-other-isps-boost-... | . hn_urls.domain.value_counts() . github.com 1008 medium.com 825 www.nytimes.com 525 www.theguardian.com 248 techcrunch.com 245 ... pss-camera.appspot.com 1 www.mrrrgn.com 1 ams-ix.net 1 www.codeshare.co.uk 1 lambdaschool.com 1 Name: domain, Length: 7251, dtype: int64 .",
            "url": "https://victoromondi1997.github.io/blog/data-analysis/hacker-news/2020/08/30/Analyzing-Hacker-News-Dataset.html",
            "relUrl": "/data-analysis/hacker-news/2020/08/30/Analyzing-Hacker-News-Dataset.html",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Visualizing Women in USA Degrees",
            "content": "Introduction to data . The Department of Education Statistics releases a data set annually containing the percentage of bachelor&#39;s degrees granted to women from 1970 to 2012. The data set is broken up into 17 categories of degrees, with each column as a separate category. . Randal Olson, a data scientist at University of Pennsylvania, has cleaned the data set and made it available on his personal website. You can download the dataset Randal compiled here. . Goal . Explore how we can communicate the nuanced narrative of gender gap using effective data visualization. . import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) %matplotlib inline . women_degrees = pd.read_csv(&#39;../data/percent-bachelors-degrees-women-usa.csv&#39;) women_degrees.head() . Year Agriculture Architecture Art and Performance Biology Business Communications and Journalism Computer Science Education Engineering English Foreign Languages Health Professions Math and Statistics Physical Sciences Psychology Public Administration Social Sciences and History . 0 1970 | 4.229798 | 11.921005 | 59.7 | 29.088363 | 9.064439 | 35.3 | 13.6 | 74.535328 | 0.8 | 65.570923 | 73.8 | 77.1 | 38.0 | 13.8 | 44.4 | 68.4 | 36.8 | . 1 1971 | 5.452797 | 12.003106 | 59.9 | 29.394403 | 9.503187 | 35.5 | 13.6 | 74.149204 | 1.0 | 64.556485 | 73.9 | 75.5 | 39.0 | 14.9 | 46.2 | 65.5 | 36.2 | . 2 1972 | 7.420710 | 13.214594 | 60.4 | 29.810221 | 10.558962 | 36.6 | 14.9 | 73.554520 | 1.2 | 63.664263 | 74.6 | 76.9 | 40.2 | 14.8 | 47.6 | 62.6 | 36.1 | . 3 1973 | 9.653602 | 14.791613 | 60.2 | 31.147915 | 12.804602 | 38.4 | 16.4 | 73.501814 | 1.6 | 62.941502 | 74.9 | 77.4 | 40.9 | 16.5 | 50.4 | 64.3 | 36.4 | . 4 1974 | 14.074623 | 17.444688 | 61.9 | 32.996183 | 16.204850 | 40.5 | 18.9 | 73.336811 | 2.2 | 62.413412 | 75.3 | 77.9 | 41.8 | 18.2 | 52.6 | 66.1 | 37.3 | . a line chart that visualizes the historical percentage of Biology degrees awarded to women . fig, ax = plt.subplots(figsize=(20,10)) ax.plot(women_degrees.Year, women_degrees.Biology) ax.set_title(&#39;The historical percentage of Biology degrees&#39;) ax.set_ylabel(&#39;Biology&#39;) ax.set_xlabel(&#39;Year&#39;) plt.show() . Observation . From the plot, we can tell that Biology degrees increased steadily from 1970 and peaked in the early 2000&#39;s. We can also tell that the percentage has stayed above 50% since around 1987. . Visualizing the gender gap . fig, ax = plt.subplots(figsize=(20,10)) ax.plot(women_degrees.Year, women_degrees.Biology, label=&#39;Women&#39;) ax.plot(women_degrees.Year, 100-women_degrees.Biology, label=&#39;Men&#39;) ax.set_title(&#39;Percentage of Biology Degrees Awarded By Gender&#39;) ax.set_ylabel(&#39;Biology&#39;) ax.set_xlabel(&#39;Year&#39;) ax.set_xlim(left=1970) ax.legend() plt.show() . In the first period, from 1970 to around 1987, women were a minority when it came to majoring in Biology while in the second period, from around 1987 to around 2012, women became a majority . Data-ink ration . To improve the data-ink ratio, let&#39;s make the following changes to the plot already created. . Remove all of the axis tick marks. | Hide the spines, which are the lines that connects the tick marks, on each axis. | fig, ax = plt.subplots(figsize=(20,10)) ax.plot(women_degrees.Year, women_degrees.Biology, label=&#39;Women&#39;) ax.plot(women_degrees.Year, 100-women_degrees.Biology, label=&#39;Men&#39;) ax.set_title(&#39;Percentage of Biology Degrees Awarded By Gender&#39;) ax.set_ylabel(&#39;Biology&#39;) ax.set_xlabel(&#39;Year&#39;) ax.set_xlim(left=1970) ax.tick_params(top=False, bottom=False, left=False, right=False) ax.legend(loc=&#39;upper right&#39;) plt.show() . Hiding spines . fig, ax = plt.subplots(figsize=(20,10)) ax.plot(women_degrees.Year, women_degrees.Biology, label=&#39;Women&#39;) ax.plot(women_degrees.Year, 100-women_degrees.Biology, label=&#39;Men&#39;) ax.set_title(&#39;Percentage of Biology Degrees Awarded By Gender&#39;) ax.set_ylabel(&#39;Biology&#39;) ax.set_xlabel(&#39;Year&#39;) ax.set_xlim(left=1970) for position in [&#39;top&#39;, &#39;bottom&#39;, &#39;right&#39;, &#39;left&#39;]: ax.spines[position].set_visible(False) ax.legend() plt.show() . Comparing gender gap across degree categories . major_cats = [&#39;Biology&#39;, &#39;Computer Science&#39;, &#39;Engineering&#39;, &#39;Math and Statistics&#39;] fig = plt.figure(figsize=(20, 20)) for sp in range(0,4): ax = fig.add_subplot(2,2,sp+1) ax.plot(women_degrees[&#39;Year&#39;], women_degrees[major_cats[sp]], c=(0/255, 107/255, 164/255), label=&#39;Women&#39;, linewidth=3) ax.plot(women_degrees[&#39;Year&#39;], 100-women_degrees[major_cats[sp]], c=(255/255, 128/255, 14/255), linewidth=3, label=&#39;Men&#39;) # Add your code here. ax.set_xlim(1968, 2011) ax.set_ylim(0,100) for position in [&#39;top&#39;, &#39;bottom&#39;, &#39;left&#39;, &#39;right&#39;]: ax.spines[position].set_visible(False) ax.tick_params(top=False, bottom=False, left=False, right=False) ax.set_title(major_cats[sp]) # Calling pyplot.legend() here will add the legend to the last subplot that was created. plt.legend(loc=&#39;upper right&#39;) plt.show() . Observation . we can conclude that the gender gap in Computer Science and Engineering have big gender gaps while the gap in Biology and Math and Statistics is quite small. In addition, the first two degree categories are dominated by men while the latter degree categories are much more balanced. . stem_cats = [&#39;Engineering&#39;, &#39;Computer Science&#39;, &#39;Psychology&#39;, &#39;Biology&#39;, &#39;Physical Sciences&#39;, &#39;Math and Statistics&#39;] fig = plt.figure(figsize=(20, 3)) for sp in range(0,6): ax = fig.add_subplot(1,6,sp+1) ax.plot(women_degrees[&#39;Year&#39;], women_degrees[stem_cats[sp]], c=(0/255, 107/255, 164/255), label=&#39;Women&#39;, linewidth=3) ax.plot(women_degrees[&#39;Year&#39;], 100-women_degrees[stem_cats[sp]], c=(255/255, 128/255, 14/255), label=&#39;Men&#39;, linewidth=3) for key,spine in ax.spines.items(): spine.set_visible(False) ax.set_xlim(1968, 2011) ax.set_ylim(0,100) ax.set_title(stem_cats[sp]) ax.tick_params(bottom=False, top=False, left=False, right=False) if sp==0: ax.text(2005,87,&quot;Men&quot;) ax.text(2002,8, &quot;Women&quot;) if sp==5: ax.text(2005,62,&quot;Men&quot;) ax.text(2001,35, &quot;Women&quot;) plt.show() .",
            "url": "https://victoromondi1997.github.io/blog/data-visualization/women/2020/08/25/visualizing_women_in_USA_degrees.html",
            "relUrl": "/data-visualization/women/2020/08/25/visualizing_women_in_USA_degrees.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Working with Missing Data",
            "content": "Libraries . import pandas as pd # to display all columns pd.set_option(&#39;display.max.columns&#39;, None) # to display the entire contents of a cell pd.set_option(&#39;display.max_colwidth&#39;, None) import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline plt.style.use(&quot;ggplot&quot;) . mvc = pd.read_csv(&quot;../datasets/nypd_mvc_2018.csv&quot;) mvc.head() . unique_key date time borough location on_street cross_street off_street pedestrians_injured cyclist_injured motorist_injured total_injured pedestrians_killed cyclist_killed motorist_killed total_killed vehicle_1 vehicle_2 vehicle_3 vehicle_4 vehicle_5 cause_vehicle_1 cause_vehicle_2 cause_vehicle_3 cause_vehicle_4 cause_vehicle_5 . 0 3869058 | 2018-03-23 | 21:40 | MANHATTAN | (40.742832, -74.00771) | WEST 15 STREET | 10 AVENUE | NaN | 0 | 0 | 0 | 0.0 | 0 | 0 | 0 | 0.0 | PASSENGER VEHICLE | NaN | NaN | NaN | NaN | Following Too Closely | Unspecified | NaN | NaN | NaN | . 1 3847947 | 2018-02-13 | 14:45 | BROOKLYN | (40.623714, -73.99314) | 16 AVENUE | 62 STREET | NaN | 0 | 0 | 0 | 0.0 | 0 | 0 | 0 | 0.0 | SPORT UTILITY / STATION WAGON | DS | NaN | NaN | NaN | Backing Unsafely | Unspecified | NaN | NaN | NaN | . 2 3914294 | 2018-06-04 | 0:00 | NaN | (40.591755, -73.9083) | BELT PARKWAY | NaN | NaN | 0 | 0 | 1 | 1.0 | 0 | 0 | 0 | 0.0 | Station Wagon/Sport Utility Vehicle | Sedan | NaN | NaN | NaN | Following Too Closely | Unspecified | NaN | NaN | NaN | . 3 3915069 | 2018-06-05 | 6:36 | QUEENS | (40.73602, -73.87954) | GRAND AVENUE | VANLOON STREET | NaN | 0 | 0 | 0 | 0.0 | 0 | 0 | 0 | 0.0 | Sedan | Sedan | NaN | NaN | NaN | Glare | Passing Too Closely | NaN | NaN | NaN | . 4 3923123 | 2018-06-16 | 15:45 | BRONX | (40.884727, -73.89945) | NaN | NaN | 208 WEST 238 STREET | 0 | 0 | 0 | 0.0 | 0 | 0 | 0 | 0.0 | Station Wagon/Sport Utility Vehicle | Sedan | NaN | NaN | NaN | Turning Improperly | Unspecified | NaN | NaN | NaN | . A summary of the columns and their data is below: . unique_key: A unique identifier for each collision. | date, time: Date and time of the collision. | borough: The borough, or area of New York City, where the collision occurred. | location: Latitude and longitude coordinates for the collision. | on_street, cross_street, off_street: Details of the street or intersection where the collision occurred. | pedestrians_injured: Number of pedestrians who were injured. | cyclist_injured: Number of people traveling on a bicycle who were injured. | motorist_injured: Number of people traveling in a vehicle who were injured. | total_injured: Total number of people injured. | pedestrians_killed: Number of pedestrians who were killed. | cyclist_killed: Number of people traveling on a bicycle who were killed. | motorist_killed: Number of people traveling in a vehicle who were killed. | total_killed: Total number of people killed. | vehicle_1 through vehicle_5: Type of each vehicle involved in the accident. | cause_vehicle_1 through cause_vehicle_5: Contributing factor for each vehicle in the accident. | . Missing Values . mvc.isna().sum() . unique_key 0 date 0 time 0 borough 20646 location 3885 on_street 13961 cross_street 29249 off_street 44093 pedestrians_injured 0 cyclist_injured 0 motorist_injured 0 total_injured 1 pedestrians_killed 0 cyclist_killed 0 motorist_killed 0 total_killed 5 vehicle_1 355 vehicle_2 12262 vehicle_3 54352 vehicle_4 57158 vehicle_5 57681 cause_vehicle_1 175 cause_vehicle_2 8692 cause_vehicle_3 54134 cause_vehicle_4 57111 cause_vehicle_5 57671 dtype: int64 . To give us a better picture of the null values in the data, let&#39;s calculate the percentage of null values in each column. . null_df = pd.DataFrame({&#39;null_counts&#39;:mvc.isna().sum(), &#39;null_pct&#39;:mvc.isna().sum()/mvc.shape[0] * 100}).T.astype(int).T null_df . null_counts null_pct . unique_key 0 | 0 | . date 0 | 0 | . time 0 | 0 | . borough 20646 | 35 | . location 3885 | 6 | . on_street 13961 | 24 | . cross_street 29249 | 50 | . off_street 44093 | 76 | . pedestrians_injured 0 | 0 | . cyclist_injured 0 | 0 | . motorist_injured 0 | 0 | . total_injured 1 | 0 | . pedestrians_killed 0 | 0 | . cyclist_killed 0 | 0 | . motorist_killed 0 | 0 | . total_killed 5 | 0 | . vehicle_1 355 | 0 | . vehicle_2 12262 | 21 | . vehicle_3 54352 | 93 | . vehicle_4 57158 | 98 | . vehicle_5 57681 | 99 | . cause_vehicle_1 175 | 0 | . cause_vehicle_2 8692 | 15 | . cause_vehicle_3 54134 | 93 | . cause_vehicle_4 57111 | 98 | . cause_vehicle_5 57671 | 99 | . About a third of the columns have no null values, with the rest ranging from less than 1% to 99%! To make things easier, let&#39;s start by looking at the group of columns that relate to people killed in collisions. . null_df.loc[[column for column in mvc.columns if &quot;killed&quot; in column]] . null_counts null_pct . pedestrians_killed 0 | 0 | . cyclist_killed 0 | 0 | . motorist_killed 0 | 0 | . total_killed 5 | 0 | . We can see that each of the individual categories have no missing values, but the total_killed column has five missing values. . If you think about it, the total number of people killed should be the sum of each of the individual categories. We might be able to &quot;fill in&quot; the missing values with the sums of the individual columns for that row. . Important: The technical name for filling in a missing value with a replacement value is called imputation. . Verifying the total columns . killed = mvc[[col for col in mvc.columns if &#39;killed&#39; in col]].copy() killed_manual_sum = killed.iloc[:, :3].sum(axis=&quot;columns&quot;) killed_mask = killed_manual_sum != killed.total_killed killed_non_eq = killed[killed_mask] killed_non_eq . pedestrians_killed cyclist_killed motorist_killed total_killed . 3508 0 | 0 | 0 | NaN | . 20163 0 | 0 | 0 | NaN | . 22046 0 | 0 | 1 | 0.0 | . 48719 0 | 0 | 0 | NaN | . 55148 0 | 0 | 0 | NaN | . 55699 0 | 0 | 0 | NaN | . Filling and Verifying the killed and the injured data . The killed_non_eq dataframe has six rows. We can categorize these into two categories: . Five rows where the total_killed is not equal to the sum of the other columns because the total value is missing. | One row where the total_killed is less than the sum of the other columns. | . From this, we can conclude that filling null values with the sum of the columns is a fairly good choice for our imputation, given that only six rows out of around 58,000 don&#39;t match this pattern. . We&#39;ve also identified a row that has suspicious data - one that doesn&#39;t sum correctly. Once we have imputed values for all rows with missing values for total_killed, we&#39;ll mark this suspect row by setting its value to NaN. . # fix the killed values killed[&#39;total_killed&#39;] = killed[&#39;total_killed&#39;].mask(killed[&#39;total_killed&#39;].isnull(), killed_manual_sum) killed[&#39;total_killed&#39;] = killed[&#39;total_killed&#39;].mask(killed[&#39;total_killed&#39;] != killed_manual_sum, np.nan) # Create an injured dataframe and manually sum values injured = mvc[[col for col in mvc.columns if &#39;injured&#39; in col]].copy() injured_manual_sum = injured.iloc[:,:3].sum(axis=1) injured[&#39;total_injured&#39;] = injured.total_injured.mask(injured.total_injured.isnull(), injured_manual_sum) injured[&#39;total_injured&#39;] = injured.total_injured.mask(injured.total_injured != injured_manual_sum, np.nan) . Let&#39;s summarize the count of null values before and after our changes: . summary = { &#39;injured&#39;: [ mvc[&#39;total_injured&#39;].isnull().sum(), injured[&#39;total_injured&#39;].isnull().sum() ], &#39;killed&#39;: [ mvc[&#39;total_killed&#39;].isnull().sum(), killed[&#39;total_killed&#39;].isnull().sum() ] } pd.DataFrame(summary, index=[&#39;before&#39;,&#39;after&#39;]) . injured killed . before 1 | 5 | . after 21 | 1 | . For the total_killed column, the number of values has gone down from 5 to 1. For the total_injured column, the number of values has actually gone up — from 1 to 21. This might sound like we&#39;ve done the opposite of what we set out to do, but what we&#39;ve actually done is fill all the null values and identify values that have suspect data. This will make any analysis we do on this data more accurate in the long run. . Let&#39;s assign the values from the killed and injured dataframe back to the main mvc dataframe: . mvc[&#39;total_injured&#39;]=injured.total_injured mvc[&#39;total_killed&#39;]=killed.total_killed . Visualizing the missing data with plots . Earlier, we used a table of numbers to understand the number of missing values in our dataframe. A different approach we can take is to use a plot to visualize the missing values. The function below uses seaborn.heatmap() to represent null values as dark squares and non-null values as light squares: . def plot_null_matrix(df, figsize=(20,15)): &quot;&quot;&quot;Plot null values as light squares and non-null values as dark squares&quot;&quot;&quot; plt.figure(figsize=figsize) df_null = df.isnull() sns.heatmap(~df_null, cbar=False, yticklabels=False) plt.xticks(rotation=90, size=&quot;x-large&quot;) plt.show() . Let&#39;s look at how the function works by using it to plot just the first row of our mvc dataframe. We&#39;ll display the first row as a table immediately below so it&#39;s easy to compare: . plot_null_matrix(mvc.head(1), figsize=(20,2)) . mvc.head(1) . unique_key date time borough location on_street cross_street off_street pedestrians_injured cyclist_injured motorist_injured total_injured pedestrians_killed cyclist_killed motorist_killed total_killed vehicle_1 vehicle_2 vehicle_3 vehicle_4 vehicle_5 cause_vehicle_1 cause_vehicle_2 cause_vehicle_3 cause_vehicle_4 cause_vehicle_5 . 0 3869058 | 2018-03-23 | 21:40 | MANHATTAN | (40.742832, -74.00771) | WEST 15 STREET | 10 AVENUE | NaN | 0 | 0 | 0 | 0.0 | 0 | 0 | 0 | 0.0 | PASSENGER VEHICLE | NaN | NaN | NaN | NaN | Following Too Closely | Unspecified | NaN | NaN | NaN | . Each value is represented by a light square, and each missing value is represented by a dark square. . Let&#39;s look at what a plot matrix looks like for the whole dataframe: . plot_null_matrix(mvc) . We can make some immediate interpretations about our dataframe: . The first three columns have few to no missing values. | The next five columns have missing values scattered throughout, with each column seeming to have its own density of missing values. | The next eight columns are the injury and killed columns we just cleaned, and only have a few missing values. | The last 10 columns seem to break into two groups of five, with each group of five having similar patterns of null/non-null values. | . Let&#39;s examine the pattern in the last 10 columns a little more closely. We can calculate the relationship between two sets of columns, known as correlation. . cols_with_missing_vals = mvc.columns[mvc.isnull().sum()&gt;0] missing_corr = mvc[cols_with_missing_vals].isnull().corr() missing_corr . borough location on_street cross_street off_street total_injured total_killed vehicle_1 vehicle_2 vehicle_3 vehicle_4 vehicle_5 cause_vehicle_1 cause_vehicle_2 cause_vehicle_3 cause_vehicle_4 cause_vehicle_5 . borough 1.000000 | 0.190105 | -0.350190 | 0.409107 | 0.362189 | -0.002827 | 0.005582 | -0.018325 | -0.077516 | -0.061932 | -0.020406 | -0.010733 | -0.012115 | -0.058596 | -0.060542 | -0.020158 | -0.011348 | . location 0.190105 | 1.000000 | -0.073975 | -0.069719 | 0.084579 | -0.001486 | 0.015496 | -0.010466 | -0.033842 | -0.000927 | 0.004655 | -0.005797 | -0.003458 | -0.021373 | 0.000684 | 0.004604 | -0.004841 | . on_street -0.350190 | -0.073975 | 1.000000 | 0.557767 | -0.991030 | 0.006220 | -0.002344 | -0.001889 | 0.119647 | 0.020867 | 0.004172 | -0.002768 | 0.001307 | 0.087374 | 0.017426 | 0.002737 | -0.003107 | . cross_street 0.409107 | -0.069719 | 0.557767 | 1.000000 | -0.552763 | 0.002513 | 0.004112 | -0.017018 | 0.043799 | -0.049910 | -0.021137 | -0.012003 | -0.009102 | 0.031189 | -0.052159 | -0.022074 | -0.013455 | . off_street 0.362189 | 0.084579 | -0.991030 | -0.552763 | 1.000000 | -0.004266 | 0.002323 | 0.001812 | -0.121129 | -0.022404 | -0.004074 | 0.002492 | -0.001738 | -0.088187 | -0.019120 | -0.002580 | 0.002863 | . total_injured -0.002827 | -0.001486 | 0.006220 | 0.002513 | -0.004266 | 1.000000 | -0.000079 | 0.079840 | 0.025644 | -0.002757 | 0.002118 | 0.001073 | 0.131140 | 0.030082 | -0.002388 | 0.002188 | 0.001102 | . total_killed 0.005582 | 0.015496 | -0.002344 | 0.004112 | 0.002323 | -0.000079 | 1.000000 | -0.000327 | 0.008017 | 0.001057 | 0.000462 | 0.000234 | -0.000229 | 0.009888 | 0.001091 | 0.000477 | 0.000240 | . vehicle_1 -0.018325 | -0.010466 | -0.001889 | -0.017018 | 0.001812 | 0.079840 | -0.000327 | 1.000000 | 0.151516 | 0.019972 | 0.008732 | 0.004425 | 0.604281 | 0.180678 | 0.020624 | 0.009022 | 0.004545 | . vehicle_2 -0.077516 | -0.033842 | 0.119647 | 0.043799 | -0.121129 | 0.025644 | 0.008017 | 0.151516 | 1.000000 | 0.131813 | 0.057631 | 0.029208 | 0.106214 | 0.784402 | 0.132499 | 0.058050 | 0.029264 | . vehicle_3 -0.061932 | -0.000927 | 0.020867 | -0.049910 | -0.022404 | -0.002757 | 0.001057 | 0.019972 | 0.131813 | 1.000000 | 0.437214 | 0.221585 | 0.014000 | 0.106874 | 0.961316 | 0.448525 | 0.225067 | . vehicle_4 -0.020406 | 0.004655 | 0.004172 | -0.021137 | -0.004074 | 0.002118 | 0.000462 | 0.008732 | 0.057631 | 0.437214 | 1.000000 | 0.506810 | 0.006121 | 0.046727 | 0.423394 | 0.963723 | 0.515058 | . vehicle_5 -0.010733 | -0.005797 | -0.002768 | -0.012003 | 0.002492 | 0.001073 | 0.000234 | 0.004425 | 0.029208 | 0.221585 | 0.506810 | 1.000000 | 0.003102 | 0.023682 | 0.214580 | 0.490537 | 0.973664 | . cause_vehicle_1 -0.012115 | -0.003458 | 0.001307 | -0.009102 | -0.001738 | 0.131140 | -0.000229 | 0.604281 | 0.106214 | 0.014000 | 0.006121 | 0.003102 | 1.000000 | 0.131000 | 0.014457 | 0.006324 | 0.003186 | . cause_vehicle_2 -0.058596 | -0.021373 | 0.087374 | 0.031189 | -0.088187 | 0.030082 | 0.009888 | 0.180678 | 0.784402 | 0.106874 | 0.046727 | 0.023682 | 0.131000 | 1.000000 | 0.110362 | 0.048277 | 0.024322 | . cause_vehicle_3 -0.060542 | 0.000684 | 0.017426 | -0.052159 | -0.019120 | -0.002388 | 0.001091 | 0.020624 | 0.132499 | 0.961316 | 0.423394 | 0.214580 | 0.014457 | 0.110362 | 1.000000 | 0.437440 | 0.220384 | . cause_vehicle_4 -0.020158 | 0.004604 | 0.002737 | -0.022074 | -0.002580 | 0.002188 | 0.000477 | 0.009022 | 0.058050 | 0.448525 | 0.963723 | 0.490537 | 0.006324 | 0.048277 | 0.437440 | 1.000000 | 0.503805 | . cause_vehicle_5 -0.011348 | -0.004841 | -0.003107 | -0.013455 | 0.002863 | 0.001102 | 0.000240 | 0.004545 | 0.029264 | 0.225067 | 0.515058 | 0.973664 | 0.003186 | 0.024322 | 0.220384 | 0.503805 | 1.000000 | . Each value is between -1 and 1, and represents the relationship between two columns. A number close to 1 or -1 represents a strong relationship, where a number in the middle (close to 0) represents a weak relationship. . If you look closely, you can see a diagonal line of 1s going from top left to bottom right. These values represent each columns relationship with itself, which of course is a perfect relationship. The values on the top/right of this &quot;line of 1s&quot; mirror the values on the bottom/left of this line: The table actually repeats every value twice! . Let&#39;s create a correlation plot of just those last 10 columns to see if we can more closely identify the pattern we saw earlier in the matrix plot. . def plot_null_correlations(df): &quot;&quot;&quot;create a correlation matrix only for columns with at least one missing value&quot;&quot;&quot; cols_with_missing_vals = df.columns[df.isnull().sum() &gt; 0] missing_corr = df[cols_with_missing_vals].isnull().corr() # create a mask to avoid repeated values and make # the plot easier to read missing_corr = missing_corr.iloc[1:, :-1] mask = np.triu(np.ones_like(missing_corr), k=1) # plot a heatmap of the values plt.figure(figsize=(20,14)) ax = sns.heatmap(missing_corr, vmin=-1, vmax=1, cbar=False, cmap=&#39;RdBu&#39;, mask=mask, annot=True) # format the text in the plot to make it easier to read for text in ax.texts: t = float(text.get_text()) if -0.05 &lt; t &lt; 0.01: text.set_text(&#39;&#39;) else: text.set_text(round(t, 2)) text.set_fontsize(&#39;x-large&#39;) plt.xticks(rotation=90, size=&#39;x-large&#39;) plt.yticks(rotation=0, size=&#39;x-large&#39;) plt.show() . plot_null_correlations(mvc[[column for column in mvc.columns if &#39;vehicle&#39; in column]]) . In our correlation plot: . The &quot;line of 1s&quot; and the repeated values are removed so that it&#39;s not visually overwhelming. | Values very close to 0, where there is little to no relationship, aren&#39;t labeled. | Values close to 1 are dark blue and values close to -1 are light blue — the depth of color represents the strength of the relationship. | . Analyzing Correlation in Missing Values . When a vehicle is in an accident, there is likely to be a cause, and vice-versa. . Let&#39;s explore the variations in missing values from these five pairs of columns. We&#39;ll create a dataframe that counts, for each pair: . The number of values where the vehicle is missing when the cause is not missing. | The number of values where the cause is missing when the vehicle is not missing. | . col_labels = [&#39;v_number&#39;, &#39;vehicle_missing&#39;, &#39;cause_missing&#39;] vc_null_data = [] for v in range(1,6): v_col = &#39;vehicle_{}&#39;.format(v) c_col = &#39;cause_vehicle_{}&#39;.format(v) v_null = mvc[mvc[v_col].isnull() &amp; mvc[c_col].notnull()].shape[0] c_null = mvc[mvc[v_col].notnull() &amp; mvc[c_col].isnull()].shape[0] vc_null_data.append([v, v_null, c_null]) vc_null_df = pd.DataFrame(vc_null_data, columns=col_labels) vc_null_df . v_number vehicle_missing cause_missing . 0 1 | 204 | 24 | . 1 2 | 3793 | 223 | . 2 3 | 242 | 24 | . 3 4 | 50 | 3 | . 4 5 | 10 | 0 | . Finding The Most Common Value Across multiple Columns . The analysis we indicates that there are roughly 4,500 missing values across the 10 columns. The easiest option for handling these would be to drop the rows with missing values. This would mean losing almost 10% of the total data, which is something we ideally want to avoid. . A better option is to impute the data, like we did earlier. Because the data in these columns is text data, we can&#39;t perform a numeric calculation to impute missing data. . One common option when imputing is to use the most common value to fill in data. Let&#39;s look at the common values across these columns and see if we can use that to make a decision. . Let&#39;s count the most common values for the cause set of columns. We&#39;ll start by selecting only the columns containing the substring cause. . cause = mvc[[c for c in mvc.columns if &quot;cause_&quot; in c]] cause.head() . cause_vehicle_1 cause_vehicle_2 cause_vehicle_3 cause_vehicle_4 cause_vehicle_5 . 0 Following Too Closely | Unspecified | NaN | NaN | NaN | . 1 Backing Unsafely | Unspecified | NaN | NaN | NaN | . 2 Following Too Closely | Unspecified | NaN | NaN | NaN | . 3 Glare | Passing Too Closely | NaN | NaN | NaN | . 4 Turning Improperly | Unspecified | NaN | NaN | NaN | . Next, we&#39;ll stack the values into a single series object: . cause_1d = cause.stack() cause_1d.head() . 0 cause_vehicle_1 Following Too Closely cause_vehicle_2 Unspecified 1 cause_vehicle_1 Backing Unsafely cause_vehicle_2 Unspecified 2 cause_vehicle_1 Following Too Closely dtype: object . You may notice that the stacked version omits null values - this is fine, as we&#39;re just interested in the most common non-null values. . Finally, we count the values in the series: . cause_counts = cause_1d.value_counts() top10_causes = cause_counts.head(10) top10_causes . Unspecified 57481 Driver Inattention/Distraction 17650 Following Too Closely 6567 Failure to Yield Right-of-Way 4566 Passing or Lane Usage Improper 3260 Passing Too Closely 3045 Backing Unsafely 3001 Other Vehicular 2523 Unsafe Lane Changing 2372 Turning Improperly 1590 dtype: int64 . The most common non-null value for the cause columns is Unspecified, which presumably indicates that the officer reporting the collision was unable to determine the cause for that vehicle. . Let&#39;s identify the most common non-null value for the vehicle columns. . v_cols = [c for c in mvc.columns if c.startswith(&quot;vehicle&quot;)] top10_vehicles = mvc[v_cols].stack().value_counts().head(10) top10_vehicles . Sedan 33133 Station Wagon/Sport Utility Vehicle 26124 PASSENGER VEHICLE 16026 SPORT UTILITY / STATION WAGON 12356 Taxi 3482 Pick-up Truck 2373 TAXI 1892 Box Truck 1659 Bike 1190 Bus 1162 dtype: int64 . Filling Unknow values with a placeholder . The top &quot;cause&quot; is an &quot;Unspecified&quot; placeholder. This is useful instead of a null value as it makes the distinction between a value that is missing because there were only a certain number of vehicles in the collision versus one that is because the contributing cause for a particular vehicle is unknown. . The vehicles columns don&#39;t have an equivalent, but we can still use the same technique. Here&#39;s the logic we&#39;ll need to do for each pair of vehicle/cause columns: . For values where the vehicle is null and the cause is non-null, set the vehicle to Unspecified. | For values where the cause is null and the vehicle is not-null, set the cause to Unspecified. | . def summarize_missing(): v_missing_data = [] for v in range(1,6): v_col = &#39;vehicle_{}&#39;.format(v) c_col = &#39;cause_vehicle_{}&#39;.format(v) v_missing = (mvc[v_col].isnull() &amp; mvc[c_col].notnull()).sum() c_missing = (mvc[c_col].isnull() &amp; mvc[v_col].notnull()).sum() v_missing_data.append([v, v_missing, c_missing]) col_labels = columns=[&quot;vehicle_number&quot;, &quot;vehicle_missing&quot;, &quot;cause_missing&quot;] return pd.DataFrame(v_missing_data, columns=col_labels) summarize_missing() . vehicle_number vehicle_missing cause_missing . 0 1 | 204 | 24 | . 1 2 | 3793 | 223 | . 2 3 | 242 | 24 | . 3 4 | 50 | 3 | . 4 5 | 10 | 0 | . for v in range(1,6): v_col = &#39;vehicle_{}&#39;.format(v) c_col = &#39;cause_vehicle_{}&#39;.format(v) mvc[v_col] = mvc[v_col].mask( mvc[v_col].isnull() &amp; mvc[c_col].notnull(), &#39;Unspecified&#39;) mvc[c_col] = mvc[c_col].mask(mvc[v_col].notnull() &amp; mvc[c_col].isnull(), &quot;Unspecified&quot;) summarize_missing() . vehicle_number vehicle_missing cause_missing . 0 1 | 0 | 0 | . 1 2 | 0 | 0 | . 2 3 | 0 | 0 | . 3 4 | 0 | 0 | . 4 5 | 0 | 0 | . Missing data in the &quot;location&quot; column . Let&#39;s view the work we&#39;ve done across the past few screens by looking at the null correlation plot for the last 10 columns: . veh_cols = [c for c in mvc.columns if &#39;vehicle&#39; in c] plot_null_correlations(mvc[veh_cols]) . You can see the perfect correlation between each pair of vehicle/cause columns represented by 1.0 in each square, which means that there is a perfect relationship between the five pairs of vehicle/cause columns . Let&#39;s now turn our focus to the final set of columns that contain missing values — the columns that relate to the location of the accident. We&#39;ll start by looking at the first few rows to refamiliarize ourselves with the data: . loc_cols = [&#39;borough&#39;, &#39;location&#39;, &#39;on_street&#39;, &#39;off_street&#39;, &#39;cross_street&#39;] location_data = mvc[loc_cols] location_data.head() . borough location on_street off_street cross_street . 0 MANHATTAN | (40.742832, -74.00771) | WEST 15 STREET | NaN | 10 AVENUE | . 1 BROOKLYN | (40.623714, -73.99314) | 16 AVENUE | NaN | 62 STREET | . 2 NaN | (40.591755, -73.9083) | BELT PARKWAY | NaN | NaN | . 3 QUEENS | (40.73602, -73.87954) | GRAND AVENUE | NaN | VANLOON STREET | . 4 BRONX | (40.884727, -73.89945) | NaN | 208 WEST 238 STREET | NaN | . Next, let&#39;s look at counts of the null values in each column: . location_data.isnull().sum() . borough 20646 location 3885 on_street 13961 off_street 44093 cross_street 29249 dtype: int64 . These columns have a lot of missing values! Keep in mind that all of these five columns represent the same thing — the location of the collision. We can potentially use the non-null values to impute some of the null values. . To see where we might be able to do this, let&#39;s look for correlations between the missing values: . plot_null_correlations(location_data) . None of these columns have strong correlations except for off_street and on_street which have a near perfect negative correlation. That means for almost every row that has a null value in one column, the other has a non-null value and vice-versa. . The final way we&#39;ll look at the null values in these columns is to plot a null matrix, but we&#39;ll sort the data first. This will gather some of the null and non-null values together and make patterns more obvious: . sorted_location_data = location_data.sort_values(loc_cols) plot_null_matrix(sorted_location_data) . Let&#39;s make some observations about the missing values across these columns: . About two-thirds of rows have non-null values for borough, but of those values that are missing, most have non-null values for location and one or more of the street name columns. | Less than one-tenth of rows have missing values in the location column, but most of these have non-null values in one or more of the street name columns. | Most rows have a non-null value for either on_street or off_street, and some also have a value for cross_street. | . Combined, this means that we will be able to impute a lot of the missing values by using the other columns in each row. To do this, we can use geolocation APIs that take either an address or location coordinates, and return information about that location. . Imputing Location Data . sup_data = pd.read_csv(&#39;../datasets/supplemental_data.csv&#39;) sup_data.head() . unique_key location on_street off_street borough . 0 3869058 | NaN | NaN | NaN | NaN | . 1 3847947 | NaN | NaN | NaN | NaN | . 2 3914294 | NaN | BELT PARKWAY | NaN | BROOKLYN | . 3 3915069 | NaN | NaN | NaN | NaN | . 4 3923123 | NaN | NaN | NaN | NaN | . The supplemental data has five columns from our original data set — the unique_key that identifies each collision, and four of the five location columns. The cross_street column is not included because the geocoding APIs we used don&#39;t include data on the nearest cross street to any single location. . Let&#39;s take a look at a null matrix for the supplemental data: . plot_null_matrix(sup_data) . Apart from the unique_key column, you&#39;ll notice that there are a lot more missing values than our main data set. This makes sense, as we didn&#39;t prepare supplemental data where the original data set had non-null values. . mvc.unique_key.equals(sup_data.unique_key) . True . both the original and supplemental data has the same values in the same order, we&#39;ll be able to use Series.mask() to add our supplemental data to our original data. . location_cols = [&#39;location&#39;, &#39;on_street&#39;, &#39;off_street&#39;, &#39;borough&#39;] mvc[location_cols].isnull().sum() . location 3885 on_street 13961 off_street 44093 borough 20646 dtype: int64 . for c in location_cols: mvc[c] = mvc[c].mask(mvc[c].isnull(), sup_data[c]) mvc[location_cols].isnull().sum() . location 77 on_street 13734 off_street 36131 borough 232 dtype: int64 . we&#39;ve imputed thousands of values to reduce the number of missing values across our data set. Let&#39;s look at a summary of the null values . mvc.isnull().sum() . unique_key 0 date 0 time 0 borough 232 location 77 on_street 13734 cross_street 29249 off_street 36131 pedestrians_injured 0 cyclist_injured 0 motorist_injured 0 total_injured 21 pedestrians_killed 0 cyclist_killed 0 motorist_killed 0 total_killed 1 vehicle_1 151 vehicle_2 8469 vehicle_3 54110 vehicle_4 57108 vehicle_5 57671 cause_vehicle_1 151 cause_vehicle_2 8469 cause_vehicle_3 54110 cause_vehicle_4 57108 cause_vehicle_5 57671 dtype: int64 . If we&#39;d like to continue working with this data, we can: . Drop the rows that had suspect values for injured and killed totals. | Clean the values in the vehicle_1 through vehicle_5 columns by analyzing the different values and merging duplicates and near-duplicates. | Analyze whether collisions are more likely in certain locations, at certain times, or for certain vehicle types. | .",
            "url": "https://victoromondi1997.github.io/blog/missing-data/data-analysis/2020/08/25/Working-with-Missing-Data.html",
            "relUrl": "/missing-data/data-analysis/2020/08/25/Working-with-Missing-Data.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Writing Functions in Python",
            "content": "Best Practices . Best practices when writing functions. We&#39;ll cover docstrings and why they matter and how to know when you need to turn a chunk of code into a function. We will also code how Python passes arguments to functions, as well as some common gotchas that can cause debugging headaches when calling functions. . Docstrings . Crafting a docstring . The first function is count_letter(). It takes a string and a single letter and returns the number of times the letter appears in the string. We want the users of our open-source package to be able to understand how this function works easily, so we will need to give it a docstring. . import inspect import pandas as pd import numpy as np import time import contextlib import os . def count_letter(content, letter): &quot;&quot;&quot;Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. &quot;&quot;&quot; if (not isinstance(letter, str)) or len(letter) != 1: raise ValueError(&#39;`letter` must be a single character string.&#39;) return len([char for char in content if char == letter]) . help(count_letter) . Help on function count_letter in module __main__: count_letter(content, letter) Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. . Retrieving docstrings . My friends and I are working on building an amazing new Python IDE (integrated development environment -- like PyCharm, Spyder, Eclipse, Visual Studio, etc.). Our team wants to add a feature that displays a tooltip with a function&#39;s docstring whenever the user starts typing the function name. That way, the user doesn&#39;t have to go elsewhere to look up the documentation for the function they are trying to use. We&#39;ve been asked to complete the build_tooltip() function that retrieves a docstring from an arbitrary function. . # Get the docstring with an attribute of count_letter() docstring = count_letter.__doc__ border = &#39;#&#39; * 28 print(&#39;{} n{} n{}&#39;.format(border, docstring, border)) . ############################ Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. ############################ . # Get the docstring with a function from the inspect module docstring = inspect.getdoc(count_letter) border = &#39;#&#39; * 28 print(&#39;{} n{} n{}&#39;.format(border, docstring, border)) . ############################ Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. ############################ . def build_tooltip(function): &quot;&quot;&quot;Create a tooltip for any function that shows the function&#39;s docstring. Args: function (callable): The function we want a tooltip for. Returns: str &quot;&quot;&quot; # Use &#39;inspect&#39; to get the docstring docstring = inspect.getdoc(function) border = &#39;#&#39; * 28 return &#39;{} n{} n{}&#39;.format(border, docstring, border) print(build_tooltip(count_letter)) print(build_tooltip(range)) print(build_tooltip(print)) . ############################ Count the number of times `letter` appears in `content`. Args: content (str): The string to search. letter (str): The letter to search for. Returns: int Raises: ValueError: If `letter` is not a one-character string. ############################ ############################ range(stop) -&gt; range object range(start, stop[, step]) -&gt; range object Return an object that produces a sequence of integers from start (inclusive) to stop (exclusive) by step. range(i, j) produces i, i+1, i+2, ..., j-1. start defaults to 0, and stop is omitted! range(4) produces 0, 1, 2, 3. These are exactly the valid indices for a list of 4 elements. When step is given, it specifies the increment (or decrement). ############################ ############################ print(value, ..., sep=&#39; &#39;, end=&#39; n&#39;, file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. ############################ . DRY and &quot;Do one thing&quot; . While we were developing a model to predict the likelihood of a student graduating from college, we wrote this bit of code to get the z-scores of students&#39; yearly GPAs. Now we&#39;re ready to turn it into a production-quality system, so we need to do something about the repetition. Writing a function to calculate the z-scores would improve this code. . # Standardize the GPAs for each year df[&#39;y1_z&#39;] = (df.y1_gpa - df.y1_gpa.mean()) / df.y1_gpa.std() df[&#39;y2_z&#39;] = (df.y2_gpa - df.y2_gpa.mean()) / df.y2_gpa.std() df[&#39;y3_z&#39;] = (df.y3_gpa - df.y3_gpa.mean()) / df.y3_gpa.std() df[&#39;y4_z&#39;] = (df.y4_gpa - df.y4_gpa.mean()) / df.y4_gpa.std() . Note: df is a pandas DataFrame where each row is a student with 4 columns of yearly student GPAs: y1_gpa, y2_gpa, y3_gpa, y4_gpa . df = pd.read_csv(&#39;students.csv&#39;, index_col=0) df.head() . y1_gpa y2_gpa y3_gpa y4_gpa . 0 2.785877 | 2.052513 | 2.170544 | 0.065570 | . 1 1.144557 | 2.666498 | 0.267098 | 2.884737 | . 2 0.907406 | 0.423634 | 2.613459 | 0.030950 | . 3 2.205259 | 0.523580 | 3.984345 | 0.339289 | . 4 2.877876 | 1.287922 | 3.077589 | 0.901994 | . def standardize(column): &quot;&quot;&quot;Standardize the values in a column. Args: column (pandas Series): The data to standardize. Returns: pandas Series: the values as z-scores &quot;&quot;&quot; # Finish the function so that it returns the z-scores z_score = (df[column] - df[column].mean()) / df[column].std() return z_score # Use the standardize() function to calculate the z-scores df[&#39;y1_z&#39;] = standardize(&quot;y1_gpa&quot;) df[&#39;y2_z&#39;] = standardize(&quot;y2_gpa&quot;) df[&#39;y3_z&#39;] = standardize(&quot;y3_gpa&quot;) df[&#39;y4_z&#39;] = standardize(&quot;y4_gpa&quot;) df.head() . y1_gpa y2_gpa y3_gpa y4_gpa y1_z y2_z y3_z y4_z . 0 2.785877 | 2.052513 | 2.170544 | 0.065570 | 0.790863 | 0.028021 | 0.172322 | -1.711179 | . 1 1.144557 | 2.666498 | 0.267098 | 2.884737 | -0.872971 | 0.564636 | -1.347122 | 0.824431 | . 2 0.907406 | 0.423634 | 2.613459 | 0.030950 | -1.113376 | -1.395595 | 0.525883 | -1.742317 | . 3 2.205259 | 0.523580 | 3.984345 | 0.339289 | 0.202281 | -1.308243 | 1.620206 | -1.464991 | . 4 2.877876 | 1.287922 | 3.077589 | 0.901994 | 0.884124 | -0.640219 | 0.896379 | -0.958885 | . standardize() will probably be useful in other places in our code, and now it is easy to use, test, and update if we need to. It&#39;s also easier to tell what the code is doing because of the docstring and the name of the function. . Split up a function . Another engineer on our team has written this function to calculate the mean and median of a list. We want to show them how to split it into two simpler functions: mean() and median() . def mean_and_median(values): &quot;&quot;&quot;Get the mean and median of a list of `values` Args: values (iterable of float): A list of numbers Returns: tuple (float, float): The mean and median &quot;&quot;&quot; mean = sum(values) / len(values) midpoint = int(len(values) / 2) if len(values) % 2 == 0: median = (values[midpoint - 1] + values[midpoint]) / 2 else: median = values[midpoint] return mean, median . def mean(values): &quot;&quot;&quot;Get the mean of a list of values Args: values (iterable of float): A list of numbers Returns: float &quot;&quot;&quot; # Write the mean() function mean = sum(values) / len(values) return mean . def median(values): &quot;&quot;&quot;Get the median of a list of values Args: values (iterable of float): A list of numbers Returns: float &quot;&quot;&quot; # Write the median() function midpoint = int(len(values) /2) if len(values) % 2 == 0: median = (values[midpoint-1] + values[midpoint]) / 2 else: median = values[midpoint] return median . Each function does one thing and does it well. Using, testing, and maintaining these will be a breeze (although we&#39;ll probably just use numpy.mean() and numpy.median() for this in real life). . Pass by assignment . Best practice for default arguments . One of our co-workers has written this function for adding a column to a panda&#39;s DataFrame. Unfortunately, they used a mutable variable as a default argument value! . def add_column(values, df=pandas.DataFrame()): &quot;&quot;&quot;Add a column of `values` to a DataFrame `df`. The column will be named &quot;col_&lt;n&gt;&quot; where &quot;n&quot; is the numerical index of the column. Args: values (iterable): The values of the new column df (DataFrame, optional): The DataFrame to update. If no DataFrame is passed, one is created by default. Returns: DataFrame &quot;&quot;&quot; df[&#39;col_{}&#39;.format(len(df.columns))] = values return df . # Use an immutable variable for the default argument def better_add_column(values, df=None): &quot;&quot;&quot;Add a column of `values` to a DataFrame `df`. The column will be named &quot;col_&lt;n&gt;&quot; where &quot;n&quot; is the numerical index of the column. Args: values (iterable): The values of the new column df (DataFrame, optional): The DataFrame to update. If no DataFrame is passed, one is created by default. Returns: DataFrame &quot;&quot;&quot; # Update the function to create a default DataFrame if df is None: df = pandas.DataFrame() df[&#39;col_{}&#39;.format(len(df.columns))] = values return df . When you need to set a mutable variable as a default argument, always use None and then set the value in the body of the function. This prevents unexpected behavior like adding multiple columns if you call the function more than once. . Context Managers . Using context managers . We are working on a natural language processing project to determine what makes great writers so great. Our current hypothesis is that great writers talk about cats a lot. To prove it, we will count the number of times the word &quot;cat&quot; appears in &quot;Alice&#39;s Adventures in Wonderland&quot; by Lewis Carroll. We have already downloaded a text file, alice.txt, with the entire contents of this great book. . # Open &quot;alice.txt&quot; and assign the file to &quot;file&quot; with open(&#39;alice.txt&#39;, encoding=&quot;utf8&quot;) as file: text = file.read() n = 0 for word in text.split(): if word.lower() in [&#39;cat&#39;, &#39;cats&#39;]: n += 1 print(&#39;Lewis Carroll uses the word &quot;cat&quot; {} times&#39;.format(n)) . Lewis Carroll uses the word &#34;cat&#34; 24 times . The speed of cats . We&#39;re working on a new web service that processes Instagram feeds to identify which pictures contain cats (don&#39;t ask why -- it&#39;s the internet). The code that processes the data is slower than we would like it to be, so we are working on tuning it up to run faster. Given an image, image, we have two functions that can process it: . process_with_numpy(image) | process_with_pytorch(image) | . Our colleague wrote a context manager, timer(), that will print out how long the code inside the context block takes to run. She is suggesting we use it to see which of the two options is faster. Time each function to determine which one to use in your web service. . def get_image_from_instagram(): return np.random.rand(84, 84) . Writing context managers . The timer() context manager . A colleague of ours is working on a web service that processes Instagram photos. Customers are complaining that the service takes too long to identify whether or not an image has a cat in it, so our colleague has come to us for help. We decided to write a context manager that they can use to time how long their functions take to run. . @contextlib.contextmanager def timer(): &quot;&quot;&quot;Time how long code in the context block takes to run.&quot;&quot;&quot; t0 = time.time() try: yield except: raise finally: t1 = time.time() print(&#39;Elapsed: {:.2f} seconds&#39;.format(t1 - t0)) . our colleague can now use our timer() context manager to figure out which of their functions is running too slow. The three elements of a context manager are all here: . a function definition, | a yield statement, and the | @contextlib.contextmanager decorator. timer() is a context manager that does not return an explicit value, so yield is written by itself without specifying anything to return. | . def process_with_numpy(p): _process_pic(0.1521) . def _process_pic(n_sec): print(&#39;Processing&#39;, end=&#39;&#39;, flush=True) for i in range(10): print(&#39;.&#39;, end=&#39;&#39; if i &lt; 9 else &#39;done! n&#39;, flush=True) time.sleep(n_sec) . def process_with_pytorch(p): _process_pic(0.0328) . image = get_image_from_instagram() # Time how long process_with_numpy(image) takes to run with timer(): print(&#39;Numpy version&#39;) process_with_numpy(image) # Time how long process_with_pytorch(image) takes to run with timer(): print(&#39;Pytorch version&#39;) process_with_pytorch(image) . Numpy version Processing..........done! Elapsed: 1.66 seconds Pytorch version Processing..........done! Elapsed: 0.49 seconds . Now that we know the pytorch version is faster, we can use it in our web service to ensure our users get the rapid response time they expect. . There is no as &lt;variable name&gt; at the end of the with statement in timer() context manager. That is because timer() is a context manager that does not return a value, so the as &lt;variable name&gt; at the end of the with statement isn&#39;t necessary. . A read-only open() context manager . We have a bunch of data files for our next deep learning project that took us months to collect and clean. It would be terrible if we accidentally overwrote one of those files when trying to read it in for training, so we decided to create a read-only version of the open() context manager to use in our project. . The regular open() context manager: takes a filename and a mode (&#39;r&#39; for read, &#39;w&#39; for write, or &#39;a&#39; for append) opens the file for reading, writing, or appending . sends control back to the context, along with a reference to the file | waits for the context to finish | and then closes the file before exiting | . Our context manager will do the same thing, except it will only take the filename as an argument and it will only open the file for reading. . @contextlib.contextmanager def open_read_only(filename): &quot;&quot;&quot;Open a file in read-only mode. Args: filename (str): The location of the file to read Yields: file object &quot;&quot;&quot; read_only_file = open(filename, mode=&#39;r&#39;) # Yield read_only_file so it can be assigned to my_file yield read_only_file # Close read_only_file read_only_file.close() with open_read_only(&#39;my_file.txt&#39;) as my_file: print(my_file.read()) . Congratulations! You wrote a context manager that acts like &#34;open()&#34; but operates in read-only mode! . Scraping the NASDAQ . Training deep neural nets is expensive! We might as well invest in NVIDIA stock since we&#39;re spending so much on GPUs. To pick the best time to invest, we are going to collect and analyze some data on how their stock is doing. The context manager stock(&#39;NVDA&#39;) will connect to the NASDAQ and return an object that you can use to get the latest price by calling its .price() method. . You want to connect to stock(&#39;NVDA&#39;) and record 10 timesteps of price data by writing it to the file NVDA.txt . class MockStock: def __init__(self, loc, scale): self.loc = loc self.scale = scale self.recent = list(np.random.laplace(loc, scale, 2)) def price(self): sign = np.sign(self.recent[1] - self.recent[0]) # 70% chance of going same direction sign = 1 if sign == 0 else (sign if np.random.rand() &gt; 0.3 else -1 * sign) new = self.recent[1] + sign * np.random.rand() / 10.0 self.recent = [self.recent[1], new] return new . @contextlib.contextmanager def stock(symbol): base = 140.00 scale = 1.0 mock = MockStock(base, scale) print(&#39;Opening stock ticker for {}&#39;.format(symbol)) yield mock print(&#39;Closing stock ticker&#39;) . # Use the &quot;stock(&#39;NVDA&#39;)&quot; context manager # and assign the result to the variable &quot;nvda&quot; with stock(&#39;NVDA&#39;) as nvda: # Open &quot;NVDA.txt&quot; for writing as f_out with open(&#39;NVDA.txt&#39;, &#39;w&#39;) as f_out: for _ in range(10): value = nvda.price() print(&#39;Logging ${:.2f} for NVDA&#39;.format(value)) f_out.write(&#39;{:.2f} n&#39;.format(value)) . Opening stock ticker for NVDA Logging $140.25 for NVDA Logging $140.17 for NVDA Logging $140.12 for NVDA Logging $140.04 for NVDA Logging $140.01 for NVDA Logging $139.93 for NVDA Logging $139.86 for NVDA Logging $139.80 for NVDA Logging $139.89 for NVDA Logging $139.89 for NVDA Closing stock ticker . Now we can monitor the NVIDIA stock price and decide when is the exact right time to buy. Nesting context managers like this allows us to connect to the stock market (the CONNECT/DISCONNECT pattern) and write to a file (the OPEN/CLOSE pattern) at the same time. . Changing the working directory . We are using an open-source library that lets us train deep neural networks on our data. Unfortunately, during training, this library writes out checkpoint models (i.e., models that have been trained on a portion of the data) to the current working directory. We found that behavior frustrating because we don&#39;t want to have to launch the script from the directory where the models will be saved. . We decided that one way to fix this is to write a context manager that changes the current working directory, lets us build our models, and then resets the working directory to its original location. We&#39;ll want to be sure that any errors that occur during model training don&#39;t prevent you from resetting the working directory to its original location. . def in_dir(directory): &quot;&quot;&quot;Change current working directory to `directory`, allow the user to run some code, and change back. Args: directory (str): The path to a directory to work in. &quot;&quot;&quot; current_dir = os.getcwd() os.chdir(directory) # Add code that lets you handle errors try: yield # Ensure the directory is reset, # whether there was an error or not finally: os.chdir(current_dir) . Now, even if someone writes buggy code when using our context manager, we will be sure to change the current working directory back to what it was when they called in_dir(). This is important to do because our users might be relying on their working directory being what it was when they started the script. in_dir() is a great example of the CHANGE/RESET pattern that indicates you should use a context manager. . Decorators . Decorators are an extremely powerful concept in Python. They allow us to modify the behavior of a function without changing the code of the function itself. . Functions are objects . Building a command line data app . We are building a command line tool that lets a user interactively explore a data set. We&#39;ve defined four functions: mean(), std(), minimum(), and maximum() that users can call to analyze their data. users can call any of these functions by typing the function name at the input prompt. . Note:The function get_user_input() in this exercise is a mock version of asking the user to enter a command. It randomly returns one of the four function names. In real life, we would ask for input and wait until the user entered a value. . import random def get_user_input(prompt=&#39;Type a command: &#39;): command = random.choice([&#39;mean&#39;, &#39;std&#39;, &#39;minimum&#39;, &#39;maximum&#39;]) print(prompt) print(&#39;&gt; {}&#39;.format(command)) return command . def mean(data): print(data.mean()) . def std(data): print(data.std()) . def minimum(data): print(data.min()) . def maximum(data): print(data.max()) . def load_data(): df = pd.DataFrame() df[&#39;height&#39;] = [72.1, 69.8, 63.2, 64.7] df[&#39;weight&#39;] = [198, 204, 164, 238] return df . # Add the missing function references to the function map function_map = { &#39;mean&#39;: mean, &#39;std&#39;: std, &#39;minimum&#39;: minimum, &#39;maximum&#39;: maximum } data = load_data() print(data) func_name = get_user_input() # Call the chosen function and pass &quot;data&quot; as an argument function_map[func_name](data) . height weight 0 72.1 198 1 69.8 204 2 63.2 164 3 64.7 238 Type a command: &gt; std height 4.194043 weight 30.309514 dtype: float64 . Reviewing our co-worker&#39;s code . Our co-worker is asking us to review some code that they&#39;ve written and give them some tips on how to get it ready for production. We know that having a docstring is considered best practice for maintainable, reusable functions, so as a sanity check we decided to run this has_docstring() function on all of their functions. . def has_docstring(func): &quot;&quot;&quot;Check to see if the function `func` has a docstring. Args: func (callable): A function. Returns: bool &quot;&quot;&quot; return func.__doc__ is not None . def load_and_plot_data(filename): &quot;&quot;&quot;Load a data frame and plot each column. Args: filename (str): Path to a CSV file of data. Returns: pandas.DataFrame &quot;&quot;&quot; df = pd.load_csv(filename, index_col=0) df.hist() return df . # Call has_docstring() on the load_and_plot_data() function ok = has_docstring(load_and_plot_data) if not ok: print(&quot;load_and_plot_data() doesn&#39;t have a docstring!&quot;) else: print(&quot;load_and_plot_data() looks ok&quot;) . load_and_plot_data() looks ok . def as_2D(arr): &quot;&quot;&quot;Reshape an array to 2 dimensions&quot;&quot;&quot; return np.array(arr).reshape(1, -1) . # Call has_docstring() on the as_2D() function ok = has_docstring(as_2D) if not ok: print(&quot;as_2D() doesn&#39;t have a docstring!&quot;) else: print(&quot;as_2D() looks ok&quot;) . as_2D() looks ok . def log_product(arr): return np.exp(np.sum(np.log(arr))) . # Call has_docstring() on the log_product() function ok = has_docstring(log_product) if not ok: print(&quot;log_product() doesn&#39;t have a docstring!&quot;) else: print(&quot;log_product() looks ok&quot;) . log_product() doesn&#39;t have a docstring! . Returning functions for a math game . We are building an educational math game where the player enters a math term, and our program returns a function that matches that term. For instance, if the user types &quot;add&quot;, our program returns a function that adds two numbers. So far we&#39;ve only implemented the &quot;add&quot; function. Now we want to include a &quot;subtract&quot; function. . def create_math_function(func_name): if func_name == &#39;add&#39;: def add(a, b): return a + b return add elif func_name == &#39;subtract&#39;: # Define the subtract() function def subtract(a,b): return a-b return subtract else: print(&quot;I don&#39;t know that one&quot;) add = create_math_function(&#39;add&#39;) print(&#39;5 + 2 = {}&#39;.format(add(5, 2))) subtract = create_math_function(&#39;subtract&#39;) print(&#39;5 - 2 = {}&#39;.format(subtract(5, 2))) . 5 + 2 = 7 5 - 2 = 3 . Scope . Modifying variables outside local scope . call_count = 0 def my_function(): # Use a keyword that lets us update call_count global call_count call_count += 1 print(&quot;You&#39;ve called my_function() {} times!&quot;.format( call_count )) for _ in range(20): my_function() . You&#39;ve called my_function() 1 times! You&#39;ve called my_function() 2 times! You&#39;ve called my_function() 3 times! You&#39;ve called my_function() 4 times! You&#39;ve called my_function() 5 times! You&#39;ve called my_function() 6 times! You&#39;ve called my_function() 7 times! You&#39;ve called my_function() 8 times! You&#39;ve called my_function() 9 times! You&#39;ve called my_function() 10 times! You&#39;ve called my_function() 11 times! You&#39;ve called my_function() 12 times! You&#39;ve called my_function() 13 times! You&#39;ve called my_function() 14 times! You&#39;ve called my_function() 15 times! You&#39;ve called my_function() 16 times! You&#39;ve called my_function() 17 times! You&#39;ve called my_function() 18 times! You&#39;ve called my_function() 19 times! You&#39;ve called my_function() 20 times! . def read_files(): file_contents = None def save_contents(filename): # Add a keyword that lets us modify file_contents nonlocal file_contents if file_contents is None: file_contents = [] with open(filename) as fin: file_contents.append(fin.read()) for filename in [&#39;1984.txt&#39;, &#39;MobyDick.txt&#39;, &#39;CatsEye.txt&#39;]: save_contents(filename) return file_contents print(&#39; n&#39;.join(read_files())) . It was a bright day in April, and the clocks were striking thirteen. Call me Ishmael. Time is not a line but a dimension, like the dimensions of space. . def wait_until_done(): def check_is_done(): # Add a keyword so that wait_until_done() # doesn&#39;t run forever global done if random.random() &lt; 0.1: done = True while not done: check_is_done() done = False wait_until_done() print(&#39;Work done? {}&#39;.format(done)) . Work done? True . Closures . Checking for closures . def return_a_func(arg1, arg2): def new_func(): print(&#39;arg1 was {}&#39;.format(arg1)) print(&#39;arg2 was {}&#39;.format(arg2)) return new_func my_func = return_a_func(2, 17) print(my_func.__closure__ is not None) print(len(my_func.__closure__) == 2) # Get the values of the variables in the closure closure_values = [ my_func.__closure__[i].cell_contents for i in range(2) ] print(closure_values == [2, 17]) . True True True . Closures keep your values safe . def my_special_function(): print(&#39;You are running my_special_function()&#39;) def get_new_func(func): def call_func(): func() return call_func new_func = get_new_func(my_special_function) # Redefine my_special_function() to just print &quot;hello&quot; def my_special_function(): print(&quot;hello&quot;) new_func() . You are running my_special_function() . def my_special_function(): print(&#39;You are running my_special_function()&#39;) def get_new_func(func): def call_func(): func() return call_func new_func = get_new_func(my_special_function) # Delete my_special_function() del(my_special_function) new_func() . You are running my_special_function() . def my_special_function(): print(&#39;You are running my_special_function()&#39;) def get_new_func(func): def call_func(): func() return call_func # Overwrite `my_special_function` with the new function my_special_function = get_new_func(my_special_function) my_special_function() . You are running my_special_function() . Decorators . Using decorator syntax . print_args prints out all of the arguments and their values any time a function that it is decorating gets called. . def print_args(func): sig = inspect.signature(func) def wrapper(*args, **kwargs): bound = sig.bind(*args, **kwargs).arguments str_args = &#39;, &#39;.join([&#39;{}={}&#39;.format(k, v) for k, v in bound.items()]) print(&#39;{} was called with {}&#39;.format(func.__name__, str_args)) return func(*args, **kwargs) return wrapper . def my_function(a, b, c): print(a + b + c) # Decorate my_function() with the print_args() decorator my_function = print_args(my_function) my_function(1, 2, 3) . my_function was called with a=1, b=2, c=3 6 . # Decorate my_function() with the print_args() decorator @print_args def my_function(a, b, c): print(a + b + c) my_function(1, 2, 3) . my_function was called with a=1, b=2, c=3 6 . even though decorators are functions themselves, when you use decorator syntax with the@ symbol we do not include the parentheses after the decorator name. . Defining a decorator . Our buddy has been working on a decorator that prints a &quot;before&quot; message before the decorated function is called and prints an &quot;after&quot; message after the decorated function is called. They are having trouble remembering how wrapping the decorated function is supposed to work. . def print_before_and_after(func): def wrapper(*args): print(&#39;Before {}&#39;.format(func.__name__)) # Call the function being decorated with *args func(*args) print(&#39;After {}&#39;.format(func.__name__)) # Return the nested function return wrapper @print_before_and_after def multiply(a, b): print(a * b) multiply(5, 10) . Before multiply 50 After multiply . Print the return type . def print_return_type(func): # Define wrapper(), the decorated function def wrapper(*args, **kwargs): # Call the function being decorated result = func(*args, **kwargs) print(&#39;{}() returned type {}&#39;.format( func.__name__, type(result) )) return result # Return the decorated function return wrapper @print_return_type def foo(value): return value print(foo(42)) print(foo([1, 2, 3])) print(foo({&#39;a&#39;: 42})) . foo() returned type &lt;class &#39;int&#39;&gt; 42 foo() returned type &lt;class &#39;list&#39;&gt; [1, 2, 3] foo() returned type &lt;class &#39;dict&#39;&gt; {&#39;a&#39;: 42} . Counter . how many times each of the functions in it gets called in a web app . def counter(func): def wrapper(*args, **kwargs): wrapper.count += 1 # Call the function being decorated and return the result return func(*args, **kwargs) wrapper.count = 0 # Return the new decorated function return wrapper # Decorate foo() with the counter() decorator @counter def foo(): print(&#39;calling foo()&#39;) foo() foo() print(&#39;foo() was called {} times.&#39;.format(foo.count)) . calling foo() calling foo() foo() was called 2 times. . Decorators and metadata . Preserving docstrings when decorating functions . def add_hello(func): def wrapper(*args, **kwargs): print(&#39;Hello&#39;) return func(*args, **kwargs) return wrapper # Decorate print_sum() with the add_hello() decorator @add_hello def print_sum(a, b): &quot;&quot;&quot;Adds two numbers and prints the sum&quot;&quot;&quot; print(a + b) print_sum(10, 20) print(print_sum.__doc__) . Hello 30 None . def add_hello(func): # Add a docstring to wrapper def wrapper(*args, **kwargs): &quot;&quot;&quot;Print &#39;hello&#39; and then call the decorated function.&quot;&quot;&quot; print(&#39;Hello&#39;) return func(*args, **kwargs) return wrapper @add_hello def print_sum(a, b): &quot;&quot;&quot;Adds two numbers and prints the sum&quot;&quot;&quot; print(a + b) print_sum(10, 20) print(print_sum.__doc__) . Hello 30 Print &#39;hello&#39; and then call the decorated function. . from functools import wraps def add_hello(func): # Decorate wrapper() so that it keeps func()&#39;s metadata @wraps(func) def wrapper(*args, **kwargs): &quot;&quot;&quot;Print &#39;hello&#39; and then call the decorated function.&quot;&quot;&quot; print(&#39;Hello&#39;) return func(*args, **kwargs) return wrapper @add_hello def print_sum(a, b): &quot;&quot;&quot;Adds two numbers and prints the sum&quot;&quot;&quot; print(a + b) print_sum(10, 20) print(print_sum.__doc__) . Hello 30 Adds two numbers and prints the sum . Measuring decorator overhead . def check_inputs(a, *args, **kwargs): for value in a: time.sleep(0.01) print(&#39;Finished checking inputs&#39;) . def check_outputs(a, *args, **kwargs): for value in a: time.sleep(0.01) print(&#39;Finished checking outputs&#39;) . def check_everything(func): @wraps(func) def wrapper(*args, **kwargs): check_inputs(*args, **kwargs) result = func(*args, **kwargs) check_outputs(result) return result return wrapper . @check_everything def duplicate(my_list): &quot;&quot;&quot;Return a new list that repeats the input twice&quot;&quot;&quot; return my_list + my_list t_start = time.time() duplicated_list = duplicate(list(range(50))) t_end = time.time() decorated_time = t_end - t_start t_start = time.time() # Call the original function instead of the decorated one duplicated_list = duplicate.__wrapped__(list(range(50))) t_end = time.time() undecorated_time = t_end - t_start print(&#39;Decorated time: {:.5f}s&#39;.format(decorated_time)) print(&#39;Undecorated time: {:.5f}s&#39;.format(undecorated_time)) . Finished checking inputs Finished checking outputs Decorated time: 2.40551s Undecorated time: 0.00000s . Decorators that take arguments . Run_n_times() . def run_n_times(n): &quot;&quot;&quot;Define and return a decorator&quot;&quot;&quot; def decorator(func): def wrapper(*args, **kwargs): for i in range(n): func(*args, **kwargs) return wrapper return decorator . # Make print_sum() run 10 times with the run_n_times() decorator @run_n_times(10) def print_sum(a, b): print(a + b) print_sum(15, 20) . 35 35 35 35 35 35 35 35 35 35 . # Use run_n_times() to create the run_five_times() decorator run_five_times = run_n_times(5) @run_five_times def print_sum(a, b): print(a + b) print_sum(4, 100) . 104 104 104 104 104 . # Modify the print() function to always run 20 times print = run_n_times(20)(print) print(&#39;What is happening?!?!&#39;) . What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! What is happening?!?! . print = run_n_times(1)(print) . HTML Generator . a script that generates HTML for a webpage on the fly. . def html(open_tag, close_tag): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): msg = func(*args, **kwargs) return &#39;{}{}{}&#39;.format(open_tag, msg, close_tag) # Return the decorated function return wrapper # Return the decorator return decorator . # Make hello() return bolded text @html(&quot;&lt;b&gt;&quot;, &quot;&lt;/b&gt;&quot;) def hello(name): return &#39;Hello {}!&#39;.format(name) print(hello(&#39;Alice&#39;)) . &lt;b&gt;Hello Alice!&lt;/b&gt; . %%html &lt;b&gt;Hello Alice!&lt;/b&gt; . Hello Alice! # Make goodbye() return italicized text @html(&quot;&lt;i&gt;&quot;, &quot;&lt;/i&gt;&quot;) def goodbye(name): return &#39;Goodbye {}.&#39;.format(name) print(goodbye(&#39;Alice&#39;)) . &lt;i&gt;Goodbye Alice.&lt;/i&gt; . %%html &lt;i&gt;Goodbye Alice.&lt;/i&gt; . Goodbye Alice. # Wrap the result of hello_goodbye() in &lt;div&gt; and &lt;/div&gt; @html(&quot;&lt;div&gt;&quot;, &quot;&lt;/div&quot;) def hello_goodbye(name): return &#39; n{} n{} n&#39;.format(hello(name), goodbye(name)) print(hello_goodbye(&#39;Alice&#39;)) . &lt;div&gt; &lt;b&gt;Hello Alice!&lt;/b&gt; &lt;i&gt;Goodbye Alice.&lt;/i&gt; &lt;/div . %%html &lt;div&gt; &lt;b&gt;Hello Alice!&lt;/b&gt; &lt;i&gt;Goodbye Alice.&lt;/i&gt; &lt;/div . Hello Alice! Goodbye Alice. &lt;/div Timeout(): a real world example . Tag your functions . Tagging something means that you have given that thing one or more strings that act as labels. For instance, we often tag emails or photos so that we can search for them later. You&#39;ve decided to write a decorator that will let you tag your functions with an arbitrary list of tags. You could use these tags for many things: . Adding information about who has worked on the function, so a user can look up who to ask if they run into trouble using it. | Labeling functions as &quot;experimental&quot; so that users know that the inputs and outputs might change in the future. | Marking any functions that you plan to remove in a future version of the code. | Etc. | . def tag(*tags): # Define a new decorator, named &quot;decorator&quot;, to return def decorator(func): # Ensure the decorated function keeps its metadata @wraps(func) def wrapper(*args, **kwargs): # Call the function being decorated and return the result return func(*args, **kwargs) wrapper.tags = tags return wrapper # Return the new decorator return decorator @tag(&#39;test&#39;, &#39;this is a tag&#39;) def foo(): pass print(foo.tags) . (&#39;test&#39;, &#39;this is a tag&#39;) . Check the return type . def returns(return_type): # Complete the returns() decorator def decorator(func): def wrapper(*args, **kwargs): result = func(*args, **kwargs) assert(type(result) == return_type) return result return wrapper return decorator @returns(dict) def foo(value): return value try: print(foo([1,2,3])) except AssertionError: print(&#39;foo() did not return a dict!&#39;) . foo() did not return a dict! .",
            "url": "https://victoromondi1997.github.io/blog/functions/python/2020/07/26/Writing-Functions-in-Python.html",
            "relUrl": "/functions/python/2020/07/26/Writing-Functions-in-Python.html",
            "date": " • Jul 26, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Working with Dates and Times in R",
            "content": "Overview . Dates and times are abundant in data and essential for answering questions that start with when, how long, or how often. However, they can be tricky, as they come in a variety of formats and can behave in unintuitive ways. This article teaches you the essentials of parsing, manipulating, and computing with dates and times in R. By the end, we&#39;ll have mastered the lubridate package, a member of the tidyverse, specifically designed to handle dates and times. we&#39;ll also have applied the new skills to explore how often R versions are released, when the weather is good in Auckland (the birthplace of R), and how long monarchs ruled in Britain. . Libraries . library(readr) library(anytime) library(ggplot2) library(dplyr) library(lubridate) library(ggridges) library(fasttime) library(tidyverse) library(microbenchmark) . Warning message: &#34;package &#39;microbenchmark&#39; was built under R version 3.6.3&#34; . install.packages(&quot;microbenchmark&quot;) . package &#39;microbenchmark&#39; successfully unpacked and MD5 sums checked The downloaded binary packages are in C: Users user AppData Local Temp RtmpUtbNCC downloaded_packages . Dates and Times in R . R doesn&#39;t know something is a date or time unless you tell it. We&#39;ll explore some of the ways R stores dates and times by exploring how often R versions are released, and how quickly people download them. . Introduction to dates . Dates . Different conventions in different places | 27th Feb 2013 NZ:27/2/2013 - USA: 2/27/2013 ### The global standard numeric date format 1 | . | . ISO 8601 YYYY-MM-DD . Values ordered from the largest to smallest unit of time | Each has a xed number of digits, must be padded with leadingzeros | Either, no separators for computers, or - in dates 1st of January 2011 -&gt; 2011-01-01 | . | . Dates in R . Packages that importd ates:readr,anytime | . 2003-02-27 . 1974 as.Date(&quot;2003-02-27&quot;) . 2003-02-27 &quot;2003-02-27&quot; . &#39;2003-02-27&#39; str(&quot;2003-02-27&quot;) . chr &#34;2003-02-27&#34; . str(as.Date(&quot;2003-02-27&quot;)) . Date[1:1], format: &#34;2003-02-27&#34; . Specifying dates . R doesn&#39;t know something is a date unless you tell it. If you have a character string that represents a date in the ISO 8601 standard you can turn it into a Date using the as.Date() function. Just pass the character string (or a vector of character strings) as the first argument. . We&#39;ll convert a character string representation of a date to a Date object. . # The date R 3.0.0 was released x &lt;- &quot;2013-04-03&quot; # Examine structure of x str(x) # Use as.Date() to interpret x as a date x_date &lt;- as.Date(x) # Examine structure of x_date str(x_date) # Store April 10 2014 as a Date april_10_2014 &lt;- as.Date(&quot;2014-04-10&quot;) april_10_2014 . chr &#34;2013-04-03&#34; Date[1:1], format: &#34;2013-04-03&#34; . 2014-04-10 Automatic import . Sometimes we&#39;ll need to input a couple of dates by hand using as.Date() but it&#39;s much more common to have a column of dates in a data file. . Some functions that read in data will automatically recognize and parse dates in a variety of formats. In particular the import functions, like read_csv(), in the readr package will recognize dates in a few common formats. . There is also the anytime() function in the anytime package whose sole goal is to automatically parse strings as dates regardless of the format. . # Use read_csv() to import rversions.csv releases &lt;- read_csv(&#39;datasets/rversions.csv&#39;) # Examine the structure of the date column str(releases$date) # Various ways of writing Sep 10 2009 sep_10_2009 &lt;- c(&quot;September 10 2009&quot;, &quot;2009-09-10&quot;, &quot;10 Sep 2009&quot;, &quot;09-10-2009&quot;) # Use anytime() to parse sep_10_2009 anytime(sep_10_2009) . Parsed with column specification: cols( major = col_double(), minor = col_double(), patch = col_double(), date = col_date(format = &#34;&#34;), datetime = col_datetime(format = &#34;&#34;), time = col_time(format = &#34;&#34;), type = col_character() ) . Date[1:105], format: &#34;1997-12-04&#34; &#34;1997-12-21&#34; &#34;1998-01-10&#34; &#34;1998-03-14&#34; &#34;1998-05-02&#34; ... . [1] &#34;2009-09-10 EAT&#34; &#34;2009-09-10 EAT&#34; &#34;2009-09-10 EAT&#34; &#34;2009-09-10 EAT&#34; . Why use dates? . Dates act like numbers . Date objects are stored as days since 1970-01-01 | . as.Date(&quot;2020-07-13&quot;) &gt; as.Date(&quot;2020-01-01&quot;) . TRUE as.Date(&quot;2020-07-10&quot;) + 3 . 2020-07-13 as.Date(&quot;2020-07-13&quot;) - as.Date(&quot;2019-07-13&quot;) . Time difference of 366 days . Plotting with dates . first_days = c(as.Date(&quot;2020-01-01&quot;), as.Date(&quot;2020-02-01&quot;), as.Date(&quot;2020-03-01&quot;), as.Date(&quot;2020-04-01&quot;), as.Date(&quot;2020-05-01&quot;), as.Date(&quot;2020-06-01&quot;), as.Date(&quot;2020-07-01&quot;), as.Date(&quot;2020-08-01&quot;)) plot(first_days, 1:8) . ggplot() + geom_point(aes(x=first_days, y=1:8)) . R releases . head(releases) . majorminorpatchdatedatetimetimetype . 0 | 60 | NA | 1997-12-04 | 1997-12-04 08:47:58 | 08:47:58 | patch | . 0 | 61 | NA | 1997-12-21 | 1997-12-21 13:09:22 | 13:09:22 | minor | . 0 | 61 | 1 | 1998-01-10 | 1998-01-10 00:31:55 | 00:31:55 | patch | . 0 | 61 | 2 | 1998-03-14 | 1998-03-14 19:25:55 | 19:25:55 | patch | . 0 | 61 | 3 | 1998-05-02 | 1998-05-02 07:58:17 | 07:58:17 | patch | . 0 | 62 | NA | 1998-06-14 | 1998-06-14 12:56:20 | 12:56:20 | minor | . Plotting . If you plot a Date on the axis of a plot, you expect the dates to be in calendar order, and that&#39;s exactly what happens with plot() or ggplot(). . We&#39;ll make some plots with the R version releases data using ggplot2. There are two big differences when a Date is on an axis: . If you specify limits they must be Date objects. | To control the behavior of the scale we&#39;ll use the scale_x_date() function. | # Set the x axis to the date column ggplot(releases, aes(x = date, y = type)) + geom_line(aes(group = 1, color = factor(major))) # Limit the axis to between 2010-01-01 and 2014-01-01 ggplot(releases, aes(x = date, y = type)) + geom_line(aes(group = 1, color = factor(major))) + xlim(as.Date(&quot;2010-01-01&quot;), as.Date(&quot;2014-01-01&quot;)) # Specify breaks every ten years and labels with &quot;%Y&quot; ggplot(releases, aes(x = date, y = type)) + geom_line(aes(group = 1, color = factor(major))) + scale_x_date(date_breaks = &quot;10 years&quot;, date_labels = &quot;%Y&quot;) . Warning message: &#34;Removed 87 row(s) containing missing values (geom_path).&#34; . Arithmetic and logical operators . Since Date objects are internally represented as the number of days since 1970-01-01 we can do basic math and comparisons with dates. We can compare dates with the usual logical operators (&lt;, ==, &gt; etc.), find extremes with min() and max(), and even subtract two dates to find out the time between them. . # Find the largest date last_release_date &lt;- max(releases$date) # Filter row for last release last_release &lt;- filter(releases, date==last_release_date) # Print last_release last_release # How long since last release? Sys.Date() - last_release_date . majorminorpatchdatedatetimetimetype . 3 | 4 | 1 | 2017-06-30 | 2017-06-30 07:04:11 | 07:04:11 | patch | . Time difference of 1110 days . `Sys.date()`- in the code, it simply returns today&#39;s date. . What about times? . ISO 8601 . HH:MM:SS - Largest unit to smallest Fixed digits Hours: 00 -- 24 | Minutes: 00 -- 59 | Seconds: 00 -- 60 (60 only for leap seconds) | . | No separator or : ### Datetimes in R | . | Two objects types:- POSIXlt - list with named components - POSIXct - seconds since 1970-01-01 00:00:00 | POSIXct will go in a data frame | as.POSIXct() turns a string into a POSIXct object | . str(as.POSIXct(&quot;1997-06-15 00:01:00&quot;)) . POSIXct[1:1], format: &#34;1997-06-15 00:01:00&#34; . Timezones . &quot;2013-02-27T18:00:00&quot;-6pm localtime- &quot;2013-02-27T18:00:00Z&quot;-6pm UTC | &quot;2013-02-27T18:00:00-08:00&quot;-6pmin Oregon | . as.POSIXct(&quot;1997-06-15T18:00:59Z&quot;) . [1] &#34;1997-06-15 EAT&#34; . as.POSIXct(&quot;1997-06-15T18:00:59Z&quot;, tz=&quot;UTC&quot;) . [1] &#34;1997-06-15 UTC&#34; . Datetimes behave nicely too . Once a POSIXct object,datetimes can be:- Compared - Subtracted Plotted | . | . Getting datetimes into R . Just like dates without times, if you want R to recognize a string as a datetime you need to convert it, although now you use `as.POSIXct()`. as.POSIXct() expects strings to be in the format YYYY-MM-DD HH:MM:SS. . The only tricky thing is that times will be interpreted in local time based on your machine&#39;s set up. You can check your timezone with Sys.timezone(). If you want the time to be interpreted in a different timezone, you just set the tz argument of as.POSIXct(). . Sys.timezone() . &#39;Africa/Nairobi&#39; # Use as.POSIXct to enter the datetime as.POSIXct(&quot;2010-10-01 12:12:00&quot;) # Use as.POSIXct again but set the timezone to `&quot;America/Los_Angeles&quot;` as.POSIXct(&quot;2010-10-01 12:12:00&quot;, tz = &quot;America/Los_Angeles&quot;) # Examine structure of datetime column str(releases$datetime) . [1] &#34;2010-10-01 12:12:00 EAT&#34; . [1] &#34;2010-10-01 12:12:00 PDT&#34; . POSIXct[1:105], format: &#34;1997-12-04 08:47:58&#34; &#34;1997-12-21 13:09:22&#34; &#34;1998-01-10 00:31:55&#34; ... . Datetimes behave nicely too . Just like Date objects, you can plot and do math with POSIXct objects. We&#39;ll see how quickly people download new versions of R, by examining the download logs from the RStudio CRAN mirror. R 3.2.0 was released at &quot;2015-04-16 07:13:33&quot; so cran-logs_2015-04-17.csv contains a random sample of downloads on the 16th, 17th and 18th. . # Import &quot;cran-logs_2015-04-17.csv&quot; with read_csv() logs &lt;- read_csv(&quot;datasets/cran-logs_2015-04-17.csv&quot;) # Print logs head(logs) . Parsed with column specification: cols( datetime = col_datetime(format = &#34;&#34;), r_version = col_character(), country = col_character() ) . datetimer_versioncountry . 2015-04-16 22:40:19 | 3.1.3 | CO | . 2015-04-16 09:11:04 | 3.1.3 | GB | . 2015-04-16 17:12:37 | 3.1.3 | DE | . 2015-04-18 12:34:43 | 3.2.0 | GB | . 2015-04-16 04:49:18 | 3.1.3 | PE | . 2015-04-16 06:40:44 | 3.1.3 | TW | . # Store the release time as a POSIXct object release_time &lt;- as.POSIXct(&quot;2015-04-16 07:13:33&quot;, tz = &quot;UTC&quot;) # When is the first download of 3.2.0? logs %&gt;% filter(datetime==release_time, r_version == &quot;3.2.0&quot;) # Examine histograms of downloads by version ggplot(logs, aes(x = datetime)) + geom_histogram() + geom_vline(aes(xintercept = as.numeric(release_time)))+ facet_wrap(~ r_version, ncol = 1) . datetimer_versioncountry . `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. . it takes about two days for downloads of the new version (3.2.0) to overtake downloads of the old version (3.1.3) . Why lubridate? . lubridate . Make working with dates and times in R easy! | tidyverse package Plays nicely with builtin datetime objects | Designed for humans not computers | . | Plays nicely with other tidyverse packages | Consistent behaviour regardless of underlying object | . Parsing a wide range of formats . ymd(&quot;1997-06-15&quot;) . 1997-06-15 dmy(&quot;15/06/97&quot;) . 1997-06-15 parse_date_time(c(&quot;June 15th, 1997&quot;, &quot;15th June, 1997&quot;), order=c(&quot;mdy&quot;, &quot;dmy&quot;)) . [1] &#34;1997-06-15 UTC&#34; &#34;1997-06-15 UTC&#34; . Manipulating datetimes . akl_daily = read_csv(&quot;datasets/akl_weather_daily.csv&quot;) head(akl_daily) . Parsed with column specification: cols( date = col_character(), max_temp = col_double(), min_temp = col_double(), mean_temp = col_double(), mean_rh = col_double(), events = col_character(), cloud_cover = col_double() ) . datemax_tempmin_tempmean_tempmean_rheventscloud_cover . 2007-9-1 | 60 | 51 | 56 | 75 | NA | 4 | . 2007-9-2 | 60 | 53 | 56 | 82 | Rain | 4 | . 2007-9-3 | 57 | 51 | 54 | 78 | NA | 6 | . 2007-9-4 | 64 | 50 | 57 | 80 | Rain | 6 | . 2007-9-5 | 53 | 48 | 50 | 90 | Rain | 7 | . 2007-9-6 | 57 | 42 | 50 | 69 | NA | 1 | . akl_daily_m &lt;- akl_daily %&gt;% mutate( year = year(date), yday = yday(date), month = month(date, label=TRUE) ) head(akl_daily_m) . datemax_tempmin_tempmean_tempmean_rheventscloud_coveryearydaymonth . 2007-9-1 | 60 | 51 | 56 | 75 | NA | 4 | 2007 | 244 | Sep | . 2007-9-2 | 60 | 53 | 56 | 82 | Rain | 4 | 2007 | 245 | Sep | . 2007-9-3 | 57 | 51 | 54 | 78 | NA | 6 | 2007 | 246 | Sep | . 2007-9-4 | 64 | 50 | 57 | 80 | Rain | 6 | 2007 | 247 | Sep | . 2007-9-5 | 53 | 48 | 50 | 90 | Rain | 7 | 2007 | 248 | Sep | . 2007-9-6 | 57 | 42 | 50 | 69 | NA | 1 | 2007 | 249 | Sep | . Other lubridate features . Handling timezones | Fast parsing of standard formats | Outputting datetimes | . Parsing and Manipulating Dates and Times with lubridate . Dates and times come in a huge assortment of formats, so the first hurdle is often to parse the format you have into an R datetime. We will explore how to import dates and times with the lubridate package. We&#39;ll also explore how to extract parts of a datetime. We&#39;ll practice by exploring the weather in R&#39;s birthplace, Auckland NZ. . Parsing dates with lubridate . ymd() . 27th of February 2013 | ymd() - year, then month, then day | . ymd(&quot;1997-06-15&quot;) . 1997-06-15 ymd(&quot;97/6/15&quot;) . 1997-06-15 ymd(&quot;97t6t15&quot;) . 1997-06-15 Friends of ymd() . ymd(),ydm(),mdy(),myd(),dmy(),dym(), dmy_hm() | parse_date_time(x = ___, order = ___) | . Formatting characters . Character Meaning Character Meaning . d | numeric day of the month | | a | Abbreviated weekday | . m | month of the year | | A | Full weekday | . y | Year with century | | b | Abbreviated month name | . Y | year without century | | B | full month name | . H | hours (24 hours) | | I | hours (12 hour) | . M | minutes | | P | AM/PM | . z | Timezone, offset | | | | . Selecting the right parsing function . lubridate provides a set of functions for parsing dates of a known order. For example, ymd() will parse dates with year first, followed by month and then day. The parsing is flexible, for example, it will parse the m whether it is numeric (e.g. 9 or 09), a full month name (e.g. September), or an abbreviated month name (e.g. Sep). . All the functions with y, m and d in any order exist. If the dates have times as well, you can use the functions that start with ymd, dmy, mdy or ydm and are followed by any of _h, _hm or _hms. . # Parse x x &lt;- &quot;2010 September 20th&quot; # 2010-09-20 ymd(x) # Parse y y &lt;- &quot;02.01.2010&quot; # 2010-01-02 dmy(y) # Parse z z &lt;- &quot;Sep, 12th 2010 14:00&quot; # 2010-09-12T14:00 mdy_hm(z) . 2010-09-20 2010-01-02 [1] &#34;2010-09-12 14:00:00 UTC&#34; . Specifying an order with parse_date_time() . What about if you have something in a really weird order like dym_msh?There&#39;s no named function just for that order, but that is where parse_date_time() comes in. parse_date_time() takes an additional argument, orders, where you can specify the order of the components in the date. . For example, to parse &quot;2010 September 20th&quot; you could say parse_date_time(&quot;2010 September 20th&quot;, orders = &quot;ymd&quot;) and that would be equivalent to using the ymd() function from the previous exercise. . One advantage of parse_date_time() is that you can use more format characters. For example, you can specify weekday names with A, I for 12 hour time, am/pm indicators with p and many others. Another big advantage is that you can specify a vector of orders, and that allows parsing of dates where multiple formats might be used. . # Specify an order string to parse x x &lt;- &quot;Monday June 1st 2010 at 4pm&quot; parse_date_time(x, orders = &quot;ABdyIp&quot;) # Specify order to include both &quot;mdy&quot; and &quot;dmy&quot; two_orders &lt;- c(&quot;October 7, 2001&quot;, &quot;October 13, 2002&quot;, &quot;April 13, 2003&quot;, &quot;17 April 2005&quot;, &quot;23 April 2017&quot;) parse_date_time(two_orders, orders = c(&quot;Bdy&quot;, &quot;Bdy&quot;, &quot;Bdy&quot;, &quot;dBy&quot;, &quot;dBy&quot;)) # Specify order to include &quot;dOmY&quot;, &quot;OmY&quot; and &quot;Y&quot; short_dates &lt;- c(&quot;11 December 1282&quot;, &quot;May 1372&quot;, &quot;1253&quot;) parse_date_time(short_dates, orders = c(&quot;d0mY&quot;, &quot;0mY&quot;, &quot;Y&quot;)) . [1] &#34;2010-06-01 16:00:00 UTC&#34; . [1] &#34;2001-10-07 UTC&#34; &#34;2002-10-13 UTC&#34; &#34;2003-04-13 UTC&#34; &#34;2005-04-17 UTC&#34; [5] &#34;2017-04-23 UTC&#34; . [1] &#34;1282-12-11 UTC&#34; &#34;1372-05-01 UTC&#34; &#34;1253-01-01 UTC&#34; . . Note: when a date component is missing, it&#8217;s just set to 1? For example, the input 1253 resulted in the date 1253-01-01. . Weather in Auckland . make_date(year, month, day) . make_date(year=1997, month=6, day=15) . 1997-06-15 make_datetime(year, month, day, hour, min, sec) for datetimes . dplyr Review . mutate() - add new columns (or overwrite old ones) | filter() - subset rows | select() - subset columns | arrange() - order rows | summarise() - summarise rows | group_by() - useful in conjuction with summarise() | . Import daily weather data . In practice you won&#39;t be parsing isolated dates and times, they&#39;ll be part of a larger dataset. We&#39;ll be working with weather data from Auckland NZ. . There are two data sets: . akl_weather_daily.csv a set of once daily summaries for 10 years, and | akl_weather_hourly_2016.csv observations every half hour for 2016. | . # Import CSV with read_csv() akl_daily_raw &lt;- read_csv(&quot;datasets/akl_weather_daily.csv&quot;) # Print akl_daily_raw head(akl_daily_raw) . Parsed with column specification: cols( date = col_character(), max_temp = col_double(), min_temp = col_double(), mean_temp = col_double(), mean_rh = col_double(), events = col_character(), cloud_cover = col_double() ) . datemax_tempmin_tempmean_tempmean_rheventscloud_cover . 2007-9-1 | 60 | 51 | 56 | 75 | NA | 4 | . 2007-9-2 | 60 | 53 | 56 | 82 | Rain | 4 | . 2007-9-3 | 57 | 51 | 54 | 78 | NA | 6 | . 2007-9-4 | 64 | 50 | 57 | 80 | Rain | 6 | . 2007-9-5 | 53 | 48 | 50 | 90 | Rain | 7 | . 2007-9-6 | 57 | 42 | 50 | 69 | NA | 1 | . # Parse date akl_daily &lt;- akl_daily_raw %&gt;% mutate(date = ymd(date)) # Print akl_daily head(akl_daily) . datemax_tempmin_tempmean_tempmean_rheventscloud_cover . 2007-09-01 | 60 | 51 | 56 | 75 | NA | 4 | . 2007-09-02 | 60 | 53 | 56 | 82 | Rain | 4 | . 2007-09-03 | 57 | 51 | 54 | 78 | NA | 6 | . 2007-09-04 | 64 | 50 | 57 | 80 | Rain | 6 | . 2007-09-05 | 53 | 48 | 50 | 90 | Rain | 7 | . 2007-09-06 | 57 | 42 | 50 | 69 | NA | 1 | . # Plot to check work akl_daily %&gt;% ggplot(aes(x = date, y = max_temp)) + geom_line() . Warning message: &#34;Removed 1 row(s) containing missing values (geom_path).&#34; . The temperatures are in farenheit. Yup, summer falls in Dec-Jan-Feb. . Import hourly weather data . The hourly data is a little different. The date information is spread over three columns year, month and mday, so we&#39;ll need to use make_date() to combine them. . akl_hourly_raw &lt;- read_csv(&quot;datasets/akl_weather_hourly_2016.csv&quot;) head(akl_hourly_raw) . Parsed with column specification: cols( year = col_double(), month = col_double(), mday = col_double(), time = col_time(format = &#34;&#34;), temperature = col_double(), weather = col_character(), conditions = col_character(), events = col_character(), humidity = col_double(), date_utc = col_datetime(format = &#34;&#34;) ) . yearmonthmdaytimetemperatureweatherconditionseventshumiditydate_utc . 2016 | 1 | 1 | 00:00:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 11:00:00 | . 2016 | 1 | 1 | 00:30:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 11:30:00 | . 2016 | 1 | 1 | 01:00:00 | 68 | Clear | Clear | NA | 73 | 2015-12-31 12:00:00 | . 2016 | 1 | 1 | 01:30:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 12:30:00 | . 2016 | 1 | 1 | 02:00:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 13:00:00 | . 2016 | 1 | 1 | 02:30:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 13:30:00 | . Then the time information is in a separate column again, time. It&#39;s quite common to find date and time split across different variables. One way to construct the datetimes is to paste the date and time together and then parse them. . # Use make_date() to combine year, month and mday akl_hourly &lt;- akl_hourly_raw %&gt;% mutate(date = make_date(year = year, month = month, day = mday)) # Parse datetime_string akl_hourly &lt;- akl_hourly %&gt;% mutate( datetime_string = paste(date, time, sep = &quot;T&quot;), datetime = ymd_hms(datetime_string) ) # Print date, time and datetime columns of akl_hourly akl_hourly %&gt;% select(date, time, datetime) %&gt;% head() . datetimedatetime . 2016-01-01 | 00:00:00 | 2016-01-01 00:00:00 | . 2016-01-01 | 00:30:00 | 2016-01-01 00:30:00 | . 2016-01-01 | 01:00:00 | 2016-01-01 01:00:00 | . 2016-01-01 | 01:30:00 | 2016-01-01 01:30:00 | . 2016-01-01 | 02:00:00 | 2016-01-01 02:00:00 | . 2016-01-01 | 02:30:00 | 2016-01-01 02:30:00 | . # Plot to check work akl_hourly %&gt;% ggplot(aes(x = datetime, y = temperature)) + geom_line() . It&#39;s interesting how the day to day variation is about half the size of the yearly variation. . Extracting parts of a datetime . Extracting parts of a datetime . yob = ymd(&quot;1997-06-15&quot;) yob . 1997-06-15 year(yob) . 1997 month(yob) . 6 day(yob) . 15 Extracting parts of a datetime . Function Extracts . year() | Year with century | . month() | Month (1-12) | . day() | Day of month (1-31) | . hour() | Hour (0-23) | . min() | Minute (0-59) | . second() | Second (0-59) | . wday() | Weekday (1-7) | . Setting parts of a datetime . yob . 1997-06-15 year(yob) &lt;- 2020 . yob . 2020-06-15 Other useful functions . Function Extracts . leap_year() | In leap year (TRUE or FALSE) | . am() | In morning (TRUE or FALSE) | . pm() | In afternoon (TRUE or FALSE) | . dst() | During daylight savings (TRUE or FALSE) | . quarter() | Quarter of year (1-4) | . semester() | Half of year (1-2) | . # Examine the head() of release_time head(releases$datetime) . [1] &#34;1997-12-04 08:47:58 UTC&#34; &#34;1997-12-21 13:09:22 UTC&#34; [3] &#34;1998-01-10 00:31:55 UTC&#34; &#34;1998-03-14 19:25:55 UTC&#34; [5] &#34;1998-05-02 07:58:17 UTC&#34; &#34;1998-06-14 12:56:20 UTC&#34; . # Examine the head() of the months of release_time head(month(releases$datetime)) . &lt;ol class=list-inline&gt; 12 | 12 | 1 | 3 | 5 | 6 | &lt;/ol&gt; # Extract the month of releases month(releases$datetime) %&gt;% table() . . 1 2 3 4 5 6 7 8 9 10 11 12 5 6 8 18 5 16 4 7 2 15 6 13 . # Extract the year of releases year(releases$datetime) %&gt;% table() . . 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2 10 9 6 6 5 5 4 4 4 4 6 5 4 6 4 2013 2014 2015 2016 2017 4 4 5 5 3 . # How often is the hour before 12 (noon)? mean(hour(releases$datetime) &lt; 12) . 0.752380952380952 # How often is the release in am? mean(am(releases$datetime)) . 0.752380952380952 R versions have historically been released most in April, June, October and December, 1998 saw 10 releases and about 75% of releases happen in the morning (at least according to UTC). . Adding useful labels . Sometimes it&#39;s nicer (especially for plotting or tables) to have named months. Both the month() and wday() (day of the week) functions have additional arguments label and abbr to achieve just that. Set label = TRUE to have the output labelled with month (or weekday) names, and abbr = FALSE for those names to be written in full rather than abbreviated. . # Use wday() to tabulate release by day of the week wday(releases$datetime) %&gt;% table() . . 1 2 3 4 5 6 7 3 29 9 12 18 31 3 . # Add label = TRUE to make table more readable wday(releases$datetime, label=TRUE) %&gt;% table() . . Sun Mon Tue Wed Thu Fri Sat 3 29 9 12 18 31 3 . # Create column wday to hold labelled week days releases$wday &lt;- wday(releases$datetime, label=T) # Plot barchart of weekday by type of release releases %&gt;% ggplot(aes(wday)) + geom_bar() + facet_wrap(~ type, ncol = 1, scale = &quot;free_y&quot;) . Looks like not too many releases occur on the weekends, and there is quite a different weekday pattern between minor and patch releases. . Extracting for plotting . Extracting components from a datetime is particularly useful when exploring data. Earlier we imported daily data for weather in Auckland, and created a time series plot of ten years of daily maximum temperature. While that plot gives a good overview of the whole ten years, it&#39;s hard to see the annual pattern. . We&#39;ll use components of the dates to help explore the pattern of maximum temperature over the year. The first step is to create some new columns to hold the extracted pieces, then we&#39;ll use them in a couple of plots. . # Add columns for year, yday and month akl_daily &lt;- akl_daily %&gt;% mutate( year = year(date), yday = yday(date), month = month(date, label=T)) # Plot max_temp by yday for all years akl_daily %&gt;% ggplot(aes(x = yday, y = max_temp)) + geom_line(aes(group = year), alpha = 0.5) . Warning message: &#34;Removed 1 row(s) containing missing values (geom_path).&#34; . # Examine distribution of max_temp by month akl_daily %&gt;% ggplot(aes(x = max_temp, y = month, height = ..density..)) + geom_density_ridges(stat = &quot;density&quot;) . Warning message: &#34;Removed 10 rows containing non-finite values (stat_density).&#34; . Both plots give a great view into both the expected temperatures and how much they vary. Looks like Jan, Feb and Mar are great months to visit if you want warm temperatures. Did you notice the warning messages? These are a consequence of some missing values in the max_temp column. They are a reminder to think carefully about what you might miss by ignoring missing values. . Extracting for filtering and summarizing . Another reason to extract components is to help with filtering observations or creating summaries. For example, if you are only interested in observations made on weekdays (i.e. not on weekends) you could extract the weekdays then filter out weekends, e.g. wday(date) %in% 2:6. . We saw that January, February and March were great times to visit Auckland for warm temperatures, but will you need a raincoat? . We&#39;ll use the hourly data to calculate how many days in each month there was any rain during the day. . # Create new columns hour, month and rainy akl_hourly &lt;- akl_hourly %&gt;% mutate( hour = hour(datetime), month = month(datetime, label=T), rainy = weather == &quot;Precipitation&quot; ) # Filter for hours between 8am and 10pm (inclusive) akl_day &lt;- akl_hourly %&gt;% filter(hour&gt;=8, hour&lt;=22) # Summarise for each date if there is any rain rainy_days &lt;- akl_day %&gt;% group_by(month, date) %&gt;% summarise( any_rain = any(rainy) ) # Summarise for each month, the number of days with rain rainy_days&lt;- rainy_days %&gt;% summarise( days_rainy = sum(any_rain) ) rainy_days . `summarise()` regrouping output by &#39;month&#39; (override with `.groups` argument) `summarise()` ungrouping output (override with `.groups` argument) . monthdays_rainy . Jan | 15 | . Feb | 13 | . Mar | 12 | . Apr | 15 | . May | 21 | . Jun | 19 | . Jul | 22 | . Aug | 16 | . Sep | 25 | . Oct | 20 | . Nov | 19 | . Dec | 11 | . At least in 2016, it looks like you&#39;ll still need to pack a raincoat if you visit in Jan, Feb or March. Months of course are different lengths so we should really correct for that, take a look at days_in_month() for helping with that. . Rounding datetimes . Rounding versus extracting . release_time = releases$datetime head(release_time) . [1] &#34;1997-12-04 08:47:58 UTC&#34; &#34;1997-12-21 13:09:22 UTC&#34; [3] &#34;1998-01-10 00:31:55 UTC&#34; &#34;1998-03-14 19:25:55 UTC&#34; [5] &#34;1998-05-02 07:58:17 UTC&#34; &#34;1998-06-14 12:56:20 UTC&#34; . release_time %&gt;% head() %&gt;% hour() . &lt;ol class=list-inline&gt; 8 | 13 | 0 | 19 | 7 | 12 | &lt;/ol&gt; release_time %&gt;% head() %&gt;% floor_date(unit=&quot;hour&quot;) . [1] &#34;1997-12-04 08:00:00 UTC&#34; &#34;1997-12-21 13:00:00 UTC&#34; [3] &#34;1998-01-10 00:00:00 UTC&#34; &#34;1998-03-14 19:00:00 UTC&#34; [5] &#34;1998-05-02 07:00:00 UTC&#34; &#34;1998-06-14 12:00:00 UTC&#34; . Rounding in lubridate . round_date() - round to nearest | ceiling_date() - round up | floor_date() - round to down | Possible values of unit:- &quot;second&quot;, &quot;minute&quot;, &quot;hour&quot;, &quot;day&quot;, &quot;week&quot;, &quot;month&quot;, &quot;bimonth&quot;, &quot;quarter&quot;, &quot;halfyear&quot;, or &quot;year&quot;. - Or multiples, e.g &quot;2 years&quot;, &quot;5 minutes&quot; | . r_3_4_1 &lt;- ymd_hms(&quot;2016-05-03 07:13:28 UTC&quot;) # Round down to day floor_date(r_3_4_1, unit = &quot;day&quot;) . [1] &#34;2016-05-03 UTC&#34; . # Round to nearest 5 minutes round_date(r_3_4_1, unit = &quot;5 minutes&quot;) . [1] &#34;2016-05-03 07:15:00 UTC&#34; . # Round up to week ceiling_date(r_3_4_1, unit = &quot;week&quot;) . [1] &#34;2016-05-08 UTC&#34; . # Subtract r_3_4_1 rounded down to day r_3_4_1 - floor_date(r_3_4_1, unit = &quot;day&quot;) . Time difference of 7.224444 hours . That last technique of subtracting a rounded datetime from an unrounded one is a really useful trick to remember. . Rounding with the weather data . When is rounding useful? In a lot of the same situations extracting date components is useful. The advantage of rounding over extracting is that it maintains the context of the unit. For example, extracting the hour gives you the hour the datetime occurred, but you lose the day that hour occurred on (unless you extract that too), on the other hand, rounding to the nearest hour maintains the day, month and year. . We&#39;ll explore how many observations per hour there really are in the hourly Auckland weather data. . # Create day_hour, datetime rounded down to hour akl_hourly &lt;- akl_hourly %&gt;% mutate( day_hour = floor_date(datetime, unit = &quot;hour&quot;) ) # Count observations per hour akl_hourly %&gt;% count(day_hour) %&gt;% head() . day_hourn . 2016-01-01 00:00:00 | 2 | . 2016-01-01 01:00:00 | 2 | . 2016-01-01 02:00:00 | 2 | . 2016-01-01 03:00:00 | 2 | . 2016-01-01 04:00:00 | 2 | . 2016-01-01 05:00:00 | 2 | . # Find day_hours with n != 2 akl_hourly %&gt;% count(day_hour) %&gt;% filter(n!=2) %&gt;% arrange(desc(n)) %&gt;% head() . day_hourn . 2016-04-03 02:00:00 | 4 | . 2016-09-25 00:00:00 | 4 | . 2016-06-26 09:00:00 | 1 | . 2016-09-01 23:00:00 | 1 | . 2016-09-02 01:00:00 | 1 | . 2016-09-04 11:00:00 | 1 | . Interestingly there are four measurements on 2016-04-03 and 2016-09-25, they happen to be the days Daylight Saving starts and ends. . Arithmetic with Dates and Times . Getting datetimes into R is just the first step. Now we&#39;ve explored how to parse datetimes, we need to explore how to do calculations with them. Here we&#39;ll learn the different ways of representing spans of time with lubridate and how to leverage them to do arithmetic on datetimes. By the end we&#39;ll have calculated how long it&#39;s been since the first man stepped on the moon, generated sequences of dates to help schedule reminders, calculated when an eclipse occurs, and explored the reigns of monarch&#39;s of England (and which ones might have seen Halley&#39;s comet!). . Taking differences of datetimes . Arithmetic for datetimes . datetime_1 - datetime2:Subtraction for time elapsed- datetime_1 + (2 * timespan): Addition and multiplication for generating new datetimes in the past or future | timespan1 / timespan2: Division for change of units | . Subtraction of datetimes . last_release_date . 2017-06-30 Sys.Date() - last_release_date . Time difference of 1110 days . difftime(Sys.Date(), last_release_date) . Time difference of 1110 days . difftime() . units =&quot;secs&quot;,&quot;mins&quot;,&quot;hours&quot;,&quot;days&quot;, or &quot;weeks&quot; | . year(yob) &lt;- 1997 yob . 1997-06-15 difftime(Sys.Date(), yob, units=&quot;weeks&quot;) . Time difference of 1204.286 weeks . difftime(Sys.Date(), yob, units=&quot;days&quot;) . Time difference of 8430 days . difftime(Sys.Date(), yob, units=&quot;secs&quot;) . Time difference of 728352000 secs . now() and today() . now() . [1] &#34;2020-07-14 09:07:16 EAT&#34; . today() . 2020-07-14 str(today()) . Date[1:1], format: &#34;2020-07-14&#34; . How long has it been? . To get finer control over a difference between datetimes use the base function difftime(). For example instead of time1 - time2, we can use difftime(time1, time2). . difftime() takes an argument units which specifies the units for the difference. Your options are &quot;secs&quot;, &quot;mins&quot;, &quot;hours&quot;, &quot;days&quot;, or &quot;weeks&quot;. . We&#39;ll find the time since the first man stepped on the moon. . # The date of landing and moment of step date_landing &lt;- mdy(&quot;July 20, 1969&quot;) moment_step &lt;- mdy_hms(&quot;July 20, 1969, 02:56:15&quot;, tz = &quot;UTC&quot;) # How many days since the first man on the moon? difftime(today(), date_landing, units = &quot;days&quot;) . Time difference of 18622 days . # How many seconds since the first man on the moon? difftime(now(), moment_step, units=&quot;secs&quot;) . Time difference of 1608952266 secs . How many seconds are in a day? . How many seconds are in a day? There are 24 hours in a day, 60 minutes in an hour, and 60 seconds in a minute, so there should be 246060 = 86400 seconds, right? . Not always! We&#39;ll see a counter example. . # Three dates mar_11 &lt;- ymd_hms(&quot;2017-03-11 12:00:00&quot;, tz = &quot;America/Los_Angeles&quot;) mar_12 &lt;- ymd_hms(&quot;2017-03-12 12:00:00&quot;, tz = &quot;America/Los_Angeles&quot;) mar_13 &lt;- ymd_hms(&quot;2017-03-13 12:00:00&quot;, tz = &quot;America/Los_Angeles&quot;) # Difference between mar_13 and mar_12 in seconds difftime(mar_13, mar_12, units = &quot;secs&quot;) . Time difference of 86400 secs . # Difference between mar_12 and mar_11 in seconds difftime(mar_12, mar_11, units = &quot;secs&quot;) . Time difference of 82800 secs . Why would a day only have 82800 seconds? At 2am on Mar 12th 2017, Daylight Savings started in the Pacific timezone. That means a whole hour of seconds gets skipped between noon on the 11th and noon on the 12th. . Time spans. . Time spans in lubridate . period Duration . Human concept of a timespan | Stopwatch concept of a time span | . datetime + period of oneday = same time on the next date | datetime + duration of oneday = datetime + 86400seconds | . variable length | fixed number of seconds | . Creating a time span . days() . 1d 0H 0M 0S days(x=2) . 2d 0H 0M 0S ddays(2) . 172800s (~2 days) Arithmetic with time spans . 2 * days() . 2d 0H 0M 0S days() + days() . 2d 0H 0M 0S ymd(&quot;2020-07-13&quot;) + days() . 2020-07-14 Functions to create time spans . Time span Duration Period . Seconds | dseconds() | seconds() | . Minutes | dminutes() | minutes() | . Hours | dhours() | hours() | . Days | ddays() | days() | . Weeks | dweeks() | weeks() | . Months | - | months() | . Years | dyears() | years() | . ddays() . 86400s (~1 days) Adding or subtracting a time span to a datetime . A common use of time spans is to add or subtract them from a moment in time. For, example to calculate the time one day in the future from mar_11, you could do either of: . mar_11+days() . [1] &#34;2017-03-12 12:00:00 PDT&#34; . mar_11+ddays() . [1] &#34;2017-03-12 13:00:00 PDT&#34; . But which one is the right one? It depends on your intent. If you want to account for the fact that time units, in this case days, have different lengths (i.e. due to daylight savings), you want a period days(). If you want the time 86400 seconds in the future you use a duration ddays(). . We&#39;ll add and subtract timespans from dates and datetimes. . # Add a period of one week to mon_2pm mon_2pm &lt;- dmy_hm(&quot;27 Aug 2018 14:00&quot;) mon_2pm + weeks() . [1] &#34;2018-09-03 14:00:00 UTC&#34; . # Add a duration of 81 hours to tue_9am tue_9am &lt;- dmy_hm(&quot;28 Aug 2018 9:00&quot;) tue_9am + dhours(81) . [1] &#34;2018-08-31 18:00:00 UTC&#34; . # Subtract a period of five years from today() today() - years(5) . 2015-07-14 # Subtract a duration of five years from today() today() - dyears(5) . [1] &#34;2015-07-14 18:00:00 UTC&#34; . Why did subtracting a duration of five years from today, give a different answer to subtracting a period of five years? Periods know about leap years, and since five years ago includes at least one leap year, the period of five years is longer than the duration of 365*5 days. . . Note: when dealing with human interpretaions of dates and time you want to use periods. . Arithmetic with timespans . You can add and subtract timespans to create different length timespans, and even multiply them by numbers. For example, to create a duration of three days and three hours you could do: . ddays(3) + dhours(3) . 270000s (~3.12 days) or . 3*ddays(1) + 3*dhours(1) . 270000s (~3.12 days) or even . 3*(ddays(1) + dhours(1)) . 270000s (~3.12 days) There was an eclipse over North America on 2017-08-21 at 18:26:40. It&#39;s possible to predict the next eclipse with similar geometry by calculating the time and date one Saros in the future. A Saros is a length of time that corresponds to 223 Synodic months, a Synodic month being the period of the Moon&#39;s phases, a duration of 29 days, 12 hours, 44 minutes and 3 seconds. . # Time of North American Eclipse 2017 eclipse_2017 &lt;- ymd_hms(&quot;2017-08-21 18:26:40&quot;) # Duration of 29 days, 12 hours, 44 mins and 3 secs synodic &lt;- ddays(29) + dhours(12) + dminutes(44) + dseconds(3) # 223 synodic months saros &lt;- 223 * synodic # Add saros to eclipse_2017 saros + eclipse_2017 . [1] &#34;2035-09-02 02:09:49 UTC&#34; . 2035 is a long way away for an eclipse, but luckily there are eclipses on different Saros cycles, so you can see one much sooner. . Generating sequences of datetimes . By combining addition and multiplication with sequences you can generate sequences of datetimes. For example, you can generate a sequence of periods from 1 day up to 10 days with, . 1:10 * days() . &lt;ol class=list-inline&gt; 1d 0H 0M 0S | 2d 0H 0M 0S | 3d 0H 0M 0S | 4d 0H 0M 0S | 5d 0H 0M 0S | 6d 0H 0M 0S | 7d 0H 0M 0S | 8d 0H 0M 0S | 9d 0H 0M 0S | 10d 0H 0M 0S | &lt;/ol&gt; Then by adding this sequence to a specific datetime, you can construct a sequence of datetimes from 1 day up to 10 days into the future . today() + 1:10 * days() . &lt;ol class=list-inline&gt; 2020-07-15 | 2020-07-16 | 2020-07-17 | 2020-07-18 | 2020-07-19 | 2020-07-20 | 2020-07-21 | 2020-07-22 | 2020-07-23 | 2020-07-24 | &lt;/ol&gt; We had a meeting this morning at 8am and we&#39;d like to have that meeting at the same time and day every two weeks for a year. . # Add a period of 8 hours to today today_8am &lt;- today() + hours(8) # Sequence of two weeks from 1 to 26 every_two_weeks &lt;- 1:26 * weeks() # Create datetime for every two weeks for a year every_two_weeks + today_8am . [1] &#34;2020-07-21 08:00:00 UTC&#34; &#34;2020-07-28 08:00:00 UTC&#34; [3] &#34;2020-08-04 08:00:00 UTC&#34; &#34;2020-08-11 08:00:00 UTC&#34; [5] &#34;2020-08-18 08:00:00 UTC&#34; &#34;2020-08-25 08:00:00 UTC&#34; [7] &#34;2020-09-01 08:00:00 UTC&#34; &#34;2020-09-08 08:00:00 UTC&#34; [9] &#34;2020-09-15 08:00:00 UTC&#34; &#34;2020-09-22 08:00:00 UTC&#34; [11] &#34;2020-09-29 08:00:00 UTC&#34; &#34;2020-10-06 08:00:00 UTC&#34; [13] &#34;2020-10-13 08:00:00 UTC&#34; &#34;2020-10-20 08:00:00 UTC&#34; [15] &#34;2020-10-27 08:00:00 UTC&#34; &#34;2020-11-03 08:00:00 UTC&#34; [17] &#34;2020-11-10 08:00:00 UTC&#34; &#34;2020-11-17 08:00:00 UTC&#34; [19] &#34;2020-11-24 08:00:00 UTC&#34; &#34;2020-12-01 08:00:00 UTC&#34; [21] &#34;2020-12-08 08:00:00 UTC&#34; &#34;2020-12-15 08:00:00 UTC&#34; [23] &#34;2020-12-22 08:00:00 UTC&#34; &#34;2020-12-29 08:00:00 UTC&#34; [25] &#34;2021-01-05 08:00:00 UTC&#34; &#34;2021-01-12 08:00:00 UTC&#34; . The tricky thing about months . What should ymd(&quot;2018-01-31&quot;) + months(1) return? . ymd(&quot;2018-01-31&quot;) + months(1) . &lt;NA&gt; In general lubridate returns the same day of the month in the next month, but since the 31st of February doesn&#39;t exist lubridate returns a missing value, NA. . There are alternative addition and subtraction operators: %m+% and %m-% that have different behavior. Rather than returning an NA for a non-existent date, they roll back to the last existing date . ymd(&quot;2018-01-31&quot;) %m+% months(1) . 2018-02-28 jan_31 = ymd(&quot;2020-01-31&quot;) # A sequence of 1 to 12 periods of 1 month month_seq &lt;- 1:12 * months(1) # Add 1 to 12 months to jan_31 month_seq + jan_31 . &lt;ol class=list-inline&gt; &lt;NA&gt; | 2020-03-31 | &lt;NA&gt; | 2020-05-31 | &lt;NA&gt; | 2020-07-31 | 2020-08-31 | &lt;NA&gt; | 2020-10-31 | &lt;NA&gt; | 2020-12-31 | 2021-01-31 | &lt;/ol&gt; # Replace + with %m+% month_seq %m+% jan_31 . &lt;ol class=list-inline&gt; 2020-02-29 | 2020-03-31 | 2020-04-30 | 2020-05-31 | 2020-06-30 | 2020-07-31 | 2020-08-31 | 2020-09-30 | 2020-10-31 | 2020-11-30 | 2020-12-31 | 2021-01-31 | &lt;/ol&gt; # Replace + with %m-% jan_31 %m-% month_seq . &lt;ol class=list-inline&gt; 2019-12-31 | 2019-11-30 | 2019-10-31 | 2019-09-30 | 2019-08-31 | 2019-07-31 | 2019-06-30 | 2019-05-31 | 2019-04-30 | 2019-03-31 | 2019-02-28 | 2019-01-31 | &lt;/ol&gt; . Warning: use these operators with caution, unlike + and -, you might not get x back from x %m+% months(1) %m-% months(1). If you&#8217;d prefer that the date was rolled forward check out add_with_rollback() which has roll_to_first argument. . Intervals . Creating intervals . datetime1%--%datetime2,or | interval(datetime1, datetime2) | . yob %--% today() . 1997-06-15 UTC--2020-07-14 UTC interval(yob, today()) . 1997-06-15 UTC--2020-07-14 UTC Operating on an interval . yob_interval = interval(yob, today()) int_start(yob_interval) . [1] &#34;1997-06-15 UTC&#34; . int_end(yob_interval) . [1] &#34;2020-07-14 UTC&#34; . Operating on an interval . int_length(yob_interval) . 728352000 as.period(yob_interval) . 23y 0m 29d 0H 0M 0S as.duration(yob_interval) . 728352000s (~23.08 years) Comparing intervals . enter_mmu &lt;- ymd(&quot;2017-07-30&quot;) enter_mmu %within% yob_interval . TRUE been_mmu &lt;- ymd(&quot;2017-07-30&quot;) %--% today() . int_overlaps(been_mmu, yob_interval) . TRUE Which kind of time span? . Use:- Intervals when you have a start and end - Periods when you are interested in human units Durations if you are interested in seconds elapsed | . | . Monarchs of England 2 | Halley&#39;s cometHalley&#39;scomet:3 | . Examining intervals. Reigns of kings and queens . # Print monarchs monarchs # Create an interval for reign monarchs &lt;- monarchs %&gt;% mutate(reign = from %--% to) # Find the length of reign, and arrange monarchs %&gt;% mutate(length = int_length(reign)) %&gt;% arrange(desc(length)) %&gt;% select(name, length, dominion) . The current queen, Elizabeth II, has ruled for 2070144000 seconds…we&#39;ll see a better way to display the length later. If you know your British monarchs, you might notice George III doesn&#39;t appear in the the top 5. In this data, his reign is spread over two rows for U.K. And Great Britain and you would need to add their lengths to see his total reign. . Comparing intervals and datetimes . A common task with intervals is to ask if a certain time is inside the interval or whether it overlaps with another interval. . The operator %within% tests if the datetime (or interval) on the left hand side is within the interval of the right hand side. For example, if y2001 is the interval covering the year 2001, . y2001 &lt;- ymd(&quot;2001-01-01&quot;) %--% ymd(&quot;2001-12-31&quot;) ymd(&quot;2001-03-30&quot;) %within% y2001 . TRUE ymd(&quot;2002-03-30&quot;) %within% y2001 . FALSE int_overlaps() performs a similar test, but will return true if two intervals overlap at all. . # Print halleys halleys # New column for interval from start to end date halleys &lt;- halleys %&gt;% mutate(visible = interval(start_date, end_date)) # The visitation of 1066 halleys_1066 &lt;- halleys[14, ] # Monarchs in power on perihelion date monarchs %&gt;% filter(halleys_1066$perihelion_date %within%reign) %&gt;% select(name, from, to, dominion) # Monarchs whose reign overlaps visible time monarchs %&gt;% filter(int_overlaps(halleys_1066$visible,reign)) %&gt;% select(name, from, to, dominion) . Looks like the Kings of England Edward the Confessor and Harold II would have been able to see the comet. It may have been a bad omen, neither were in power by 1067. . Converting to durations and periods . Intervals are the most specific way to represent a span of time since they retain information about the exact start and end moments. They can be converted to periods and durations exactly: it&#39;s possible to calculate both the exact number of seconds elapsed between the start and end date, as well as the perceived change in clock time . # New columns for duration and period monarchs &lt;- monarchs %&gt;% mutate( duration = as.duration(reign), period = as.period(reign)) # Examine results monarchs %&gt;% select(name, duration, period) . Problems in practice . You now know most of what you need to tackle data that includes dates and times, but there are a few other problems we might encounter. We&#39;ll explore a little more about these problems by returning to some of the earlier data sets and explore how to handle time zones, deal with times when you don&#39;t care about dates, parse dates quickly, and output dates and times. . Time zones . Sys.timezone() . &#39;Africa/Nairobi&#39; IANA Timezones . OlsonNames() %&gt;% head(10) . &lt;ol class=list-inline&gt; &#39;Africa/Abidjan&#39; | &#39;Africa/Accra&#39; | &#39;Africa/Addis_Ababa&#39; | &#39;Africa/Algiers&#39; | &#39;Africa/Asmara&#39; | &#39;Africa/Asmera&#39; | &#39;Africa/Bamako&#39; | &#39;Africa/Bangui&#39; | &#39;Africa/Banjul&#39; | &#39;Africa/Bissau&#39; | &lt;/ol&gt; length(OlsonNames()) . 593 Setting and extracting . mar_11 . [1] &#34;2017-03-11 12:00:00 PST&#34; . tz(mar_11) . &#39;America/Los_Angeles&#39; Manipulating timezones . force_tz() - change thetimezone without changing the clock time | with_tz() - view the sameinstant in a different timezone | . with_tz(mar_11, tzone=&quot;Africa/Nairobi&quot;) . [1] &#34;2017-03-11 23:00:00 EAT&#34; . force_tz(mar_11, tzone=&quot;Africa/Nairobi&quot;) . [1] &#34;2017-03-11 12:00:00 EAT&#34; . Setting the timezone . If you import a datetime and it has the wrong timezone, you can set it with force_tz(). Pass in the datetime as the first argument and the appropriate timezone to the tzone argument. Remember the timezone needs to be one from OlsonNames(). . I wanted to watch New Zealand in the Women&#39;s World Cup Soccer games in 2015, but the times listed on the FIFA website were all in times local to the venues. We&#39;ll set the timezones, then figure out what time I needed to tune in to watch them. . # Game2: CAN vs NZL in Edmonton game2 &lt;- mdy_hm(&quot;June 11 2015 19:00&quot;) # Game3: CHN vs NZL in Winnipeg game3 &lt;- mdy_hm(&quot;June 15 2015 18:30&quot;) # Set the timezone to &quot;America/Edmonton&quot; game2_local &lt;- force_tz(game2, tzone = &quot;America/Edmonton&quot;) game2_local . [1] &#34;2015-06-11 19:00:00 MDT&#34; . # Set the timezone to &quot;America/Winnipeg&quot; game3_local &lt;- force_tz(game3, tzone = &quot;America/Winnipeg&quot;) game3_local . [1] &#34;2015-06-15 18:30:00 CDT&#34; . # How long does the team have to rest? as.period(interval(game2_local, game3_local)) . 3d 22H 30M 0S Edmonton and Winnipeg are in different timezones, so even though the start times of the games only look 30 minutes apart, they are in fact 1 hour and 30 minutes apart, and the team only has 3 days, 22 hours and 30 minutes to prepare. . Viewing in a timezone . To view a datetime in another timezone use with_tz(). The syntax of with_tz() is the same as force_tz(), passing a datetime and set the tzone argument to the desired timezone. Unlike force_tz(), with_tz() isn&#39;t changing the underlying moment of time, just how it is displayed. . # What time is game2_local in NZ? with_tz(game2_local, tzone = &quot;Pacific/Auckland&quot;) . [1] &#34;2015-06-12 13:00:00 NZST&#34; . # What time is game2_local in Corvallis, Oregon? with_tz(game2_local, tzone = &quot;America/Los_Angeles&quot;) . [1] &#34;2015-06-11 18:00:00 PDT&#34; . # What time is game3_local in NZ? with_tz(game3_local, tzone=&quot;Pacific/Auckland&quot;) . [1] &#34;2015-06-16 11:30:00 NZST&#34; . Timezones in the weather data . head(akl_hourly) . yearmonthmdaytimetemperatureweatherconditionseventshumiditydate_utcdatedatetime_stringdatetimehourrainyday_hour . 2016 | Jan | 1 | 00:00:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 11:00:00 | 2016-01-01 | 2016-01-01T00:00:00 | 2016-01-01 00:00:00 | 0 | FALSE | 2016-01-01 00:00:00 | . 2016 | Jan | 1 | 00:30:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 11:30:00 | 2016-01-01 | 2016-01-01T00:30:00 | 2016-01-01 00:30:00 | 0 | FALSE | 2016-01-01 00:00:00 | . 2016 | Jan | 1 | 01:00:00 | 68 | Clear | Clear | NA | 73 | 2015-12-31 12:00:00 | 2016-01-01 | 2016-01-01T01:00:00 | 2016-01-01 01:00:00 | 1 | FALSE | 2016-01-01 01:00:00 | . 2016 | Jan | 1 | 01:30:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 12:30:00 | 2016-01-01 | 2016-01-01T01:30:00 | 2016-01-01 01:30:00 | 1 | FALSE | 2016-01-01 01:00:00 | . 2016 | Jan | 1 | 02:00:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 13:00:00 | 2016-01-01 | 2016-01-01T02:00:00 | 2016-01-01 02:00:00 | 2 | FALSE | 2016-01-01 02:00:00 | . 2016 | Jan | 1 | 02:30:00 | 68 | Clear | Clear | NA | 68 | 2015-12-31 13:30:00 | 2016-01-01 | 2016-01-01T02:30:00 | 2016-01-01 02:30:00 | 2 | FALSE | 2016-01-01 02:00:00 | . The datetime column we created represented local time in Auckland, NZ. I suspect this additional column, date_utc represents the observation time in UTC (the name seems a big clue). But does it really? . # Examine datetime and date_utc columns head(akl_hourly$datetime) . [1] &#34;2016-01-01 00:00:00 UTC&#34; &#34;2016-01-01 00:30:00 UTC&#34; [3] &#34;2016-01-01 01:00:00 UTC&#34; &#34;2016-01-01 01:30:00 UTC&#34; [5] &#34;2016-01-01 02:00:00 UTC&#34; &#34;2016-01-01 02:30:00 UTC&#34; . head(akl_hourly$date_utc) . [1] &#34;2015-12-31 11:00:00 UTC&#34; &#34;2015-12-31 11:30:00 UTC&#34; [3] &#34;2015-12-31 12:00:00 UTC&#34; &#34;2015-12-31 12:30:00 UTC&#34; [5] &#34;2015-12-31 13:00:00 UTC&#34; &#34;2015-12-31 13:30:00 UTC&#34; . # Force datetime to Pacific/Auckland akl_hourly &lt;- akl_hourly %&gt;% mutate( datetime = force_tz(datetime, tzone = &quot;Pacific/Auckland&quot;)) # Reexamine datetime head(akl_hourly$datetime) . [1] &#34;2016-01-01 00:00:00 NZDT&#34; &#34;2016-01-01 00:30:00 NZDT&#34; [3] &#34;2016-01-01 01:00:00 NZDT&#34; &#34;2016-01-01 01:30:00 NZDT&#34; [5] &#34;2016-01-01 02:00:00 NZDT&#34; &#34;2016-01-01 02:30:00 NZDT&#34; . # Are datetime and date_utc the same moments table(akl_hourly$datetime - akl_hourly$date_utc) . -82800 0 3600 2 17450 2 . Looks like for 17,450 rows datetime and date_utc describe the same moment, but for 4 rows they are different. Can you guess which? Yup, the times where DST kicks in. . Times without dates . # Examine structure of time column str(akl_hourly$time) . &#39;hms&#39; num [1:17454] 00:00:00 00:30:00 01:00:00 01:30:00 ... - attr(*, &#34;units&#34;)= chr &#34;secs&#34; . # Examine head of time column head(akl_hourly$time) . 00:00:00 00:30:00 01:00:00 01:30:00 02:00:00 02:30:00 . # A plot using just time akl_hourly %&gt;% ggplot(aes(x = time, y = temperature)) + geom_line(aes(group = make_date(year, month, mday)), alpha = 0.2) . Using time without date is a great way to examine daily patterns. . More on importing and exporting datetimes . Fast parsing . parse_date_time() can be slow because it&#39;s designed to be forgiving and flexible. | . fastPOSIXct(&quot;1997-06-16&quot;) . [1] &#34;1997-06-16 03:00:00 EAT&#34; . fast_strptime() . yob_str &lt;- &quot;1997-06-15&quot; parse_date_time(yob_str, order=&quot;ymd&quot;) . [1] &#34;1997-06-15 UTC&#34; . fast_strptime(yob_str, format = &quot;%Y-%m-%d&quot;) . [1] &#34;1997-06-15 UTC&#34; . Exporting datetimes . akl_hourly %&gt;% select(datetime) %&gt;% write_csv(&quot;datasets/tmp.csv&quot;) . Formatting datetimes . yob_stamp &lt;- stamp(&quot;Sunday June 15 1997&quot;) . Multiple formats matched: &#34;%A %B %d %Y&#34;(1), &#34;Sunday %Om %d %Y&#34;(1), &#34;Sunday %B %d %Y&#34;(1), &#34;%A %Om %d %Y&#34;(0) Using: &#34;%A %B %d %Y&#34; . yob_stamp(ymd(&quot;1997-06-15&quot;)) . &#39;Sunday June 15 1997&#39; yob_stamp . &lt;pre class=language-r&gt;function (x, locale = &quot;English_United States.1252&quot;) { &lt;span style=white-space:pre-wrap&gt; {&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; old_lc_time &lt;- Sys.getlocale(&quot;LC_TIME&quot;)&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; if (old_lc_time != locale) {&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; on.exit(Sys.setlocale(&quot;LC_TIME&quot;, old_lc_time))&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; Sys.setlocale(&quot;LC_TIME&quot;, locale)&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; }&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; }&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; format(x, format = &quot;%A %B %d %Y&quot;)&lt;/span&gt; }&lt;/pre&gt; Fast parsing with fasttime . The fasttime package provides a single function fastPOSIXct(), designed to read in datetimes formatted according to ISO 8601. Because it only reads in one format, and doesn&#39;t have to guess a format, it is really fast! . We&#39;ll see how fast by comparing how fast it reads in the dates from the Auckland hourly weather data (over 17,000 dates) to lubridates ymd_hms(). . # Examine structure of dates str(akl_hourly$datetime_string) . chr [1:17454] &#34;2016-01-01T00:00:00&#34; &#34;2016-01-01T00:30:00&#34; ... . # Use fastPOSIXct() to parse dates fastPOSIXct(akl_hourly$datetime_string) %&gt;% str() . POSIXct[1:17454], format: &#34;2016-01-01 03:00:00&#34; &#34;2016-01-01 03:30:00&#34; &#34;2016-01-01 04:00:00&#34; ... . # Compare speed of fastPOSIXct() to ymd_hms() microbenchmark( ymd_hms = ymd_hms(akl_hourly$datetime_string), fasttime = fastPOSIXct(akl_hourly$datetime_string), times = 20) . exprtime . fasttime | 2455100 | . ymd_hms | 30602000 | . fasttime | 2577700 | . ymd_hms | 31090200 | . fasttime | 2762600 | . fasttime | 2779400 | . fasttime | 2796400 | . fasttime | 2786900 | . ymd_hms | 35160800 | . fasttime | 2590900 | . ymd_hms | 32037000 | . ymd_hms | 31251200 | . ymd_hms | 32154400 | . fasttime | 2638200 | . ymd_hms | 32626700 | . ymd_hms | 31518000 | . fasttime | 2517700 | . ymd_hms | 32616500 | . fasttime | 2396500 | . ymd_hms | 33101100 | . fasttime | 2503900 | . fasttime | 2429800 | . ymd_hms | 38136000 | . ymd_hms | 28758100 | . fasttime | 2131100 | . ymd_hms | 26896600 | . fasttime | 2219700 | . ymd_hms | 26722900 | . ymd_hms | 26774200 | . ymd_hms | 28291400 | . ymd_hms | 35462800 | . ymd_hms | 35749400 | . fasttime | 2591500 | . fasttime | 2532400 | . ymd_hms | 32089700 | . fasttime | 2776700 | . fasttime | 2697700 | . fasttime | 2550500 | . ymd_hms | 33218200 | . fasttime | 2475800 | . fasttime is about 20 times faster than ymd_hms(). . Fast parsing with lubridate::fast_strptime . lubridate provides its own fast datetime parser: fast_strptime(). Instead of taking an order argument like parse_date_time() it takes a format argument and the format must comply with the strptime() style. . # Head of dates head(akl_hourly$datetime_string) . &lt;ol class=list-inline&gt; &#39;2016-01-01T00:00:00&#39; | &#39;2016-01-01T00:30:00&#39; | &#39;2016-01-01T01:00:00&#39; | &#39;2016-01-01T01:30:00&#39; | &#39;2016-01-01T02:00:00&#39; | &#39;2016-01-01T02:30:00&#39; | &lt;/ol&gt; # Parse dates with fast_strptime fast_strptime(akl_hourly$datetime_string, format = &quot;%Y-%m-%dT%H:%M:%S&quot;) %&gt;% str() . POSIXlt[1:17454], format: &#34;2016-01-01 00:00:00&#34; &#34;2016-01-01 00:30:00&#34; &#34;2016-01-01 01:00:00&#34; ... . # Comparse speed to ymd_hms() and fasttime microbenchmark( ymd_hms = ymd_hms(akl_hourly$datetime_string), fasttime = fastPOSIXct(akl_hourly$datetime_string), fast_strptime = fast_strptime(akl_hourly$datetime_string, format = &quot;%Y-%m-%dT%H:%M:%S&quot;), times = 20) . exprtime . ymd_hms | 42620600 | . ymd_hms | 46698300 | . fast_strptime | 3006900 | . ymd_hms | 29005100 | . ymd_hms | 28695300 | . fasttime | 2406000 | . fasttime | 2340100 | . fasttime | 2312100 | . fasttime | 2310500 | . ymd_hms | 31183400 | . fast_strptime | 3118800 | . fasttime | 2325100 | . ymd_hms | 29844300 | . fast_strptime | 3363600 | . fast_strptime | 3071200 | . ymd_hms | 30061000 | . fasttime | 2251400 | . ymd_hms | 36800700 | . fast_strptime | 3555000 | . ymd_hms | 34068400 | . fasttime | 2512300 | . fast_strptime | 3111700 | . fast_strptime | 3095400 | . fasttime | 2264900 | . fast_strptime | 3090500 | . ymd_hms | 36769000 | . ymd_hms | 27374000 | . fast_strptime | 2777400 | . ymd_hms | 27985100 | . fasttime | 2200400 | . fasttime | 1994100 | . fast_strptime | 2602500 | . fast_strptime | 2617500 | . ymd_hms | 26029600 | . ymd_hms | 26985200 | . fast_strptime | 3005900 | . fasttime | 2201400 | . fasttime | 2131400 | . ymd_hms | 29039200 | . fasttime | 2134500 | . fasttime | 2064700 | . fast_strptime | 2821100 | . fasttime | 2143200 | . fasttime | 2044300 | . ymd_hms | 29941500 | . fast_strptime | 3029800 | . ymd_hms | 41443300 | . fasttime | 2978200 | . fasttime | 3032300 | . fast_strptime | 4292500 | . fast_strptime | 3543800 | . ymd_hms | 35316000 | . ymd_hms | 37035500 | . fast_strptime | 3009300 | . ymd_hms | 27633800 | . fasttime | 2117500 | . fast_strptime | 2832600 | . fasttime | 2035900 | . fast_strptime | 2714100 | . fast_strptime | 2799400 | . Outputting pretty dates and times . An easy way to output dates is to use the stamp() function in lubridate. stamp() takes a string which should be an example of how the date should be formatted, and returns a function that can be used to format dates. . # Create a stamp based on &quot;Saturday, Jan 1, 2000&quot; date_stamp &lt;- stamp(&quot;Saturday, Jan 1, 2000&quot;) # Print date_stamp date_stamp . Multiple formats matched: &#34;%A, %b %d, %Y&#34;(1), &#34;Saturday, Jan %Om, %Y&#34;(1), &#34;Saturday, %Om %d, %Y&#34;(1), &#34;Saturday, %b %d, %Y&#34;(1), &#34;Saturday, Jan %m, %Y&#34;(1), &#34;%A, Jan %Om, %Y&#34;(0), &#34;%A, %Om %d, %Y&#34;(0), &#34;%A, Jan %m, %Y&#34;(0) Using: &#34;%A, %b %d, %Y&#34; . &lt;pre class=language-r&gt;function (x, locale = &quot;English_United States.1252&quot;) { &lt;span style=white-space:pre-wrap&gt; {&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; old_lc_time &lt;- Sys.getlocale(&quot;LC_TIME&quot;)&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; if (old_lc_time != locale) {&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; on.exit(Sys.setlocale(&quot;LC_TIME&quot;, old_lc_time))&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; Sys.setlocale(&quot;LC_TIME&quot;, locale)&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; }&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; }&lt;/span&gt; &lt;span style=white-space:pre-wrap&gt; format(x, format = &quot;%A, %b %d, %Y&quot;)&lt;/span&gt; }&lt;/pre&gt; # Call date_stamp on today() date_stamp(today()) . &#39;Tuesday, Jul 14, 2020&#39; # Create and call a stamp based on &quot;12/31/1999&quot; stamp(&quot;12/31/1999&quot;)(today()) . Multiple formats matched: &#34;%Om/%d/%Y&#34;(1), &#34;%m/%d/%Y&#34;(1) Using: &#34;%Om/%d/%Y&#34; . &#39;07/14/2020&#39; finished = &quot;I finished &#39;Dates and Times in R&#39; on Thursday, September 4, 2017!&quot; . # Use string finished for stamp() stamp(finished)(today()) . Multiple formats matched: &#34;I finished &#39;Dates and Times in R&#39; on %A, %B %d, %Y!&#34;(1), &#34;I finished &#39;Dates and Times in R&#39; on Thursday, September %Om, %Y!&#34;(1), &#34;I finished &#39;Dates and Times in R&#39; on Thursday, %Om %d, %Y!&#34;(1), &#34;I finished &#39;Dates and Times in R&#39; on Thursday, %B %d, %Y!&#34;(1), &#34;I finished &#39;Dates and Times in R&#39; on Thursday, September %m, %Y!&#34;(1), &#34;I finished &#39;Dates and Times in R&#39; on %A, September %Om, %Y!&#34;(0), &#34;I finished &#39;Dates and Times in R&#39; on %A, %Om %d, %Y!&#34;(0), &#34;I finished &#39;Dates and Times in R&#39; on %A, September %m, %Y!&#34;(0) Using: &#34;I finished &#39;Dates and Times in R&#39; on %A, %B %d, %Y!&#34; . &#39;I finished &#39;Dates and Times in R &#39; on Tuesday, July 14, 2020!&#39; Wrapping-up . Chapter 1: base R objects Date, POSIXct | Chapter 2: importing and manipulating datetimes | Chapter 3: arithmetic with datetimes, periods, durations andintervals | Chapter 4: time zones, fast parsing, outputting datetimes | . 1. source: https://xkcd.com/1179↩ . 2. Source: Monarchs of England↩ . 3. Source: Halley↩&lt;/p&gt;&lt;/div&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; .",
            "url": "https://victoromondi1997.github.io/blog/dates/times/r/2020/07/26/Working-with-Dates-and-Times-in-R.html",
            "relUrl": "/dates/times/r/2020/07/26/Working-with-Dates-and-Times-in-R.html",
            "date": " • Jul 26, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Working With Dates and Times in Python",
            "content": "Dates and Calendars . Hurricanes (also known as cyclones or typhoons) hit the U.S. state of Florida several times per year. To start off we will work with date objects in Python, starting with the dates of every hurricane to hit Florida since 1950. . Dates in Python . Which day of the week? . Hurricane Andrew, which hit Florida on August 24, 1992, was one of the costliest and deadliest hurricanes in US history. Which day of the week did it make landfall? . import datetime . hurricane_andrew = datetime.date(1992, 8, 24) hurricane_andrew.weekday() . 0 . How many hurricanes come early? . florida_hurricane_dates = [datetime.date(1950, 8, 31), datetime.date(1950, 9, 5), datetime.date(1950, 10, 18), datetime.date(1950, 10, 21), datetime.date(1951, 5, 18), datetime.date(1951, 10, 2), datetime.date(1952, 2, 3), datetime.date(1952, 8, 30), datetime.date(1953, 6, 6), datetime.date(1953, 8, 29), datetime.date(1953, 9, 20), datetime.date(1953, 9, 26), datetime.date(1953, 10, 9), datetime.date(1955, 8, 21), datetime.date(1956, 7, 6), datetime.date(1956, 9, 24), datetime.date(1956, 10, 15), datetime.date(1957, 6, 8), datetime.date(1957, 9, 8), datetime.date(1958, 9, 4), datetime.date(1959, 6, 18), datetime.date(1959, 10, 8), datetime.date(1959, 10, 18), datetime.date(1960, 7, 29), datetime.date(1960, 9, 10), datetime.date(1960, 9, 15), datetime.date(1960, 9, 23), datetime.date(1961, 9, 11), datetime.date(1961, 10, 29), datetime.date(1962, 8, 26), datetime.date(1963, 10, 21), datetime.date(1964, 6, 6), datetime.date(1964, 8, 27), datetime.date(1964, 9, 10), datetime.date(1964, 9, 20), datetime.date(1964, 10, 5), datetime.date(1964, 10, 14), datetime.date(1965, 6, 15), datetime.date(1965, 9, 8), datetime.date(1965, 9, 30), datetime.date(1966, 6, 9), datetime.date(1966, 6, 30), datetime.date(1966, 7, 24), datetime.date(1966, 10, 4), datetime.date(1968, 6, 4), datetime.date(1968, 6, 18), datetime.date(1968, 7, 5), datetime.date(1968, 8, 10), datetime.date(1968, 8, 28), datetime.date(1968, 9, 26), datetime.date(1968, 10, 19), datetime.date(1969, 6, 9), datetime.date(1969, 8, 18), datetime.date(1969, 8, 29), datetime.date(1969, 9, 7), datetime.date(1969, 9, 21), datetime.date(1969, 10, 1), datetime.date(1969, 10, 2), datetime.date(1969, 10, 21), datetime.date(1970, 5, 25), datetime.date(1970, 7, 22), datetime.date(1970, 8, 6), datetime.date(1970, 9, 13), datetime.date(1970, 9, 27), datetime.date(1971, 8, 10), datetime.date(1971, 8, 13), datetime.date(1971, 8, 29), datetime.date(1971, 9, 1), datetime.date(1971, 9, 16), datetime.date(1971, 10, 13), datetime.date(1972, 5, 28), datetime.date(1972, 6, 19), datetime.date(1972, 9, 5), datetime.date(1973, 6, 7), datetime.date(1973, 6, 23), datetime.date(1973, 9, 3), datetime.date(1973, 9, 25), datetime.date(1974, 6, 25), datetime.date(1974, 9, 8), datetime.date(1974, 9, 27), datetime.date(1974, 10, 7), datetime.date(1975, 6, 27), datetime.date(1975, 7, 29), datetime.date(1975, 9, 23), datetime.date(1975, 10, 1), datetime.date(1975, 10, 16), datetime.date(1976, 5, 23), datetime.date(1976, 6, 11), datetime.date(1976, 8, 19), datetime.date(1976, 9, 13), datetime.date(1977, 8, 27), datetime.date(1977, 9, 5), datetime.date(1978, 6, 22), datetime.date(1979, 7, 11), datetime.date(1979, 9, 3), datetime.date(1979, 9, 12), datetime.date(1979, 9, 24), datetime.date(1980, 8, 7), datetime.date(1980, 11, 18), datetime.date(1981, 8, 17), datetime.date(1982, 6, 18), datetime.date(1982, 9, 11), datetime.date(1983, 8, 28), datetime.date(1984, 9, 9), datetime.date(1984, 9, 27), datetime.date(1984, 10, 26), datetime.date(1985, 7, 23), datetime.date(1985, 8, 15), datetime.date(1985, 10, 10), datetime.date(1985, 11, 21), datetime.date(1986, 6, 26), datetime.date(1986, 8, 13), datetime.date(1987, 8, 14), datetime.date(1987, 9, 7), datetime.date(1987, 10, 12), datetime.date(1987, 11, 4), datetime.date(1988, 5, 30), datetime.date(1988, 8, 4), datetime.date(1988, 8, 13), datetime.date(1988, 8, 23), datetime.date(1988, 9, 4), datetime.date(1988, 9, 10), datetime.date(1988, 9, 13), datetime.date(1988, 11, 23), datetime.date(1989, 9, 22), datetime.date(1990, 5, 25), datetime.date(1990, 10, 9), datetime.date(1990, 10, 12), datetime.date(1991, 6, 30), datetime.date(1991, 10, 16), datetime.date(1992, 6, 25), datetime.date(1992, 8, 24), datetime.date(1992, 9, 29), datetime.date(1993, 6, 1), datetime.date(1994, 7, 3), datetime.date(1994, 8, 15), datetime.date(1994, 10, 2), datetime.date(1994, 11, 16), datetime.date(1995, 6, 5), datetime.date(1995, 7, 27), datetime.date(1995, 8, 2), datetime.date(1995, 8, 23), datetime.date(1995, 10, 4), datetime.date(1996, 7, 11), datetime.date(1996, 9, 2), datetime.date(1996, 10, 8), datetime.date(1996, 10, 18), datetime.date(1997, 7, 19), datetime.date(1998, 9, 3), datetime.date(1998, 9, 20), datetime.date(1998, 9, 25), datetime.date(1998, 11, 5), datetime.date(1999, 8, 29), datetime.date(1999, 9, 15), datetime.date(1999, 9, 21), datetime.date(1999, 10, 15), datetime.date(2000, 8, 23), datetime.date(2000, 9, 9), datetime.date(2000, 9, 18), datetime.date(2000, 9, 22), datetime.date(2000, 10, 3), datetime.date(2001, 6, 12), datetime.date(2001, 8, 6), datetime.date(2001, 9, 14), datetime.date(2001, 11, 5), datetime.date(2002, 7, 13), datetime.date(2002, 8, 4), datetime.date(2002, 9, 4), datetime.date(2002, 9, 14), datetime.date(2002, 9, 26), datetime.date(2002, 10, 3), datetime.date(2002, 10, 11), datetime.date(2003, 4, 20), datetime.date(2003, 6, 30), datetime.date(2003, 7, 25), datetime.date(2003, 8, 14), datetime.date(2003, 8, 30), datetime.date(2003, 9, 6), datetime.date(2003, 9, 13), datetime.date(2004, 8, 12), datetime.date(2004, 8, 13), datetime.date(2004, 9, 5), datetime.date(2004, 9, 13), datetime.date(2004, 9, 16), datetime.date(2004, 10, 10), datetime.date(2005, 6, 11), datetime.date(2005, 7, 6), datetime.date(2005, 7, 10), datetime.date(2005, 8, 25), datetime.date(2005, 9, 12), datetime.date(2005, 9, 20), datetime.date(2005, 10, 5), datetime.date(2005, 10, 24), datetime.date(2006, 6, 13), datetime.date(2006, 8, 30), datetime.date(2007, 5, 9), datetime.date(2007, 6, 2), datetime.date(2007, 8, 23), datetime.date(2007, 9, 8), datetime.date(2007, 9, 13), datetime.date(2007, 9, 22), datetime.date(2007, 10, 31), datetime.date(2007, 12, 13), datetime.date(2008, 7, 16), datetime.date(2008, 7, 22), datetime.date(2008, 8, 18), datetime.date(2008, 8, 31), datetime.date(2008, 9, 2), datetime.date(2009, 8, 16), datetime.date(2009, 8, 21), datetime.date(2009, 11, 9), datetime.date(2010, 6, 30), datetime.date(2010, 7, 23), datetime.date(2010, 8, 10), datetime.date(2010, 8, 31), datetime.date(2010, 9, 29), datetime.date(2011, 7, 18), datetime.date(2011, 8, 25), datetime.date(2011, 9, 3), datetime.date(2011, 10, 28), datetime.date(2011, 11, 9), datetime.date(2012, 5, 28), datetime.date(2012, 6, 23), datetime.date(2012, 8, 25), datetime.date(2012, 10, 25), datetime.date(2015, 8, 30), datetime.date(2015, 10, 1), datetime.date(2016, 6, 6), datetime.date(2016, 9, 1), datetime.date(2016, 9, 14), datetime.date(2016, 10, 7), datetime.date(2017, 6, 21), datetime.date(2017, 7, 31), datetime.date(2017, 9, 10), datetime.date(2017, 10, 29)] . Atlantic hurricane season officially begins on June 1. How many hurricanes since 1950 have made landfall in Florida before the official start of hurricane season? . early_hurricanes = 0 # We loop over dates for hurricane in florida_hurricane_dates: # Check if the month if before june if hurricane.month &lt; 6: early_hurricanes+=1 early_hurricanes . 10 . Math with dates . Subtracting dates . # Create a date object for May 9th, 2007 start = datetime.date(2007, 5, 9) # Create a date object for December 13th, 2007 end = datetime.date(2007, 12, 13) # Subtract the two dates and print the number of days (end - start).days . 218 . Counting events per calendar month . Hurricanes can make landfall in Florida throughout the year. As we&#39;ve already discussed, some months are more hurricane-prone than others. let&#39;s see how hurricanes in Florida were distributed across months throughout the year. We&#39;ve created a dictionary called hurricanes_each_month to hold counts and set the initial counts to zero. We will loop over the list of hurricanes, incrementing the correct month in hurricanes_each_month as we go, and then print the result. . # A dictionary to count hurricanes per calendar month hurricanes_each_month = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6:0,7: 0, 8:0, 9:0, 10:0, 11:0, 12:0} # Loop over all hurricanes for hurricane in florida_hurricane_dates: # Pull out the month month = hurricane.month # Increment the count in your dictionary by one hurricanes_each_month[month] += 1 print(hurricanes_each_month) . {1: 0, 2: 1, 3: 0, 4: 1, 5: 8, 6: 32, 7: 21, 8: 49, 9: 70, 10: 43, 11: 9, 12: 1} . Turning dates into strings . Printing dates in a friendly format . Let&#39;s see what event was recorded first in the Florida hurricane data set. We will format the earliest date in the florida_hurriance_dates list in two ways so we can decide which one you want to use: either the ISO standard or the typical US style. . # Assign the earliest date to first_date first_date = min(florida_hurricane_dates) # Convert to ISO and US formats iso = &quot;Our earliest hurricane date: &quot; + first_date.isoformat() us = &quot;Our earliest hurricane date: &quot; + first_date.strftime(&quot;%m/%d/%Y&quot;) print(&quot;ISO: &quot; + iso) print(&quot;US: &quot; + us) . ISO: Our earliest hurricane date: 1950-08-31 US: Our earliest hurricane date: 08/31/1950 . printing out the same date, August 26, 1992 (the day that Hurricane Andrew made landfall in Florida), in a number of different ways, to practice using the .strftime() method. . # Create a date object andrew = datetime.date(1992, 8, 26) # Print the date in the format &#39;YYYY-DDD&#39; print(andrew.strftime(&#39;%Y-%j&#39;)) . 1992-239 . Combining Dates and Times . Bike sharing programs have swept through cities around the world -- and luckily for us, every trip gets recorded! Working with all of the comings and goings of one bike in Washington, D.C., we&#39;ll be working with dates and times together. We&#39;ll parse dates and times from text, analyze peak trip times, calculate ride durations, and more. . Dates and times . Creating Datetimes by hand . # Create a datetime object dt = datetime.datetime(year=2017, month=10, day=1, hour=15, minute=26, second=26) # Print the results in ISO 8601 format print(dt.isoformat()) . 2017-10-01T15:26:26 . # Create a datetime object dt = datetime.datetime(2017, 12, 31, 15, 19, 13) # Replace the year with 1917 dt_old = dt.replace(year=1917) # Print the results in ISO 8601 format print(dt_old) . 1917-12-31 15:19:13 . # Pull out the start of the first trip first_start = onebike_datetimes[0][&#39;start&#39;] # Format to feed to strftime() fmt = &quot;%Y-%m-%dT%H:%M:%S&quot; # Print out date with .isoformat(), then with .strftime() to compare print(first_start.isoformat()) print(first_start.strftime(fmt)) . NameError Traceback (most recent call last) &lt;ipython-input-20-4a515eea12f7&gt; in &lt;module&gt; 1 # Pull out the start of the first trip -&gt; 2 first_start = onebike_datetimes[0][&#39;start&#39;] 3 4 # Format to feed to strftime() 5 fmt = &#34;%Y-%m-%dT%H:%M:%S&#34; NameError: name &#39;onebike_datetimes&#39; is not defined . Unix timestamps . Datetimes are sometimes stored as Unix timestamps: the number of seconds since January 1, 1970. This is especially common with computer infrastructure, like the log files that websites keep when they get visitors. . The largest number that some older computers can hold in one variable is 2147483648, which as a Unix timestamp is in January 2038. On that day, many computers which haven&#39;t been upgraded will fail. Hopefully, none of them are running anything critical! . # Starting timestamps timestamps = [1514665153, 1514664543] # Datetime objects dts = [] # Loop for ts in timestamps: dts.append(datetime.datetime.fromtimestamp(ts)) # Print results print(dts) . [datetime.datetime(2017, 12, 30, 23, 19, 13), datetime.datetime(2017, 12, 30, 23, 9, 3)] . Working with durations . Turning pairs of datetimes into durations . Remember that timedelta objects are represented in Python as a number of days and seconds of elapsed time. Be careful not to use .seconds on a timedelta object, since you&#39;ll just get the number of seconds without the days! . onebike_datetimes = [{&#39;end&#39;: datetime.datetime(2017, 10, 1, 15, 26, 26), &#39;start&#39;: datetime.datetime(2017, 10, 1, 15, 23, 25)}, {&#39;end&#39;: datetime.datetime(2017, 10, 1, 17, 49, 59), &#39;start&#39;: datetime.datetime(2017, 10, 1, 15, 42, 57)}, {&#39;end&#39;: datetime.datetime(2017, 10, 2, 6, 42, 53), &#39;start&#39;: datetime.datetime(2017, 10, 2, 6, 37, 10)}, {&#39;end&#39;: datetime.datetime(2017, 10, 2, 9, 18, 3), &#39;start&#39;: datetime.datetime(2017, 10, 2, 8, 56, 45)}, {&#39;end&#39;: datetime.datetime(2017, 10, 2, 18, 45, 5), &#39;start&#39;: datetime.datetime(2017, 10, 2, 18, 23, 48)}, {&#39;end&#39;: datetime.datetime(2017, 10, 2, 19, 10, 54), &#39;start&#39;: datetime.datetime(2017, 10, 2, 18, 48, 8)}, {&#39;end&#39;: datetime.datetime(2017, 10, 2, 19, 31, 45), &#39;start&#39;: datetime.datetime(2017, 10, 2, 19, 18, 10)}, {&#39;end&#39;: datetime.datetime(2017, 10, 2, 19, 46, 37), &#39;start&#39;: datetime.datetime(2017, 10, 2, 19, 37, 32)}, {&#39;end&#39;: datetime.datetime(2017, 10, 3, 8, 32, 27), &#39;start&#39;: datetime.datetime(2017, 10, 3, 8, 24, 16)}, {&#39;end&#39;: datetime.datetime(2017, 10, 3, 18, 27, 46), &#39;start&#39;: datetime.datetime(2017, 10, 3, 18, 17, 7)}, {&#39;end&#39;: datetime.datetime(2017, 10, 3, 19, 52, 8), &#39;start&#39;: datetime.datetime(2017, 10, 3, 19, 24, 10)}, {&#39;end&#39;: datetime.datetime(2017, 10, 3, 20, 23, 52), &#39;start&#39;: datetime.datetime(2017, 10, 3, 20, 17, 6)}, {&#39;end&#39;: datetime.datetime(2017, 10, 3, 20, 57, 10), &#39;start&#39;: datetime.datetime(2017, 10, 3, 20, 45, 21)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 7, 13, 31), &#39;start&#39;: datetime.datetime(2017, 10, 4, 7, 4, 57)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 7, 21, 54), &#39;start&#39;: datetime.datetime(2017, 10, 4, 7, 13, 42)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 14, 50), &#39;start&#39;: datetime.datetime(2017, 10, 4, 14, 22, 12)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 15, 44, 49), &#39;start&#39;: datetime.datetime(2017, 10, 4, 15, 7, 27)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 16, 32, 33), &#39;start&#39;: datetime.datetime(2017, 10, 4, 15, 46, 41)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 16, 46, 59), &#39;start&#39;: datetime.datetime(2017, 10, 4, 16, 34, 44)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 17, 31, 36), &#39;start&#39;: datetime.datetime(2017, 10, 4, 17, 26, 6)}, {&#39;end&#39;: datetime.datetime(2017, 10, 4, 17, 50, 41), &#39;start&#39;: datetime.datetime(2017, 10, 4, 17, 42, 3)}, {&#39;end&#39;: datetime.datetime(2017, 10, 5, 8, 12, 55), &#39;start&#39;: datetime.datetime(2017, 10, 5, 7, 49, 2)}, {&#39;end&#39;: datetime.datetime(2017, 10, 5, 8, 29, 45), &#39;start&#39;: datetime.datetime(2017, 10, 5, 8, 26, 21)}, {&#39;end&#39;: datetime.datetime(2017, 10, 5, 8, 38, 31), &#39;start&#39;: datetime.datetime(2017, 10, 5, 8, 33, 27)}, {&#39;end&#39;: datetime.datetime(2017, 10, 5, 16, 51, 52), &#39;start&#39;: datetime.datetime(2017, 10, 5, 16, 35, 35)}, {&#39;end&#39;: datetime.datetime(2017, 10, 5, 18, 16, 50), &#39;start&#39;: datetime.datetime(2017, 10, 5, 17, 53, 31)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 8, 38, 1), &#39;start&#39;: datetime.datetime(2017, 10, 6, 8, 17, 17)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 11, 50, 38), &#39;start&#39;: datetime.datetime(2017, 10, 6, 11, 39, 40)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 13, 13, 14), &#39;start&#39;: datetime.datetime(2017, 10, 6, 12, 59, 54)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 14, 14, 56), &#39;start&#39;: datetime.datetime(2017, 10, 6, 13, 43, 5)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 15, 9, 26), &#39;start&#39;: datetime.datetime(2017, 10, 6, 14, 28, 15)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 16, 12, 34), &#39;start&#39;: datetime.datetime(2017, 10, 6, 15, 50, 10)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 16, 39, 31), &#39;start&#39;: datetime.datetime(2017, 10, 6, 16, 32, 16)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 16, 48, 39), &#39;start&#39;: datetime.datetime(2017, 10, 6, 16, 44, 8)}, {&#39;end&#39;: datetime.datetime(2017, 10, 6, 17, 9, 3), &#39;start&#39;: datetime.datetime(2017, 10, 6, 16, 53, 43)}, {&#39;end&#39;: datetime.datetime(2017, 10, 7, 11, 53, 6), &#39;start&#39;: datetime.datetime(2017, 10, 7, 11, 38, 55)}, {&#39;end&#39;: datetime.datetime(2017, 10, 7, 14, 7, 5), &#39;start&#39;: datetime.datetime(2017, 10, 7, 14, 3, 36)}, {&#39;end&#39;: datetime.datetime(2017, 10, 7, 14, 27, 36), &#39;start&#39;: datetime.datetime(2017, 10, 7, 14, 20, 3)}, {&#39;end&#39;: datetime.datetime(2017, 10, 7, 14, 44, 51), &#39;start&#39;: datetime.datetime(2017, 10, 7, 14, 30, 50)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 0, 30, 48), &#39;start&#39;: datetime.datetime(2017, 10, 8, 0, 28, 26)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 11, 33, 24), &#39;start&#39;: datetime.datetime(2017, 10, 8, 11, 16, 21)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 13, 1, 29), &#39;start&#39;: datetime.datetime(2017, 10, 8, 12, 37, 3)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 13, 57, 53), &#39;start&#39;: datetime.datetime(2017, 10, 8, 13, 30, 37)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 15, 7, 19), &#39;start&#39;: datetime.datetime(2017, 10, 8, 14, 16, 40)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 15, 50, 1), &#39;start&#39;: datetime.datetime(2017, 10, 8, 15, 23, 50)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 16, 17, 42), &#39;start&#39;: datetime.datetime(2017, 10, 8, 15, 54, 12)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 16, 35, 18), &#39;start&#39;: datetime.datetime(2017, 10, 8, 16, 28, 52)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 23, 33, 41), &#39;start&#39;: datetime.datetime(2017, 10, 8, 23, 8, 14)}, {&#39;end&#39;: datetime.datetime(2017, 10, 8, 23, 45, 11), &#39;start&#39;: datetime.datetime(2017, 10, 8, 23, 34, 49)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 0, 10, 57), &#39;start&#39;: datetime.datetime(2017, 10, 8, 23, 46, 47)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 0, 36, 40), &#39;start&#39;: datetime.datetime(2017, 10, 9, 0, 12, 58)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 0, 53, 33), &#39;start&#39;: datetime.datetime(2017, 10, 9, 0, 37, 2)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 1, 48, 13), &#39;start&#39;: datetime.datetime(2017, 10, 9, 1, 23, 29)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 2, 13, 35), &#39;start&#39;: datetime.datetime(2017, 10, 9, 1, 49, 25)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 2, 29, 40), &#39;start&#39;: datetime.datetime(2017, 10, 9, 2, 14, 11)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 13, 13, 25), &#39;start&#39;: datetime.datetime(2017, 10, 9, 13, 4, 32)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 14, 38, 55), &#39;start&#39;: datetime.datetime(2017, 10, 9, 14, 30, 10)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 15, 11, 30), &#39;start&#39;: datetime.datetime(2017, 10, 9, 15, 6, 47)}, {&#39;end&#39;: datetime.datetime(2017, 10, 9, 16, 45, 38), &#39;start&#39;: datetime.datetime(2017, 10, 9, 16, 43, 25)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 15, 51, 24), &#39;start&#39;: datetime.datetime(2017, 10, 10, 15, 32, 58)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 17, 3, 47), &#39;start&#39;: datetime.datetime(2017, 10, 10, 16, 47, 55)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 18, 0, 18), &#39;start&#39;: datetime.datetime(2017, 10, 10, 17, 51, 5)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 18, 19, 11), &#39;start&#39;: datetime.datetime(2017, 10, 10, 18, 8, 12)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 19, 14, 32), &#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 9, 35)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 19, 23, 8), &#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 17, 11)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 19, 44, 40), &#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 28, 11)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 20, 11, 54), &#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 55, 35)}, {&#39;end&#39;: datetime.datetime(2017, 10, 10, 22, 33, 23), &#39;start&#39;: datetime.datetime(2017, 10, 10, 22, 20, 43)}, {&#39;end&#39;: datetime.datetime(2017, 10, 11, 4, 59, 22), &#39;start&#39;: datetime.datetime(2017, 10, 11, 4, 40, 52)}, {&#39;end&#39;: datetime.datetime(2017, 10, 11, 6, 40, 13), &#39;start&#39;: datetime.datetime(2017, 10, 11, 6, 28, 58)}, {&#39;end&#39;: datetime.datetime(2017, 10, 11, 17, 1, 14), &#39;start&#39;: datetime.datetime(2017, 10, 11, 16, 41, 7)}, {&#39;end&#39;: datetime.datetime(2017, 10, 12, 8, 35, 3), &#39;start&#39;: datetime.datetime(2017, 10, 12, 8, 8, 30)}, {&#39;end&#39;: datetime.datetime(2017, 10, 12, 8, 59, 50), &#39;start&#39;: datetime.datetime(2017, 10, 12, 8, 47, 2)}, {&#39;end&#39;: datetime.datetime(2017, 10, 12, 13, 37, 45), &#39;start&#39;: datetime.datetime(2017, 10, 12, 13, 13, 39)}, {&#39;end&#39;: datetime.datetime(2017, 10, 12, 13, 48, 17), &#39;start&#39;: datetime.datetime(2017, 10, 12, 13, 40, 12)}, {&#39;end&#39;: datetime.datetime(2017, 10, 12, 13, 53, 16), &#39;start&#39;: datetime.datetime(2017, 10, 12, 13, 49, 56)}, {&#39;end&#39;: datetime.datetime(2017, 10, 12, 14, 39, 57), &#39;start&#39;: datetime.datetime(2017, 10, 12, 14, 33, 18)}, {&#39;end&#39;: datetime.datetime(2017, 10, 13, 15, 59, 41), &#39;start&#39;: datetime.datetime(2017, 10, 13, 15, 55, 39)}, {&#39;end&#39;: datetime.datetime(2017, 10, 17, 18, 1, 38), &#39;start&#39;: datetime.datetime(2017, 10, 17, 17, 58, 48)}, {&#39;end&#39;: datetime.datetime(2017, 10, 19, 20, 29, 15), &#39;start&#39;: datetime.datetime(2017, 10, 19, 20, 21, 45)}, {&#39;end&#39;: datetime.datetime(2017, 10, 19, 21, 29, 37), &#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 11, 39)}, {&#39;end&#39;: datetime.datetime(2017, 10, 19, 21, 47, 23), &#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 30, 1)}, {&#39;end&#39;: datetime.datetime(2017, 10, 19, 21, 57, 7), &#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 47, 34)}, {&#39;end&#39;: datetime.datetime(2017, 10, 19, 22, 9, 52), &#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 57, 24)}, {&#39;end&#39;: datetime.datetime(2017, 10, 21, 12, 36, 24), &#39;start&#39;: datetime.datetime(2017, 10, 21, 12, 24, 9)}, {&#39;end&#39;: datetime.datetime(2017, 10, 21, 12, 42, 13), &#39;start&#39;: datetime.datetime(2017, 10, 21, 12, 36, 37)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 11, 9, 36), &#39;start&#39;: datetime.datetime(2017, 10, 21, 13, 47, 43)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 13, 31, 44), &#39;start&#39;: datetime.datetime(2017, 10, 22, 13, 28, 53)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 13, 56, 33), &#39;start&#39;: datetime.datetime(2017, 10, 22, 13, 47, 5)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 14, 32, 39), &#39;start&#39;: datetime.datetime(2017, 10, 22, 14, 26, 41)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 15, 9, 58), &#39;start&#39;: datetime.datetime(2017, 10, 22, 14, 54, 41)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 16, 51, 40), &#39;start&#39;: datetime.datetime(2017, 10, 22, 16, 40, 29)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 18, 28, 37), &#39;start&#39;: datetime.datetime(2017, 10, 22, 17, 58, 46)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 18, 50, 34), &#39;start&#39;: datetime.datetime(2017, 10, 22, 18, 45, 16)}, {&#39;end&#39;: datetime.datetime(2017, 10, 22, 19, 11, 10), &#39;start&#39;: datetime.datetime(2017, 10, 22, 18, 56, 22)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 10, 35, 32), &#39;start&#39;: datetime.datetime(2017, 10, 23, 10, 14, 8)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 14, 38, 34), &#39;start&#39;: datetime.datetime(2017, 10, 23, 11, 29, 36)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 15, 32, 58), &#39;start&#39;: datetime.datetime(2017, 10, 23, 15, 4, 52)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 17, 6, 47), &#39;start&#39;: datetime.datetime(2017, 10, 23, 15, 33, 48)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 19, 31, 26), &#39;start&#39;: datetime.datetime(2017, 10, 23, 17, 13, 16)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 20, 25, 53), &#39;start&#39;: datetime.datetime(2017, 10, 23, 19, 55, 3)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 22, 18, 4), &#39;start&#39;: datetime.datetime(2017, 10, 23, 21, 47, 54)}, {&#39;end&#39;: datetime.datetime(2017, 10, 23, 22, 48, 42), &#39;start&#39;: datetime.datetime(2017, 10, 23, 22, 34, 12)}, {&#39;end&#39;: datetime.datetime(2017, 10, 24, 7, 2, 17), &#39;start&#39;: datetime.datetime(2017, 10, 24, 6, 55, 1)}, {&#39;end&#39;: datetime.datetime(2017, 10, 24, 15, 3, 16), &#39;start&#39;: datetime.datetime(2017, 10, 24, 14, 56, 7)}, {&#39;end&#39;: datetime.datetime(2017, 10, 24, 15, 59, 50), &#39;start&#39;: datetime.datetime(2017, 10, 24, 15, 51, 36)}, {&#39;end&#39;: datetime.datetime(2017, 10, 24, 16, 55, 9), &#39;start&#39;: datetime.datetime(2017, 10, 24, 16, 31, 10)}, {&#39;end&#39;: datetime.datetime(2017, 10, 28, 14, 32, 34), &#39;start&#39;: datetime.datetime(2017, 10, 28, 14, 26, 14)}, {&#39;end&#39;: datetime.datetime(2017, 11, 1, 9, 52, 23), &#39;start&#39;: datetime.datetime(2017, 11, 1, 9, 41, 54)}, {&#39;end&#39;: datetime.datetime(2017, 11, 1, 20, 32, 13), &#39;start&#39;: datetime.datetime(2017, 11, 1, 20, 16, 11)}, {&#39;end&#39;: datetime.datetime(2017, 11, 2, 19, 50, 56), &#39;start&#39;: datetime.datetime(2017, 11, 2, 19, 44, 29)}, {&#39;end&#39;: datetime.datetime(2017, 11, 2, 20, 30, 29), &#39;start&#39;: datetime.datetime(2017, 11, 2, 20, 14, 37)}, {&#39;end&#39;: datetime.datetime(2017, 11, 2, 21, 38, 57), &#39;start&#39;: datetime.datetime(2017, 11, 2, 21, 35, 47)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 10, 11, 46), &#39;start&#39;: datetime.datetime(2017, 11, 3, 9, 59, 27)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 10, 32, 2), &#39;start&#39;: datetime.datetime(2017, 11, 3, 10, 13, 22)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 10, 50, 34), &#39;start&#39;: datetime.datetime(2017, 11, 3, 10, 44, 25)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 16, 44, 38), &#39;start&#39;: datetime.datetime(2017, 11, 3, 16, 6, 43)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 17, 0, 27), &#39;start&#39;: datetime.datetime(2017, 11, 3, 16, 45, 54)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 17, 35, 5), &#39;start&#39;: datetime.datetime(2017, 11, 3, 17, 7, 15)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 17, 46, 48), &#39;start&#39;: datetime.datetime(2017, 11, 3, 17, 36, 5)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 18, 0, 3), &#39;start&#39;: datetime.datetime(2017, 11, 3, 17, 50, 31)}, {&#39;end&#39;: datetime.datetime(2017, 11, 3, 19, 45, 51), &#39;start&#39;: datetime.datetime(2017, 11, 3, 19, 22, 56)}, {&#39;end&#39;: datetime.datetime(2017, 11, 4, 13, 26, 15), &#39;start&#39;: datetime.datetime(2017, 11, 4, 13, 14, 10)}, {&#39;end&#39;: datetime.datetime(2017, 11, 4, 14, 30, 5), &#39;start&#39;: datetime.datetime(2017, 11, 4, 14, 18, 37)}, {&#39;end&#39;: datetime.datetime(2017, 11, 4, 15, 3, 20), &#39;start&#39;: datetime.datetime(2017, 11, 4, 14, 45, 59)}, {&#39;end&#39;: datetime.datetime(2017, 11, 4, 15, 44, 30), &#39;start&#39;: datetime.datetime(2017, 11, 4, 15, 16, 3)}, {&#39;end&#39;: datetime.datetime(2017, 11, 4, 16, 58, 22), &#39;start&#39;: datetime.datetime(2017, 11, 4, 16, 37, 46)}, {&#39;end&#39;: datetime.datetime(2017, 11, 4, 17, 34, 50), &#39;start&#39;: datetime.datetime(2017, 11, 4, 17, 13, 19)}, {&#39;end&#39;: datetime.datetime(2017, 11, 4, 18, 58, 44), &#39;start&#39;: datetime.datetime(2017, 11, 4, 18, 10, 34)}, {&#39;end&#39;: datetime.datetime(2017, 11, 5, 1, 1, 4), &#39;start&#39;: datetime.datetime(2017, 11, 5, 1, 56, 50)}, {&#39;end&#39;: datetime.datetime(2017, 11, 5, 8, 53, 46), &#39;start&#39;: datetime.datetime(2017, 11, 5, 8, 33, 33)}, {&#39;end&#39;: datetime.datetime(2017, 11, 5, 9, 3, 39), &#39;start&#39;: datetime.datetime(2017, 11, 5, 8, 58, 8)}, {&#39;end&#39;: datetime.datetime(2017, 11, 5, 11, 30, 5), &#39;start&#39;: datetime.datetime(2017, 11, 5, 11, 5, 8)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 8, 59, 5), &#39;start&#39;: datetime.datetime(2017, 11, 6, 8, 50, 18)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 9, 13, 47), &#39;start&#39;: datetime.datetime(2017, 11, 6, 9, 4, 3)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 17, 2, 55), &#39;start&#39;: datetime.datetime(2017, 11, 6, 16, 19, 36)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 17, 34, 6), &#39;start&#39;: datetime.datetime(2017, 11, 6, 17, 21, 27)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 17, 57, 32), &#39;start&#39;: datetime.datetime(2017, 11, 6, 17, 36, 1)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 18, 15, 8), &#39;start&#39;: datetime.datetime(2017, 11, 6, 17, 59, 52)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 18, 21, 17), &#39;start&#39;: datetime.datetime(2017, 11, 6, 18, 18, 36)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 19, 37, 57), &#39;start&#39;: datetime.datetime(2017, 11, 6, 19, 24, 31)}, {&#39;end&#39;: datetime.datetime(2017, 11, 6, 20, 3, 14), &#39;start&#39;: datetime.datetime(2017, 11, 6, 19, 49, 16)}, {&#39;end&#39;: datetime.datetime(2017, 11, 7, 8, 1, 32), &#39;start&#39;: datetime.datetime(2017, 11, 7, 7, 50, 48)}, {&#39;end&#39;: datetime.datetime(2017, 11, 8, 13, 18, 5), &#39;start&#39;: datetime.datetime(2017, 11, 8, 13, 11, 51)}, {&#39;end&#39;: datetime.datetime(2017, 11, 8, 21, 46, 5), &#39;start&#39;: datetime.datetime(2017, 11, 8, 21, 34, 47)}, {&#39;end&#39;: datetime.datetime(2017, 11, 8, 22, 4, 47), &#39;start&#39;: datetime.datetime(2017, 11, 8, 22, 2, 30)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 7, 12, 10), &#39;start&#39;: datetime.datetime(2017, 11, 9, 7, 1, 11)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 8, 8, 28), &#39;start&#39;: datetime.datetime(2017, 11, 9, 8, 2, 2)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 8, 32, 24), &#39;start&#39;: datetime.datetime(2017, 11, 9, 8, 19, 59)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 8, 48, 59), &#39;start&#39;: datetime.datetime(2017, 11, 9, 8, 41, 31)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 9, 9, 24), &#39;start&#39;: datetime.datetime(2017, 11, 9, 9, 0, 6)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 9, 24, 25), &#39;start&#39;: datetime.datetime(2017, 11, 9, 9, 9, 37)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 13, 25, 39), &#39;start&#39;: datetime.datetime(2017, 11, 9, 13, 14, 37)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 15, 31, 10), &#39;start&#39;: datetime.datetime(2017, 11, 9, 15, 20, 7)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 18, 53, 10), &#39;start&#39;: datetime.datetime(2017, 11, 9, 18, 47, 8)}, {&#39;end&#39;: datetime.datetime(2017, 11, 9, 23, 43, 35), &#39;start&#39;: datetime.datetime(2017, 11, 9, 23, 35, 2)}, {&#39;end&#39;: datetime.datetime(2017, 11, 10, 8, 2, 28), &#39;start&#39;: datetime.datetime(2017, 11, 10, 7, 51, 33)}, {&#39;end&#39;: datetime.datetime(2017, 11, 10, 8, 42, 9), &#39;start&#39;: datetime.datetime(2017, 11, 10, 8, 38, 28)}, {&#39;end&#39;: datetime.datetime(2017, 11, 11, 18, 13, 14), &#39;start&#39;: datetime.datetime(2017, 11, 11, 18, 5, 25)}, {&#39;end&#39;: datetime.datetime(2017, 11, 11, 19, 46, 22), &#39;start&#39;: datetime.datetime(2017, 11, 11, 19, 39, 12)}, {&#39;end&#39;: datetime.datetime(2017, 11, 11, 21, 16, 31), &#39;start&#39;: datetime.datetime(2017, 11, 11, 21, 13, 19)}, {&#39;end&#39;: datetime.datetime(2017, 11, 12, 9, 51, 43), &#39;start&#39;: datetime.datetime(2017, 11, 12, 9, 46, 19)}, {&#39;end&#39;: datetime.datetime(2017, 11, 13, 13, 54, 15), &#39;start&#39;: datetime.datetime(2017, 11, 13, 13, 33, 42)}, {&#39;end&#39;: datetime.datetime(2017, 11, 14, 8, 55, 52), &#39;start&#39;: datetime.datetime(2017, 11, 14, 8, 40, 29)}, {&#39;end&#39;: datetime.datetime(2017, 11, 15, 6, 30, 6), &#39;start&#39;: datetime.datetime(2017, 11, 15, 6, 14, 5)}, {&#39;end&#39;: datetime.datetime(2017, 11, 15, 8, 23, 44), &#39;start&#39;: datetime.datetime(2017, 11, 15, 8, 14, 59)}, {&#39;end&#39;: datetime.datetime(2017, 11, 15, 10, 33, 41), &#39;start&#39;: datetime.datetime(2017, 11, 15, 10, 16, 44)}, {&#39;end&#39;: datetime.datetime(2017, 11, 15, 10, 54, 14), &#39;start&#39;: datetime.datetime(2017, 11, 15, 10, 33, 58)}, {&#39;end&#39;: datetime.datetime(2017, 11, 15, 11, 14, 42), &#39;start&#39;: datetime.datetime(2017, 11, 15, 11, 2, 15)}, {&#39;end&#39;: datetime.datetime(2017, 11, 16, 9, 38, 49), &#39;start&#39;: datetime.datetime(2017, 11, 16, 9, 27, 41)}, {&#39;end&#39;: datetime.datetime(2017, 11, 16, 10, 18), &#39;start&#39;: datetime.datetime(2017, 11, 16, 9, 57, 41)}, {&#39;end&#39;: datetime.datetime(2017, 11, 16, 17, 44, 47), &#39;start&#39;: datetime.datetime(2017, 11, 16, 17, 25, 5)}, {&#39;end&#39;: datetime.datetime(2017, 11, 17, 16, 36, 56), &#39;start&#39;: datetime.datetime(2017, 11, 17, 13, 45, 54)}, {&#39;end&#39;: datetime.datetime(2017, 11, 17, 19, 31, 15), &#39;start&#39;: datetime.datetime(2017, 11, 17, 19, 12, 49)}, {&#39;end&#39;: datetime.datetime(2017, 11, 18, 10, 55, 45), &#39;start&#39;: datetime.datetime(2017, 11, 18, 10, 49, 6)}, {&#39;end&#39;: datetime.datetime(2017, 11, 18, 11, 44, 16), &#39;start&#39;: datetime.datetime(2017, 11, 18, 11, 32, 12)}, {&#39;end&#39;: datetime.datetime(2017, 11, 18, 18, 14, 31), &#39;start&#39;: datetime.datetime(2017, 11, 18, 18, 9, 1)}, {&#39;end&#39;: datetime.datetime(2017, 11, 18, 19, 1, 29), &#39;start&#39;: datetime.datetime(2017, 11, 18, 18, 53, 10)}, {&#39;end&#39;: datetime.datetime(2017, 11, 19, 14, 31, 49), &#39;start&#39;: datetime.datetime(2017, 11, 19, 14, 15, 41)}, {&#39;end&#39;: datetime.datetime(2017, 11, 20, 21, 41, 9), &#39;start&#39;: datetime.datetime(2017, 11, 20, 21, 19, 19)}, {&#39;end&#39;: datetime.datetime(2017, 11, 20, 23, 23, 37), &#39;start&#39;: datetime.datetime(2017, 11, 20, 22, 39, 48)}, {&#39;end&#39;: datetime.datetime(2017, 11, 21, 17, 51, 32), &#39;start&#39;: datetime.datetime(2017, 11, 21, 17, 44, 25)}, {&#39;end&#39;: datetime.datetime(2017, 11, 21, 18, 34, 51), &#39;start&#39;: datetime.datetime(2017, 11, 21, 18, 20, 52)}, {&#39;end&#39;: datetime.datetime(2017, 11, 21, 18, 51, 50), &#39;start&#39;: datetime.datetime(2017, 11, 21, 18, 47, 32)}, {&#39;end&#39;: datetime.datetime(2017, 11, 21, 19, 14, 33), &#39;start&#39;: datetime.datetime(2017, 11, 21, 19, 7, 57)}, {&#39;end&#39;: datetime.datetime(2017, 11, 21, 20, 8, 54), &#39;start&#39;: datetime.datetime(2017, 11, 21, 20, 4, 56)}, {&#39;end&#39;: datetime.datetime(2017, 11, 21, 22, 8, 12), &#39;start&#39;: datetime.datetime(2017, 11, 21, 21, 55, 47)}, {&#39;end&#39;: datetime.datetime(2017, 11, 23, 23, 57, 56), &#39;start&#39;: datetime.datetime(2017, 11, 23, 23, 47, 43)}, {&#39;end&#39;: datetime.datetime(2017, 11, 24, 6, 53, 15), &#39;start&#39;: datetime.datetime(2017, 11, 24, 6, 41, 25)}, {&#39;end&#39;: datetime.datetime(2017, 11, 24, 7, 33, 24), &#39;start&#39;: datetime.datetime(2017, 11, 24, 6, 58, 56)}, {&#39;end&#39;: datetime.datetime(2017, 11, 26, 12, 41, 36), &#39;start&#39;: datetime.datetime(2017, 11, 26, 12, 25, 49)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 5, 54, 13), &#39;start&#39;: datetime.datetime(2017, 11, 27, 5, 29, 4)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 6, 11, 1), &#39;start&#39;: datetime.datetime(2017, 11, 27, 6, 6, 47)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 6, 55, 39), &#39;start&#39;: datetime.datetime(2017, 11, 27, 6, 45, 14)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 9, 47, 43), &#39;start&#39;: datetime.datetime(2017, 11, 27, 9, 39, 44)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 11, 20, 46), &#39;start&#39;: datetime.datetime(2017, 11, 27, 11, 9, 18)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 11, 35, 44), &#39;start&#39;: datetime.datetime(2017, 11, 27, 11, 31, 46)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 12, 12, 36), &#39;start&#39;: datetime.datetime(2017, 11, 27, 12, 7, 14)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 12, 26, 44), &#39;start&#39;: datetime.datetime(2017, 11, 27, 12, 21, 40)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 17, 36, 7), &#39;start&#39;: datetime.datetime(2017, 11, 27, 17, 26, 31)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 18, 29, 4), &#39;start&#39;: datetime.datetime(2017, 11, 27, 18, 11, 49)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 19, 47, 17), &#39;start&#39;: datetime.datetime(2017, 11, 27, 19, 36, 16)}, {&#39;end&#39;: datetime.datetime(2017, 11, 27, 20, 17, 33), &#39;start&#39;: datetime.datetime(2017, 11, 27, 20, 12, 57)}, {&#39;end&#39;: datetime.datetime(2017, 11, 28, 8, 41, 53), &#39;start&#39;: datetime.datetime(2017, 11, 28, 8, 18, 6)}, {&#39;end&#39;: datetime.datetime(2017, 11, 28, 19, 34, 1), &#39;start&#39;: datetime.datetime(2017, 11, 28, 19, 17, 23)}, {&#39;end&#39;: datetime.datetime(2017, 11, 28, 19, 46, 24), &#39;start&#39;: datetime.datetime(2017, 11, 28, 19, 34, 15)}, {&#39;end&#39;: datetime.datetime(2017, 11, 28, 21, 39, 32), &#39;start&#39;: datetime.datetime(2017, 11, 28, 21, 27, 29)}, {&#39;end&#39;: datetime.datetime(2017, 11, 29, 7, 51, 18), &#39;start&#39;: datetime.datetime(2017, 11, 29, 7, 47, 38)}, {&#39;end&#39;: datetime.datetime(2017, 11, 29, 9, 53, 44), &#39;start&#39;: datetime.datetime(2017, 11, 29, 9, 50, 12)}, {&#39;end&#39;: datetime.datetime(2017, 11, 29, 17, 16, 21), &#39;start&#39;: datetime.datetime(2017, 11, 29, 17, 3, 42)}, {&#39;end&#39;: datetime.datetime(2017, 11, 29, 18, 23, 43), &#39;start&#39;: datetime.datetime(2017, 11, 29, 18, 19, 15)}, {&#39;end&#39;: datetime.datetime(2017, 12, 1, 17, 10, 12), &#39;start&#39;: datetime.datetime(2017, 12, 1, 17, 3, 58)}, {&#39;end&#39;: datetime.datetime(2017, 12, 2, 8, 1, 1), &#39;start&#39;: datetime.datetime(2017, 12, 2, 7, 55, 56)}, {&#39;end&#39;: datetime.datetime(2017, 12, 2, 9, 21, 18), &#39;start&#39;: datetime.datetime(2017, 12, 2, 9, 16, 14)}, {&#39;end&#39;: datetime.datetime(2017, 12, 2, 19, 53, 18), &#39;start&#39;: datetime.datetime(2017, 12, 2, 19, 48, 29)}, {&#39;end&#39;: datetime.datetime(2017, 12, 3, 15, 20, 9), &#39;start&#39;: datetime.datetime(2017, 12, 3, 14, 36, 29)}, {&#39;end&#39;: datetime.datetime(2017, 12, 3, 16, 25, 30), &#39;start&#39;: datetime.datetime(2017, 12, 3, 16, 4, 2)}, {&#39;end&#39;: datetime.datetime(2017, 12, 3, 16, 43, 58), &#39;start&#39;: datetime.datetime(2017, 12, 3, 16, 40, 26)}, {&#39;end&#39;: datetime.datetime(2017, 12, 3, 18, 4, 33), &#39;start&#39;: datetime.datetime(2017, 12, 3, 17, 20, 17)}, {&#39;end&#39;: datetime.datetime(2017, 12, 4, 8, 51), &#39;start&#39;: datetime.datetime(2017, 12, 4, 8, 34, 24)}, {&#39;end&#39;: datetime.datetime(2017, 12, 4, 17, 53, 57), &#39;start&#39;: datetime.datetime(2017, 12, 4, 17, 49, 26)}, {&#39;end&#39;: datetime.datetime(2017, 12, 4, 18, 50, 33), &#39;start&#39;: datetime.datetime(2017, 12, 4, 18, 38, 52)}, {&#39;end&#39;: datetime.datetime(2017, 12, 4, 21, 46, 58), &#39;start&#39;: datetime.datetime(2017, 12, 4, 21, 39, 20)}, {&#39;end&#39;: datetime.datetime(2017, 12, 4, 21, 56, 17), &#39;start&#39;: datetime.datetime(2017, 12, 4, 21, 54, 21)}, {&#39;end&#39;: datetime.datetime(2017, 12, 5, 8, 52, 54), &#39;start&#39;: datetime.datetime(2017, 12, 5, 8, 50, 50)}, {&#39;end&#39;: datetime.datetime(2017, 12, 6, 8, 24, 14), &#39;start&#39;: datetime.datetime(2017, 12, 6, 8, 19, 38)}, {&#39;end&#39;: datetime.datetime(2017, 12, 6, 18, 28, 11), &#39;start&#39;: datetime.datetime(2017, 12, 6, 18, 19, 19)}, {&#39;end&#39;: datetime.datetime(2017, 12, 6, 18, 33, 12), &#39;start&#39;: datetime.datetime(2017, 12, 6, 18, 28, 55)}, {&#39;end&#39;: datetime.datetime(2017, 12, 6, 20, 21, 38), &#39;start&#39;: datetime.datetime(2017, 12, 6, 20, 3, 29)}, {&#39;end&#39;: datetime.datetime(2017, 12, 6, 20, 39, 57), &#39;start&#39;: datetime.datetime(2017, 12, 6, 20, 36, 42)}, {&#39;end&#39;: datetime.datetime(2017, 12, 7, 6, 1, 15), &#39;start&#39;: datetime.datetime(2017, 12, 7, 5, 54, 51)}, {&#39;end&#39;: datetime.datetime(2017, 12, 8, 16, 55, 49), &#39;start&#39;: datetime.datetime(2017, 12, 8, 16, 47, 18)}, {&#39;end&#39;: datetime.datetime(2017, 12, 8, 19, 29, 12), &#39;start&#39;: datetime.datetime(2017, 12, 8, 19, 15, 2)}, {&#39;end&#39;: datetime.datetime(2017, 12, 9, 22, 47, 19), &#39;start&#39;: datetime.datetime(2017, 12, 9, 22, 39, 37)}, {&#39;end&#39;: datetime.datetime(2017, 12, 9, 23, 5, 32), &#39;start&#39;: datetime.datetime(2017, 12, 9, 23, 0, 10)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 0, 56, 2), &#39;start&#39;: datetime.datetime(2017, 12, 10, 0, 39, 24)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 1, 8, 9), &#39;start&#39;: datetime.datetime(2017, 12, 10, 1, 2, 42)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 1, 11, 30), &#39;start&#39;: datetime.datetime(2017, 12, 10, 1, 8, 57)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 13, 51, 41), &#39;start&#39;: datetime.datetime(2017, 12, 10, 13, 49, 9)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 15, 18, 19), &#39;start&#39;: datetime.datetime(2017, 12, 10, 15, 14, 29)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 15, 36, 28), &#39;start&#39;: datetime.datetime(2017, 12, 10, 15, 31, 7)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 16, 30, 31), &#39;start&#39;: datetime.datetime(2017, 12, 10, 16, 20, 6)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 17, 14, 25), &#39;start&#39;: datetime.datetime(2017, 12, 10, 17, 7, 54)}, {&#39;end&#39;: datetime.datetime(2017, 12, 10, 17, 45, 25), &#39;start&#39;: datetime.datetime(2017, 12, 10, 17, 23, 47)}, {&#39;end&#39;: datetime.datetime(2017, 12, 11, 6, 34, 4), &#39;start&#39;: datetime.datetime(2017, 12, 11, 6, 17, 6)}, {&#39;end&#39;: datetime.datetime(2017, 12, 11, 9, 12, 21), &#39;start&#39;: datetime.datetime(2017, 12, 11, 9, 8, 41)}, {&#39;end&#39;: datetime.datetime(2017, 12, 11, 9, 20, 18), &#39;start&#39;: datetime.datetime(2017, 12, 11, 9, 15, 41)}, {&#39;end&#39;: datetime.datetime(2017, 12, 12, 8, 59, 34), &#39;start&#39;: datetime.datetime(2017, 12, 12, 8, 55, 53)}, {&#39;end&#39;: datetime.datetime(2017, 12, 13, 17, 18, 32), &#39;start&#39;: datetime.datetime(2017, 12, 13, 17, 14, 56)}, {&#39;end&#39;: datetime.datetime(2017, 12, 13, 19, 0, 45), &#39;start&#39;: datetime.datetime(2017, 12, 13, 18, 52, 16)}, {&#39;end&#39;: datetime.datetime(2017, 12, 14, 9, 11, 6), &#39;start&#39;: datetime.datetime(2017, 12, 14, 9, 1, 10)}, {&#39;end&#39;: datetime.datetime(2017, 12, 14, 9, 19, 6), &#39;start&#39;: datetime.datetime(2017, 12, 14, 9, 12, 59)}, {&#39;end&#39;: datetime.datetime(2017, 12, 14, 12, 2), &#39;start&#39;: datetime.datetime(2017, 12, 14, 11, 54, 33)}, {&#39;end&#39;: datetime.datetime(2017, 12, 14, 14, 44, 40), &#39;start&#39;: datetime.datetime(2017, 12, 14, 14, 40, 23)}, {&#39;end&#39;: datetime.datetime(2017, 12, 14, 15, 26, 24), &#39;start&#39;: datetime.datetime(2017, 12, 14, 15, 8, 55)}, {&#39;end&#39;: datetime.datetime(2017, 12, 14, 18, 9, 4), &#39;start&#39;: datetime.datetime(2017, 12, 14, 17, 46, 17)}, {&#39;end&#39;: datetime.datetime(2017, 12, 15, 9, 23, 45), &#39;start&#39;: datetime.datetime(2017, 12, 15, 9, 8, 12)}, {&#39;end&#39;: datetime.datetime(2017, 12, 16, 9, 36, 17), &#39;start&#39;: datetime.datetime(2017, 12, 16, 9, 33, 46)}, {&#39;end&#39;: datetime.datetime(2017, 12, 16, 11, 5, 4), &#39;start&#39;: datetime.datetime(2017, 12, 16, 11, 2, 31)}, {&#39;end&#39;: datetime.datetime(2017, 12, 17, 10, 32, 3), &#39;start&#39;: datetime.datetime(2017, 12, 17, 10, 9, 47)}, {&#39;end&#39;: datetime.datetime(2017, 12, 18, 8, 7, 34), &#39;start&#39;: datetime.datetime(2017, 12, 18, 8, 2, 36)}, {&#39;end&#39;: datetime.datetime(2017, 12, 18, 16, 9, 20), &#39;start&#39;: datetime.datetime(2017, 12, 18, 16, 3)}, {&#39;end&#39;: datetime.datetime(2017, 12, 18, 16, 53, 12), &#39;start&#39;: datetime.datetime(2017, 12, 18, 16, 30, 7)}, {&#39;end&#39;: datetime.datetime(2017, 12, 18, 19, 22, 8), &#39;start&#39;: datetime.datetime(2017, 12, 18, 19, 18, 23)}, {&#39;end&#39;: datetime.datetime(2017, 12, 18, 20, 17, 47), &#39;start&#39;: datetime.datetime(2017, 12, 18, 20, 14, 46)}, {&#39;end&#39;: datetime.datetime(2017, 12, 19, 19, 23, 49), &#39;start&#39;: datetime.datetime(2017, 12, 19, 19, 14, 8)}, {&#39;end&#39;: datetime.datetime(2017, 12, 19, 19, 43, 46), &#39;start&#39;: datetime.datetime(2017, 12, 19, 19, 39, 36)}, {&#39;end&#39;: datetime.datetime(2017, 12, 20, 8, 10, 46), &#39;start&#39;: datetime.datetime(2017, 12, 20, 8, 5, 14)}, {&#39;end&#39;: datetime.datetime(2017, 12, 20, 8, 29, 50), &#39;start&#39;: datetime.datetime(2017, 12, 20, 8, 15, 45)}, {&#39;end&#39;: datetime.datetime(2017, 12, 20, 8, 38, 9), &#39;start&#39;: datetime.datetime(2017, 12, 20, 8, 33, 32)}, {&#39;end&#39;: datetime.datetime(2017, 12, 20, 13, 54, 39), &#39;start&#39;: datetime.datetime(2017, 12, 20, 13, 43, 36)}, {&#39;end&#39;: datetime.datetime(2017, 12, 20, 19, 6, 54), &#39;start&#39;: datetime.datetime(2017, 12, 20, 18, 57, 53)}, {&#39;end&#39;: datetime.datetime(2017, 12, 21, 7, 32, 3), &#39;start&#39;: datetime.datetime(2017, 12, 21, 7, 21, 11)}, {&#39;end&#39;: datetime.datetime(2017, 12, 21, 8, 6, 15), &#39;start&#39;: datetime.datetime(2017, 12, 21, 8, 1, 58)}, {&#39;end&#39;: datetime.datetime(2017, 12, 21, 13, 33, 49), &#39;start&#39;: datetime.datetime(2017, 12, 21, 13, 20, 54)}, {&#39;end&#39;: datetime.datetime(2017, 12, 21, 15, 34, 27), &#39;start&#39;: datetime.datetime(2017, 12, 21, 15, 26, 8)}, {&#39;end&#39;: datetime.datetime(2017, 12, 21, 18, 38, 50), &#39;start&#39;: datetime.datetime(2017, 12, 21, 18, 9, 46)}, {&#39;end&#39;: datetime.datetime(2017, 12, 22, 16, 21, 46), &#39;start&#39;: datetime.datetime(2017, 12, 22, 16, 14, 21)}, {&#39;end&#39;: datetime.datetime(2017, 12, 22, 16, 34, 14), &#39;start&#39;: datetime.datetime(2017, 12, 22, 16, 29, 17)}, {&#39;end&#39;: datetime.datetime(2017, 12, 25, 13, 18, 27), &#39;start&#39;: datetime.datetime(2017, 12, 25, 12, 49, 51)}, {&#39;end&#39;: datetime.datetime(2017, 12, 25, 14, 20, 50), &#39;start&#39;: datetime.datetime(2017, 12, 25, 13, 46, 44)}, {&#39;end&#39;: datetime.datetime(2017, 12, 26, 10, 53, 45), &#39;start&#39;: datetime.datetime(2017, 12, 26, 10, 40, 16)}, {&#39;end&#39;: datetime.datetime(2017, 12, 27, 17, 17, 39), &#39;start&#39;: datetime.datetime(2017, 12, 27, 16, 56, 12)}, {&#39;end&#39;: datetime.datetime(2017, 12, 29, 6, 12, 30), &#39;start&#39;: datetime.datetime(2017, 12, 29, 6, 2, 34)}, {&#39;end&#39;: datetime.datetime(2017, 12, 29, 12, 46, 16), &#39;start&#39;: datetime.datetime(2017, 12, 29, 12, 21, 3)}, {&#39;end&#39;: datetime.datetime(2017, 12, 29, 14, 43, 46), &#39;start&#39;: datetime.datetime(2017, 12, 29, 14, 32, 55)}, {&#39;end&#39;: datetime.datetime(2017, 12, 29, 15, 18, 51), &#39;start&#39;: datetime.datetime(2017, 12, 29, 15, 8, 26)}, {&#39;end&#39;: datetime.datetime(2017, 12, 29, 20, 38, 13), &#39;start&#39;: datetime.datetime(2017, 12, 29, 20, 33, 34)}, {&#39;end&#39;: datetime.datetime(2017, 12, 30, 13, 54, 33), &#39;start&#39;: datetime.datetime(2017, 12, 30, 13, 51, 3)}, {&#39;end&#39;: datetime.datetime(2017, 12, 30, 15, 19, 13), &#39;start&#39;: datetime.datetime(2017, 12, 30, 15, 9, 3)}] . # Initialize a list for all the trip durations onebike_durations = [] for trip in onebike_datetimes: # Create a timedelta object corresponding to the length of the trip trip_duration = trip[&#39;end&#39;] - trip[&#39;start&#39;] # Get the total elapsed seconds in trip_duration trip_length_seconds = trip_duration.total_seconds() # Append the results to our list onebike_durations.append(trip_length_seconds) onebike_durations . [181.0, 7622.0, 343.0, 1278.0, 1277.0, 1366.0, 815.0, 545.0, 491.0, 639.0, 1678.0, 406.0, 709.0, 514.0, 492.0, 1668.0, 2242.0, 2752.0, 735.0, 330.0, 518.0, 1433.0, 204.0, 304.0, 977.0, 1399.0, 1244.0, 658.0, 800.0, 1911.0, 2471.0, 1344.0, 435.0, 271.0, 920.0, 851.0, 209.0, 453.0, 841.0, 142.0, 1023.0, 1466.0, 1636.0, 3039.0, 1571.0, 1410.0, 386.0, 1527.0, 622.0, 1450.0, 1422.0, 991.0, 1484.0, 1450.0, 929.0, 533.0, 525.0, 283.0, 133.0, 1106.0, 952.0, 553.0, 659.0, 297.0, 357.0, 989.0, 979.0, 760.0, 1110.0, 675.0, 1207.0, 1593.0, 768.0, 1446.0, 485.0, 200.0, 399.0, 242.0, 170.0, 450.0, 1078.0, 1042.0, 573.0, 748.0, 735.0, 336.0, 76913.0, 171.0, 568.0, 358.0, 917.0, 671.0, 1791.0, 318.0, 888.0, 1284.0, 11338.0, 1686.0, 5579.0, 8290.0, 1850.0, 1810.0, 870.0, 436.0, 429.0, 494.0, 1439.0, 380.0, 629.0, 962.0, 387.0, 952.0, 190.0, 739.0, 1120.0, 369.0, 2275.0, 873.0, 1670.0, 643.0, 572.0, 1375.0, 725.0, 688.0, 1041.0, 1707.0, 1236.0, 1291.0, 2890.0, -3346.0, 1213.0, 331.0, 1497.0, 527.0, 584.0, 2599.0, 759.0, 1291.0, 916.0, 161.0, 806.0, 838.0, 644.0, 374.0, 678.0, 137.0, 659.0, 386.0, 745.0, 448.0, 558.0, 888.0, 662.0, 663.0, 362.0, 513.0, 655.0, 221.0, 469.0, 430.0, 192.0, 324.0, 1233.0, 923.0, 961.0, 525.0, 1017.0, 1216.0, 747.0, 668.0, 1219.0, 1182.0, 10262.0, 1106.0, 399.0, 724.0, 330.0, 499.0, 968.0, 1310.0, 2629.0, 427.0, 839.0, 258.0, 396.0, 238.0, 745.0, 613.0, 710.0, 2068.0, 947.0, 1509.0, 254.0, 625.0, 479.0, 688.0, 238.0, 322.0, 304.0, 576.0, 1035.0, 661.0, 276.0, 1427.0, 998.0, 729.0, 723.0, 220.0, 212.0, 759.0, 268.0, 374.0, 305.0, 304.0, 289.0, 2620.0, 1288.0, 212.0, 2656.0, 996.0, 271.0, 701.0, 458.0, 116.0, 124.0, 276.0, 532.0, 257.0, 1089.0, 195.0, 384.0, 511.0, 850.0, 462.0, 322.0, 998.0, 327.0, 153.0, 152.0, 230.0, 321.0, 625.0, 391.0, 1298.0, 1018.0, 220.0, 277.0, 221.0, 216.0, 509.0, 596.0, 367.0, 447.0, 257.0, 1049.0, 1367.0, 933.0, 151.0, 153.0, 1336.0, 298.0, 380.0, 1385.0, 225.0, 181.0, 581.0, 250.0, 332.0, 845.0, 277.0, 663.0, 541.0, 652.0, 257.0, 775.0, 499.0, 1744.0, 445.0, 297.0, 1716.0, 2046.0, 809.0, 1287.0, 596.0, 1513.0, 651.0, 625.0, 279.0, 210.0, 610.0] . Average trip time . # What was the total duration of all trips? total_elapsed_time = sum(onebike_durations) # What was the total number of trips? number_of_trips = len(onebike_durations) # Divide the total duration by the number of trips print(total_elapsed_time / number_of_trips) . 1178.9310344827586 . For the average to be a helpful summary of the data, we need for all of our durations to be reasonable numbers, and not a few that are way too big, way too small, or even malformed. For example, if there is anything fishy happening in the data, and our trip ended before it started, we&#39;d have a negative trip length. . The long and the short of why time is hard . Out of 291 trips taken by W20529, how long was the longest? How short was the shortest? Does anything look fishy? . # Calculate shortest and longest trips shortest_trip = min(onebike_durations) longest_trip = max(onebike_durations) # Print out the results print(&quot;The shortest trip was &quot; + str(shortest_trip) + &quot; seconds&quot;) print(&quot;The longest trip was &quot; + str(longest_trip) + &quot; seconds&quot;) . The shortest trip was -3346.0 seconds The longest trip was 76913.0 seconds . For at least one trip, the bike returned before it left. Why could that be? Here&#39;s a hint: it happened in early November, around 2AM local time. What happens to clocks around that time each year? . Counting events before and after noon . We will be working with a list of all bike trips for one Capital Bikeshare bike, W20529, from October 1, 2017 to December 31, 2017. This list has been loaded as onebike_datetimes. Each element of the list is a dictionary with two entries: start is a datetime object corresponding to the start of a trip (when a bike is removed from the dock) and end is a datetime object corresponding to the end of a trip (when a bike is put back into a dock). We can use this data set to understand better how this bike was used. . Did more trips start before noon or after noon? . # Create dictionary to hold results trip_counts = {&#39;AM&#39;: 0, &#39;PM&#39;: 0} # Loop over all trips for trip in onebike_datetimes: # Check to see if the trip starts before noon if trip[&#39;start&#39;].hour &lt; 12: # Increment the counter for before noon trip_counts[&#39;AM&#39;] += 1 else: # Increment the counter for after noon trip_counts[&#39;PM&#39;] += 1 print(trip_counts) . {&#39;AM&#39;: 94, &#39;PM&#39;: 196} . It looks like this bike is used about twice as much after noon than it is before noon. One obvious follow up would be to see which hours the bike is most likely to be taken out for a ride. . Printing and Parsing Datetimes . Turning strings into datetimes . # Starting string, in YYYY-MM-DD HH:MM:SS format s = &#39;2017-02-03 00:00:01&#39; # Write a format string to parse s fmt = &#39;%Y-%m-%d %H:%M:%S&#39; # Create a datetime object d d = datetime.datetime.strptime(s, fmt) # Print d print(d) . 2017-02-03 00:00:01 . # Starting string, in YYYY-MM-DD format s = &#39;2030-10-15&#39; # Write a format string to parse s fmt = &#39;%Y-%m-%d&#39; # Create a datetime object d d = datetime.datetime.strptime(s, fmt) # Print d print(d) . 2030-10-15 00:00:00 . # Starting string, in MM/DD/YYYY HH:MM:SS format s = &#39;12/15/1986 08:00:00&#39; # Write a format string to parse s fmt = &#39;%m/%d/%Y %H:%M:%S&#39; # Create a datetime object d d = datetime.datetime.strptime(s, fmt) # Print d print(d) . 1986-12-15 08:00:00 . Parsing pairs of strings as datetimes . onebike_datetime_strings = [(&#39;2017-10-01 15:23:25&#39;, &#39;2017-10-01 15:26:26&#39;), (&#39;2017-10-01 15:42:57&#39;, &#39;2017-10-01 17:49:59&#39;), (&#39;2017-10-02 06:37:10&#39;, &#39;2017-10-02 06:42:53&#39;), (&#39;2017-10-02 08:56:45&#39;, &#39;2017-10-02 09:18:03&#39;), (&#39;2017-10-02 18:23:48&#39;, &#39;2017-10-02 18:45:05&#39;), (&#39;2017-10-02 18:48:08&#39;, &#39;2017-10-02 19:10:54&#39;), (&#39;2017-10-02 19:18:10&#39;, &#39;2017-10-02 19:31:45&#39;), (&#39;2017-10-02 19:37:32&#39;, &#39;2017-10-02 19:46:37&#39;), (&#39;2017-10-03 08:24:16&#39;, &#39;2017-10-03 08:32:27&#39;), (&#39;2017-10-03 18:17:07&#39;, &#39;2017-10-03 18:27:46&#39;), (&#39;2017-10-03 19:24:10&#39;, &#39;2017-10-03 19:52:08&#39;), (&#39;2017-10-03 20:17:06&#39;, &#39;2017-10-03 20:23:52&#39;), (&#39;2017-10-03 20:45:21&#39;, &#39;2017-10-03 20:57:10&#39;), (&#39;2017-10-04 07:04:57&#39;, &#39;2017-10-04 07:13:31&#39;), (&#39;2017-10-04 07:13:42&#39;, &#39;2017-10-04 07:21:54&#39;), (&#39;2017-10-04 14:22:12&#39;, &#39;2017-10-04 14:50:00&#39;), (&#39;2017-10-04 15:07:27&#39;, &#39;2017-10-04 15:44:49&#39;), (&#39;2017-10-04 15:46:41&#39;, &#39;2017-10-04 16:32:33&#39;), (&#39;2017-10-04 16:34:44&#39;, &#39;2017-10-04 16:46:59&#39;), (&#39;2017-10-04 17:26:06&#39;, &#39;2017-10-04 17:31:36&#39;), (&#39;2017-10-04 17:42:03&#39;, &#39;2017-10-04 17:50:41&#39;), (&#39;2017-10-05 07:49:02&#39;, &#39;2017-10-05 08:12:55&#39;), (&#39;2017-10-05 08:26:21&#39;, &#39;2017-10-05 08:29:45&#39;), (&#39;2017-10-05 08:33:27&#39;, &#39;2017-10-05 08:38:31&#39;), (&#39;2017-10-05 16:35:35&#39;, &#39;2017-10-05 16:51:52&#39;), (&#39;2017-10-05 17:53:31&#39;, &#39;2017-10-05 18:16:50&#39;), (&#39;2017-10-06 08:17:17&#39;, &#39;2017-10-06 08:38:01&#39;), (&#39;2017-10-06 11:39:40&#39;, &#39;2017-10-06 11:50:38&#39;), (&#39;2017-10-06 12:59:54&#39;, &#39;2017-10-06 13:13:14&#39;), (&#39;2017-10-06 13:43:05&#39;, &#39;2017-10-06 14:14:56&#39;), (&#39;2017-10-06 14:28:15&#39;, &#39;2017-10-06 15:09:26&#39;), (&#39;2017-10-06 15:50:10&#39;, &#39;2017-10-06 16:12:34&#39;), (&#39;2017-10-06 16:32:16&#39;, &#39;2017-10-06 16:39:31&#39;), (&#39;2017-10-06 16:44:08&#39;, &#39;2017-10-06 16:48:39&#39;), (&#39;2017-10-06 16:53:43&#39;, &#39;2017-10-06 17:09:03&#39;), (&#39;2017-10-07 11:38:55&#39;, &#39;2017-10-07 11:53:06&#39;), (&#39;2017-10-07 14:03:36&#39;, &#39;2017-10-07 14:07:05&#39;), (&#39;2017-10-07 14:20:03&#39;, &#39;2017-10-07 14:27:36&#39;), (&#39;2017-10-07 14:30:50&#39;, &#39;2017-10-07 14:44:51&#39;), (&#39;2017-10-08 00:28:26&#39;, &#39;2017-10-08 00:30:48&#39;), (&#39;2017-10-08 11:16:21&#39;, &#39;2017-10-08 11:33:24&#39;), (&#39;2017-10-08 12:37:03&#39;, &#39;2017-10-08 13:01:29&#39;), (&#39;2017-10-08 13:30:37&#39;, &#39;2017-10-08 13:57:53&#39;), (&#39;2017-10-08 14:16:40&#39;, &#39;2017-10-08 15:07:19&#39;), (&#39;2017-10-08 15:23:50&#39;, &#39;2017-10-08 15:50:01&#39;), (&#39;2017-10-08 15:54:12&#39;, &#39;2017-10-08 16:17:42&#39;), (&#39;2017-10-08 16:28:52&#39;, &#39;2017-10-08 16:35:18&#39;), (&#39;2017-10-08 23:08:14&#39;, &#39;2017-10-08 23:33:41&#39;), (&#39;2017-10-08 23:34:49&#39;, &#39;2017-10-08 23:45:11&#39;), (&#39;2017-10-08 23:46:47&#39;, &#39;2017-10-09 00:10:57&#39;), (&#39;2017-10-09 00:12:58&#39;, &#39;2017-10-09 00:36:40&#39;), (&#39;2017-10-09 00:37:02&#39;, &#39;2017-10-09 00:53:33&#39;), (&#39;2017-10-09 01:23:29&#39;, &#39;2017-10-09 01:48:13&#39;), (&#39;2017-10-09 01:49:25&#39;, &#39;2017-10-09 02:13:35&#39;), (&#39;2017-10-09 02:14:11&#39;, &#39;2017-10-09 02:29:40&#39;), (&#39;2017-10-09 13:04:32&#39;, &#39;2017-10-09 13:13:25&#39;), (&#39;2017-10-09 14:30:10&#39;, &#39;2017-10-09 14:38:55&#39;), (&#39;2017-10-09 15:06:47&#39;, &#39;2017-10-09 15:11:30&#39;), (&#39;2017-10-09 16:43:25&#39;, &#39;2017-10-09 16:45:38&#39;), (&#39;2017-10-10 15:32:58&#39;, &#39;2017-10-10 15:51:24&#39;), (&#39;2017-10-10 16:47:55&#39;, &#39;2017-10-10 17:03:47&#39;), (&#39;2017-10-10 17:51:05&#39;, &#39;2017-10-10 18:00:18&#39;), (&#39;2017-10-10 18:08:12&#39;, &#39;2017-10-10 18:19:11&#39;), (&#39;2017-10-10 19:09:35&#39;, &#39;2017-10-10 19:14:32&#39;), (&#39;2017-10-10 19:17:11&#39;, &#39;2017-10-10 19:23:08&#39;), (&#39;2017-10-10 19:28:11&#39;, &#39;2017-10-10 19:44:40&#39;), (&#39;2017-10-10 19:55:35&#39;, &#39;2017-10-10 20:11:54&#39;), (&#39;2017-10-10 22:20:43&#39;, &#39;2017-10-10 22:33:23&#39;), (&#39;2017-10-11 04:40:52&#39;, &#39;2017-10-11 04:59:22&#39;), (&#39;2017-10-11 06:28:58&#39;, &#39;2017-10-11 06:40:13&#39;), (&#39;2017-10-11 16:41:07&#39;, &#39;2017-10-11 17:01:14&#39;), (&#39;2017-10-12 08:08:30&#39;, &#39;2017-10-12 08:35:03&#39;), (&#39;2017-10-12 08:47:02&#39;, &#39;2017-10-12 08:59:50&#39;), (&#39;2017-10-12 13:13:39&#39;, &#39;2017-10-12 13:37:45&#39;), (&#39;2017-10-12 13:40:12&#39;, &#39;2017-10-12 13:48:17&#39;), (&#39;2017-10-12 13:49:56&#39;, &#39;2017-10-12 13:53:16&#39;), (&#39;2017-10-12 14:33:18&#39;, &#39;2017-10-12 14:39:57&#39;), (&#39;2017-10-13 15:55:39&#39;, &#39;2017-10-13 15:59:41&#39;), (&#39;2017-10-17 17:58:48&#39;, &#39;2017-10-17 18:01:38&#39;), (&#39;2017-10-19 20:21:45&#39;, &#39;2017-10-19 20:29:15&#39;), (&#39;2017-10-19 21:11:39&#39;, &#39;2017-10-19 21:29:37&#39;), (&#39;2017-10-19 21:30:01&#39;, &#39;2017-10-19 21:47:23&#39;), (&#39;2017-10-19 21:47:34&#39;, &#39;2017-10-19 21:57:07&#39;), (&#39;2017-10-19 21:57:24&#39;, &#39;2017-10-19 22:09:52&#39;), (&#39;2017-10-21 12:24:09&#39;, &#39;2017-10-21 12:36:24&#39;), (&#39;2017-10-21 12:36:37&#39;, &#39;2017-10-21 12:42:13&#39;), (&#39;2017-10-21 13:47:43&#39;, &#39;2017-10-22 11:09:36&#39;), (&#39;2017-10-22 13:28:53&#39;, &#39;2017-10-22 13:31:44&#39;), (&#39;2017-10-22 13:47:05&#39;, &#39;2017-10-22 13:56:33&#39;), (&#39;2017-10-22 14:26:41&#39;, &#39;2017-10-22 14:32:39&#39;), (&#39;2017-10-22 14:54:41&#39;, &#39;2017-10-22 15:09:58&#39;), (&#39;2017-10-22 16:40:29&#39;, &#39;2017-10-22 16:51:40&#39;), (&#39;2017-10-22 17:58:46&#39;, &#39;2017-10-22 18:28:37&#39;), (&#39;2017-10-22 18:45:16&#39;, &#39;2017-10-22 18:50:34&#39;), (&#39;2017-10-22 18:56:22&#39;, &#39;2017-10-22 19:11:10&#39;), (&#39;2017-10-23 10:14:08&#39;, &#39;2017-10-23 10:35:32&#39;), (&#39;2017-10-23 11:29:36&#39;, &#39;2017-10-23 14:38:34&#39;), (&#39;2017-10-23 15:04:52&#39;, &#39;2017-10-23 15:32:58&#39;), (&#39;2017-10-23 15:33:48&#39;, &#39;2017-10-23 17:06:47&#39;), (&#39;2017-10-23 17:13:16&#39;, &#39;2017-10-23 19:31:26&#39;), (&#39;2017-10-23 19:55:03&#39;, &#39;2017-10-23 20:25:53&#39;), (&#39;2017-10-23 21:47:54&#39;, &#39;2017-10-23 22:18:04&#39;), (&#39;2017-10-23 22:34:12&#39;, &#39;2017-10-23 22:48:42&#39;), (&#39;2017-10-24 06:55:01&#39;, &#39;2017-10-24 07:02:17&#39;), (&#39;2017-10-24 14:56:07&#39;, &#39;2017-10-24 15:03:16&#39;), (&#39;2017-10-24 15:51:36&#39;, &#39;2017-10-24 15:59:50&#39;), (&#39;2017-10-24 16:31:10&#39;, &#39;2017-10-24 16:55:09&#39;), (&#39;2017-10-28 14:26:14&#39;, &#39;2017-10-28 14:32:34&#39;), (&#39;2017-11-01 09:41:54&#39;, &#39;2017-11-01 09:52:23&#39;), (&#39;2017-11-01 20:16:11&#39;, &#39;2017-11-01 20:32:13&#39;), (&#39;2017-11-02 19:44:29&#39;, &#39;2017-11-02 19:50:56&#39;), (&#39;2017-11-02 20:14:37&#39;, &#39;2017-11-02 20:30:29&#39;), (&#39;2017-11-02 21:35:47&#39;, &#39;2017-11-02 21:38:57&#39;), (&#39;2017-11-03 09:59:27&#39;, &#39;2017-11-03 10:11:46&#39;), (&#39;2017-11-03 10:13:22&#39;, &#39;2017-11-03 10:32:02&#39;), (&#39;2017-11-03 10:44:25&#39;, &#39;2017-11-03 10:50:34&#39;), (&#39;2017-11-03 16:06:43&#39;, &#39;2017-11-03 16:44:38&#39;), (&#39;2017-11-03 16:45:54&#39;, &#39;2017-11-03 17:00:27&#39;), (&#39;2017-11-03 17:07:15&#39;, &#39;2017-11-03 17:35:05&#39;), (&#39;2017-11-03 17:36:05&#39;, &#39;2017-11-03 17:46:48&#39;), (&#39;2017-11-03 17:50:31&#39;, &#39;2017-11-03 18:00:03&#39;), (&#39;2017-11-03 19:22:56&#39;, &#39;2017-11-03 19:45:51&#39;), (&#39;2017-11-04 13:14:10&#39;, &#39;2017-11-04 13:26:15&#39;), (&#39;2017-11-04 14:18:37&#39;, &#39;2017-11-04 14:30:05&#39;), (&#39;2017-11-04 14:45:59&#39;, &#39;2017-11-04 15:03:20&#39;), (&#39;2017-11-04 15:16:03&#39;, &#39;2017-11-04 15:44:30&#39;), (&#39;2017-11-04 16:37:46&#39;, &#39;2017-11-04 16:58:22&#39;), (&#39;2017-11-04 17:13:19&#39;, &#39;2017-11-04 17:34:50&#39;), (&#39;2017-11-04 18:10:34&#39;, &#39;2017-11-04 18:58:44&#39;), (&#39;2017-11-05 01:56:50&#39;, &#39;2017-11-05 01:01:04&#39;), (&#39;2017-11-05 08:33:33&#39;, &#39;2017-11-05 08:53:46&#39;), (&#39;2017-11-05 08:58:08&#39;, &#39;2017-11-05 09:03:39&#39;), (&#39;2017-11-05 11:05:08&#39;, &#39;2017-11-05 11:30:05&#39;), (&#39;2017-11-06 08:50:18&#39;, &#39;2017-11-06 08:59:05&#39;), (&#39;2017-11-06 09:04:03&#39;, &#39;2017-11-06 09:13:47&#39;), (&#39;2017-11-06 16:19:36&#39;, &#39;2017-11-06 17:02:55&#39;), (&#39;2017-11-06 17:21:27&#39;, &#39;2017-11-06 17:34:06&#39;), (&#39;2017-11-06 17:36:01&#39;, &#39;2017-11-06 17:57:32&#39;), (&#39;2017-11-06 17:59:52&#39;, &#39;2017-11-06 18:15:08&#39;), (&#39;2017-11-06 18:18:36&#39;, &#39;2017-11-06 18:21:17&#39;), (&#39;2017-11-06 19:24:31&#39;, &#39;2017-11-06 19:37:57&#39;), (&#39;2017-11-06 19:49:16&#39;, &#39;2017-11-06 20:03:14&#39;), (&#39;2017-11-07 07:50:48&#39;, &#39;2017-11-07 08:01:32&#39;), (&#39;2017-11-08 13:11:51&#39;, &#39;2017-11-08 13:18:05&#39;), (&#39;2017-11-08 21:34:47&#39;, &#39;2017-11-08 21:46:05&#39;), (&#39;2017-11-08 22:02:30&#39;, &#39;2017-11-08 22:04:47&#39;), (&#39;2017-11-09 07:01:11&#39;, &#39;2017-11-09 07:12:10&#39;), (&#39;2017-11-09 08:02:02&#39;, &#39;2017-11-09 08:08:28&#39;), (&#39;2017-11-09 08:19:59&#39;, &#39;2017-11-09 08:32:24&#39;), (&#39;2017-11-09 08:41:31&#39;, &#39;2017-11-09 08:48:59&#39;), (&#39;2017-11-09 09:00:06&#39;, &#39;2017-11-09 09:09:24&#39;), (&#39;2017-11-09 09:09:37&#39;, &#39;2017-11-09 09:24:25&#39;), (&#39;2017-11-09 13:14:37&#39;, &#39;2017-11-09 13:25:39&#39;), (&#39;2017-11-09 15:20:07&#39;, &#39;2017-11-09 15:31:10&#39;), (&#39;2017-11-09 18:47:08&#39;, &#39;2017-11-09 18:53:10&#39;), (&#39;2017-11-09 23:35:02&#39;, &#39;2017-11-09 23:43:35&#39;), (&#39;2017-11-10 07:51:33&#39;, &#39;2017-11-10 08:02:28&#39;), (&#39;2017-11-10 08:38:28&#39;, &#39;2017-11-10 08:42:09&#39;), (&#39;2017-11-11 18:05:25&#39;, &#39;2017-11-11 18:13:14&#39;), (&#39;2017-11-11 19:39:12&#39;, &#39;2017-11-11 19:46:22&#39;), (&#39;2017-11-11 21:13:19&#39;, &#39;2017-11-11 21:16:31&#39;), (&#39;2017-11-12 09:46:19&#39;, &#39;2017-11-12 09:51:43&#39;), (&#39;2017-11-13 13:33:42&#39;, &#39;2017-11-13 13:54:15&#39;), (&#39;2017-11-14 08:40:29&#39;, &#39;2017-11-14 08:55:52&#39;), (&#39;2017-11-15 06:14:05&#39;, &#39;2017-11-15 06:30:06&#39;), (&#39;2017-11-15 08:14:59&#39;, &#39;2017-11-15 08:23:44&#39;), (&#39;2017-11-15 10:16:44&#39;, &#39;2017-11-15 10:33:41&#39;), (&#39;2017-11-15 10:33:58&#39;, &#39;2017-11-15 10:54:14&#39;), (&#39;2017-11-15 11:02:15&#39;, &#39;2017-11-15 11:14:42&#39;), (&#39;2017-11-16 09:27:41&#39;, &#39;2017-11-16 09:38:49&#39;), (&#39;2017-11-16 09:57:41&#39;, &#39;2017-11-16 10:18:00&#39;), (&#39;2017-11-16 17:25:05&#39;, &#39;2017-11-16 17:44:47&#39;), (&#39;2017-11-17 13:45:54&#39;, &#39;2017-11-17 16:36:56&#39;), (&#39;2017-11-17 19:12:49&#39;, &#39;2017-11-17 19:31:15&#39;), (&#39;2017-11-18 10:49:06&#39;, &#39;2017-11-18 10:55:45&#39;), (&#39;2017-11-18 11:32:12&#39;, &#39;2017-11-18 11:44:16&#39;), (&#39;2017-11-18 18:09:01&#39;, &#39;2017-11-18 18:14:31&#39;), (&#39;2017-11-18 18:53:10&#39;, &#39;2017-11-18 19:01:29&#39;), (&#39;2017-11-19 14:15:41&#39;, &#39;2017-11-19 14:31:49&#39;), (&#39;2017-11-20 21:19:19&#39;, &#39;2017-11-20 21:41:09&#39;), (&#39;2017-11-20 22:39:48&#39;, &#39;2017-11-20 23:23:37&#39;), (&#39;2017-11-21 17:44:25&#39;, &#39;2017-11-21 17:51:32&#39;), (&#39;2017-11-21 18:20:52&#39;, &#39;2017-11-21 18:34:51&#39;), (&#39;2017-11-21 18:47:32&#39;, &#39;2017-11-21 18:51:50&#39;), (&#39;2017-11-21 19:07:57&#39;, &#39;2017-11-21 19:14:33&#39;), (&#39;2017-11-21 20:04:56&#39;, &#39;2017-11-21 20:08:54&#39;), (&#39;2017-11-21 21:55:47&#39;, &#39;2017-11-21 22:08:12&#39;), (&#39;2017-11-23 23:47:43&#39;, &#39;2017-11-23 23:57:56&#39;), (&#39;2017-11-24 06:41:25&#39;, &#39;2017-11-24 06:53:15&#39;), (&#39;2017-11-24 06:58:56&#39;, &#39;2017-11-24 07:33:24&#39;), (&#39;2017-11-26 12:25:49&#39;, &#39;2017-11-26 12:41:36&#39;), (&#39;2017-11-27 05:29:04&#39;, &#39;2017-11-27 05:54:13&#39;), (&#39;2017-11-27 06:06:47&#39;, &#39;2017-11-27 06:11:01&#39;), (&#39;2017-11-27 06:45:14&#39;, &#39;2017-11-27 06:55:39&#39;), (&#39;2017-11-27 09:39:44&#39;, &#39;2017-11-27 09:47:43&#39;), (&#39;2017-11-27 11:09:18&#39;, &#39;2017-11-27 11:20:46&#39;), (&#39;2017-11-27 11:31:46&#39;, &#39;2017-11-27 11:35:44&#39;), (&#39;2017-11-27 12:07:14&#39;, &#39;2017-11-27 12:12:36&#39;), (&#39;2017-11-27 12:21:40&#39;, &#39;2017-11-27 12:26:44&#39;), (&#39;2017-11-27 17:26:31&#39;, &#39;2017-11-27 17:36:07&#39;), (&#39;2017-11-27 18:11:49&#39;, &#39;2017-11-27 18:29:04&#39;), (&#39;2017-11-27 19:36:16&#39;, &#39;2017-11-27 19:47:17&#39;), (&#39;2017-11-27 20:12:57&#39;, &#39;2017-11-27 20:17:33&#39;), (&#39;2017-11-28 08:18:06&#39;, &#39;2017-11-28 08:41:53&#39;), (&#39;2017-11-28 19:17:23&#39;, &#39;2017-11-28 19:34:01&#39;), (&#39;2017-11-28 19:34:15&#39;, &#39;2017-11-28 19:46:24&#39;), (&#39;2017-11-28 21:27:29&#39;, &#39;2017-11-28 21:39:32&#39;), (&#39;2017-11-29 07:47:38&#39;, &#39;2017-11-29 07:51:18&#39;), (&#39;2017-11-29 09:50:12&#39;, &#39;2017-11-29 09:53:44&#39;), (&#39;2017-11-29 17:03:42&#39;, &#39;2017-11-29 17:16:21&#39;), (&#39;2017-11-29 18:19:15&#39;, &#39;2017-11-29 18:23:43&#39;), (&#39;2017-12-01 17:03:58&#39;, &#39;2017-12-01 17:10:12&#39;), (&#39;2017-12-02 07:55:56&#39;, &#39;2017-12-02 08:01:01&#39;), (&#39;2017-12-02 09:16:14&#39;, &#39;2017-12-02 09:21:18&#39;), (&#39;2017-12-02 19:48:29&#39;, &#39;2017-12-02 19:53:18&#39;), (&#39;2017-12-03 14:36:29&#39;, &#39;2017-12-03 15:20:09&#39;), (&#39;2017-12-03 16:04:02&#39;, &#39;2017-12-03 16:25:30&#39;), (&#39;2017-12-03 16:40:26&#39;, &#39;2017-12-03 16:43:58&#39;), (&#39;2017-12-03 17:20:17&#39;, &#39;2017-12-03 18:04:33&#39;), (&#39;2017-12-04 08:34:24&#39;, &#39;2017-12-04 08:51:00&#39;), (&#39;2017-12-04 17:49:26&#39;, &#39;2017-12-04 17:53:57&#39;), (&#39;2017-12-04 18:38:52&#39;, &#39;2017-12-04 18:50:33&#39;), (&#39;2017-12-04 21:39:20&#39;, &#39;2017-12-04 21:46:58&#39;), (&#39;2017-12-04 21:54:21&#39;, &#39;2017-12-04 21:56:17&#39;), (&#39;2017-12-05 08:50:50&#39;, &#39;2017-12-05 08:52:54&#39;), (&#39;2017-12-06 08:19:38&#39;, &#39;2017-12-06 08:24:14&#39;), (&#39;2017-12-06 18:19:19&#39;, &#39;2017-12-06 18:28:11&#39;), (&#39;2017-12-06 18:28:55&#39;, &#39;2017-12-06 18:33:12&#39;), (&#39;2017-12-06 20:03:29&#39;, &#39;2017-12-06 20:21:38&#39;), (&#39;2017-12-06 20:36:42&#39;, &#39;2017-12-06 20:39:57&#39;), (&#39;2017-12-07 05:54:51&#39;, &#39;2017-12-07 06:01:15&#39;), (&#39;2017-12-08 16:47:18&#39;, &#39;2017-12-08 16:55:49&#39;), (&#39;2017-12-08 19:15:02&#39;, &#39;2017-12-08 19:29:12&#39;), (&#39;2017-12-09 22:39:37&#39;, &#39;2017-12-09 22:47:19&#39;), (&#39;2017-12-09 23:00:10&#39;, &#39;2017-12-09 23:05:32&#39;), (&#39;2017-12-10 00:39:24&#39;, &#39;2017-12-10 00:56:02&#39;), (&#39;2017-12-10 01:02:42&#39;, &#39;2017-12-10 01:08:09&#39;), (&#39;2017-12-10 01:08:57&#39;, &#39;2017-12-10 01:11:30&#39;), (&#39;2017-12-10 13:49:09&#39;, &#39;2017-12-10 13:51:41&#39;), (&#39;2017-12-10 15:14:29&#39;, &#39;2017-12-10 15:18:19&#39;), (&#39;2017-12-10 15:31:07&#39;, &#39;2017-12-10 15:36:28&#39;), (&#39;2017-12-10 16:20:06&#39;, &#39;2017-12-10 16:30:31&#39;), (&#39;2017-12-10 17:07:54&#39;, &#39;2017-12-10 17:14:25&#39;), (&#39;2017-12-10 17:23:47&#39;, &#39;2017-12-10 17:45:25&#39;), (&#39;2017-12-11 06:17:06&#39;, &#39;2017-12-11 06:34:04&#39;), (&#39;2017-12-11 09:08:41&#39;, &#39;2017-12-11 09:12:21&#39;), (&#39;2017-12-11 09:15:41&#39;, &#39;2017-12-11 09:20:18&#39;), (&#39;2017-12-12 08:55:53&#39;, &#39;2017-12-12 08:59:34&#39;), (&#39;2017-12-13 17:14:56&#39;, &#39;2017-12-13 17:18:32&#39;), (&#39;2017-12-13 18:52:16&#39;, &#39;2017-12-13 19:00:45&#39;), (&#39;2017-12-14 09:01:10&#39;, &#39;2017-12-14 09:11:06&#39;), (&#39;2017-12-14 09:12:59&#39;, &#39;2017-12-14 09:19:06&#39;), (&#39;2017-12-14 11:54:33&#39;, &#39;2017-12-14 12:02:00&#39;), (&#39;2017-12-14 14:40:23&#39;, &#39;2017-12-14 14:44:40&#39;), (&#39;2017-12-14 15:08:55&#39;, &#39;2017-12-14 15:26:24&#39;), (&#39;2017-12-14 17:46:17&#39;, &#39;2017-12-14 18:09:04&#39;), (&#39;2017-12-15 09:08:12&#39;, &#39;2017-12-15 09:23:45&#39;), (&#39;2017-12-16 09:33:46&#39;, &#39;2017-12-16 09:36:17&#39;), (&#39;2017-12-16 11:02:31&#39;, &#39;2017-12-16 11:05:04&#39;), (&#39;2017-12-17 10:09:47&#39;, &#39;2017-12-17 10:32:03&#39;), (&#39;2017-12-18 08:02:36&#39;, &#39;2017-12-18 08:07:34&#39;), (&#39;2017-12-18 16:03:00&#39;, &#39;2017-12-18 16:09:20&#39;), (&#39;2017-12-18 16:30:07&#39;, &#39;2017-12-18 16:53:12&#39;), (&#39;2017-12-18 19:18:23&#39;, &#39;2017-12-18 19:22:08&#39;), (&#39;2017-12-18 20:14:46&#39;, &#39;2017-12-18 20:17:47&#39;), (&#39;2017-12-19 19:14:08&#39;, &#39;2017-12-19 19:23:49&#39;), (&#39;2017-12-19 19:39:36&#39;, &#39;2017-12-19 19:43:46&#39;), (&#39;2017-12-20 08:05:14&#39;, &#39;2017-12-20 08:10:46&#39;), (&#39;2017-12-20 08:15:45&#39;, &#39;2017-12-20 08:29:50&#39;), (&#39;2017-12-20 08:33:32&#39;, &#39;2017-12-20 08:38:09&#39;), (&#39;2017-12-20 13:43:36&#39;, &#39;2017-12-20 13:54:39&#39;), (&#39;2017-12-20 18:57:53&#39;, &#39;2017-12-20 19:06:54&#39;), (&#39;2017-12-21 07:21:11&#39;, &#39;2017-12-21 07:32:03&#39;), (&#39;2017-12-21 08:01:58&#39;, &#39;2017-12-21 08:06:15&#39;), (&#39;2017-12-21 13:20:54&#39;, &#39;2017-12-21 13:33:49&#39;), (&#39;2017-12-21 15:26:08&#39;, &#39;2017-12-21 15:34:27&#39;), (&#39;2017-12-21 18:09:46&#39;, &#39;2017-12-21 18:38:50&#39;), (&#39;2017-12-22 16:14:21&#39;, &#39;2017-12-22 16:21:46&#39;), (&#39;2017-12-22 16:29:17&#39;, &#39;2017-12-22 16:34:14&#39;), (&#39;2017-12-25 12:49:51&#39;, &#39;2017-12-25 13:18:27&#39;), (&#39;2017-12-25 13:46:44&#39;, &#39;2017-12-25 14:20:50&#39;), (&#39;2017-12-26 10:40:16&#39;, &#39;2017-12-26 10:53:45&#39;), (&#39;2017-12-27 16:56:12&#39;, &#39;2017-12-27 17:17:39&#39;), (&#39;2017-12-29 06:02:34&#39;, &#39;2017-12-29 06:12:30&#39;), (&#39;2017-12-29 12:21:03&#39;, &#39;2017-12-29 12:46:16&#39;), (&#39;2017-12-29 14:32:55&#39;, &#39;2017-12-29 14:43:46&#39;), (&#39;2017-12-29 15:08:26&#39;, &#39;2017-12-29 15:18:51&#39;), (&#39;2017-12-29 20:33:34&#39;, &#39;2017-12-29 20:38:13&#39;), (&#39;2017-12-30 13:51:03&#39;, &#39;2017-12-30 13:54:33&#39;), (&#39;2017-12-30 15:09:03&#39;, &#39;2017-12-30 15:19:13&#39;)] . # Write down the format string fmt = &quot;%Y-%m-%d %H:%M:%S&quot; # Initialize a list for holding the pairs of datetime objects onebike_datetimes = [] # Loop over all trips for (start, end) in onebike_datetime_strings: trip = {&#39;start&#39;: datetime.datetime.strptime(start, fmt), &#39;end&#39;: datetime.datetime.strptime(end, fmt)} # Append the trip onebike_datetimes.append(trip) onebike_datetimes . [{&#39;start&#39;: datetime.datetime(2017, 10, 1, 15, 23, 25), &#39;end&#39;: datetime.datetime(2017, 10, 1, 15, 26, 26)}, {&#39;start&#39;: datetime.datetime(2017, 10, 1, 15, 42, 57), &#39;end&#39;: datetime.datetime(2017, 10, 1, 17, 49, 59)}, {&#39;start&#39;: datetime.datetime(2017, 10, 2, 6, 37, 10), &#39;end&#39;: datetime.datetime(2017, 10, 2, 6, 42, 53)}, {&#39;start&#39;: datetime.datetime(2017, 10, 2, 8, 56, 45), &#39;end&#39;: datetime.datetime(2017, 10, 2, 9, 18, 3)}, {&#39;start&#39;: datetime.datetime(2017, 10, 2, 18, 23, 48), &#39;end&#39;: datetime.datetime(2017, 10, 2, 18, 45, 5)}, {&#39;start&#39;: datetime.datetime(2017, 10, 2, 18, 48, 8), &#39;end&#39;: datetime.datetime(2017, 10, 2, 19, 10, 54)}, {&#39;start&#39;: datetime.datetime(2017, 10, 2, 19, 18, 10), &#39;end&#39;: datetime.datetime(2017, 10, 2, 19, 31, 45)}, {&#39;start&#39;: datetime.datetime(2017, 10, 2, 19, 37, 32), &#39;end&#39;: datetime.datetime(2017, 10, 2, 19, 46, 37)}, {&#39;start&#39;: datetime.datetime(2017, 10, 3, 8, 24, 16), &#39;end&#39;: datetime.datetime(2017, 10, 3, 8, 32, 27)}, {&#39;start&#39;: datetime.datetime(2017, 10, 3, 18, 17, 7), &#39;end&#39;: datetime.datetime(2017, 10, 3, 18, 27, 46)}, {&#39;start&#39;: datetime.datetime(2017, 10, 3, 19, 24, 10), &#39;end&#39;: datetime.datetime(2017, 10, 3, 19, 52, 8)}, {&#39;start&#39;: datetime.datetime(2017, 10, 3, 20, 17, 6), &#39;end&#39;: datetime.datetime(2017, 10, 3, 20, 23, 52)}, {&#39;start&#39;: datetime.datetime(2017, 10, 3, 20, 45, 21), &#39;end&#39;: datetime.datetime(2017, 10, 3, 20, 57, 10)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 7, 4, 57), &#39;end&#39;: datetime.datetime(2017, 10, 4, 7, 13, 31)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 7, 13, 42), &#39;end&#39;: datetime.datetime(2017, 10, 4, 7, 21, 54)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 14, 22, 12), &#39;end&#39;: datetime.datetime(2017, 10, 4, 14, 50)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 15, 7, 27), &#39;end&#39;: datetime.datetime(2017, 10, 4, 15, 44, 49)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 15, 46, 41), &#39;end&#39;: datetime.datetime(2017, 10, 4, 16, 32, 33)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 16, 34, 44), &#39;end&#39;: datetime.datetime(2017, 10, 4, 16, 46, 59)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 17, 26, 6), &#39;end&#39;: datetime.datetime(2017, 10, 4, 17, 31, 36)}, {&#39;start&#39;: datetime.datetime(2017, 10, 4, 17, 42, 3), &#39;end&#39;: datetime.datetime(2017, 10, 4, 17, 50, 41)}, {&#39;start&#39;: datetime.datetime(2017, 10, 5, 7, 49, 2), &#39;end&#39;: datetime.datetime(2017, 10, 5, 8, 12, 55)}, {&#39;start&#39;: datetime.datetime(2017, 10, 5, 8, 26, 21), &#39;end&#39;: datetime.datetime(2017, 10, 5, 8, 29, 45)}, {&#39;start&#39;: datetime.datetime(2017, 10, 5, 8, 33, 27), &#39;end&#39;: datetime.datetime(2017, 10, 5, 8, 38, 31)}, {&#39;start&#39;: datetime.datetime(2017, 10, 5, 16, 35, 35), &#39;end&#39;: datetime.datetime(2017, 10, 5, 16, 51, 52)}, {&#39;start&#39;: datetime.datetime(2017, 10, 5, 17, 53, 31), &#39;end&#39;: datetime.datetime(2017, 10, 5, 18, 16, 50)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 8, 17, 17), &#39;end&#39;: datetime.datetime(2017, 10, 6, 8, 38, 1)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 11, 39, 40), &#39;end&#39;: datetime.datetime(2017, 10, 6, 11, 50, 38)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 12, 59, 54), &#39;end&#39;: datetime.datetime(2017, 10, 6, 13, 13, 14)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 13, 43, 5), &#39;end&#39;: datetime.datetime(2017, 10, 6, 14, 14, 56)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 14, 28, 15), &#39;end&#39;: datetime.datetime(2017, 10, 6, 15, 9, 26)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 15, 50, 10), &#39;end&#39;: datetime.datetime(2017, 10, 6, 16, 12, 34)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 16, 32, 16), &#39;end&#39;: datetime.datetime(2017, 10, 6, 16, 39, 31)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 16, 44, 8), &#39;end&#39;: datetime.datetime(2017, 10, 6, 16, 48, 39)}, {&#39;start&#39;: datetime.datetime(2017, 10, 6, 16, 53, 43), &#39;end&#39;: datetime.datetime(2017, 10, 6, 17, 9, 3)}, {&#39;start&#39;: datetime.datetime(2017, 10, 7, 11, 38, 55), &#39;end&#39;: datetime.datetime(2017, 10, 7, 11, 53, 6)}, {&#39;start&#39;: datetime.datetime(2017, 10, 7, 14, 3, 36), &#39;end&#39;: datetime.datetime(2017, 10, 7, 14, 7, 5)}, {&#39;start&#39;: datetime.datetime(2017, 10, 7, 14, 20, 3), &#39;end&#39;: datetime.datetime(2017, 10, 7, 14, 27, 36)}, {&#39;start&#39;: datetime.datetime(2017, 10, 7, 14, 30, 50), &#39;end&#39;: datetime.datetime(2017, 10, 7, 14, 44, 51)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 0, 28, 26), &#39;end&#39;: datetime.datetime(2017, 10, 8, 0, 30, 48)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 11, 16, 21), &#39;end&#39;: datetime.datetime(2017, 10, 8, 11, 33, 24)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 12, 37, 3), &#39;end&#39;: datetime.datetime(2017, 10, 8, 13, 1, 29)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 13, 30, 37), &#39;end&#39;: datetime.datetime(2017, 10, 8, 13, 57, 53)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 14, 16, 40), &#39;end&#39;: datetime.datetime(2017, 10, 8, 15, 7, 19)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 15, 23, 50), &#39;end&#39;: datetime.datetime(2017, 10, 8, 15, 50, 1)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 15, 54, 12), &#39;end&#39;: datetime.datetime(2017, 10, 8, 16, 17, 42)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 16, 28, 52), &#39;end&#39;: datetime.datetime(2017, 10, 8, 16, 35, 18)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 23, 8, 14), &#39;end&#39;: datetime.datetime(2017, 10, 8, 23, 33, 41)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 23, 34, 49), &#39;end&#39;: datetime.datetime(2017, 10, 8, 23, 45, 11)}, {&#39;start&#39;: datetime.datetime(2017, 10, 8, 23, 46, 47), &#39;end&#39;: datetime.datetime(2017, 10, 9, 0, 10, 57)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 0, 12, 58), &#39;end&#39;: datetime.datetime(2017, 10, 9, 0, 36, 40)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 0, 37, 2), &#39;end&#39;: datetime.datetime(2017, 10, 9, 0, 53, 33)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 1, 23, 29), &#39;end&#39;: datetime.datetime(2017, 10, 9, 1, 48, 13)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 1, 49, 25), &#39;end&#39;: datetime.datetime(2017, 10, 9, 2, 13, 35)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 2, 14, 11), &#39;end&#39;: datetime.datetime(2017, 10, 9, 2, 29, 40)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 13, 4, 32), &#39;end&#39;: datetime.datetime(2017, 10, 9, 13, 13, 25)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 14, 30, 10), &#39;end&#39;: datetime.datetime(2017, 10, 9, 14, 38, 55)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 15, 6, 47), &#39;end&#39;: datetime.datetime(2017, 10, 9, 15, 11, 30)}, {&#39;start&#39;: datetime.datetime(2017, 10, 9, 16, 43, 25), &#39;end&#39;: datetime.datetime(2017, 10, 9, 16, 45, 38)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 15, 32, 58), &#39;end&#39;: datetime.datetime(2017, 10, 10, 15, 51, 24)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 16, 47, 55), &#39;end&#39;: datetime.datetime(2017, 10, 10, 17, 3, 47)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 17, 51, 5), &#39;end&#39;: datetime.datetime(2017, 10, 10, 18, 0, 18)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 18, 8, 12), &#39;end&#39;: datetime.datetime(2017, 10, 10, 18, 19, 11)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 9, 35), &#39;end&#39;: datetime.datetime(2017, 10, 10, 19, 14, 32)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 17, 11), &#39;end&#39;: datetime.datetime(2017, 10, 10, 19, 23, 8)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 28, 11), &#39;end&#39;: datetime.datetime(2017, 10, 10, 19, 44, 40)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 19, 55, 35), &#39;end&#39;: datetime.datetime(2017, 10, 10, 20, 11, 54)}, {&#39;start&#39;: datetime.datetime(2017, 10, 10, 22, 20, 43), &#39;end&#39;: datetime.datetime(2017, 10, 10, 22, 33, 23)}, {&#39;start&#39;: datetime.datetime(2017, 10, 11, 4, 40, 52), &#39;end&#39;: datetime.datetime(2017, 10, 11, 4, 59, 22)}, {&#39;start&#39;: datetime.datetime(2017, 10, 11, 6, 28, 58), &#39;end&#39;: datetime.datetime(2017, 10, 11, 6, 40, 13)}, {&#39;start&#39;: datetime.datetime(2017, 10, 11, 16, 41, 7), &#39;end&#39;: datetime.datetime(2017, 10, 11, 17, 1, 14)}, {&#39;start&#39;: datetime.datetime(2017, 10, 12, 8, 8, 30), &#39;end&#39;: datetime.datetime(2017, 10, 12, 8, 35, 3)}, {&#39;start&#39;: datetime.datetime(2017, 10, 12, 8, 47, 2), &#39;end&#39;: datetime.datetime(2017, 10, 12, 8, 59, 50)}, {&#39;start&#39;: datetime.datetime(2017, 10, 12, 13, 13, 39), &#39;end&#39;: datetime.datetime(2017, 10, 12, 13, 37, 45)}, {&#39;start&#39;: datetime.datetime(2017, 10, 12, 13, 40, 12), &#39;end&#39;: datetime.datetime(2017, 10, 12, 13, 48, 17)}, {&#39;start&#39;: datetime.datetime(2017, 10, 12, 13, 49, 56), &#39;end&#39;: datetime.datetime(2017, 10, 12, 13, 53, 16)}, {&#39;start&#39;: datetime.datetime(2017, 10, 12, 14, 33, 18), &#39;end&#39;: datetime.datetime(2017, 10, 12, 14, 39, 57)}, {&#39;start&#39;: datetime.datetime(2017, 10, 13, 15, 55, 39), &#39;end&#39;: datetime.datetime(2017, 10, 13, 15, 59, 41)}, {&#39;start&#39;: datetime.datetime(2017, 10, 17, 17, 58, 48), &#39;end&#39;: datetime.datetime(2017, 10, 17, 18, 1, 38)}, {&#39;start&#39;: datetime.datetime(2017, 10, 19, 20, 21, 45), &#39;end&#39;: datetime.datetime(2017, 10, 19, 20, 29, 15)}, {&#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 11, 39), &#39;end&#39;: datetime.datetime(2017, 10, 19, 21, 29, 37)}, {&#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 30, 1), &#39;end&#39;: datetime.datetime(2017, 10, 19, 21, 47, 23)}, {&#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 47, 34), &#39;end&#39;: datetime.datetime(2017, 10, 19, 21, 57, 7)}, {&#39;start&#39;: datetime.datetime(2017, 10, 19, 21, 57, 24), &#39;end&#39;: datetime.datetime(2017, 10, 19, 22, 9, 52)}, {&#39;start&#39;: datetime.datetime(2017, 10, 21, 12, 24, 9), &#39;end&#39;: datetime.datetime(2017, 10, 21, 12, 36, 24)}, {&#39;start&#39;: datetime.datetime(2017, 10, 21, 12, 36, 37), &#39;end&#39;: datetime.datetime(2017, 10, 21, 12, 42, 13)}, {&#39;start&#39;: datetime.datetime(2017, 10, 21, 13, 47, 43), &#39;end&#39;: datetime.datetime(2017, 10, 22, 11, 9, 36)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 13, 28, 53), &#39;end&#39;: datetime.datetime(2017, 10, 22, 13, 31, 44)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 13, 47, 5), &#39;end&#39;: datetime.datetime(2017, 10, 22, 13, 56, 33)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 14, 26, 41), &#39;end&#39;: datetime.datetime(2017, 10, 22, 14, 32, 39)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 14, 54, 41), &#39;end&#39;: datetime.datetime(2017, 10, 22, 15, 9, 58)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 16, 40, 29), &#39;end&#39;: datetime.datetime(2017, 10, 22, 16, 51, 40)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 17, 58, 46), &#39;end&#39;: datetime.datetime(2017, 10, 22, 18, 28, 37)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 18, 45, 16), &#39;end&#39;: datetime.datetime(2017, 10, 22, 18, 50, 34)}, {&#39;start&#39;: datetime.datetime(2017, 10, 22, 18, 56, 22), &#39;end&#39;: datetime.datetime(2017, 10, 22, 19, 11, 10)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 10, 14, 8), &#39;end&#39;: datetime.datetime(2017, 10, 23, 10, 35, 32)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 11, 29, 36), &#39;end&#39;: datetime.datetime(2017, 10, 23, 14, 38, 34)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 15, 4, 52), &#39;end&#39;: datetime.datetime(2017, 10, 23, 15, 32, 58)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 15, 33, 48), &#39;end&#39;: datetime.datetime(2017, 10, 23, 17, 6, 47)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 17, 13, 16), &#39;end&#39;: datetime.datetime(2017, 10, 23, 19, 31, 26)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 19, 55, 3), &#39;end&#39;: datetime.datetime(2017, 10, 23, 20, 25, 53)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 21, 47, 54), &#39;end&#39;: datetime.datetime(2017, 10, 23, 22, 18, 4)}, {&#39;start&#39;: datetime.datetime(2017, 10, 23, 22, 34, 12), &#39;end&#39;: datetime.datetime(2017, 10, 23, 22, 48, 42)}, {&#39;start&#39;: datetime.datetime(2017, 10, 24, 6, 55, 1), &#39;end&#39;: datetime.datetime(2017, 10, 24, 7, 2, 17)}, {&#39;start&#39;: datetime.datetime(2017, 10, 24, 14, 56, 7), &#39;end&#39;: datetime.datetime(2017, 10, 24, 15, 3, 16)}, {&#39;start&#39;: datetime.datetime(2017, 10, 24, 15, 51, 36), &#39;end&#39;: datetime.datetime(2017, 10, 24, 15, 59, 50)}, {&#39;start&#39;: datetime.datetime(2017, 10, 24, 16, 31, 10), &#39;end&#39;: datetime.datetime(2017, 10, 24, 16, 55, 9)}, {&#39;start&#39;: datetime.datetime(2017, 10, 28, 14, 26, 14), &#39;end&#39;: datetime.datetime(2017, 10, 28, 14, 32, 34)}, {&#39;start&#39;: datetime.datetime(2017, 11, 1, 9, 41, 54), &#39;end&#39;: datetime.datetime(2017, 11, 1, 9, 52, 23)}, {&#39;start&#39;: datetime.datetime(2017, 11, 1, 20, 16, 11), &#39;end&#39;: datetime.datetime(2017, 11, 1, 20, 32, 13)}, {&#39;start&#39;: datetime.datetime(2017, 11, 2, 19, 44, 29), &#39;end&#39;: datetime.datetime(2017, 11, 2, 19, 50, 56)}, {&#39;start&#39;: datetime.datetime(2017, 11, 2, 20, 14, 37), &#39;end&#39;: datetime.datetime(2017, 11, 2, 20, 30, 29)}, {&#39;start&#39;: datetime.datetime(2017, 11, 2, 21, 35, 47), &#39;end&#39;: datetime.datetime(2017, 11, 2, 21, 38, 57)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 9, 59, 27), &#39;end&#39;: datetime.datetime(2017, 11, 3, 10, 11, 46)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 10, 13, 22), &#39;end&#39;: datetime.datetime(2017, 11, 3, 10, 32, 2)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 10, 44, 25), &#39;end&#39;: datetime.datetime(2017, 11, 3, 10, 50, 34)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 16, 6, 43), &#39;end&#39;: datetime.datetime(2017, 11, 3, 16, 44, 38)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 16, 45, 54), &#39;end&#39;: datetime.datetime(2017, 11, 3, 17, 0, 27)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 17, 7, 15), &#39;end&#39;: datetime.datetime(2017, 11, 3, 17, 35, 5)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 17, 36, 5), &#39;end&#39;: datetime.datetime(2017, 11, 3, 17, 46, 48)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 17, 50, 31), &#39;end&#39;: datetime.datetime(2017, 11, 3, 18, 0, 3)}, {&#39;start&#39;: datetime.datetime(2017, 11, 3, 19, 22, 56), &#39;end&#39;: datetime.datetime(2017, 11, 3, 19, 45, 51)}, {&#39;start&#39;: datetime.datetime(2017, 11, 4, 13, 14, 10), &#39;end&#39;: datetime.datetime(2017, 11, 4, 13, 26, 15)}, {&#39;start&#39;: datetime.datetime(2017, 11, 4, 14, 18, 37), &#39;end&#39;: datetime.datetime(2017, 11, 4, 14, 30, 5)}, {&#39;start&#39;: datetime.datetime(2017, 11, 4, 14, 45, 59), &#39;end&#39;: datetime.datetime(2017, 11, 4, 15, 3, 20)}, {&#39;start&#39;: datetime.datetime(2017, 11, 4, 15, 16, 3), &#39;end&#39;: datetime.datetime(2017, 11, 4, 15, 44, 30)}, {&#39;start&#39;: datetime.datetime(2017, 11, 4, 16, 37, 46), &#39;end&#39;: datetime.datetime(2017, 11, 4, 16, 58, 22)}, {&#39;start&#39;: datetime.datetime(2017, 11, 4, 17, 13, 19), &#39;end&#39;: datetime.datetime(2017, 11, 4, 17, 34, 50)}, {&#39;start&#39;: datetime.datetime(2017, 11, 4, 18, 10, 34), &#39;end&#39;: datetime.datetime(2017, 11, 4, 18, 58, 44)}, {&#39;start&#39;: datetime.datetime(2017, 11, 5, 1, 56, 50), &#39;end&#39;: datetime.datetime(2017, 11, 5, 1, 1, 4)}, {&#39;start&#39;: datetime.datetime(2017, 11, 5, 8, 33, 33), &#39;end&#39;: datetime.datetime(2017, 11, 5, 8, 53, 46)}, {&#39;start&#39;: datetime.datetime(2017, 11, 5, 8, 58, 8), &#39;end&#39;: datetime.datetime(2017, 11, 5, 9, 3, 39)}, {&#39;start&#39;: datetime.datetime(2017, 11, 5, 11, 5, 8), &#39;end&#39;: datetime.datetime(2017, 11, 5, 11, 30, 5)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 8, 50, 18), &#39;end&#39;: datetime.datetime(2017, 11, 6, 8, 59, 5)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 9, 4, 3), &#39;end&#39;: datetime.datetime(2017, 11, 6, 9, 13, 47)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 16, 19, 36), &#39;end&#39;: datetime.datetime(2017, 11, 6, 17, 2, 55)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 17, 21, 27), &#39;end&#39;: datetime.datetime(2017, 11, 6, 17, 34, 6)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 17, 36, 1), &#39;end&#39;: datetime.datetime(2017, 11, 6, 17, 57, 32)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 17, 59, 52), &#39;end&#39;: datetime.datetime(2017, 11, 6, 18, 15, 8)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 18, 18, 36), &#39;end&#39;: datetime.datetime(2017, 11, 6, 18, 21, 17)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 19, 24, 31), &#39;end&#39;: datetime.datetime(2017, 11, 6, 19, 37, 57)}, {&#39;start&#39;: datetime.datetime(2017, 11, 6, 19, 49, 16), &#39;end&#39;: datetime.datetime(2017, 11, 6, 20, 3, 14)}, {&#39;start&#39;: datetime.datetime(2017, 11, 7, 7, 50, 48), &#39;end&#39;: datetime.datetime(2017, 11, 7, 8, 1, 32)}, {&#39;start&#39;: datetime.datetime(2017, 11, 8, 13, 11, 51), &#39;end&#39;: datetime.datetime(2017, 11, 8, 13, 18, 5)}, {&#39;start&#39;: datetime.datetime(2017, 11, 8, 21, 34, 47), &#39;end&#39;: datetime.datetime(2017, 11, 8, 21, 46, 5)}, {&#39;start&#39;: datetime.datetime(2017, 11, 8, 22, 2, 30), &#39;end&#39;: datetime.datetime(2017, 11, 8, 22, 4, 47)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 7, 1, 11), &#39;end&#39;: datetime.datetime(2017, 11, 9, 7, 12, 10)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 8, 2, 2), &#39;end&#39;: datetime.datetime(2017, 11, 9, 8, 8, 28)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 8, 19, 59), &#39;end&#39;: datetime.datetime(2017, 11, 9, 8, 32, 24)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 8, 41, 31), &#39;end&#39;: datetime.datetime(2017, 11, 9, 8, 48, 59)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 9, 0, 6), &#39;end&#39;: datetime.datetime(2017, 11, 9, 9, 9, 24)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 9, 9, 37), &#39;end&#39;: datetime.datetime(2017, 11, 9, 9, 24, 25)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 13, 14, 37), &#39;end&#39;: datetime.datetime(2017, 11, 9, 13, 25, 39)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 15, 20, 7), &#39;end&#39;: datetime.datetime(2017, 11, 9, 15, 31, 10)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 18, 47, 8), &#39;end&#39;: datetime.datetime(2017, 11, 9, 18, 53, 10)}, {&#39;start&#39;: datetime.datetime(2017, 11, 9, 23, 35, 2), &#39;end&#39;: datetime.datetime(2017, 11, 9, 23, 43, 35)}, {&#39;start&#39;: datetime.datetime(2017, 11, 10, 7, 51, 33), &#39;end&#39;: datetime.datetime(2017, 11, 10, 8, 2, 28)}, {&#39;start&#39;: datetime.datetime(2017, 11, 10, 8, 38, 28), &#39;end&#39;: datetime.datetime(2017, 11, 10, 8, 42, 9)}, {&#39;start&#39;: datetime.datetime(2017, 11, 11, 18, 5, 25), &#39;end&#39;: datetime.datetime(2017, 11, 11, 18, 13, 14)}, {&#39;start&#39;: datetime.datetime(2017, 11, 11, 19, 39, 12), &#39;end&#39;: datetime.datetime(2017, 11, 11, 19, 46, 22)}, {&#39;start&#39;: datetime.datetime(2017, 11, 11, 21, 13, 19), &#39;end&#39;: datetime.datetime(2017, 11, 11, 21, 16, 31)}, {&#39;start&#39;: datetime.datetime(2017, 11, 12, 9, 46, 19), &#39;end&#39;: datetime.datetime(2017, 11, 12, 9, 51, 43)}, {&#39;start&#39;: datetime.datetime(2017, 11, 13, 13, 33, 42), &#39;end&#39;: datetime.datetime(2017, 11, 13, 13, 54, 15)}, {&#39;start&#39;: datetime.datetime(2017, 11, 14, 8, 40, 29), &#39;end&#39;: datetime.datetime(2017, 11, 14, 8, 55, 52)}, {&#39;start&#39;: datetime.datetime(2017, 11, 15, 6, 14, 5), &#39;end&#39;: datetime.datetime(2017, 11, 15, 6, 30, 6)}, {&#39;start&#39;: datetime.datetime(2017, 11, 15, 8, 14, 59), &#39;end&#39;: datetime.datetime(2017, 11, 15, 8, 23, 44)}, {&#39;start&#39;: datetime.datetime(2017, 11, 15, 10, 16, 44), &#39;end&#39;: datetime.datetime(2017, 11, 15, 10, 33, 41)}, {&#39;start&#39;: datetime.datetime(2017, 11, 15, 10, 33, 58), &#39;end&#39;: datetime.datetime(2017, 11, 15, 10, 54, 14)}, {&#39;start&#39;: datetime.datetime(2017, 11, 15, 11, 2, 15), &#39;end&#39;: datetime.datetime(2017, 11, 15, 11, 14, 42)}, {&#39;start&#39;: datetime.datetime(2017, 11, 16, 9, 27, 41), &#39;end&#39;: datetime.datetime(2017, 11, 16, 9, 38, 49)}, {&#39;start&#39;: datetime.datetime(2017, 11, 16, 9, 57, 41), &#39;end&#39;: datetime.datetime(2017, 11, 16, 10, 18)}, {&#39;start&#39;: datetime.datetime(2017, 11, 16, 17, 25, 5), &#39;end&#39;: datetime.datetime(2017, 11, 16, 17, 44, 47)}, {&#39;start&#39;: datetime.datetime(2017, 11, 17, 13, 45, 54), &#39;end&#39;: datetime.datetime(2017, 11, 17, 16, 36, 56)}, {&#39;start&#39;: datetime.datetime(2017, 11, 17, 19, 12, 49), &#39;end&#39;: datetime.datetime(2017, 11, 17, 19, 31, 15)}, {&#39;start&#39;: datetime.datetime(2017, 11, 18, 10, 49, 6), &#39;end&#39;: datetime.datetime(2017, 11, 18, 10, 55, 45)}, {&#39;start&#39;: datetime.datetime(2017, 11, 18, 11, 32, 12), &#39;end&#39;: datetime.datetime(2017, 11, 18, 11, 44, 16)}, {&#39;start&#39;: datetime.datetime(2017, 11, 18, 18, 9, 1), &#39;end&#39;: datetime.datetime(2017, 11, 18, 18, 14, 31)}, {&#39;start&#39;: datetime.datetime(2017, 11, 18, 18, 53, 10), &#39;end&#39;: datetime.datetime(2017, 11, 18, 19, 1, 29)}, {&#39;start&#39;: datetime.datetime(2017, 11, 19, 14, 15, 41), &#39;end&#39;: datetime.datetime(2017, 11, 19, 14, 31, 49)}, {&#39;start&#39;: datetime.datetime(2017, 11, 20, 21, 19, 19), &#39;end&#39;: datetime.datetime(2017, 11, 20, 21, 41, 9)}, {&#39;start&#39;: datetime.datetime(2017, 11, 20, 22, 39, 48), &#39;end&#39;: datetime.datetime(2017, 11, 20, 23, 23, 37)}, {&#39;start&#39;: datetime.datetime(2017, 11, 21, 17, 44, 25), &#39;end&#39;: datetime.datetime(2017, 11, 21, 17, 51, 32)}, {&#39;start&#39;: datetime.datetime(2017, 11, 21, 18, 20, 52), &#39;end&#39;: datetime.datetime(2017, 11, 21, 18, 34, 51)}, {&#39;start&#39;: datetime.datetime(2017, 11, 21, 18, 47, 32), &#39;end&#39;: datetime.datetime(2017, 11, 21, 18, 51, 50)}, {&#39;start&#39;: datetime.datetime(2017, 11, 21, 19, 7, 57), &#39;end&#39;: datetime.datetime(2017, 11, 21, 19, 14, 33)}, {&#39;start&#39;: datetime.datetime(2017, 11, 21, 20, 4, 56), &#39;end&#39;: datetime.datetime(2017, 11, 21, 20, 8, 54)}, {&#39;start&#39;: datetime.datetime(2017, 11, 21, 21, 55, 47), &#39;end&#39;: datetime.datetime(2017, 11, 21, 22, 8, 12)}, {&#39;start&#39;: datetime.datetime(2017, 11, 23, 23, 47, 43), &#39;end&#39;: datetime.datetime(2017, 11, 23, 23, 57, 56)}, {&#39;start&#39;: datetime.datetime(2017, 11, 24, 6, 41, 25), &#39;end&#39;: datetime.datetime(2017, 11, 24, 6, 53, 15)}, {&#39;start&#39;: datetime.datetime(2017, 11, 24, 6, 58, 56), &#39;end&#39;: datetime.datetime(2017, 11, 24, 7, 33, 24)}, {&#39;start&#39;: datetime.datetime(2017, 11, 26, 12, 25, 49), &#39;end&#39;: datetime.datetime(2017, 11, 26, 12, 41, 36)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 5, 29, 4), &#39;end&#39;: datetime.datetime(2017, 11, 27, 5, 54, 13)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 6, 6, 47), &#39;end&#39;: datetime.datetime(2017, 11, 27, 6, 11, 1)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 6, 45, 14), &#39;end&#39;: datetime.datetime(2017, 11, 27, 6, 55, 39)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 9, 39, 44), &#39;end&#39;: datetime.datetime(2017, 11, 27, 9, 47, 43)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 11, 9, 18), &#39;end&#39;: datetime.datetime(2017, 11, 27, 11, 20, 46)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 11, 31, 46), &#39;end&#39;: datetime.datetime(2017, 11, 27, 11, 35, 44)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 12, 7, 14), &#39;end&#39;: datetime.datetime(2017, 11, 27, 12, 12, 36)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 12, 21, 40), &#39;end&#39;: datetime.datetime(2017, 11, 27, 12, 26, 44)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 17, 26, 31), &#39;end&#39;: datetime.datetime(2017, 11, 27, 17, 36, 7)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 18, 11, 49), &#39;end&#39;: datetime.datetime(2017, 11, 27, 18, 29, 4)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 19, 36, 16), &#39;end&#39;: datetime.datetime(2017, 11, 27, 19, 47, 17)}, {&#39;start&#39;: datetime.datetime(2017, 11, 27, 20, 12, 57), &#39;end&#39;: datetime.datetime(2017, 11, 27, 20, 17, 33)}, {&#39;start&#39;: datetime.datetime(2017, 11, 28, 8, 18, 6), &#39;end&#39;: datetime.datetime(2017, 11, 28, 8, 41, 53)}, {&#39;start&#39;: datetime.datetime(2017, 11, 28, 19, 17, 23), &#39;end&#39;: datetime.datetime(2017, 11, 28, 19, 34, 1)}, {&#39;start&#39;: datetime.datetime(2017, 11, 28, 19, 34, 15), &#39;end&#39;: datetime.datetime(2017, 11, 28, 19, 46, 24)}, {&#39;start&#39;: datetime.datetime(2017, 11, 28, 21, 27, 29), &#39;end&#39;: datetime.datetime(2017, 11, 28, 21, 39, 32)}, {&#39;start&#39;: datetime.datetime(2017, 11, 29, 7, 47, 38), &#39;end&#39;: datetime.datetime(2017, 11, 29, 7, 51, 18)}, {&#39;start&#39;: datetime.datetime(2017, 11, 29, 9, 50, 12), &#39;end&#39;: datetime.datetime(2017, 11, 29, 9, 53, 44)}, {&#39;start&#39;: datetime.datetime(2017, 11, 29, 17, 3, 42), &#39;end&#39;: datetime.datetime(2017, 11, 29, 17, 16, 21)}, {&#39;start&#39;: datetime.datetime(2017, 11, 29, 18, 19, 15), &#39;end&#39;: datetime.datetime(2017, 11, 29, 18, 23, 43)}, {&#39;start&#39;: datetime.datetime(2017, 12, 1, 17, 3, 58), &#39;end&#39;: datetime.datetime(2017, 12, 1, 17, 10, 12)}, {&#39;start&#39;: datetime.datetime(2017, 12, 2, 7, 55, 56), &#39;end&#39;: datetime.datetime(2017, 12, 2, 8, 1, 1)}, {&#39;start&#39;: datetime.datetime(2017, 12, 2, 9, 16, 14), &#39;end&#39;: datetime.datetime(2017, 12, 2, 9, 21, 18)}, {&#39;start&#39;: datetime.datetime(2017, 12, 2, 19, 48, 29), &#39;end&#39;: datetime.datetime(2017, 12, 2, 19, 53, 18)}, {&#39;start&#39;: datetime.datetime(2017, 12, 3, 14, 36, 29), &#39;end&#39;: datetime.datetime(2017, 12, 3, 15, 20, 9)}, {&#39;start&#39;: datetime.datetime(2017, 12, 3, 16, 4, 2), &#39;end&#39;: datetime.datetime(2017, 12, 3, 16, 25, 30)}, {&#39;start&#39;: datetime.datetime(2017, 12, 3, 16, 40, 26), &#39;end&#39;: datetime.datetime(2017, 12, 3, 16, 43, 58)}, {&#39;start&#39;: datetime.datetime(2017, 12, 3, 17, 20, 17), &#39;end&#39;: datetime.datetime(2017, 12, 3, 18, 4, 33)}, {&#39;start&#39;: datetime.datetime(2017, 12, 4, 8, 34, 24), &#39;end&#39;: datetime.datetime(2017, 12, 4, 8, 51)}, {&#39;start&#39;: datetime.datetime(2017, 12, 4, 17, 49, 26), &#39;end&#39;: datetime.datetime(2017, 12, 4, 17, 53, 57)}, {&#39;start&#39;: datetime.datetime(2017, 12, 4, 18, 38, 52), &#39;end&#39;: datetime.datetime(2017, 12, 4, 18, 50, 33)}, {&#39;start&#39;: datetime.datetime(2017, 12, 4, 21, 39, 20), &#39;end&#39;: datetime.datetime(2017, 12, 4, 21, 46, 58)}, {&#39;start&#39;: datetime.datetime(2017, 12, 4, 21, 54, 21), &#39;end&#39;: datetime.datetime(2017, 12, 4, 21, 56, 17)}, {&#39;start&#39;: datetime.datetime(2017, 12, 5, 8, 50, 50), &#39;end&#39;: datetime.datetime(2017, 12, 5, 8, 52, 54)}, {&#39;start&#39;: datetime.datetime(2017, 12, 6, 8, 19, 38), &#39;end&#39;: datetime.datetime(2017, 12, 6, 8, 24, 14)}, {&#39;start&#39;: datetime.datetime(2017, 12, 6, 18, 19, 19), &#39;end&#39;: datetime.datetime(2017, 12, 6, 18, 28, 11)}, {&#39;start&#39;: datetime.datetime(2017, 12, 6, 18, 28, 55), &#39;end&#39;: datetime.datetime(2017, 12, 6, 18, 33, 12)}, {&#39;start&#39;: datetime.datetime(2017, 12, 6, 20, 3, 29), &#39;end&#39;: datetime.datetime(2017, 12, 6, 20, 21, 38)}, {&#39;start&#39;: datetime.datetime(2017, 12, 6, 20, 36, 42), &#39;end&#39;: datetime.datetime(2017, 12, 6, 20, 39, 57)}, {&#39;start&#39;: datetime.datetime(2017, 12, 7, 5, 54, 51), &#39;end&#39;: datetime.datetime(2017, 12, 7, 6, 1, 15)}, {&#39;start&#39;: datetime.datetime(2017, 12, 8, 16, 47, 18), &#39;end&#39;: datetime.datetime(2017, 12, 8, 16, 55, 49)}, {&#39;start&#39;: datetime.datetime(2017, 12, 8, 19, 15, 2), &#39;end&#39;: datetime.datetime(2017, 12, 8, 19, 29, 12)}, {&#39;start&#39;: datetime.datetime(2017, 12, 9, 22, 39, 37), &#39;end&#39;: datetime.datetime(2017, 12, 9, 22, 47, 19)}, {&#39;start&#39;: datetime.datetime(2017, 12, 9, 23, 0, 10), &#39;end&#39;: datetime.datetime(2017, 12, 9, 23, 5, 32)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 0, 39, 24), &#39;end&#39;: datetime.datetime(2017, 12, 10, 0, 56, 2)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 1, 2, 42), &#39;end&#39;: datetime.datetime(2017, 12, 10, 1, 8, 9)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 1, 8, 57), &#39;end&#39;: datetime.datetime(2017, 12, 10, 1, 11, 30)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 13, 49, 9), &#39;end&#39;: datetime.datetime(2017, 12, 10, 13, 51, 41)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 15, 14, 29), &#39;end&#39;: datetime.datetime(2017, 12, 10, 15, 18, 19)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 15, 31, 7), &#39;end&#39;: datetime.datetime(2017, 12, 10, 15, 36, 28)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 16, 20, 6), &#39;end&#39;: datetime.datetime(2017, 12, 10, 16, 30, 31)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 17, 7, 54), &#39;end&#39;: datetime.datetime(2017, 12, 10, 17, 14, 25)}, {&#39;start&#39;: datetime.datetime(2017, 12, 10, 17, 23, 47), &#39;end&#39;: datetime.datetime(2017, 12, 10, 17, 45, 25)}, {&#39;start&#39;: datetime.datetime(2017, 12, 11, 6, 17, 6), &#39;end&#39;: datetime.datetime(2017, 12, 11, 6, 34, 4)}, {&#39;start&#39;: datetime.datetime(2017, 12, 11, 9, 8, 41), &#39;end&#39;: datetime.datetime(2017, 12, 11, 9, 12, 21)}, {&#39;start&#39;: datetime.datetime(2017, 12, 11, 9, 15, 41), &#39;end&#39;: datetime.datetime(2017, 12, 11, 9, 20, 18)}, {&#39;start&#39;: datetime.datetime(2017, 12, 12, 8, 55, 53), &#39;end&#39;: datetime.datetime(2017, 12, 12, 8, 59, 34)}, {&#39;start&#39;: datetime.datetime(2017, 12, 13, 17, 14, 56), &#39;end&#39;: datetime.datetime(2017, 12, 13, 17, 18, 32)}, {&#39;start&#39;: datetime.datetime(2017, 12, 13, 18, 52, 16), &#39;end&#39;: datetime.datetime(2017, 12, 13, 19, 0, 45)}, {&#39;start&#39;: datetime.datetime(2017, 12, 14, 9, 1, 10), &#39;end&#39;: datetime.datetime(2017, 12, 14, 9, 11, 6)}, {&#39;start&#39;: datetime.datetime(2017, 12, 14, 9, 12, 59), &#39;end&#39;: datetime.datetime(2017, 12, 14, 9, 19, 6)}, {&#39;start&#39;: datetime.datetime(2017, 12, 14, 11, 54, 33), &#39;end&#39;: datetime.datetime(2017, 12, 14, 12, 2)}, {&#39;start&#39;: datetime.datetime(2017, 12, 14, 14, 40, 23), &#39;end&#39;: datetime.datetime(2017, 12, 14, 14, 44, 40)}, {&#39;start&#39;: datetime.datetime(2017, 12, 14, 15, 8, 55), &#39;end&#39;: datetime.datetime(2017, 12, 14, 15, 26, 24)}, {&#39;start&#39;: datetime.datetime(2017, 12, 14, 17, 46, 17), &#39;end&#39;: datetime.datetime(2017, 12, 14, 18, 9, 4)}, {&#39;start&#39;: datetime.datetime(2017, 12, 15, 9, 8, 12), &#39;end&#39;: datetime.datetime(2017, 12, 15, 9, 23, 45)}, {&#39;start&#39;: datetime.datetime(2017, 12, 16, 9, 33, 46), &#39;end&#39;: datetime.datetime(2017, 12, 16, 9, 36, 17)}, {&#39;start&#39;: datetime.datetime(2017, 12, 16, 11, 2, 31), &#39;end&#39;: datetime.datetime(2017, 12, 16, 11, 5, 4)}, {&#39;start&#39;: datetime.datetime(2017, 12, 17, 10, 9, 47), &#39;end&#39;: datetime.datetime(2017, 12, 17, 10, 32, 3)}, {&#39;start&#39;: datetime.datetime(2017, 12, 18, 8, 2, 36), &#39;end&#39;: datetime.datetime(2017, 12, 18, 8, 7, 34)}, {&#39;start&#39;: datetime.datetime(2017, 12, 18, 16, 3), &#39;end&#39;: datetime.datetime(2017, 12, 18, 16, 9, 20)}, {&#39;start&#39;: datetime.datetime(2017, 12, 18, 16, 30, 7), &#39;end&#39;: datetime.datetime(2017, 12, 18, 16, 53, 12)}, {&#39;start&#39;: datetime.datetime(2017, 12, 18, 19, 18, 23), &#39;end&#39;: datetime.datetime(2017, 12, 18, 19, 22, 8)}, {&#39;start&#39;: datetime.datetime(2017, 12, 18, 20, 14, 46), &#39;end&#39;: datetime.datetime(2017, 12, 18, 20, 17, 47)}, {&#39;start&#39;: datetime.datetime(2017, 12, 19, 19, 14, 8), &#39;end&#39;: datetime.datetime(2017, 12, 19, 19, 23, 49)}, {&#39;start&#39;: datetime.datetime(2017, 12, 19, 19, 39, 36), &#39;end&#39;: datetime.datetime(2017, 12, 19, 19, 43, 46)}, {&#39;start&#39;: datetime.datetime(2017, 12, 20, 8, 5, 14), &#39;end&#39;: datetime.datetime(2017, 12, 20, 8, 10, 46)}, {&#39;start&#39;: datetime.datetime(2017, 12, 20, 8, 15, 45), &#39;end&#39;: datetime.datetime(2017, 12, 20, 8, 29, 50)}, {&#39;start&#39;: datetime.datetime(2017, 12, 20, 8, 33, 32), &#39;end&#39;: datetime.datetime(2017, 12, 20, 8, 38, 9)}, {&#39;start&#39;: datetime.datetime(2017, 12, 20, 13, 43, 36), &#39;end&#39;: datetime.datetime(2017, 12, 20, 13, 54, 39)}, {&#39;start&#39;: datetime.datetime(2017, 12, 20, 18, 57, 53), &#39;end&#39;: datetime.datetime(2017, 12, 20, 19, 6, 54)}, {&#39;start&#39;: datetime.datetime(2017, 12, 21, 7, 21, 11), &#39;end&#39;: datetime.datetime(2017, 12, 21, 7, 32, 3)}, {&#39;start&#39;: datetime.datetime(2017, 12, 21, 8, 1, 58), &#39;end&#39;: datetime.datetime(2017, 12, 21, 8, 6, 15)}, {&#39;start&#39;: datetime.datetime(2017, 12, 21, 13, 20, 54), &#39;end&#39;: datetime.datetime(2017, 12, 21, 13, 33, 49)}, {&#39;start&#39;: datetime.datetime(2017, 12, 21, 15, 26, 8), &#39;end&#39;: datetime.datetime(2017, 12, 21, 15, 34, 27)}, {&#39;start&#39;: datetime.datetime(2017, 12, 21, 18, 9, 46), &#39;end&#39;: datetime.datetime(2017, 12, 21, 18, 38, 50)}, {&#39;start&#39;: datetime.datetime(2017, 12, 22, 16, 14, 21), &#39;end&#39;: datetime.datetime(2017, 12, 22, 16, 21, 46)}, {&#39;start&#39;: datetime.datetime(2017, 12, 22, 16, 29, 17), &#39;end&#39;: datetime.datetime(2017, 12, 22, 16, 34, 14)}, {&#39;start&#39;: datetime.datetime(2017, 12, 25, 12, 49, 51), &#39;end&#39;: datetime.datetime(2017, 12, 25, 13, 18, 27)}, {&#39;start&#39;: datetime.datetime(2017, 12, 25, 13, 46, 44), &#39;end&#39;: datetime.datetime(2017, 12, 25, 14, 20, 50)}, {&#39;start&#39;: datetime.datetime(2017, 12, 26, 10, 40, 16), &#39;end&#39;: datetime.datetime(2017, 12, 26, 10, 53, 45)}, {&#39;start&#39;: datetime.datetime(2017, 12, 27, 16, 56, 12), &#39;end&#39;: datetime.datetime(2017, 12, 27, 17, 17, 39)}, {&#39;start&#39;: datetime.datetime(2017, 12, 29, 6, 2, 34), &#39;end&#39;: datetime.datetime(2017, 12, 29, 6, 12, 30)}, {&#39;start&#39;: datetime.datetime(2017, 12, 29, 12, 21, 3), &#39;end&#39;: datetime.datetime(2017, 12, 29, 12, 46, 16)}, {&#39;start&#39;: datetime.datetime(2017, 12, 29, 14, 32, 55), &#39;end&#39;: datetime.datetime(2017, 12, 29, 14, 43, 46)}, {&#39;start&#39;: datetime.datetime(2017, 12, 29, 15, 8, 26), &#39;end&#39;: datetime.datetime(2017, 12, 29, 15, 18, 51)}, {&#39;start&#39;: datetime.datetime(2017, 12, 29, 20, 33, 34), &#39;end&#39;: datetime.datetime(2017, 12, 29, 20, 38, 13)}, {&#39;start&#39;: datetime.datetime(2017, 12, 30, 13, 51, 3), &#39;end&#39;: datetime.datetime(2017, 12, 30, 13, 54, 33)}, {&#39;start&#39;: datetime.datetime(2017, 12, 30, 15, 9, 3), &#39;end&#39;: datetime.datetime(2017, 12, 30, 15, 19, 13)}] . Recreating ISO format with strftime() . Time Zones and Daylight Saving . We confidently tackle the time-related topic that causes people the most trouble: time zones and daylight saving. Continuing with our bike data, we will compare clocks around the world, how to gracefully handle &quot;spring forward&quot; and &quot;fall back,&quot; and how to get up-to-date timezone data from the dateutil library. . UTC offsets . Creating timezone aware datetimes . # Import datetime, timezone from datetime import datetime, timezone, timedelta # October 1, 2017 at 15:26:26, UTC dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=timezone.utc) # Print results print(dt.isoformat()) . 2017-10-01T15:26:26+00:00 . # Import datetime, timedelta, timezone from datetime import datetime, timedelta, timezone # Create a timezone for Pacific Standard Time, or UTC-8 pst = timezone(timedelta(hours=-8)) # October 1, 2017 at 15:26:26, UTC-8 dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=pst) # Print results print(dt.isoformat()) . 2017-10-01T15:26:26-08:00 . # Import datetime, timedelta, timezone from datetime import datetime, timedelta, timezone # Create a timezone for Australian Eastern Daylight Time, or UTC+11 aedt = timezone(timedelta(hours=11)) # October 1, 2017 at 15:26:26, UTC+11 dt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=aedt) # Print results print(dt.isoformat()) . 2017-10-01T15:26:26+11:00 . Did you know that Russia and France are tied for the most number of time zones, with 12 each? The French mainland only has one timezone, but because France has so many overseas dependencies they really add up! . Setting timezones . # Create a timezone object corresponding to UTC-4 edt = timezone(timedelta(hours=-4)) # Loop over trips, updating the start and end datetimes to be in UTC-4 for trip in onebike_datetimes[:10]: # Update trip[&#39;start&#39;] and trip[&#39;end&#39;] trip[&#39;start&#39;] = trip[&#39;start&#39;].replace(tzinfo=edt) trip[&#39;end&#39;] = trip[&#39;end&#39;].replace(tzinfo=edt) . Did you know that despite being over 2,500 miles (4,200 km) wide (about as wide as the continential United States or the European Union) China has only one official timezone? There&#39;s a second, unofficial timezone, too. It is used by much of the Uyghurs population in the Xinjiang province in the far west of China. . What time did the bike leave in UTC? . Having set the timezone for the first ten rides that W20529 took, let&#39;s see what time the bike left in UTC. . # Loop over the trips for trip in onebike_datetimes[:10]: # Pull out the start and set it to UTC dt = trip[&#39;start&#39;].astimezone(timezone.utc) # Print the start time in UTC print(&#39;Original:&#39;, trip[&#39;start&#39;], &#39;| UTC:&#39;, dt.isoformat()) . Original: 2017-10-01 15:23:25-04:00 | UTC: 2017-10-01T19:23:25+00:00 Original: 2017-10-01 15:42:57-04:00 | UTC: 2017-10-01T19:42:57+00:00 Original: 2017-10-02 06:37:10-04:00 | UTC: 2017-10-02T10:37:10+00:00 Original: 2017-10-02 08:56:45-04:00 | UTC: 2017-10-02T12:56:45+00:00 Original: 2017-10-02 18:23:48-04:00 | UTC: 2017-10-02T22:23:48+00:00 Original: 2017-10-02 18:48:08-04:00 | UTC: 2017-10-02T22:48:08+00:00 Original: 2017-10-02 19:18:10-04:00 | UTC: 2017-10-02T23:18:10+00:00 Original: 2017-10-02 19:37:32-04:00 | UTC: 2017-10-02T23:37:32+00:00 Original: 2017-10-03 08:24:16-04:00 | UTC: 2017-10-03T12:24:16+00:00 Original: 2017-10-03 18:17:07-04:00 | UTC: 2017-10-03T22:17:07+00:00 . Did you know that there is no official time zone at the North or South pole? Since all the lines of longitude meet each other, it&#39;s up to each traveler (or research station) to decide what time they want to use. . Timezones database . Putting the bike trips into the right time zone . Instead of setting the timezones for W20529 by hand, let&#39;s assign them to their IANA timezone:&#39;America/New_York&#39;. Since we know their political jurisdiction, we don&#39;t need to look up their UTC offset. Python will do that for us. . # Import tz from dateutil import tz # Create a timezone object for Eastern Time et = tz.gettz(&#39;America/New_York&#39;) # Loop over trips, updating the datetimes to be in Eastern Time for trip in onebike_datetimes[:10]: # Update trip[&#39;start&#39;] and trip[&#39;end&#39;] trip[&#39;start&#39;] = trip[&#39;start&#39;].replace(tzinfo=et) trip[&#39;end&#39;] = trip[&#39;end&#39;].replace(tzinfo=et) . Time zone rules actually change quite frequently. IANA time zone data gets updated every 3-4 months, as different jurisdictions make changes to their laws about time or as more historical information about timezones are uncovered. tz is smart enough to use the date in your datetime to determine which rules to use historically. . What time did the bike leave? (Global edition) . # Create the timezone object uk = tz.gettz(&#39;Europe/London&#39;) # Pull out the start of the first trip local = onebike_datetimes[0][&#39;start&#39;] # What time was it in the UK? notlocal = local.astimezone(uk) # Print them out and see the difference print(local.isoformat()) print(notlocal.isoformat()) . 2017-10-01T15:23:25-04:00 2017-10-01T20:23:25+01:00 . # Create the timezone object ist = tz.gettz(&#39;Asia/Kolkata&#39;) # Pull out the start of the first trip local = onebike_datetimes[0][&#39;start&#39;] # What time was it in India? notlocal = local.astimezone(ist) # Print them out and see the difference print(local.isoformat()) print(notlocal.isoformat()) . 2017-10-01T15:23:25-04:00 2017-10-02T00:53:25+05:30 . # Create the timezone object sm = tz.gettz(&#39;Pacific/Apia&#39;) # Pull out the start of the first trip local = onebike_datetimes[0][&#39;start&#39;] # What time was it in Samoa? notlocal = local.astimezone(sm) # Print them out and see the difference print(local.isoformat()) print(notlocal.isoformat()) . 2017-10-01T15:23:25-04:00 2017-10-02T09:23:25+14:00 . Did you notice the time offset for this one? It&#39;s at UTC+14! Samoa used to be UTC-10, but in 2011 it changed to the other side of the International Date Line to better match New Zealand, its closest trading partner. However, they wanted to keep the clocks the same, so the UTC offset shifted from -10 to +14, since 24-10 is 14. Timezones... not simple! . Starting Daylight saving time . How many hours elapsed around daylight saving? . Since our bike data takes place in the fall, you&#39;ll have to do something else to learn about the start of daylight savings time. . Let&#39;s look at March 12, 2017, in the Eastern United States, when Daylight Saving kicked in at 2 AM. . # Import datetime, timedelta, tz, timezone from datetime import datetime, timedelta, timezone from dateutil import tz # Start on March 12, 2017, midnight, then add 6 hours start = datetime(2017, 3, 12, tzinfo = tz.gettz(&#39;America/New_York&#39;)) end = start + timedelta(hours=6) print(start.isoformat() + &quot; to &quot; + end.isoformat()) . 2017-03-12T00:00:00-05:00 to 2017-03-12T06:00:00-04:00 . # Import datetime, timedelta, tz, timezone from datetime import datetime, timedelta, timezone from dateutil import tz # Start on March 12, 2017, midnight, then add 6 hours start = datetime(2017, 3, 12, tzinfo = tz.gettz(&#39;America/New_York&#39;)) end = start + timedelta(hours=6) print(start.isoformat() + &quot; to &quot; + end.isoformat()) # How many hours have elapsed? print((end - start).seconds/(60*60)) . 2017-03-12T00:00:00-05:00 to 2017-03-12T06:00:00-04:00 6.0 . # Import datetime, timedelta, tz, timezone from datetime import datetime, timedelta, timezone from dateutil import tz # Start on March 12, 2017, midnight, then add 6 hours start = datetime(2017, 3, 12, tzinfo = tz.gettz(&#39;America/New_York&#39;)) end = start + timedelta(hours=6) print(start.isoformat() + &quot; to &quot; + end.isoformat()) # How many hours have elapsed? print((end - start).total_seconds()/(60*60)) # What if we move to UTC? print((end.astimezone(timezone.utc) - start.astimezone(timezone.utc)) .total_seconds()/(60*60)) . 2017-03-12T00:00:00-05:00 to 2017-03-12T06:00:00-04:00 6.0 5.0 . When we compare times in local time zones, everything gets converted into clock time. Remember if you want to get absolute time differences, always move to UTC! . March 29, throughout a decade . Daylight Saving rules are complicated:they&#39;re different in different places, they change over time, and they usually start on a Sunday (and so they move around the calendar). For example, in the United Kingdom, as of the time this code was written, Daylight Saving begins on the last Sunday in March. Let&#39;s look at the UTC offset for March 29, at midnight, for the years 2000 to 2010. . # Import datetime and tz from datetime import datetime from dateutil import tz # Create starting date dt = datetime(2000, 3, 29, tzinfo = tz.gettz(&#39;Europe/London&#39;)) # Loop over the dates, replacing the year, and print the ISO timestamp for y in range(2000, 2011): print(dt.replace(year=y).isoformat()) . 2000-03-29T00:00:00+01:00 2001-03-29T00:00:00+01:00 2002-03-29T00:00:00+00:00 2003-03-29T00:00:00+00:00 2004-03-29T00:00:00+01:00 2005-03-29T00:00:00+01:00 2006-03-29T00:00:00+01:00 2007-03-29T00:00:00+01:00 2008-03-29T00:00:00+00:00 2009-03-29T00:00:00+00:00 2010-03-29T00:00:00+01:00 . As you can see, the rules for Daylight Saving are not trivial. When in doubt, always use tz instead of hand-rolling timezones, so it will catch the Daylight Saving rules (and rule changes!) for you. . Ending daylight saving time . Finding ambiguous datetimes . we saw something anomalous in our bike trip duration data. Let&#39;s see if we can identify what the problem might be. . Note that tz.datetime_ambiguous() only catches ambiguous datetimes from Daylight Saving changes. Other weird edge cases, like jurisdictions which change their Daylight Saving rules, hopefully should be caught by tz. And if they&#39;re not, at least those kinds of things are pretty rare in most data sets! . Cleaning daylight saving data with fold . As we&#39;ve just discovered, there is a ride in our data set which is being messed up by a Daylight Savings shift. Let&#39;s clean up the data set so we actually have a correct minimum ride length. We can use the fact that we know the end of the ride happened after the beginning to fix up the duration messed up by the shift out of Daylight Savings. . Since Python does not handle tz.enfold() when doing arithmetic, we must put our datetime objects into UTC, where ambiguities have been resolved. . Easy and Powerful: Dates and Times in Pandas . Reading date and time data in Pandas . import pandas as pd . # Load CSV into the rides variable rides = pd.read_csv(&#39;../datasets/capital-onebike.csv&#39;, parse_dates = [&#39;Start date&#39;, &#39;End date&#39;]) # Print the initial (0th) row print(rides.iloc[0]) . Start date 2017-10-01 15:23:25 End date 2017-10-01 15:26:26 Start station number 31038 Start station Glebe Rd &amp; 11th St N End station number 31036 End station George Mason Dr &amp; Wilson Blvd Bike number W20529 Member type Member Name: 0, dtype: object . Making timedelta columns . rides[&#39;Duration&#39;] = (rides[&#39;End date&#39;]-rides[&#39;Start date&#39;]).dt.total_seconds() rides.Duration.head() . 0 181.0 1 7622.0 2 343.0 3 1278.0 4 1277.0 Name: Duration, dtype: float64 . Summarizing datetime data in Pandas . How many joyrides? . Suppose we have a theory that some people take long bike rides before putting their bike back in the same dock. Let&#39;s call these rides &quot;joyrides&quot;. Are there many joyrides? How long were they in our data set? Lets use the median instead of the mean, because we know there are some very long trips in our data set that might skew the answer, and the median is less sensitive to outliers. . # Create joyrides joyrides = (rides[&#39;Start station&#39;] == rides[&#39;End station&#39;]) # Total number of joyrides print(&quot;{} rides were joyrides&quot;.format(joyrides.sum())) # Median of all rides print(&quot;The median duration overall was {:.2f} seconds&quot; .format(rides[&#39;Duration&#39;].median())) # Median of joyrides print(&quot;The median duration for joyrides was {:.2f} seconds&quot; .format(rides[joyrides][&#39;Duration&#39;].median())) . 6 rides were joyrides The median duration overall was 660.00 seconds The median duration for joyrides was 2642.50 seconds . It&#39;s getting cold outside, W20529 . Washington, D.C. has mild weather overall, but the average high temperature in October (68ºF / 20ºC) is certainly higher than the average high temperature in December (47ºF / 8ºC). People also travel more in December, and they work fewer days so they commute less. . How might the weather or the season have affected the length of bike trips? . # Import matplotlib import matplotlib.pyplot as plt # Resample rides to daily, take the size, plot the results rides.resample(&#39;D&#39;, on = &#39;Start date&#39;) .size() .plot(ylim = [0, 15]) # Show the results plt.show() . Since the daily time series is so noisy for this one bike, change the resampling to be monthly. . # Import matplotlib import matplotlib.pyplot as plt # Resample rides to monthly, take the size, plot the results rides.resample(&#39;M&#39;, on = &#39;Start date&#39;) .size() .plot(ylim = [0, 150]) # Show the results plt.show() . As you can see, the pattern is clearer at the monthly level: there were fewer rides in November, and then fewer still in December, possibly because the temperature got colder. . Members vs casual riders over time . Riders can either be &quot;Members&quot;, meaning they pay yearly for the ability to take a bike at any time, or &quot;Casual&quot;, meaning they pay at the kiosk attached to the bike dock. . Do members and casual riders drop off at the same rate over October to December, or does one drop off faster than the other? . # Resample rides to be monthly on the basis of Start date monthly_rides = rides.resample(&#39;M&#39;, on=&#39;Start date&#39;)[&#39;Member type&#39;] # Take the ratio of the .value_counts() over the total number of rides print(monthly_rides.value_counts() / monthly_rides.size()) . Start date Member type 2017-10-31 Member 0.768519 Casual 0.231481 2017-11-30 Member 0.825243 Casual 0.174757 2017-12-31 Member 0.860759 Casual 0.139241 Name: Member type, dtype: float64 . .resample() labels Monthly resampling with the last day in the month and not the first. It certainly looks like the fraction of Casual riders went down as the number of rides dropped. With a little more digging, you could figure out if keeping Member rides only would be enough to stabilize the usage numbers throughout the fall. . Combining groupby() and resample() . # Group rides by member type, and resample to the month grouped = rides.groupby(&#39;Member type&#39;) .resample(&#39;M&#39;, on=&#39;Start date&#39;) # Print the median duration for each group print(grouped[&#39;Duration&#39;].median()) . Member type Start date Casual 2017-10-31 1636.0 2017-11-30 1159.5 2017-12-31 850.0 Member 2017-10-31 671.0 2017-11-30 655.0 2017-12-31 387.5 Name: Duration, dtype: float64 . It looks like casual riders consistently took longer rides, but that both groups took shorter rides as the months went by. Note that, by combining grouping and resampling, you can answer a lot of questions about nearly any data set that includes time as a feature. Keep in mind that you can also group by more than one column at once. . Timezones in Pandas . # Localize the Start date column to America/New_York rides[&#39;Start date&#39;] = rides[&#39;Start date&#39;].dt.tz_localize(&quot;America/New_York&quot;, ambiguous=&#39;NaT&#39;) # Print first value print(rides[&#39;Start date&#39;].iloc[0]) . 2017-10-01 15:23:25-04:00 . # Convert the Start date column to Europe/London rides[&#39;Start date&#39;] = rides[&#39;Start date&#39;].dt.tz_convert(&quot;Europe/London&quot;) # Print the new value print(rides[&#39;Start date&#39;].iloc[0]) . 2017-10-01 20:23:25+01:00 . dt.tzconvert() converts to a new timezone, whereas dt.tzlocalize() sets a timezone in the first place. . How long per weekday? . # Add a column for the weekday of the start of the ride rides[&#39;Ride start weekday&#39;] = rides[&#39;Start date&#39;].dt.weekday # Print the median trip time per weekday print(rides.groupby(&#39;Ride start weekday&#39;)[&#39;Duration&#39;].median()) . Ride start weekday 0.0 922.5 1.0 644.0 2.0 629.0 3.0 659.0 4.0 684.0 5.0 610.0 6.0 625.0 Name: Duration, dtype: float64 .",
            "url": "https://victoromondi1997.github.io/blog/dates/times/python/2020/07/26/Working-with-Dates-and-Times-in-Python.html",
            "relUrl": "/dates/times/python/2020/07/26/Working-with-Dates-and-Times-in-Python.html",
            "date": " • Jul 26, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Unsupervised Learning in Python",
            "content": "Overview . Say we have a collection of customers with a variety of characteristics such as age, location, and financial history, and we wish to discover patterns and sort them into clusters. Or perhaps we have a set of texts, such as wikipedia pages, and we wish to segment them into categories based on their content. This is the world of unsupervised learning, called as such because we are not guiding, or supervising, the pattern discovery by some prediction task, but instead uncovering hidden structure from unlabeled data. Unsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension reduction to matrix factorization. We&#39;ll explore the fundamentals of unsupervised learning and implement the essential algorithms using scikit-learn and scipy. We will explore how to cluster, transform, visualize, and extract insights from unlabeled datasets, and end the session by building a recommender system to recommend popular musical artists. . Libraries . from sklearn import datasets from sklearn.cluster import KMeans from sklearn.preprocessing import (StandardScaler, Normalizer, normalize, MaxAbsScaler) from sklearn.pipeline import make_pipeline from sklearn.manifold import TSNE from sklearn.decomposition import (PCA, TruncatedSVD, NMF) from sklearn.feature_extraction.text import TfidfVectorizer import numpy as np import pandas as pd from scipy.cluster.hierarchy import (linkage, dendrogram, fcluster) from scipy.stats import pearsonr from scipy.sparse import csr_matrix import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline plt.style.use(&quot;ggplot&quot;) import warnings warnings.filterwarnings(&quot;ignore&quot;, message=&quot;numpy.ufunc size changed&quot;) warnings.filterwarnings(&quot;ignore&quot;, message=&quot;invalid value encountered in sign&quot;) . Clustering for dataset exploration . Exploring how to discover the underlying groups (or &quot;clusters&quot;) in a dataset. We&#39;ll be clustering companies using their stock market prices, and distinguishing different species by clustering their measurements. . Unsupervised Learning . Unsupervised learning . Unsupervised learning finds patterns in data | E.g. clustering customers by their purchases | Compressing the data using purchase patterns (dimension reduction) | . Supervised vs unsupervised learning . Supervised learning finds patterns for a prediction task | E.g. classify tumors as benign or cancerous (labels) | Unsupervised learning finds patterns in data ... but without a specific prediction task in mind | . | . Iris dataset . Measurements of many iris plants 1 | 3 species of iris:setosa, versicolor, virginica - Petal length, petal width, sepal length, sepal width (the features of the dataset) ### Iris data is 4-dimensional | Iris samples are points in 4 dimensional space | Dimension = number of features | Dimension too high to visualize! | ... but unsupervised learning gives insight | . k-means clustering . Finds clusters of samples | Number of clusters must be specified | Implemented in sklearn (&quot;scikit-learn&quot;) | . Cluster labels for new samples . New samples can be assigned to existing clusters | k-means remembers the mean of each cluster (the &quot;centroids&quot;) | Finds the nearest centroid to each new sample | . Scatter plots . Scatter plot of sepal length vs petal length | Each point represents an iris sample | Color points by cluster labels | PyPlot (matplotlib.pyplot) TODO:add scatter plot | . iris = datasets.load_iris() iris.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) . samples = iris.data samples[:5] . array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2]]) . k-means clustering with scikit-learn . model = KMeans(n_clusters=3) model.fit(samples) . KMeans(n_clusters=3) . labels = model.predict([[5.8, 4. , 1.2, 0.2]]) labels . array([1]) . Cluster labels for new samples . new_samples = [[ 5.7,4.4,1.5,0.4] ,[ 6.5,3. ,5.5,1.8] ,[ 5.8,2.7,5.1,1.9]] model.predict(new_samples) . array([1, 2, 0]) . Scatter plots . labels_iris = model.predict(samples) . xs_iris = samples[:,0] ys_iris = samples[:,2] _ = sns.scatterplot(xs_iris, ys_iris, hue=labels_iris) plt.show() . points = pd.read_csv(&quot;datasets/points.csv&quot;).values points[:5] . array([[ 0.06544649, -0.76866376], [-1.52901547, -0.42953079], [ 1.70993371, 0.69885253], [ 1.16779145, 1.01262638], [-1.80110088, -0.31861296]]) . xs_points = points[:,0] ys_points = points[:,1] _ = sns.scatterplot(xs_points, ys_points) plt.show() . There are three clusters . Clustering 2D points . From the scatter plot we saw that the points seem to separate into 3 clusters. we&#39;ll now create a KMeans model to find 3 clusters, and fit it to the data points. After the model has been fit, we&#39;ll obtain the cluster labels for some new points using the .predict() method. . new_points = pd.read_csv(&quot;datasets/new_points.csv&quot;).values new_points[:5] . array([[ 0.40023333, -1.26544471], [ 0.80323037, 1.28260167], [-1.39507552, 0.05572929], [-0.34119268, -1.07661994], [ 1.54781747, 1.40250049]]) . # Create a KMeans instance with 3 clusters: model model_points = KMeans(n_clusters=3) # Fit model to points model_points.fit(points) # Determine the cluster labels of new_points: labels labels_points = model_points.predict(new_points) # Print cluster labels of new_points print(labels_points) . [0 2 1 0 2 0 2 2 2 1 0 2 2 1 1 2 1 1 2 2 1 2 0 2 0 1 2 1 1 0 0 2 2 2 1 0 2 2 0 2 1 0 0 1 0 2 1 1 2 2 2 2 1 1 0 0 1 1 1 0 0 2 2 2 0 2 1 2 0 1 0 0 0 2 0 1 1 0 2 1 0 1 0 2 1 2 1 0 2 2 2 0 2 2 0 1 1 1 1 0 2 0 1 1 0 0 2 0 1 1 0 1 1 1 2 2 2 2 1 1 2 0 2 1 2 0 1 2 1 1 2 1 2 1 0 2 0 0 2 1 0 2 0 0 1 2 2 0 1 0 1 2 0 1 1 0 1 2 2 1 2 1 1 2 2 0 2 2 1 0 1 0 0 2 0 2 2 0 0 1 0 0 0 1 2 2 0 1 0 1 1 2 2 2 0 2 2 2 1 1 0 2 0 0 0 1 2 2 2 2 2 2 1 1 2 1 1 1 1 2 1 1 2 2 0 1 0 0 1 0 1 0 1 2 2 1 2 2 2 1 0 0 1 2 2 1 2 1 1 2 1 1 0 1 0 0 0 2 1 1 1 0 2 0 1 0 1 1 2 0 0 0 1 2 2 2 0 2 1 1 2 0 0 1 0 0 1 0 2 0 1 1 1 1 2 1 1 2 2 0] . We&#39;ve successfully performed k-Means clustering and predicted the labels of new points. But it is not easy to inspect the clustering by just looking at the printed labels. A visualization would be far more useful. We&#39;ll inspect the clustering with a scatter plot! . Inspect clustering . Let&#39;s now inspect the clustering we performed! . # Assign the columns of new_points: xs and ys xs_np = new_points[:,0] ys_np = new_points[:,1] # Make a scatter plot of xs and ys, using labels to define the colors _ = plt.scatter(xs_np, ys_np, c=labels_points, alpha=.5) # Assign the cluster centers: centroids centroids_p = model_points.cluster_centers_ # Assign the columns of centroids: centroids_x, centroids_y centroids_x_p = centroids_p[:,0] centroids_y_p = centroids_p[:,1] # Make a scatter plot of centroids_x and centroids_y _ = plt.scatter(centroids_x_p, centroids_y_p, marker=&quot;D&quot;, s=50) plt.show() . The clustering looks great! But how can we be sure that 3 clusters is the correct choice? In other words, how can we evaluate the quality of a clustering? . Evaluating a clustering . Can check correspondence with e.g. iris species | ... but what if there are no species to check against? | Measure quality of a clustering | Informs choice of how many clusters to look forIris:clusters vs species- k-means found 3 clusters amongst the iris samples . | . Cross tabulation with pandas . Clusters vs species is a &quot;cross-tabulation&quot; | . iris_ct = pd.DataFrame({&#39;labels&#39;:labels_iris, &#39;species&#39;:iris.target}) iris_ct.head() . labels species . 0 1 | 0 | . 1 1 | 0 | . 2 1 | 0 | . 3 1 | 0 | . 4 1 | 0 | . np.unique(iris.target) . array([0, 1, 2]) . iris_ct.species.unique() . array([0, 1, 2]) . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . iris_ct[&#39;species&#39;] = iris_ct.species.map({0:&#39;setosa&#39;, 1:&#39;versicolor&#39;, 2:&#39;virginica&#39;}) iris_ct.head() . labels species . 0 1 | setosa | . 1 1 | setosa | . 2 1 | setosa | . 3 1 | setosa | . 4 1 | setosa | . Crosstab of labels and species . pd.crosstab(iris_ct.labels, iris_ct.species) . species setosa versicolor virginica . labels . 0 0 | 48 | 14 | . 1 50 | 0 | 0 | . 2 0 | 2 | 36 | . Measuring clustering quality . Using only samples and their cluster labels | A good clustering has tight clusters | ... and samples in each cluster bunched together | . Inertia measures clustering quality . Measures how spread out the clusters are (lower is better) | Distance from each sample to centroid of its cluster | After fit(), available as attribute inertia_ | k-means attempts to minimize the inertia when choosing clusters | . model.inertia_ . 78.851441426146 . The number of clusters . Clusterings of the iris dataset with different numbers of clusters | More clusters means lower inertia | What is the best number of clusters? | . How many clusters to choose? . A good clustering has tight clusters (so low inertia) | ... but not too many clusters! | Choose an &quot;elbow&quot; in the inertia plot | Where inertia begins to decrease more slowly | E.g. for iris dataset, 3 is a good choice | . How many clusters of grain? . samples_grain = pd.read_csv(&quot;datasets/samples_grain.csv&quot;).values samples_grain[:5] . array([[15.26 , 14.84 , 0.871 , 5.763 , 3.312 , 2.221 , 5.22 ], [14.88 , 14.57 , 0.8811, 5.554 , 3.333 , 1.018 , 4.956 ], [14.29 , 14.09 , 0.905 , 5.291 , 3.337 , 2.699 , 4.825 ], [13.84 , 13.94 , 0.8955, 5.324 , 3.379 , 2.259 , 4.805 ], [16.14 , 14.99 , 0.9034, 5.658 , 3.562 , 1.355 , 5.175 ]]) . an array samples contains the measurements (such as area, perimeter, length, and several others) of samples of grain. What&#39;s a good number of clusters in this case? . ks_grain = range(1, 6) inertias_grain = [] for k in ks_grain: # Create a KMeans instance with k clusters: model model_grain = KMeans(n_clusters=k) # Fit model to samples model_grain.fit(samples_grain) # Append the inertia to the list of inertias inertias_grain.append(model_grain.inertia_) # Plot ks vs inertias plt.plot(ks_grain, inertias_grain, &#39;-o&#39;) plt.xlabel(&#39;number of clusters, k&#39;) plt.ylabel(&#39;inertia&#39;) plt.xticks(ks_grain) plt.show() . The inertia decreases very slowly from 3 clusters to 4, so it looks like 3 clusters would be a good choice for this data. . Evaluating the grain clustering . In fact, the grain samples come from a mix of 3 different grain varieties: &quot;Kama&quot;, &quot;Rosa&quot; and &quot;Canadian&quot;. We will cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation. . varieties = pd.read_csv(&quot;datasets/varieties.csv&quot;)[&quot;0&quot;].to_list() varieties[:5] . [&#39;Kama wheat&#39;, &#39;Kama wheat&#39;, &#39;Kama wheat&#39;, &#39;Kama wheat&#39;, &#39;Kama wheat&#39;] . list varieties gives the grain variety for each sample. . # Create a KMeans model with 3 clusters: model model_grain = KMeans(n_clusters=3) # Use fit_predict to fit model and obtain cluster labels: labels labels_grain = model_grain.fit_predict(samples_grain) # Create a DataFrame with labels and varieties as columns: df grain_df = pd.DataFrame({&#39;labels&#39;: labels_grain, &#39;varieties&#39;: varieties}) # Create crosstab: ct ct_grain = pd.crosstab(grain_df.labels, grain_df.varieties) # Display ct ct_grain . varieties Canadian wheat Kama wheat Rosa wheat . labels . 0 0 | 1 | 60 | . 1 2 | 60 | 10 | . 2 68 | 9 | 0 | . The cross-tabulation shows that the 3 varieties of grain separate really well into 3 clusters. But depending on the type of data you are working with, the clustering may not always be this good. Is there anything we can do in such situations to improve the clustering? . Transforming features for better clusterings . Piedmont wines dataset 2 . 178 samples from 3 distinct varieties of red wine:Barolo, Grignolino and Barbera- Features measure chemical composition e.g. alcohol content | ... also visual properties like “color intensity” ### Feature variancesfeature | The wine features have very different variances! | Variance of a feature measures spread of its values | . StandardScaler . In kmeans:feature variance = feature influence - StandardScaler transforms each feature to have mean 0 and variance 1 | Features are said to be &quot;standardized&quot; ### Similar methods | StandardScaler and KMeans have similar methods | Use fit() / transform() with StandardScaler | Use fit() / predict() with KMeans | . StandardScaler, then KMeans . Need to perform two steps:StandardScaler, then KMeans- Use sklearn pipeline to combine multiple steps | Data flows from one step into the next | . sklearn preprocessing steps . StandardScaler is a &quot;preprocessing&quot; step | MaxAbsScaler and Normalizer are other examples | . Scaling fish data for clustering . samples_fish = pd.read_csv(&quot;datasets/samples_fish.csv&quot;).values samples_fish[:5] . array([[242. , 23.2, 25.4, 30. , 38.4, 13.4], [290. , 24. , 26.3, 31.2, 40. , 13.8], [340. , 23.9, 26.5, 31.1, 39.8, 15.1], [363. , 26.3, 29. , 33.5, 38. , 13.3], [430. , 26.5, 29. , 34. , 36.6, 15.1]]) . an array samples_fish 3 gives measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, we&#39;ll need to standardize these features first. We&#39;ll build a pipeline to standardize and cluster the data. . # Create scaler: scaler_fish scaler_fish = StandardScaler() # Create KMeans instance: kmeans_fish kmeans_fish = KMeans(n_clusters=4) # Create pipeline: pipeline_fish pipeline_fish = make_pipeline(scaler_fish, kmeans_fish) . Now that We&#39;ve built the pipeline, we&#39;ll use it to cluster the fish by their measurements. . Clustering the fish data . species_fish = pd.read_csv(&quot;datasets/species_fish.csv&quot;)[&quot;0&quot;].to_list() species_fish[:5] . [&#39;Bream&#39;, &#39;Bream&#39;, &#39;Bream&#39;, &#39;Bream&#39;, &#39;Bream&#39;] . We&#39;ll now use the standardization and clustering pipeline to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species. . # Fit the pipeline to samples pipeline_fish.fit(samples_fish) # Calculate the cluster labels: labels_fish labels_fish = pipeline_fish.predict(samples_fish) # Create a DataFrame with labels and species as columns: df fish_df = pd.DataFrame({&#39;labels&#39;:labels_fish, &#39;species&#39;:species_fish}) # Create crosstab: ct ct_fish = pd.crosstab(fish_df.labels, fish_df.species) # Display ct ct_fish . species Bream Pike Roach Smelt . labels . 0 33 | 0 | 1 | 0 | . 1 1 | 0 | 19 | 1 | . 2 0 | 17 | 0 | 0 | . 3 0 | 0 | 0 | 13 | . It looks like the fish data separates really well into 4 clusters! . Clustering stocks using KMeans . We&#39;ll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). . movements = pd.read_csv(&quot;datasets/movements.csv&quot;).values movements[:5] . array([[ 0.58 , -0.220005, -3.409998, ..., -5.359962, 0.840019, -19.589981], [ -0.640002, -0.65 , -0.210001, ..., -0.040001, -0.400002, 0.66 ], [ -2.350006, 1.260009, -2.350006, ..., 4.790009, -1.760009, 3.740021], [ 0.109997, 0. , 0.260002, ..., 1.849999, 0.040001, 0.540001], [ 0.459999, 1.77 , 1.549999, ..., 1.940002, 1.130005, 0.309998]]) . A NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day. . Some stocks are more expensive than others. To account for this, we will include a Normalizer at the beginning of the pipeline. The Normalizer will separately transform each company&#39;s stock price to a relative scale before the clustering begins. . Note: Normalizer() is different to StandardScaler(). While StandardScaler() standardizes features (such as the features of the fish data) by removing the mean and scaling to unit variance, Normalizer() rescales each sample - here, each company&#8217;s stock price - independently of the other. . # Create a normalizer: normalizer_movements normalizer_movements = Normalizer() # Create a KMeans model with 10 clusters: kmeans_movements kmeans_movements = KMeans(n_clusters=10) # Make a pipeline chaining normalizer and kmeans: pipeline_movements pipeline_movements = make_pipeline(normalizer_movements, kmeans_movements) # Fit pipeline to the daily price movements pipeline_movements.fit(movements) . Pipeline(steps=[(&#39;normalizer&#39;, Normalizer()), (&#39;kmeans&#39;, KMeans(n_clusters=10))]) . Now that the pipeline has been set up, we can find out which stocks move together . Which stocks move together? . So which company have stock prices that tend to change in the same way? We&#39;ll now inspect the cluster labels from the clustering to find out. . companies_movements=pd.read_csv(&quot;datasets/companies_movements.csv&quot;) companies_movements.head() . 0 . 0 Apple | . 1 AIG | . 2 Amazon | . 3 American express | . 4 Boeing | . companies_movements=companies_movements[&quot;0&quot;].to_list() companies_movements[:5] . [&#39;Apple&#39;, &#39;AIG&#39;, &#39;Amazon&#39;, &#39;American express&#39;, &#39;Boeing&#39;] . a list companies_movements of the company names . # Predict the cluster labels: labels_movements labels_movements = pipeline_movements.predict(movements) # Create a DataFrame aligning labels and companies: df movements_df = pd.DataFrame({&#39;labels&#39;: labels_movements, &#39;companies&#39;: companies_movements}) # Display df sorted by cluster label movements_df.sort_values(&quot;labels&quot;) . labels companies . 18 0 | Goldman Sachs | . 26 0 | JPMorgan Chase | . 16 0 | General Electrics | . 15 0 | Ford | . 5 0 | Bank of America | . 55 0 | Wells Fargo | . 3 0 | American express | . 1 0 | AIG | . 22 1 | HP | . 20 1 | Home Depot | . 58 1 | Xerox | . 30 1 | MasterCard | . 23 1 | IBM | . 14 1 | Dell | . 54 1 | Walgreen | . 32 1 | 3M | . 11 1 | Cisco | . 33 1 | Microsoft | . 47 1 | Symantec | . 8 1 | Caterpillar | . 13 1 | DuPont de Nemours | . 39 2 | Pfizer | . 53 2 | Valero Energy | . 37 2 | Novartis | . 42 2 | Royal Dutch Shell | . 6 2 | British American Tobacco | . 57 2 | Exxon | . 19 2 | GlaxoSmithKline | . 35 2 | Navistar | . 46 2 | Sanofi-Aventis | . 52 2 | Unilever | . 12 2 | Chevron | . 10 2 | ConocoPhillips | . 49 2 | Total | . 43 2 | SAP | . 44 2 | Schlumberger | . 40 3 | Procter Gamble | . 27 3 | Kimberly-Clark | . 56 3 | Wal-Mart | . 9 3 | Colgate-Palmolive | . 25 3 | Johnson &amp; Johnson | . 2 4 | Amazon | . 59 4 | Yahoo | . 21 5 | Honda | . 45 5 | Sony | . 34 5 | Mitsubishi | . 48 5 | Toyota | . 7 5 | Canon | . 24 6 | Intel | . 50 6 | Taiwan Semiconductor Manufacturing | . 51 6 | Texas instruments | . 28 7 | Coca Cola | . 31 7 | McDonalds | . 41 7 | Philip Morris | . 38 7 | Pepsi | . 29 8 | Lookheed Martin | . 4 8 | Boeing | . 36 8 | Northrop Grumman | . 17 9 | Google/Alphabet | . 0 9 | Apple | . Visualization with hierarchical clustering and t-SNE . We&#39;ll Explore two unsupervised learning techniques for data visualization, hierarchical clustering and t-SNE. Hierarchical clustering merges the data samples into ever-coarser clusters, yielding a tree visualization of the resulting cluster hierarchy. t-SNE maps the data samples into 2d space so that the proximity of the samples to one another can be visualized. . Visualizing hierarchies . Visualisations communicate insight . &quot;t-SNE&quot; :Creates a 2D map of a dataset (later) - &quot;Hierarchical clustering&quot; | . A hierarchy of groups . Groups of living things can form a hierarchy | Clusters are contained in one another | . Eurovision scoring dataset 4 . Countries gave scores to songs performed at the Eurovision 2016 | 2D array of scores | Rows are countries, columns are songs | . Hierarchical clustering . Every country begins in a separate cluster | At each step, the two closest clusters are merged | Continue until all countries in a single cluster | This is “agglomerative” hierarchical clustering | . The dendrogram of a hierarchical clustering . Read from the bottom up | Vertical lines represent clusters | . With 5 data samples, there would be 4 merge operations, and with 6 data samples, there would be 5 merges, and so on. . Hierarchical clustering of the grain data . the SciPy linkage() function performs hierarchical clustering on an array of samples. We will use the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to visualize the result. . # Calculate the linkage: mergings_g mergings_g = linkage(samples_grain, method=&#39;complete&#39;) # Plot the dendrogram, using varieties as labels plt.figure(figsize=(20,7)) dendrogram(mergings_g, labels=varieties, leaf_rotation=90, leaf_font_size=8, ) plt.show() . Dendrograms are a great way to illustrate the arrangement of the clusters produced by hierarchical clustering. . Hierarchies of stocks . We used k-means clustering to cluster companies according to their stock price movements. Now, we&#39;ll perform hierarchical clustering of the companies. SciPy hierarchical clustering doesn&#39;t fit into a sklearn pipeline, so we&#39;ll need to use the normalize() function from sklearn.preprocessing instead of Normalizer. . # Normalize the movements: normalized_movements normalized_movements = normalize(movements) # Calculate the linkage: mergings mergings_m = linkage(normalized_movements, method=&quot;complete&quot;) # Plot the dendrogram plt.figure(figsize=(20,10)) dendrogram(mergings_m, labels=companies_movements, leaf_font_size=12, leaf_rotation=90) plt.show() . You can produce great visualizations such as this with hierarchical clustering, but it can be used for more than just visualizations. . Cluster labels in hierarchical clustering . Cluster labels in hierarchical clustering . Not only a visualisation tool! | Cluster labels at any intermediate stage can be recovered | For use in e.g. cross-tabulations | . Intermediate clusterings &amp; height on dendrogram . E.g. at height 15:Bulgaria, Cyprus, Greece are one cluster - Russia and Moldova are another | Armenia in a cluster on its own ### Dendrograms show cluster distances | Height on dendrogram = distance between merging clusters | E.g. clusters with only Cyprus and Greece had distance approx. 6 | This new cluster distance approx. 12 from cluster with only Bulgaria | . Intermediate clusterings &amp; height on dendrogram . Height on dendrogram specifies max. distance between merging clusters | Don&#39;t merge clusters further apart than this (e.g. 15) | . Distance between clusters . Defined by a &quot;linkage method&quot; | Specified via method parameter, e.g. linkage(samples, method=&quot;complete&quot;) | In &quot;complete&quot; linkage:distance between clusters is max. distance between their samples - Different linkage method, different hierarchical clustering! | . Extracting cluster labels . Use the fcluster method | Returns a NumPy array of cluster labels | . the linkage method defines how the distance between clusters is measured. In complete linkage, the distance between clusters is the distance between the furthest points of the clusters. In single linkage, the distance between clusters is the distance between the closest points of the clusters. . Different linkage, different hierarchical clustering! . We will perform a hierarchical clustering of the voting countries with &#39;single&#39; linkage. Different linkage, different hierarchical clustering! . samples_eurovision = pd.read_csv(&quot;datasets/samples_eurovision.csv&quot;).values samples_eurovision[:5] . array([[ 2., 12., 0., 0., 0., 8., 0., 0., 0., 0., 0., 0., 1., 0., 10., 0., 4., 0., 5., 7., 0., 0., 3., 0., 6., 0.], [12., 0., 4., 0., 0., 0., 0., 6., 0., 7., 8., 0., 3., 0., 0., 0., 0., 5., 1., 12., 0., 0., 2., 0., 10., 0.], [ 0., 12., 3., 0., 12., 10., 0., 0., 0., 7., 0., 0., 0., 0., 0., 0., 1., 6., 0., 5., 0., 2., 0., 0., 8., 4.], [ 0., 3., 12., 0., 0., 5., 0., 0., 0., 1., 0., 2., 0., 0., 0., 0., 0., 0., 12., 8., 4., 0., 7., 6., 10., 0.], [ 0., 2., 0., 12., 0., 8., 0., 0., 0., 4., 1., 0., 7., 6., 0., 0., 0., 5., 3., 12., 0., 0., 0., 0., 10., 0.]]) . country_names_eurovision = pd.read_csv(&quot;datasets/country_names_eurovision.csv&quot;)[&quot;0&quot;].to_list() country_names_eurovision[:5] . [&#39;Albania&#39;, &#39;Armenia&#39;, &#39;Australia&#39;, &#39;Austria&#39;, &#39;Azerbaijan&#39;] . len(country_names_eurovision) . 42 . # Calculate the linkage: mergings mergings_ev = linkage(samples_eurovision, method=&#39;single&#39;) # Plot the dendrogram plt.figure(figsize=(20,9)) dendrogram(mergings_ev, labels=country_names_eurovision, leaf_rotation=90, leaf_font_size=12) plt.show() . Extracting the cluster labels . We saw that the intermediate clustering of the grain samples at height 6 has 3 clusters. We will use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation. . # Use fcluster to extract labels: labels_g labels_g = fcluster(mergings_g,6, criterion=&#39;distance&#39;) # Create a DataFrame with labels and varieties as columns: df grain_df = pd.DataFrame({&#39;labels&#39;: labels_g, &#39;varieties&#39;: varieties}) # Create crosstab: ct grain_ct = pd.crosstab(grain_df.labels, grain_df.varieties) # Display ct print(grain_ct) . varieties Canadian wheat Kama wheat Rosa wheat labels 1 0 0 47 2 0 52 23 3 13 1 0 4 57 17 0 . We&#39;ve now mastered the fundamentals of k-Means and agglomerative hierarchical clustering. Next, we&#39;ll explore t-SNE, which is a powerful tool for visualizing high dimensional data. . t-SNE for 2-dimensional maps . t-SNE for 2-dimensional maps . t-SNE = “t-distributed stochastic neighbor embedding” | Maps samples to 2D space (or 3D) | Map approximately preserves nearness of samples | Great for inspecting datasets | . t-SNE on the iris dataset . Iris dataset has 4 measurements, so samples are 4-dimensional | t-SNE maps samples to 2D space | t-SNE didn&#39;t know that there were different species | ... yet kept the species mostly separate | . . Interpreting t-SNE scatter plots . “versicolor” and “virginica” harder to distinguish from one another | Consistent with k-means inertia plot:could argue for 2 clusters, or for 3 | . t-SNE in sklearnIn . 2D NumPy array samples | List species giving species of labels as number (0, 1, or 2) | . samples[:5] . array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2]]) . iris.target[:5] . array([0, 0, 0, 0, 0]) . t-SNE in sklearn . model_i = TSNE(learning_rate=100) transformed_i = model_i.fit_transform(samples) xs_i = transformed_i[:,0] ys_i = transformed_i[:,1] plt.scatter(xs_i, ys_i, c=iris.target) plt.show() . t-SNE has only fit_transform() . Has a fit_transform() method | Simultaneously fits the model and transforms the data | Has no separate fit() or transform() methods | Can’t extend the map to include new data samples | Must start over each time! | . t-SNE learning rate . Choose learning rate for the dataset | Wrong choice:points bunch together- Try values between 50 and 200 ### Different every time | t-SNE features are different every time | Piedmont wines, 3 runs, 3 different scatter plots! | ... however:The wine varieties (=colors) have same position relative to one another | . t-SNE visualization of grain dataset . We&#39;ll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. . variety_numbers_g = pd.read_csv(&quot;datasets/variety_numbers_grains.csv&quot;)[&quot;0&quot;].to_list() variety_numbers_g[:5] . [1, 1, 1, 1, 1] . samples_grain[:5] . array([[15.26 , 14.84 , 0.871 , 5.763 , 3.312 , 2.221 , 5.22 ], [14.88 , 14.57 , 0.8811, 5.554 , 3.333 , 1.018 , 4.956 ], [14.29 , 14.09 , 0.905 , 5.291 , 3.337 , 2.699 , 4.825 ], [13.84 , 13.94 , 0.8955, 5.324 , 3.379 , 2.259 , 4.805 ], [16.14 , 14.99 , 0.9034, 5.658 , 3.562 , 1.355 , 5.175 ]]) . # Create a TSNE instance: model model_g = TSNE(learning_rate=200) # Apply fit_transform to samples: tsne_features tsne_features_g = model_g.fit_transform(samples_grain) # Select the 0th feature: xs xs_g = tsne_features_g[:,0] # Select the 1st feature: ys ys_g = tsne_features_g[:,1] # Scatter plot, coloring by variety_numbers_g plt.scatter(xs_g, ys_g, c=variety_numbers_g) plt.show() . the t-SNE visualization manages to separate the 3 varieties of grain samples. But how will it perform on the stock data? . t-SNE map of the stock market . t-SNE provides great visualizations when the individual samples can be labeled. We&#39;ll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives a map of the stock market! The stock price movements for each company are available as the array normalized_movements. The list companies gives the name of each company. . # Create a TSNE instance: model model_m = TSNE(learning_rate=50) # Apply fit_transform to normalized_movements: tsne_features tsne_features_m = model_m.fit_transform(normalized_movements) # Select the 0th feature: xs xs_m = tsne_features_m[:,0] # Select the 1th feature: ys ys_m = tsne_features_m[:,1] # Scatter plot plt.figure(figsize=(20,14)) plt.scatter(xs_m, ys_m) # Annotate the points for x, y, company in zip(xs_m, ys_m, companies_movements): plt.annotate(company, (x, y), fontsize=12) plt.show() . It&#39;s visualizations such as this that make t-SNE such a powerful tool for extracting quick insights from high dimensional data. . Decorrelating the data and dimension reduction . Dimension reduction summarizes a dataset using its common occuring patterns. We&#39;ll explore the most fundamental of dimension reduction techniques, &quot;Principal Component Analysis&quot; (&quot;PCA&quot;). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you&#39;ll employ a variant of PCA will allow you to cluster Wikipedia articles by their content! . Visualizing the PCA transformation . Dimension reduction . More efficient storage and computation | Remove less-informative &quot;noise&quot; features | ... which cause problems for prediction tasks, e.g. classification, regression | . Principal Component Analysis . PCA = &quot;Principal Component Analysis&quot; | Fundamental dimension reduction technique | First step &quot;decorrelation&quot; (considered below) | Second step reduces dimension (considered later) | . PCA aligns data with axes . Rotates data samples to be aligned with axes | Shifts data samples so they have mean 0 | No information is lostPCA | . . PCA follows the fit/transform pattern . PCA a scikit-learn component like KMeans or StandardScaler | fit() learns the transformation from given data | transform() applies the learned transformation | transform() can also be applied to new data | . wine = pd.read_csv(&quot;datasets/wine.csv&quot;) wine.head() . class_label class_name alcohol malic_acid ash alcalinity_of_ash magnesium total_phenols flavanoids nonflavanoid_phenols proanthocyanins color_intensity hue od280 proline . 0 1 | Barolo | 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.64 | 1.04 | 3.92 | 1065 | . 1 1 | Barolo | 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 2 1 | Barolo | 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.68 | 1.03 | 3.17 | 1185 | . 3 1 | Barolo | 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.80 | 0.86 | 3.45 | 1480 | . 4 1 | Barolo | 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.32 | 1.04 | 2.93 | 735 | . Using scikit-learn PCA . samples_wine = array of two wine features (total_phenols &amp; od280) | . samples_wine = wine[[&#39;total_phenols&#39;, &#39;od280&#39;]].values samples_wine[:5] . array([[2.8 , 3.92], [2.65, 3.4 ], [2.8 , 3.17], [3.85, 3.45], [2.8 , 2.93]]) . model_w = PCA() model_w.fit(samples_wine) . PCA() . transformed_w = model_w.transform(samples_wine) . PCA features . Rows of transformed correspond to samples | Columns of transformed are the &quot;PCA features&quot; | Row gives PCA feature values of corresponding sample | . transformed_w[:5] . array([[nan, nan], [nan, nan], [nan, nan], [nan, nan], [nan, nan]]) . PCA features are not correlated . Features of dataset are often correlated, e.g. total_phenols and od280 | PCA aligns the data with axes | Resulting PCA features are not linearly correlated (&quot;decorrelation&quot;)PCA | . . Pearson correlation . Measures linear correlation of features | Value between -1 and 1 | Value of 0 means no linear correlation | . . Principal components . &quot;Principal components&quot; = directions of variance | PCA aligns principal components with the axes | Available as components_ attribute of PCA object | Each row defines displacement from mean | . model_w.components_ . array([[nan, nan], [nan, nan]]) . Correlated data in nature . We have array grains giving the width and length of samples of grain. We suspect that width and length will be correlated. To confirm this, let&#39;s make a scatter plot of width vs length and measure their Pearson correlation. . # Assign the 0th column of grains: width width_g = samples_grain[:,4] # Assign the 1st column of grains: length length_g = samples_grain[:,3] # Scatter plot width vs length plt.scatter(width_g, length_g) plt.axis(&#39;equal&#39;) plt.show() # Calculate the Pearson correlation correlation_g, pvalue_g = pearsonr(width_g, length_g) # Display the correlation correlation_g . 0.8604149377143469 . As you would expect, the width and length of the grain samples are highly correlated. . Decorrelating the grain measurements with PCA . We observed that the width and length measurements of the grain are correlated. Now, we&#39;ll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation. . grains = pd.read_csv(&quot;datasets/grains.csv&quot;).values grains[:5] . array([[3.312, 5.763], [3.333, 5.554], [3.337, 5.291], [3.379, 5.324], [3.562, 5.658]]) . # Create PCA instance: model_g model_g = PCA() # Apply the fit_transform method of model to grains: pca_features pca_features_g = model_g.fit_transform(grains) # Assign 0th column of pca_features: xs xs_g = pca_features_g[:,0] # Assign 1st column of pca_features: ys ys_g = pca_features_g[:,1] # Scatter plot xs vs ys plt.scatter(xs_g, ys_g) plt.axis(&#39;equal&#39;) plt.show() # Calculate the Pearson correlation of xs and ys correlation_g, pvalue_g = pearsonr(xs_g, ys_g) # Display the correlation correlation_g . 1.5634195327240974e-16 . principal components have to align with the axes of the point cloud. . Intrinsic dimension . Intrinsic dimension of a flight path . 2 features:longitude and latitude at points along a flight path - Dataset appears to be 2-dimensional | But can approximate using one feature: displacement along flight path | Is intrinsically 1-dimensiona | . . Intrinsic dimension . Intrinsic dimension = number of features needed to approximate the dataset | Essential idea behind dimension reduction | What is the most compact representation of the samples? | Can be detected with PCA | . Versicolor dataset . &quot;versicolor&quot;, one of the iris species | Only 3 features:sepal length, sepal width, and petal width- Samples are points in 3D space ### Versicolor dataset has intrinsic dimension 2 | Samples lie close to a flat 2-dimensional sheet | So can be approximated using 2 features | . . PCA identifies intrinsic dimension . Scatter plots work only if samples have 2 or 3 features | PCA identifies intrinsic dimension when samples have any number of features | Intrinsic dimension = number of PCA features with significant variance | . PCA of the versicolor samples . . PCA features are ordered by variance descending . . Variance and intrinsic dimension . Intrinsic dimension is number of PCA features with significant variance | In our example:the first two PCA features - So intrinsic dimension is 2 | . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . versicolor = pd.DataFrame(iris.data, columns=iris.feature_names) versicolor[&#39;target&#39;] = iris.target versicolor = versicolor[versicolor.target==1] versicolor.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target . 50 7.0 | 3.2 | 4.7 | 1.4 | 1 | . 51 6.4 | 3.2 | 4.5 | 1.5 | 1 | . 52 6.9 | 3.1 | 4.9 | 1.5 | 1 | . 53 5.5 | 2.3 | 4.0 | 1.3 | 1 | . 54 6.5 | 2.8 | 4.6 | 1.5 | 1 | . samples_versicolor = versicolor[[&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal width (cm)&#39;]].values samples_versicolor[:5] . array([[7. , 3.2, 1.4], [6.4, 3.2, 1.5], [6.9, 3.1, 1.5], [5.5, 2.3, 1.3], [6.5, 2.8, 1.5]]) . Plotting the variances of PCA features . samples_versicolor = array of versicolor samples | . pca_versicolor = PCA() pca_versicolor.fit(samples_versicolor) . PCA() . features_versicolor = range(pca_versicolor.n_components_) plt.bar(features_versicolor, pca_versicolor.explained_variance_) plt.xticks(features_versicolor) plt.xlabel(&quot;PCA feature&quot;) plt.ylabel(&quot;Variance&quot;) plt.show() . Intrinsic dimension can be ambiguous . Intrinsic dimension is an idealization | ... there is not always one correct answer! | Piedmont wines:could argue for 2, or for 3, or more | . The first principal component . The first principal component of the data is the direction in which the data varies the most. We will use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot. . # Make a scatter plot of the untransformed points plt.scatter(grains[:,0], grains[:,1]) # Fit model to points model_g.fit(grains) # Get the mean of the grain samples: mean mean_g = model_g.mean_ # Get the first principal component: first_pc first_pc_g = model_g.components_[0,:] # Plot first_pc as an arrow, starting at mean plt.arrow(mean_g[0], mean_g[1], first_pc_g[0], first_pc_g[1], color=&#39;blue&#39;, width=0.01) # Keep axes on same scale plt.axis(&#39;equal&#39;) plt.show() . This is the direction in which the grain data varies the most. . Variance of the PCA features . The fish dataset is 6-dimensional. But what is its intrinsic dimension? We will make a plot of the variances of the PCA features to find out. We&#39;ll need to standardize the features first. . # Create scaler: scaler scaler_fish = StandardScaler() # Create a PCA instance: pca pca_fish = PCA() # Create pipeline: pipeline pipeline_fish = make_pipeline(scaler_fish, pca_fish) # Fit the pipeline to &#39;samples&#39; pipeline_fish.fit(samples_fish) # Plot the explained variances features_fish = range(pca_fish.n_components_) plt.bar(features_fish, pca_fish.explained_variance_) plt.xlabel(&#39;PCA feature&#39;) plt.ylabel(&#39;variance&#39;) plt.xticks(features_fish) plt.show() . It looks like PCA features 0 and 1 have significant variance. Since PCA features 0 and 1 have significant variance, the intrinsic dimension of this dataset appears to be 2. . Dimension reduction with PCA . Dimension reduction . Represents same data, using less features | Important part of machine-learning pipelines | Can be performed using PCA | . Dimension reduction with PCA . PCA features are in decreasing order of variance | Assumes the low variance features are &quot;noise&quot; | ... and high variance features are informative | . Dimension reduction with PCA . Specify how many features to keep | E.g. PCA(n_components=2) | Keeps the first 2 PCA features | Intrinsic dimension is a good choice | . Dimension reduction of iris dataset . samples = array of iris measurements (4 features) | species = list of iris species numbers | . Dimension reduction with PCA . Discards low variance PCA features | Assumes the high variance features are informative | Assumption typically holds in practice (e.g. for iris) | . Word frequency arrays . Rows represent documents, columns represent words | Entries measure presence of each word in each document | ... measure using &quot;tf-idf&quot; (more later) | . Sparse arrays and csr_matrix . Array is &quot;sparse&quot;:most entries are zero - Can use scipy.sparse.csr_matrix instead of NumPy array | csr_matrix remembers only the non-zero entries (saves space!) | . TruncatedSVD and csr_matrix . scikit-learn PCA doesn&#39;t support csr_matrix | Use scikit-learn TruncatedSVD instead | Performs same transformation | . Dimension reduction of the fish measurements . We saw that 2 was a reasonable choice for the &quot;intrinsic dimension&quot; of the fish measurements. We will use PCA for dimensionality reduction of the fish measurements, retaining only the 2 most important components. . scaled_samples_fish = pd.read_csv(&quot;datasets/scaled_samples_fish.csv&quot;).values scaled_samples_fish[:5] . array([[-0.50109735, -0.36878558, -0.34323399, -0.23781518, 1.0032125 , 0.25373964], [-0.37434344, -0.29750241, -0.26893461, -0.14634781, 1.15869615, 0.44376493], [-0.24230812, -0.30641281, -0.25242364, -0.15397009, 1.13926069, 1.0613471 ], [-0.18157187, -0.09256329, -0.04603648, 0.02896467, 0.96434159, 0.20623332], [-0.00464454, -0.0747425 , -0.04603648, 0.06707608, 0.8282934 , 1.0613471 ]]) . # Create a PCA model with 2 components: pca pca_fish = PCA(n_components=2) # Fit the PCA instance to the scaled samples pca_fish.fit(scaled_samples_fish) # Transform the scaled samples: pca_features pca_features_fish = pca_fish.transform(scaled_samples_fish) # Print the shape of pca_features pca_features_fish.shape . (85, 2) . We&#39;ve successfully reduced the dimensionality from 6 to 2. . A tf-idf word-frequency array . We&#39;ll create a tf-idf word frequency array for a toy collection of documents. For this, we will use the TfidfVectorizer from sklearn. It transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects. . documents = [&#39;cats say meow&#39;, &#39;dogs say woof&#39;, &#39;dogs chase cats&#39;] . # Create a TfidfVectorizer: tfidf tfidf_d = TfidfVectorizer() # Apply fit_transform to document: csr_mat csr_mat_d = tfidf_d.fit_transform(documents) # Print result of toarray() method print(csr_mat_d.toarray()) # Get the words: words words_d = tfidf_d.get_feature_names() # Print words words_d . [[0.51785612 0. 0. 0.68091856 0.51785612 0. ] [0. 0. 0.51785612 0. 0.51785612 0.68091856] [0.51785612 0.68091856 0.51785612 0. 0. 0. ]] . [&#39;cats&#39;, &#39;chase&#39;, &#39;dogs&#39;, &#39;meow&#39;, &#39;say&#39;, &#39;woof&#39;] . Clustering Wikipedia part I . TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. We will cluster some popular pages from Wikipedia 5. We will build the pipeline and apply it to the word-frequency array of some Wikipedia articles. . The Pipeline object will be consisting of a TruncatedSVD followed by KMeans. . # Create a TruncatedSVD instance: svd svd_wp = TruncatedSVD(n_components=50) # Create a KMeans instance: kmeans kmeans_wp = KMeans(n_clusters=6) # Create a pipeline: pipeline pipeline_wp = make_pipeline(svd_wp, kmeans_wp) . Now that we have set up the pipeline, we will use to cluster the articles. . Clustering Wikipedia part II . wv = pd.read_csv(&quot;datasets/Wikipedia_articles/wikipedia-vectors.csv&quot;, index_col=0) articles = csr_matrix(wv.transpose()) articles_titles = list(wv.columns) . # Fit the pipeline to articles pipeline_wp.fit(articles) # Calculate the cluster labels: labels labels_wp = pipeline_wp.predict(articles) # Create a DataFrame aligning labels and titles: df wp = pd.DataFrame({&#39;label&#39;: labels_wp, &#39;article&#39;: articles_titles}) # Display df sorted by cluster label wp.sort_values(&quot;label&quot;) . label article . 47 0 | Fever | . 40 0 | Tonsillitis | . 41 0 | Hepatitis B | . 42 0 | Doxycycline | . 43 0 | Leukemia | . 44 0 | Gout | . 45 0 | Hepatitis C | . 46 0 | Prednisone | . 49 0 | Lymphoma | . 48 0 | Gabapentin | . 58 1 | Sepsis | . 59 1 | Adam Levine | . 50 1 | Chad Kroeger | . 51 1 | Nate Ruess | . 52 1 | The Wanted | . 53 1 | Stevie Nicks | . 54 1 | Arctic Monkeys | . 55 1 | Black Sabbath | . 56 1 | Skrillex | . 57 1 | Red Hot Chili Peppers | . 28 2 | Anne Hathaway | . 27 2 | Dakota Fanning | . 26 2 | Mila Kunis | . 25 2 | Russell Crowe | . 29 2 | Jennifer Aniston | . 23 2 | Catherine Zeta-Jones | . 22 2 | Denzel Washington | . 21 2 | Michael Fassbender | . 20 2 | Angelina Jolie | . 24 2 | Jessica Biel | . 10 3 | Global warming | . 11 3 | Nationally Appropriate Mitigation Action | . 12 3 | Nigel Lawson | . 13 3 | Connie Hedegaard | . 14 3 | Climate change | . 15 3 | Kyoto Protocol | . 17 3 | Greenhouse gas emissions by the United States | . 18 3 | 2010 United Nations Climate Change Conference | . 16 3 | 350.org | . 19 3 | 2007 United Nations Climate Change Conference | . 9 4 | LinkedIn | . 1 4 | Alexa Internet | . 2 4 | Internet Explorer | . 3 4 | HTTP cookie | . 4 4 | Google Search | . 5 4 | Tumblr | . 6 4 | Hypertext Transfer Protocol | . 7 4 | Social search | . 8 4 | Firefox | . 0 4 | HTTP 404 | . 30 5 | France national football team | . 31 5 | Cristiano Ronaldo | . 32 5 | Arsenal F.C. | . 33 5 | Radamel Falcao | . 34 5 | Zlatan Ibrahimović | . 35 5 | Colombia national football team | . 36 5 | 2014 FIFA World Cup qualification | . 37 5 | Football | . 38 5 | Neymar | . 39 5 | Franck Ribéry | . Discovering interpretable features . We&#39;ll explore a dimension reduction technique called &quot;Non-negative matrix factorization&quot; (&quot;NMF&quot;) that expresses samples as combinations of interpretable parts. For example, it expresses documents as combinations of topics, and images in terms of commonly occurring visual patterns. We&#39;ll also explore how to use NMF to build recommender systems that can find us similar articles to read, or musical artists that match your listening history! . Non-negative matrix factorization (NMF) . NMF = &quot;non-negative matrix factorization&quot; | Dimension reduction technique | NMF models are interpretable (unlike PCA) | Easy to interpret means easy to explain! | However, all sample features must be non-negative (&gt;= 0) | . Interpretable parts . NMF expresses documents as combinations of topics (or &quot;themes&quot;) | NMF expresses images as combinations of patterns | . Using scikit-learn NMF . Follows fit() / transform() pattern | Must specify number of components e.g. NMF(n_components=2) | Works with NumPy arrays and with csr_matrix | . Example word-frequency array . Word frequency array, 4 words, many documents | Measure presence of words in each document using &quot;tf-idf&quot; | &quot;tf&quot; = frequency of word in document | &quot;idf&quot; reduces influence of frequent words | . NMF components . NMF has components | ... just like PCA has principal components | Dimension of components = dimension of samples | Entries are non-negative | . NMF features . NMF feature values are non-negative | Can be used to reconstruct the samples | ... combine feature values with components | . Sample reconstruction . Multiply components by feature values, and add up | Can also be expressed as a product of matrices | This is the &quot;Matrix Factorization&quot; in &quot;NMF&quot; | . NMF fits to non-negative data, only . Word frequencies in each document | Images encoded as arrays | Audio spectrograms | Purchase histories on e-commerce sites | ... and many more! | . Non-negative data . A tf-idf word-frequency array. | An array where rows are customers, columns are products and entries are 0 or 1, indicating whether a customer has purchased a product. | . NMF applied to Wikipedia articles . # Create an NMF instance: model model_wp = NMF(n_components=6) # Fit the model to articles model_wp.fit(articles) # Transform the articles: nmf_features nmf_features_wp = model_wp.transform(articles) # Print the NMF features nmf_features_wp[:5] . array([[0. , 0. , 0. , 0. , 0. , 0.44041429], [0. , 0. , 0. , 0. , 0. , 0.56653905], [0.00382072, 0. , 0. , 0. , 0. , 0.39860038], [0. , 0. , 0. , 0. , 0. , 0.3816956 ], [0. , 0. , 0. , 0. , 0. , 0.48546058]]) . NMF features of the Wikipedia articles . . Note: When investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. We&#8217;ll see why: NMF components represent topics (for instance, acting!). . # Create a pandas DataFrame: df wp_df = pd.DataFrame(nmf_features_wp, index=articles_titles) # Print the row for &#39;Anne Hathaway&#39; display(wp_df.loc[[&#39;Anne Hathaway&#39;]]) # Print the row for &#39;Denzel Washington&#39; display(wp_df.loc[[&#39;Denzel Washington&#39;]]) . 0 1 2 3 4 5 . Anne Hathaway 0.003846 | 0.0 | 0.0 | 0.575735 | 0.0 | 0.0 | . 0 1 2 3 4 5 . Denzel Washington 0.0 | 0.005601 | 0.0 | 0.422398 | 0.0 | 0.0 | . NMF learns interpretable parts . Example:NMF learns interpretable parts- Word-frequency array articles (tf-idf ) . 20,000 scientific articles (rows) | 800 words (columns) | . articles.shape . (60, 13125) . NMF components are topics . NMF components . For documents:- NMF components represent topics - NMF features combine topics into documents | For images, NMF components are parts of images | . Grayscale images . &quot;Grayscale&quot; image = no colors, only shades of gray | Measure pixel brightness | Represent with value between 0 and 1 (0 is black) | Convert to 2D array | . Encoding a collection of images . Collection of images of the same size | Encode as 2D array | Each row corresponds to an image | Each column corresponds to a pixel | ... can apply NMF! | . NMF learns topics of documents . when NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. We will using the NMF model that we built earlier using the Wikipedia articles. 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. We will identify the topic of the corresponding NMF component. . words = pd.read_csv(&quot;datasets/Wikipedia_articles/words.csv&quot;)[&quot;0&quot;].to_list() words[:5] . [&#39;aaron&#39;, &#39;abandon&#39;, &#39;abandoned&#39;, &#39;abandoning&#39;, &#39;abandonment&#39;] . # Create a DataFrame: components_df components_df = pd.DataFrame(model_wp.components_, columns=words) # Print the shape of the DataFrame print(components_df.shape) # Select row 3: component component = components_df.iloc[3] # Print result of nlargest component.nlargest() . (6, 13125) . film 0.627850 award 0.253121 starred 0.245274 role 0.211442 actress 0.186390 Name: 3, dtype: float64 . Explore the LED digits dataset . We&#39;ll use NMF to decompose grayscale images into their commonly occurring patterns. Firstly, we&#39;ll explore the image dataset and see how it is encoded as an array. . samples_images = pd.read_csv(&quot;datasets/samples_images.csv&quot;) x=samples_images.isnull().sum() x[x&gt;0] . Series([], dtype: int64) . samples_images=samples_images.values np.isinf(samples_images).any() . False . np.isnan(samples_images).any() . False . # Select the 0th row: digit digit_i = samples_images[0,:] # Print digit print(digit_i) # Reshape digit to a 13x8 array: bitmap bitmap_i = digit_i.reshape(13,8) # Print bitmap print(bitmap_i) # Use plt.imshow to display bitmap plt.imshow(bitmap_i, cmap=&#39;gray&#39;, interpolation=&#39;nearest&#39;) plt.colorbar() plt.show() . [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [[0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 1. 1. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.]] . NMF learns the parts of images . def show_as_image(sample): &quot;&quot;&quot;displays the image encoded by any 1D array&quot;&quot;&quot; bitmap = sample.reshape((13, 8)) plt.figure() plt.imshow(bitmap, cmap=&#39;gray&#39;, interpolation=&#39;nearest&#39;) plt.colorbar() plt.show() . show_as_image(samples_images[99, :]) . # Create an NMF model: model model_i = NMF(n_components=7) # Apply fit_transform to samples: features features_i = model_i.fit_transform(samples_images) # Call show_as_image on each component for component in model_i.components_: show_as_image(component) # Assign the 0th row of features: digit_features digit_features_i = features_i[0,:] # Print digit_features digit_features_i . array([4.76823559e-01, 0.00000000e+00, 0.00000000e+00, 5.90605054e-01, 4.81559442e-01, 0.00000000e+00, 7.37535093e-16]) . PCA doesn&#39;t learn parts . Unlike NMF, PCA doesn&#39;t learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. We will verify by inspecting the components of a PCA model fit to the dataset of LED digit images . # Create a PCA instance: model model_i = PCA(n_components=7) # Apply fit_transform to samples: features features_i = model_i.fit_transform(samples_images) # Call show_as_image on each component for component in model_i.components_: show_as_image(component) . the components of PCA do not represent meaningful parts of images of LED digits! . Building recommender systems using NMF . Finding similar articles . Engineer at a large online newspaper | Task:recommend articles similar to article being read by customer - Similar articles should have similar topics | . Strategy . Apply NMF to the word-frequency array | NMF feature values describe the topics | ... so similar documents have similar NMF feature values | Compare NMF feature values? | . Versions of articles . Different versions of the same document have same topic proportions | ... exact feature values may be different! | E.g. because one version uses many meaningless words | But all versions lie on the same line through the origin | . Cosine similarity . Uses the angle between the lines | Higher values means more similar | Maximum value is 1, when angle is 0 | . Which articles are similar to &#39;Cristiano Ronaldo&#39;? . finding the articles most similar to the article about the footballer Cristiano Ronaldo. . # Normalize the NMF features: norm_features norm_features_wp = normalize(nmf_features_wp) # Create a DataFrame: df wp_df = pd.DataFrame(norm_features_wp, index=articles_titles) # Select the row corresponding to &#39;Cristiano Ronaldo&#39;: article article = wp_df.loc[&#39;Cristiano Ronaldo&#39;] # Compute the dot products: similarities similarities = wp_df.dot(article) # Display those with the largest cosine similarity similarities.nlargest() . Cristiano Ronaldo 1.000000 Franck Ribéry 0.999972 Radamel Falcao 0.999942 Zlatan Ibrahimović 0.999942 France national football team 0.999923 dtype: float64 . Recommend musical artists part I . recommend popular music artists! . artists_df = pd.read_csv(&quot;datasets/Musical_artists/scrobbler-small-sample.csv&quot;) artists = csr_matrix(artists_df) . # Create a MaxAbsScaler: scaler scaler = MaxAbsScaler() # Create an NMF model: nmf nmf = NMF(n_components=20) # Create a Normalizer: normalizer normalizer = Normalizer() # Create a pipeline: pipeline pipeline = make_pipeline(scaler, nmf, normalizer) # Apply fit_transform to artists: norm_features norm_features = pipeline.fit_transform(artists) . Recommend musical artists part II . Suppose you were a big fan of Bruce Springsteen - which other musicial artists might you like? We will use the NMF features and the cosine similarity to find similar musical artists. . artist_names = pd.read_csv(&quot;datasets/Musical_artists/artists.csv&quot;)[&quot;0&quot;].to_list() . # Create a DataFrame: df df = pd.DataFrame(norm_features, index=artist_names) # Select row of &#39;Bruce Springsteen&#39;: artist artist = df.loc[&#39;Bruce Springsteen&#39;] # Compute cosine similarities: similarities similarities = df.dot(artist) # Display those with highest cosine similarity similarities.nlargest() . ValueError Traceback (most recent call last) ~ Anaconda3 lib site-packages pandas core internals managers.py in create_block_manager_from_blocks(blocks, axes) 1656 -&gt; 1657 mgr = BlockManager(blocks, axes) 1658 mgr._consolidate_inplace() ~ Anaconda3 lib site-packages pandas core internals managers.py in __init__(self, blocks, axes, do_integrity_check) 138 if do_integrity_check: --&gt; 139 self._verify_integrity() 140 ~ Anaconda3 lib site-packages pandas core internals managers.py in _verify_integrity(self) 333 if block._verify_integrity and block.shape[1:] != mgr_shape[1:]: --&gt; 334 construction_error(tot_items, block.shape[1:], self.axes) 335 if len(self.items) != tot_items: ~ Anaconda3 lib site-packages pandas core internals managers.py in construction_error(tot_items, block_shape, axes, e) 1693 raise ValueError(&#34;Empty data passed with indices specified.&#34;) -&gt; 1694 raise ValueError(f&#34;Shape of passed values is {passed}, indices imply {implied}&#34;) 1695 ValueError: Shape of passed values is (2894, 20), indices imply (111, 20) During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) &lt;ipython-input-118-67242b9cbae0&gt; in &lt;module&gt; 1 # Create a DataFrame: df -&gt; 2 df = pd.DataFrame(norm_features, index=artist_names) 3 4 # Select row of &#39;Bruce Springsteen&#39;: artist 5 artist = df.loc[&#39;Bruce Springsteen&#39;] ~ Anaconda3 lib site-packages pandas core frame.py in __init__(self, data, index, columns, dtype, copy) 462 mgr = init_dict({data.name: data}, index, columns, dtype=dtype) 463 else: --&gt; 464 mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy) 465 466 # For data is list-like, or Iterable (will consume into list) ~ Anaconda3 lib site-packages pandas core internals construction.py in init_ndarray(values, index, columns, dtype, copy) 208 block_values = [values] 209 --&gt; 210 return create_block_manager_from_blocks(block_values, [columns, index]) 211 212 ~ Anaconda3 lib site-packages pandas core internals managers.py in create_block_manager_from_blocks(blocks, axes) 1662 blocks = [getattr(b, &#34;values&#34;, b) for b in blocks] 1663 tot_items = sum(b.shape[0] for b in blocks) -&gt; 1664 construction_error(tot_items, blocks[0].shape[1:], axes, e) 1665 1666 ~ Anaconda3 lib site-packages pandas core internals managers.py in construction_error(tot_items, block_shape, axes, e) 1692 if block_shape[0] == 0: 1693 raise ValueError(&#34;Empty data passed with indices specified.&#34;) -&gt; 1694 raise ValueError(f&#34;Shape of passed values is {passed}, indices imply {implied}&#34;) 1695 1696 ValueError: Shape of passed values is (2894, 20), indices imply (111, 20) . 1. Source: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html.↩ . 2. Source: https://archive.ics.uci.edu/ml/datasets/Wine.↩ . 3. These fish measurement data were sourced from the Journal of Statistics Education..↩ . 4. Source: http://www.eurovision.tv/page/results↩ . 5. The Wikipedia dataset you will be working with was obtained from here.↩ .",
            "url": "https://victoromondi1997.github.io/blog/machine-learning/unsupervised-learning/2020/07/14/Unsupervised-Learning-in-Python.html",
            "relUrl": "/machine-learning/unsupervised-learning/2020/07/14/Unsupervised-Learning-in-Python.html",
            "date": " • Jul 14, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Supervised Learning with scikit-learn",
            "content": "Overview . Machine learning is the field that teaches machines and computers to learn from existing data to make predictions on new data: Will a tumor be benign or malignant? Which of your customers will take their business elsewhere? Is a particular email spam? We will use Python to perform supervised learning, an essential component of machine learning. We will build predictive models, tune their parameters, and determine how well they will perform with unseen data—all while using real world datasets. We be using scikit-learn, one of the most popular and user-friendly machine learning libraries for Python. . Libraries . from sklearn import datasets from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import (train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV) from sklearn.linear_model import (LinearRegression, Ridge, Lasso, LogisticRegression, ElasticNet) from sklearn.metrics import (mean_squared_error, classification_report, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, plot_precision_recall_curve) from sklearn.tree import DecisionTreeClassifier from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.svm import SVC # Support Vector Classiffication from sklearn.preprocessing import (scale, StandardScaler) import pandas as pd import numpy as np from scipy.stats import randint import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) plt.style.use(&quot;ggplot&quot;) . Classification . Introduction to classification problems and how to solve them using supervised learning techniques. We&#39;ll explore a political dataset, where we will classify the party affiliation of United States congressmen based on their voting records. . Supervised learning . What is machine learning? . The art and science of:- Giving computers the ability to learn to make decisions from data - without being explicitly programmed! | Examples: Learning to predict whether an email is spam or not | Clustering wikipedia entries into different categories | . | Supervised learning: Uses labeled data | Unsupervised learning: Uses unlabeled data ### Unsupervised learning | Uncovering hidden patterns from unlabeled data | Example:- Grouping customers into distinct categories (Clustering)&gt; ### Reinforcement learning | Software agents interact with an environment Learn how to optimize their behavior | Given a system of rewards and punishments | Draws inspiration from behavioral psychology | . | Applications Economics | Genetics | Game playing | . | AlphaGo:First computer to defeat the world champion in Go&gt; ### Supervised learning | Predictor variables/features and a target variable | Aim:- Predict the target variable, given the predictor variables - Classication: Target variable consists of categories Regression: Target variable is continuous ### Naming conventions | . | Features = predictor variables = independent variables | Target variable = dependent variable = response variable | . Supervised learning . Automate time-consuming or expensive manual tasks Example:Doctor’s diagnosis- Make predictions about the future | Example: Will a customer click on an ad or not? | . | Need labeled data Historical data with labels | Experiments to get labeled data | Crowd-sourcing labeled data | . | . Supervised learning in Python . We will use scikit-learn/sklearn Integrates well with the SciPy stack | . | Otherlibraries Tensor Flow | keras | . | . Exploratory data analysis . The Iris dataset . Features:- Petal length - Petal width Sepal length | Sepal width | . | Target variable: Species Versicolor | Virginica | Setosa | . | . The Iris dataset in scikit-learn . iris = datasets.load_iris() type(iris) . sklearn.utils.Bunch . iris.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) . type(iris.data) . numpy.ndarray . type(iris.target) . numpy.ndarray . iris.data.shape . (150, 4) . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . Exploratory data analysis (EDA) . X = iris.data y= iris.target df = pd.DataFrame(X, columns=iris.feature_names) df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . df2 = df.copy() df2[&#39;target_names&#39;] = iris.target df2.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target_names . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . df2.target_names.value_counts() . 2 50 1 50 0 50 Name: target_names, dtype: int64 . df2[&#39;target_names&#39;] = df2.target_names.map({0:&#39;setosa&#39;, 1:&#39;versicolor&#39;, 2:&#39;virginica&#39;}) df2.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target_names . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . Visual EDA . _ = pd.plotting.scatter_matrix(df, c=y, figsize=[8,8], s=150, marker=&quot;D&quot;) . Numerical EDA . We&#39;ll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. our goal will be to predict their party affiliation (&#39;Democrat&#39; or &#39;Republican&#39;) based on how they voted on certain key issues. . Note: Here, it&#8217;s worth noting that we have preprocessed this dataset to deal with missing values. This is so that our focus can be directed towards understanding how to train and evaluate supervised learning models. . Before thinking about what supervised learning models we can apply to this, however, we need to perform Exploratory data analysis (EDA) in order to understand the structure of the data. . votes = pd.read_csv(&quot;datasets/votes.csv&quot;) votes.head() . party infants water budget physician salvador religious satellite aid missile immigration synfuels education superfund crime duty_free_exports eaa_rsa . 0 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | . 1 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 | . 2 democrat | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | . 3 democrat | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 4 democrat | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | . votes.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 435 entries, 0 to 434 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 party 435 non-null object 1 infants 435 non-null int64 2 water 435 non-null int64 3 budget 435 non-null int64 4 physician 435 non-null int64 5 salvador 435 non-null int64 6 religious 435 non-null int64 7 satellite 435 non-null int64 8 aid 435 non-null int64 9 missile 435 non-null int64 10 immigration 435 non-null int64 11 synfuels 435 non-null int64 12 education 435 non-null int64 13 superfund 435 non-null int64 14 crime 435 non-null int64 15 duty_free_exports 435 non-null int64 16 eaa_rsa 435 non-null int64 dtypes: int64(16), object(1) memory usage: 57.9+ KB . votes.describe() . infants water budget physician salvador religious satellite aid missile immigration synfuels education superfund crime duty_free_exports eaa_rsa . count 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | 435.000000 | . mean 0.429885 | 0.558621 | 0.606897 | 0.406897 | 0.521839 | 0.650575 | 0.581609 | 0.590805 | 0.526437 | 0.512644 | 0.344828 | 0.393103 | 0.537931 | 0.609195 | 0.400000 | 0.857471 | . std 0.495630 | 0.497123 | 0.489002 | 0.491821 | 0.500098 | 0.477337 | 0.493863 | 0.492252 | 0.499876 | 0.500416 | 0.475859 | 0.489002 | 0.499133 | 0.488493 | 0.490462 | 0.349994 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . 50% 0.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | . 75% 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . max 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . Observations . The DataFrame has a total of 435 rows and 17 columns. | Except for &#39;party&#39;, all of the columns are of type int64. | The first two rows of the DataFrame consist of votes made by Republicans and the next three rows consist of votes made by Democrats. | The target variable in this DataFrame is &#39;party&#39;. | . Votes Visual EDA . The Numerical EDA we did gave us some very important information, such as the names and data types of the columns, and the dimensions of the DataFrame. Following this with some visual EDA will give us an even better understanding of the data. all the features in this dataset are binary; that is, they are either 0 or 1. So a different type of plot would be more useful here, such as Seaborn&#39;s countplot. . def plot_countplot(column): plt.figure() sns.countplot(x=column, hue=&#39;party&#39;, data=votes, palette=&#39;RdBu&#39;) plt.xticks([0,1], [&#39;No&#39;, &#39;Yes&#39;]) plt.show() plot_countplot(&quot;education&quot;) . It seems like Democrats voted resoundingly against this bill, compared to Republicans. This is the kind of information that our machine learning model will seek to learn when we try to predict party affiliation solely based on voting behavior. An expert in U.S politics may be able to predict this without machine learning, but probably not instantaneously - and certainly not if we are dealing with hundreds of samples! . plot_countplot(&#39;infants&#39;) . plot_countplot(&#39;water&#39;) . plot_countplot(&quot;budget&quot;) . plot_countplot(&#39;physician&#39;) . plot_countplot(&#39;salvador&#39;) . plot_countplot(&#39;religious&#39;) . plot_countplot(&#39;satellite&#39;) . plot_countplot(&#39;aid&#39;) . plot_countplot(&#39;missile&#39;) . plot_countplot(&#39;immigration&#39;) . plot_countplot(&#39;synfuels&#39;) . plot_countplot(&#39;superfund&#39;) . plot_countplot(&#39;crime&#39;) . plot_countplot(&#39;duty_free_exports&#39;) . plot_countplot(&#39;eaa_rsa&#39;) . Observations . Democrats voted in favor of both &#39;satellite&#39; and &#39;missile&#39; | . The classification challenge . k-Nearest Neighbors . Basic idea:Predict the label of a data point by - Looking at the ‘k’ closest labeled data points Taking a majority vote ### Scikit-learn fit and predict | . | All machine learning models implemented as Python classes They implement the algorithms for learning and predicting | Store the information learned from the data | . | Training a model on the data = ‘fitting’ a model to the data .fit() method | . | To predict the labels of new data:.predict() method | . Iris k-NN: Intuition . _ = sns.scatterplot(data=df2, x=&quot;petal width (cm)&quot;, y=&quot;petal length (cm)&quot;, hue=&#39;target_names&#39;) plt.show() . Iris dataset Using scikit-learn to fit a classier . knn = KNeighborsClassifier(n_neighbors=6) knn.fit(iris[&#39;data&#39;], iris[&#39;target&#39;]) . KNeighborsClassifier(n_neighbors=6) . iris[&#39;data&#39;].shape . (150, 4) . iris[&#39;target&#39;].shape . (150,) . Predicting on unlabeled data . X_new = np.array([[5.6, 2.8, 3.9, 1.1], [5.7, 2.6, 3.8, 1.3], [4.7, 3.2, 1.3, 0.2]]) prediction = knn.predict(X_new) prediction . array([1, 1, 0]) . k-Nearest Neighbors: Fit . Having explored the Congressional voting records dataset, it is time now to build our first classifier. We&#39;ll will fit a k-Nearest Neighbors classifier to the voting dataset. . The features need to be in an array where each column is a feature and each row a different observation or data point - in this case, a Congressman&#39;s voting record. The target needs to be a single column with the same number of observations as the feature data. We will name the feature array X and response variable y: This is in accordance with the common scikit-learn practice. . # Create arrays for the features and the response variable y_votes = votes[&#39;party&#39;].values X_votes = votes.drop(&#39;party&#39;, axis=1).values # Create a k-NN classifier with 6 neighbors knn_votes = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the data knn_votes.fit(X_votes, y_votes) . KNeighborsClassifier(n_neighbors=6) . Now that the k-NN classifier with 6 neighbors has been fit to the data, it can be used to predict the labels of new data points. . k-Nearest Neighbors: Predict . X_new_votes = pd.read_csv(&quot;datasets/X_new_votes.csv&quot;) X_new_votes.head() . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . 0 0.696469 | 0.286139 | 0.226851 | 0.551315 | 0.719469 | 0.423106 | 0.980764 | 0.68483 | 0.480932 | 0.392118 | 0.343178 | 0.72905 | 0.438572 | 0.059678 | 0.398044 | 0.737995 | . Having fit a k-NN classifier, we can now use it to predict the label of a new data point. . # Predict and print the label for the new data point X_new new_prediction = knn_votes.predict(X_new_votes) print(&quot;Prediction: {}&quot;.format(new_prediction)) . Prediction: [&#39;democrat&#39;] . Measuring model performance . In classication, accuracy is a commonly used metric | Accuracy = Fraction of correct predictions | Which data should be used to compute accuracy? | How well will the model perform on new data? | Could compute accuracy on data used to fit classifier | NOT indicative of ability to generalize | Split data into training and test set | Fit/train the classifier on the training set | Make predictions on test set | Compare predictions with the known labels | . Model complexity . Larger k = smoother decision boundary = less complex model | Smaller k = more complex model = can lead to overfitting | . X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X, y, test_size=.3, random_state=21, stratify=y) knn_iris = KNeighborsClassifier(n_neighbors=8) knn_iris.fit(X_train_iris, y_train_iris) y_pred_iris = knn_iris.predict(X_test_iris) print(f&quot;Test set predictions n{y_pred_iris}&quot;) . Test set predictions [2 1 2 2 1 0 1 0 0 1 0 2 0 2 2 0 0 0 1 0 2 2 2 0 1 1 1 0 0 1 2 2 0 0 1 2 2 1 1 2 1 1 0 2 1] . knn_iris.score(X_test_iris, y_test_iris) . 0.9555555555555556 . The digits recognition dataset . We&#39;ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn&#39;s included datasets. . Each sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. Helpfully for the MNIST dataset, scikit-learn provides an &#39;images&#39; key in addition to the &#39;data&#39; and &#39;target&#39; keys that we have seen with the Iris data. Because it is a 2D array of the images corresponding to each sample, this &#39;images&#39; key is useful for visualizing the images. On the other hand, the &#39;data&#39; key contains the feature array - that is, the images as a flattened array of 64 pixels. . # Load the digits dataset: digits digits = datasets.load_digits() # Print the keys and DESCR of the dataset print(digits.keys()) print(digits.DESCR) . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) .. _digits_dataset: Optical recognition of handwritten digits dataset -- **Data Set Characteristics:** :Number of Instances: 5620 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr) :Date: July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. .. topic:: References - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. - Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000. . # Print the shape of the images and data keys print(digits.images.shape) digits.data.shape . (1797, 8, 8) . (1797, 64) . # Display digit 1010 plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) plt.show() . It looks like the image in question corresponds to the digit &#39;5&#39;. Now, can we build a classifier that can make this prediction not only for this image, but for all the other ones in the dataset? . Train/Test Split + Fit/Predict/Accuracy . Now that we have learned about the importance of splitting your data into training and test sets, it&#39;s time to practice doing this on the digits dataset! After creating arrays for the features and target variable, we will split them into training and test sets, fit a k-NN classifier to the training data, and then compute its accuracy using the .score() method. . # Create feature and target arrays X_digits = digits.data y_digits = digits.target # Split into training and test set X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(X_digits, y_digits, test_size = 0.2, random_state= 42, stratify=y_digits) # Create a k-NN classifier with 7 neighbors: knn_digits knn_digits = KNeighborsClassifier(n_neighbors=7) # Fit the classifier to the training data knn_digits.fit(X_train_digits, y_train_digits) # Print the accuracy knn_digits.score(X_test_digits, y_test_digits) . 0.9833333333333333 . Incredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be. . Overfitting and underfitting . We will now construct such a model complexity curve for the digits dataset! We will compute and plot the training and testing accuracy scores for a variety of different neighbor values. . By observing how the accuracy scores differ for the training and testing sets with different values of k, we will develop your intuition for overfitting and underfitting. . # Setup arrays to store train and test accuracies neighbors_digits = np.arange(1, 9) train_accuracy_digits = np.empty(len(neighbors_digits)) test_accuracy_digits = np.empty(len(neighbors_digits)) # Loop over different values of k for i, k in enumerate(neighbors_digits): # Setup a k-NN Classifier with k neighbors: knn knn_digits = KNeighborsClassifier(n_neighbors=k) # Fit the classifier to the training data knn_digits.fit(X_train_digits, y_train_digits) #Compute accuracy on the training set train_accuracy_digits[i] = knn_digits.score(X_train_digits, y_train_digits) #Compute accuracy on the testing set test_accuracy_digits[i] = knn_digits.score(X_test_digits, y_test_digits) # Generate plot plt.title(&#39;k-NN: Varying Number of Neighbors&#39;) plt.plot(neighbors_digits, test_accuracy_digits, label = &#39;Testing Accuracy&#39;) plt.plot(neighbors_digits, train_accuracy_digits, label = &#39;Training Accuracy&#39;) plt.legend() plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . It looks like the test accuracy is highest when using 1 and35 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data. . Regression . We used image and political datasets to predict binary and multiclass outcomes. But what if our problem requires a continuous outcome? Regression is best suited to solving such problems. We will explore the fundamental concepts in regression and apply them to predict the life expectancy in a given country using Gapminder data. . Introduction to regression . Example of an regression problem: A bike share company using time and weather data to predict the number of bikes being rented at any given hour. The target variable here - the number of bike rentals at any given hour - is quantitative, so this is best framed as a regression problem. . Boston housing data . boston = datasets.load_boston() boston.data.shape . (506, 13) . boston.target.shape . (506,) . boston.feature_names . array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;, &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;) . boston_df = pd.DataFrame(boston.data, columns=boston.feature_names) boston_df[&#39;MEDV&#39;] = boston.target boston_df.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . Creating feature and target arrays for the boston dataset . X_boston = boston.data y_boston = boston.target . Predicting house value from a single feature . X_boston_rooms = X_boston[:,5] type(X_boston_rooms), type(y_boston) . (numpy.ndarray, numpy.ndarray) . y_boston = y_boston.reshape(-1,1) X_boston_rooms = X_boston_rooms.reshape(-1,1) . Plotting house value vs. number of rooms . plt.scatter(X_boston_rooms, y_boston) plt.ylabel(&#39;Value of house /1000 ($)&#39;) plt.xlabel(&#39;Number of rooms&#39;) plt.show(); . Fitting a regression model . reg_boston = LinearRegression() reg_boston.fit(X_boston_rooms, y_boston) boston_prediction_space = np.linspace(min(X_boston_rooms), max(X_boston_rooms)).reshape(-1,1) . plt.scatter(X_boston_rooms, y_boston, color=&quot;blue&quot;) plt.plot(boston_prediction_space, reg_boston.predict(boston_prediction_space), color=&#39;black&#39;, linewidth=3) plt.show() . Importing Gapminder data for supervised learning . We will work with Gapminder data that we have consolidated into one CSV file. . Specifically, our goal will be to use this data to predict the life expectancy in a given country based on features such as the country&#39;s GDP, fertility rate, and population. . Since the target variable here is quantitative, this is a regression problem. To begin, we will fit a linear regression with just one feature: &#39;fertility&#39;, which is the average number of children a woman in a given country gives birth to. . Before that, however, we need to import the data and get it into the form needed by scikit-learn. This involves creating feature and target variable arrays. Furthermore, since we are going to use only one feature to begin with, we need to do some reshaping using NumPy&#39;s .reshape() method. . # Read the CSV file into a DataFrame: gapminder_df gapminder = pd.read_csv(&quot;datasets/gapminder.csv&quot;) # Create arrays for features and target variable y_gapminder = gapminder.life.values X_gapminder = gapminder.fertility.values # Print the dimensions of X and y before reshaping print(&quot;Dimensions of y before reshaping: {}&quot;.format(y_gapminder.shape)) print(&quot;Dimensions of X before reshaping: {}&quot;.format(X_gapminder.shape)) # Reshape X and y y_gapminder = y_gapminder.reshape(-1,1) X_gapminder = X_gapminder.reshape(-1,1) # Print the dimensions of X and y after reshaping print(&quot;Dimensions of y after reshaping: {}&quot;.format(y_gapminder.shape)) print(&quot;Dimensions of X after reshaping: {}&quot;.format(X_gapminder.shape)) . Dimensions of y before reshaping: (139,) Dimensions of X before reshaping: (139,) Dimensions of y after reshaping: (139, 1) Dimensions of X after reshaping: (139, 1) . Exploring the Gapminder data . As always, it is important to explore the data before building models. . sns.heatmap(gapminder.corr(), square=True, cmap=&quot;RdYlGn&quot;) plt.show() . Cells that are in green show positive correlation, while cells that are in red show negative correlation. life and fertility are negatively correlated. GDP and life are positively correlated . gapminder.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | . gapminder.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 139 entries, 0 to 138 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 population 139 non-null float64 1 fertility 139 non-null float64 2 HIV 139 non-null float64 3 CO2 139 non-null float64 4 BMI_male 139 non-null float64 5 GDP 139 non-null float64 6 BMI_female 139 non-null float64 7 life 139 non-null float64 8 child_mortality 139 non-null float64 dtypes: float64(9) memory usage: 9.9 KB . The DataFrame has 139 samples (or rows) and 9 columns. . gapminder.describe() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality . count 1.390000e+02 | 139.000000 | 139.000000 | 139.000000 | 139.000000 | 139.000000 | 139.000000 | 139.000000 | 139.000000 | . mean 3.549977e+07 | 3.005108 | 1.915612 | 4.459874 | 24.623054 | 16638.784173 | 126.701914 | 69.602878 | 45.097122 | . std 1.095121e+08 | 1.615354 | 4.408974 | 6.268349 | 2.209368 | 19207.299083 | 4.471997 | 9.122189 | 45.724667 | . min 2.773150e+05 | 1.280000 | 0.060000 | 0.008618 | 20.397420 | 588.000000 | 117.375500 | 45.200000 | 2.700000 | . 25% 3.752776e+06 | 1.810000 | 0.100000 | 0.496190 | 22.448135 | 2899.000000 | 123.232200 | 62.200000 | 8.100000 | . 50% 9.705130e+06 | 2.410000 | 0.400000 | 2.223796 | 25.156990 | 9938.000000 | 126.519600 | 72.000000 | 24.000000 | . 75% 2.791973e+07 | 4.095000 | 1.300000 | 6.589156 | 26.497575 | 23278.500000 | 130.275900 | 76.850000 | 74.200000 | . max 1.197070e+09 | 7.590000 | 25.900000 | 48.702062 | 28.456980 | 126076.000000 | 135.492000 | 82.600000 | 192.000000 | . The mean of life is 69.602878 . The basics of linear regression . Regression mechanics . $y = ax + b$ $y$ = target | $x$ = single feature | $a$, $b$ = parameters of model | . | How do we choose $a$ and $b$? | Define an error functions for any given lineChoose the line that minimizes the error function | Ordinary least squares(OLS):Minimize sum of squares of residuals&gt; ### Linear regression in higher dimensions | $y=a_1x_1+a_2x_2+b$ | To fit a linear regression model here:- Need to specify 3 variables- In higher dimensions: Must specify coefcient for each feature and the variable $b$ | $y=a_1x_1+a_2x_2+a_3x_3+...+a_nx_n+b$ | . | Scikit-learn API works exactly the same way: Pass two arrays: Features, and target | . | . Linear regression on all features in boston dataset . X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, test_size=.3, random_state=42) reg_all_boston = LinearRegression() reg_all_boston.fit(X_train_boston, y_train_boston) y_pred_boston = reg_all_boston.predict(X_test_boston) reg_all_boston.score(X_test_boston, y_test_boston) . 0.7112260057484925 . Fit &amp; predict for regression in gapminder dataset . We will fit a linear regression and predict life expectancy using just one feature. We will use the &#39;fertility&#39; feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is &#39;life&#39;. . sns.scatterplot(data=gapminder, x=&quot;fertility&quot;, y=&quot;life&quot;) plt.show() . As you can see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Our job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. We will also compute and print the $R^2$ score using sckit-learn&#39;s .score() method. . # Create the regressor: reg reg_gapminder = LinearRegression() # Create the prediction space prediction_space = np.linspace(min(X_gapminder), max(X_gapminder)).reshape(-1,1) # Fit the model to the data reg_gapminder.fit(X_gapminder,y_gapminder) # Compute predictions over the prediction space: y_pred y_pred_gapminder = reg_gapminder.predict(prediction_space) # Print R^2 print(reg_gapminder.score(X_gapminder, y_gapminder)) . 0.6192442167740035 . # Plot regression line sns.scatterplot(data=gapminder, x=&quot;fertility&quot;, y=&quot;life&quot;) plt.plot(prediction_space, y_pred_gapminder, color=&#39;black&#39;, linewidth=3) plt.show() . Notice how the line captures the underlying trend in the data. And the performance is quite decent for this basic regression model with only one feature! . Train/test split for regression . train and test sets are vital to ensure that the supervised learning model is able to generalize well to new data. This was true for classification models, and is equally true for linear regression models. . We will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the $R^2$ score, we will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. . X_gapminder = gapminder.drop(&quot;life&quot;, axis=1).values . # Create training and test sets X_train_gapminder, X_test_gapminder, y_train_gapminder, y_test_gapminder = train_test_split(X_gapminder, y_gapminder, test_size = .3, random_state=42) # Create the regressor: reg_all reg_all_gapminder = LinearRegression() # Fit the regressor to the training data reg_all_gapminder.fit(X_train_gapminder, y_train_gapminder) # Predict on the test data: y_pred y_pred_gapminder = reg_all_gapminder.predict(X_test_gapminder) # Compute and print R^2 and RMSE print(&quot;R^2: {}&quot;.format(reg_all_gapminder.score(X_test_gapminder, y_test_gapminder))) rmse_gapminder = np.sqrt(mean_squared_error(y_test_gapminder, y_pred_gapminder)) print(&quot;Root Mean Squared Error: {}&quot;.format(rmse_gapminder)) . R^2: 0.8380468731430059 Root Mean Squared Error: 3.247601080037022 . Using all features has improved the model score. This makes sense, as the model has more information to learn from. However, there is one potential pitfall to this process. Can you spot it? . Cross-validation . Cross-validation motivation . Model performance is dependent on way the data is split | Not representative of the model’s ability to generalize | Solution:Cross-validation! | . Cross-validation and model performance . 5 folds = 5-fold CV | 10 folds = 10-fold CV | k folds = k-fold CV | More folds = More computationally expensive | . Cross-validation in scikit-learn: Boston . cv_results_boston = cross_val_score(reg_all_boston, X_boston, y_boston, cv=5) cv_results_boston . array([ 0.63919994, 0.71386698, 0.58702344, 0.07923081, -0.25294154]) . np.mean(cv_results_boston) . 0.353275924395884 . np.median(cv_results_boston) . 0.5870234363057776 . 5-fold cross-validation . Cross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data. . We will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn&#39;s cross_val_score() function uses R2 as the metric of choice for regression. Since We are performing 5-fold cross-validation, the function will return 5 scores. We will compute these 5 scores and then take their average. . # Compute 5-fold cross-validation scores: cv_scores cv_scores_gapminder = cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=5) # Print the 5-fold cross-validation scores print(cv_scores_gapminder) print(&quot;Average 5-Fold CV Score: {}&quot;.format(np.mean(cv_scores_gapminder))) . [0.81720569 0.82917058 0.90214134 0.80633989 0.94495637] Average 5-Fold CV Score: 0.8599627722793267 . Now that we have cross-validated your model, we can more confidently evaluate its predictions. . K-Fold CV comparison . . Warning: Cross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes. . %timeit cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=3) . 8.03 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 100 loops each) . %timeit cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=10) . 31.8 ms ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . # Perform 3-fold CV cvscores_3_gapminder = cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=3) print(np.mean(cvscores_3_gapminder)) # Perform 10-fold CV cvscores_10_gapminder = cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=10) print(np.mean(cvscores_10_gapminder)) . 0.8718712782621969 0.8436128620131095 . Regularized regression . Why regularize? . Recall:Linear regression minimizes a loss function- It chooses a coefcient for each feature variable | Large coefcients can lead to overtting | Penalizing large coefcients: Regularization ### Ridge regression | Loss function = OLS loss function + $ alpha * sum_{i=1}^{n} a_i^2$ | Alpha:Parameter we need to choose- Picking alpha here is similar to picking k in k-NN | Hyperparameter tuning | Alpha controls model complexity Alpha = 0: We get back OLS (Can lead to overtting) | Very high alpha: Can lead to undertting | . | . Lasso regression . Loss function = OLS loss function + $ alpha * sum_{i=1}^{n} |a_i|$ | . Lasso regression for feature selection . Can be used to select important features of a dataset | Shrinks the coefcients of less important features to exactly 0 | . Ridge regression in scikit-learn: Boston . ridge_boston = Ridge(alpha=.1, normalize=True) ridge_boston.fit(X_train_boston, y_train_boston) ridge_pred_boston = ridge_boston.predict(X_test_boston) ridge_boston.score(X_test_boston, y_test_boston) . 0.6996938275127311 . Lasso regression in scikit-learn: Boston . lasso_boston = Lasso(alpha=.1, normalize=True) lasso_boston.fit(X_train_boston, y_train_boston) lasso_pred_boston = lasso_boston.predict(X_test_boston) lasso_boston.score(X_test_boston, y_test_boston) . 0.5950229535328551 . Lasso for feature selection in scikit-learn: Boston . names_boston = boston.feature_names lasso_boston_2 = Lasso(alpha=.1) lasso_coef_boston = lasso_boston_2.fit(X_boston, y_boston).coef_ _ = plt.plot(range(len(names_boston)), lasso_coef_boston) _ = plt.xticks(range(len(names_boston)), names_boston, rotation=60) _ = plt.ylabel(&quot;Coefficients&quot;) plt.show() . Regularization I: Lasso . We saw how Lasso selected out the &#39;RM&#39; feature as being the most important for predicting Boston house prices, while shrinking the coefficients of certain other features to 0. Its ability to perform feature selection in this way becomes even more useful when you are dealing with data involving thousands of features. . We will fit a lasso regression to the Gapminder data we have been working with and plot the coefficients. Just as with the Boston data. . df_columns_gapminder = pd.Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;child_mortality&#39;], dtype=&#39;object&#39;) . # Instantiate a lasso regressor: lasso lasso_gapminder = Lasso(alpha=.4, normalize=True) # Fit the regressor to the data lasso_gapminder.fit(X_gapminder,y_gapminder) # Compute and print the coefficients lasso_coef_gapminder = lasso_gapminder.fit(X_gapminder,y_gapminder).coef_ print(lasso_coef_gapminder) # Plot the coefficients plt.plot(range(len(df_columns_gapminder)), lasso_coef_gapminder) plt.xticks(range(len(df_columns_gapminder)), df_columns_gapminder.values, rotation=60) plt.margins(0.02) plt.show() . [-0. -0. -0. 0. 0. 0. -0. -0.07087587] . According to the lasso algorithm, it seems like &#39;child_mortality&#39; is the most important feature when predicting life expectancy. . Regularization II: Ridge . Lasso is great for feature selection, but when building regression models, Ridge regression should be the first choice. . lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as $L1$ regularization because the regularization term is the $L1$ norm of the coefficients. This is not the only way to regularize, however. . def display_plot(cv_scores, cv_scores_std): &quot;&quot;&quot;plots the R^2 score as well as standard error for each alpha&quot;&quot;&quot; fig = plt.figure() ax = fig.add_subplot(1,1,1) ax.plot(alpha_space_gapminder, cv_scores) std_error = cv_scores_std / np.sqrt(10) ax.fill_between(alpha_space_gapminder, cv_scores + std_error, cv_scores - std_error, alpha=0.2) ax.set_ylabel(&#39;CV Score +/- Std Error&#39;) ax.set_xlabel(&#39;Alpha&#39;) ax.axhline(np.max(cv_scores), linestyle=&#39;--&#39;, color=&#39;.5&#39;) ax.set_xlim([alpha_space_gapminder[0], alpha_space_gapminder[-1]]) ax.set_xscale(&#39;log&#39;) plt.show() . If instead we took the sum of the squared values of the coefficients multiplied by some alpha - like in Ridge regression - we would be computing the $L2$ norm. We will fit ridge regression models over a range of different alphas, and plot cross-validated $R^2$ scores for each, using this function display_plot, which plots the $R^2$ score as well as standard error for each alpha: . # Setup the array of alphas and lists to store scores alpha_space_gapminder = np.logspace(-4, 0, 50) ridge_scores_gapminder = [] ridge_scores_std_gapminder = [] # Create a ridge regressor: ridge ridge_gapminder = Ridge(normalize=True) # Compute scores over range of alphas for alpha in alpha_space_gapminder: # Specify the alpha value to use: ridge.alpha ridge_gapminder.alpha = alpha # Perform 10-fold CV: ridge_cv_scores ridge_cv_scores_gapminder = cross_val_score(ridge_gapminder, X_gapminder, y_gapminder, cv=10) # Append the mean of ridge_cv_scores to ridge_scores ridge_scores_gapminder.append(np.mean(ridge_cv_scores_gapminder)) # Append the std of ridge_cv_scores to ridge_scores_std ridge_scores_std_gapminder.append(np.std(ridge_cv_scores_gapminder)) # Display the plot display_plot(ridge_scores_gapminder, ridge_scores_std_gapminder) . the cross-validation scores change with different alphas. . Fine-tuning model . Having trained a model, the next task is to evaluate its performance. We will explore some of the other metrics available in scikit-learn that will allow us to assess the model&#39;s performance in a more nuanced manner. . How good is your model? . Classication metrics . Measuring model performance with accuracy:- Fraction of correctly classied samples - Not always a useful metric ### Class imbalance example:Emails- Spam classication 99% of emails are real; 1% of emails are spam | . | Could build a classier that predicts ALL emails as real 99% accurate! | But horrible at actually classifying spam | Fails at its original purpose | . | Need more nuanced metrics ### Diagnosing classication predictions | Confusion matrix | Accuracy:$ frac{tp+tn}{tp+tn+fp+fn}$&gt; ### Metrics from the confusion matrix | Precision:$ frac{tp}{tp+fp}$- Recal $ frac{tp}{tp+fn}$ | F1score: $2. frac{precision.recal}{precision+recall}$ | High precision: Not many real emails predicted as spam | High recall: Predicted most spam emails correctly | . Confusion matrix in scikit-learn: iris dataset . confusion_matrix(y_test_iris, y_pred_iris) . array([[15, 0, 0], [ 0, 14, 1], [ 0, 1, 14]], dtype=int64) . print(classification_report(y_test_iris, y_pred_iris)) . precision recall f1-score support 0 1.00 1.00 1.00 15 1 0.93 0.93 0.93 15 2 0.93 0.93 0.93 15 accuracy 0.96 45 macro avg 0.96 0.96 0.96 45 weighted avg 0.96 0.96 0.96 45 . X_train_votes, X_test_votes, y_train_votes, y_test_votes = train_test_split(X_votes, y_votes, test_size=.4, random_state=42) knn_votes = KNeighborsClassifier(n_neighbors=8) knn_votes.fit(X_train_votes, y_train_votes) y_pred_votes = knn_votes.predict(X_test_votes) . confusion_matrix(y_test_votes, y_pred_votes) . array([[108, 7], [ 4, 55]], dtype=int64) . print(classification_report(y_test_votes, y_pred_votes)) . precision recall f1-score support democrat 0.96 0.94 0.95 115 republican 0.89 0.93 0.91 59 accuracy 0.94 174 macro avg 0.93 0.94 0.93 174 weighted avg 0.94 0.94 0.94 174 . The support gives the number of samples of the true response that lie in that class, the support was the number of Republicans or Democrats in the test set on which the classification report was computed. The precision, recall, and f1-score columns, then, gave the respective metrics for that particular class. . Metrics for classification . We evaluated the performance of k-NN classifier based on its accuracy. However, accuracy is not always an informative metric. We will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report. . We&#39;ll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. . pidd = pd.read_csv(&quot;datasets/pima_indians_diabetes_database.csv&quot;) pidd.head() . pregnancies glucose diastolic triceps insulin bmi dpf age diabetes . 0 6 | 148 | 72 | 35.00000 | 155.548223 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29.00000 | 155.548223 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 29.15342 | 155.548223 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23.00000 | 94.000000 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35.00000 | 168.000000 | 43.1 | 2.288 | 33 | 1 | . We will train a k-NN classifier to the data and evaluate its performance by generating a confusion matrix and classification report. . y_pidd = pidd.diabetes.values X_pidd = pidd.drop(&quot;diabetes&quot;, axis=1).values . # Create training and test set X_train_pidd, X_test_pidd, y_train_pidd, y_test_pidd = train_test_split(X_pidd, y_pidd, test_size=.4, random_state=42) # Instantiate a k-NN classifier: knn knn_pidd = KNeighborsClassifier(n_neighbors=6) # Fit the classifier to the training data knn_pidd.fit(X_train_pidd, y_train_pidd) # Predict the labels of the test data: y_pred y_pred_pidd = knn_pidd.predict(X_test_pidd) # Generate the confusion matrix and classification report print(confusion_matrix(y_test_pidd, y_pred_pidd)) print(classification_report(y_test_pidd, y_pred_pidd)) . [[176 30] [ 52 50]] precision recall f1-score support 0 0.77 0.85 0.81 206 1 0.62 0.49 0.55 102 accuracy 0.73 308 macro avg 0.70 0.67 0.68 308 weighted avg 0.72 0.73 0.72 308 . By analyzing the confusion matrix and classification report, we can get a much better understanding of your classifier&#39;s performance. . Logistic regression and the ROC curve . Logistic regression for binary classication . Logistic regression outputs probabilities | If the probability ‘p’ is greater than 0.5:- The data is labeled ‘1’- If the probability ‘p’ is less than 0.5: The data is labeled ‘0’ | . | . Probability thresholds . By default, logistic regression threshold = 0.5 | Not specific to logistic regression k-NN classifiers also have thresholds | . | What happens if we vary the threshold? | . Building a logistic regression model . Time to build our first logistic regression model! scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as &#39;estimators&#39;. . # Create the classifier: logreg logreg_pidd = LogisticRegression() # Fit the classifier to the training data logreg_pidd.fit(X_train_pidd, y_train_pidd) # Predict the labels of the test set: y_pred y_pred_logreg_pidd = logreg_pidd.predict(X_test_pidd) # Compute and print the confusion matrix and classification report print(confusion_matrix(y_test_pidd, y_pred_logreg_pidd)) print(classification_report(y_test_pidd, y_pred_logreg_pidd)) . [[171 35] [ 35 67]] precision recall f1-score support 0 0.83 0.83 0.83 206 1 0.66 0.66 0.66 102 accuracy 0.77 308 macro avg 0.74 0.74 0.74 308 weighted avg 0.77 0.77 0.77 308 . Precision-recall Curve . the precision-recall curve is generated by plotting the precision and recall for different thresholds. . $$ precision = frac{TP}{TP+FP} $$$$ recall = frac{TP}{TP+FN} $$ disp = plot_precision_recall_curve(logreg_pidd, X_test_pidd, y_test_pidd) disp.ax_.set_title(&#39;Precision-Recall curve: &#39;) . Text(0.5, 1.0, &#39;Precision-Recall curve: &#39;) . A recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes. | Precision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes. | When the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions. | . Plotting an ROC curve . Classification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models. most classifiers in scikit-learn have a .predict_proba() method which returns the probability of a given sample being in a particular class. Having built a logistic regression model, we&#39;ll now evaluate its performance by plotting an ROC curve. In doing so, we&#39;ll make use of the .predict_proba() method and become familiar with its functionality. . # Compute predicted probabilities: y_pred_prob y_pred_prob_pidd = logreg_pidd.predict_proba(X_test_pidd)[:,1] # Generate ROC curve values: fpr, tpr, thresholds fpr_pidd, tpr_pidd, thresholds_pidd = roc_curve(y_test_pidd, y_pred_prob_pidd) # Plot ROC curve plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr_pidd, tpr_pidd) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC Curve&#39;) plt.show() . Area under the ROC curve . Area under the ROC curve (AUC) . Larger area under the ROC curve = better model | . AUC computation . Say you have a binary classifier that in fact is just randomly making guesses. It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. The Area under this ROC curve would be 0.5. This is one way in which the AUC is an informative metric to evaluate a model. If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign! . We&#39;ll calculate AUC scores using the roc_auc_score() function from sklearn.metrics as well as by performing cross-validation on the diabetes dataset. . # Compute and print AUC score print(&quot;AUC: {}&quot;.format(roc_auc_score(y_test_pidd, y_pred_prob_pidd))) # Compute cross-validated AUC scores: cv_auc cv_auc_pidd = cross_val_score(logreg_pidd, X_pidd, y_pidd, cv=5, scoring=&quot;roc_auc&quot;) # Print list of AUC scores print(&quot;AUC scores computed using 5-fold cross-validation: {}&quot;.format(cv_auc_pidd)) . AUC: 0.8329050066628594 AUC scores computed using 5-fold cross-validation: [0.81962963 0.80537037 0.82555556 0.87377358 0.82509434] . Hyperparameter tuning . Hyperparameter tuning . Linear regression:Choosing parameters- Ridge/lasso regression: Choosing alpha | k-Nearest Neighbors: Choosing n_neighbors | Parameters like alpha and k: Hyperparameters | Hyperparameters cannot be learned by tting the model | . Choosing the correct hyperparameter . Try a bunch of different hyperparameter values | Fit all of them separately | See how well each performs | Choose the best performing one | It is essential to use cross-validation | . GridSearchCV in scikit-learn votes dataset . param_grid_votes = {&quot;n_neighbors&quot;:np.arange(1,50)} knn_votes = KNeighborsClassifier() knn_cv_votes = GridSearchCV(knn_votes, param_grid=param_grid_votes, cv=5) knn_cv_votes.fit(X_votes, y_votes) knn_cv_votes.best_params_ . {&#39;n_neighbors&#39;: 4} . knn_cv_votes.best_score_ . 0.9333333333333333 . Hyperparameter tuning with GridSearchCV . logistic regression also has a regularization parameter: $C$. $C$ controls the inverse of the regularization strength, and this is what we will tune. A large $C$ can lead to an overfit model, while a small $C$ can lead to an underfit model. . # Setup the hyperparameter grid c_space_pidd = np.logspace(-5, 8, 15) param_grid_pidd = {&#39;C&#39;: c_space_pidd} # Instantiate the GridSearchCV object: logreg_cv logreg_cv_pidd = GridSearchCV(logreg_pidd, param_grid_pidd, cv=5) # Fit it to the data logreg_cv_pidd.fit(X_pidd,y_pidd) # Print the tuned parameters and score print(&quot;Tuned Logistic Regression Parameters: {}&quot;.format(logreg_cv_pidd.best_params_)) print(&quot;Best score is {}&quot;.format(logreg_cv_pidd.best_score_)) . Tuned Logistic Regression Parameters: {&#39;C&#39;: 1389495.494373136} Best score is 0.7787029963500551 . Hyperparameter tuning with RandomizedSearchCV . GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. . Decision trees have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for RandomizedSearchCV. Our goal is to use RandomizedSearchCV to find the optimal hyperparameters. . # Setup the parameters and distributions to sample from: param_dist param_dist_pidd = {&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: randint(1, 9), &quot;min_samples_leaf&quot;: randint(1, 9), &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]} # Instantiate a Decision Tree classifier: tree tree_pidd = DecisionTreeClassifier() # Instantiate the RandomizedSearchCV object: tree_cv tree_cv_pidd = RandomizedSearchCV(tree_pidd, param_dist_pidd, cv=5) # Fit it to the data tree_cv_pidd.fit(X,y) # Print the tuned parameters and score print(&quot;Tuned Decision Tree Parameters: {}&quot;.format(tree_cv_pidd.best_params_)) print(&quot;Best score is {}&quot;.format(tree_cv_pidd.best_score_)) . Tuned Decision Tree Parameters: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 3, &#39;max_features&#39;: 3, &#39;min_samples_leaf&#39;: 2} Best score is 0.96 . . Note: RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time. . Hold-out set for final evaluation . Hold-out set reasoning . How well can the model perform on never before seen data? | Using ALL data for cross-validation is not ideal | Split data into training and hold-out set at the beginning | Perform grid search cross-validation on training set | Choose best hyperparameters and evaluate on hold-out set | . Hold-out set in practice I: Classification . You will now practice evaluating a model with tuned hyperparameters on a hold-out set. In addition to $C$, logistic regression has a &#39;penalty&#39; hyperparameter which specifies whether to use &#39;l1&#39; or &#39;l2&#39; regularization. Our job is to create a hold-out set, tune the &#39;C&#39; and &#39;penalty&#39; hyperparameters of a logistic regression classifier using GridSearchCV on the training set. . param_grid_pidd[&#39;penalty&#39;] = [&#39;l1&#39;, &#39;l2&#39;] # Instantiate the GridSearchCV object: logreg_cv logreg_cv_pidd = GridSearchCV(logreg_pidd, param_grid_pidd, cv=5) # Fit it to the training data logreg_cv_pidd.fit(X_train_pidd, y_train_pidd) # Print the optimal parameters and best score print(&quot;Tuned Logistic Regression Parameter: {}&quot;.format(logreg_cv_pidd.best_params_)) print(&quot;Tuned Logistic Regression Accuracy: {}&quot;.format(logreg_cv_pidd.best_score_)) . Tuned Logistic Regression Parameter: {&#39;C&#39;: 100000000.0, &#39;penalty&#39;: &#39;l2&#39;} Tuned Logistic Regression Accuracy: 0.7717391304347827 . Hold-out set in practice II: Regression . Lasso used the $L1$ penalty to regularize, while ridge used the $L2$ penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the $L1$ and $L2$ penalties: . $$ a∗L1+b∗L2 $$In scikit-learn, this term is represented by the &#39;l1_ratio&#39; parameter: An &#39;l1_ratio&#39; of 1 corresponds to an $L1$ penalty, and anything lower is a combination of $L1$ and $L2$. . We will GridSearchCV to tune the &#39;l1_ratio&#39; of an elastic net model trained on the Gapminder data. . # Create the hyperparameter grid l1_space_gapminder = np.linspace(0, 1, 30) param_grid_gapminder = {&#39;l1_ratio&#39;: l1_space_gapminder} # Instantiate the ElasticNet regressor: elastic_net elastic_net_gapminder = ElasticNet() # Setup the GridSearchCV object: gm_cv gm_cv_gapminder = GridSearchCV(elastic_net_gapminder, param_grid_gapminder, cv=5) # Fit it to the training data gm_cv_gapminder.fit(X_train_gapminder, y_train_gapminder) # Predict on the test set and compute metrics y_pred_gapminder = gm_cv_gapminder.predict(X_test_gapminder) r2_gapminder = gm_cv_gapminder.score(X_test_gapminder, y_test_gapminder) mse_gapminder = mean_squared_error(y_test_gapminder, y_pred_gapminder) print(&quot;Tuned ElasticNet l1 ratio: {}&quot;.format(gm_cv_gapminder.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2_gapminder)) print(&quot;Tuned ElasticNet MSE: {}&quot;.format(mse_gapminder)) . Tuned ElasticNet l1 ratio: {&#39;l1_ratio&#39;: 0.0} Tuned ElasticNet R squared: 0.8442220994403307 Tuned ElasticNet MSE: 10.144762014599413 . Preprocessing and pipelines . Pipelines, and how scikit-learn allows for transformers and estimators to be chained together and used as a single unit. Preprocessing techniques will be introduced as a way to enhance model performance, and pipelines will tie together concepts from previous sections. . Preprocessing data . Dealing with categorical features . Scikit-learn will not accept categorical features by default | Need to encode categorical features numerically | Convert to ‘dummy variables’ 0:Observation was NOT that category - 1: Observation was that category ### Dealing with categorical features in Python | . | scikit-learn:- OneHotEncoder()- pandas: get_dummies() | . | . Automobile dataset . mpg:Target Variable | Origin:Categorical Feature | . autos = pd.read_csv(&quot;datasets/autos.csv&quot;) autos.head() . mpg displ hp weight accel origin size . 0 18.0 | 250.0 | 88 | 3139 | 14.5 | US | 15.0 | . 1 9.0 | 304.0 | 193 | 4732 | 18.5 | US | 20.0 | . 2 36.1 | 91.0 | 60 | 1800 | 16.4 | Asia | 10.0 | . 3 18.5 | 250.0 | 98 | 3525 | 19.0 | US | 15.0 | . 4 34.3 | 97.0 | 78 | 2188 | 15.8 | Europe | 10.0 | . autos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 392 entries, 0 to 391 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 mpg 392 non-null float64 1 displ 392 non-null float64 2 hp 392 non-null int64 3 weight 392 non-null int64 4 accel 392 non-null float64 5 origin 392 non-null object 6 size 392 non-null float64 dtypes: float64(4), int64(2), object(1) memory usage: 21.6+ KB . autos.describe() . mpg displ hp weight accel size . count 392.000000 | 392.000000 | 392.000000 | 392.000000 | 392.000000 | 392.000000 | . mean 23.445918 | 194.411990 | 104.469388 | 2977.584184 | 15.541327 | 13.679847 | . std 7.805007 | 104.644004 | 38.491160 | 849.402560 | 2.758864 | 4.264458 | . min 9.000000 | 68.000000 | 46.000000 | 1613.000000 | 8.000000 | 7.500000 | . 25% 17.000000 | 105.000000 | 75.000000 | 2225.250000 | 13.775000 | 10.000000 | . 50% 22.750000 | 151.000000 | 93.500000 | 2803.500000 | 15.500000 | 10.000000 | . 75% 29.000000 | 275.750000 | 126.000000 | 3614.750000 | 17.025000 | 20.000000 | . max 46.600000 | 455.000000 | 230.000000 | 5140.000000 | 24.800000 | 20.000000 | . autos.shape . (392, 7) . EDA w/ categorical feature . _ = sns.boxplot(data=autos, x=&quot;origin&quot;, y=&quot;mpg&quot;, order=[&#39;Asia&#39;, &#39;US&#39;, &#39;Europe&#39;]) plt.show() . Encoding dummy variables . autos_origin = pd.get_dummies(autos) autos_origin.head() . mpg displ hp weight accel size origin_Asia origin_Europe origin_US . 0 18.0 | 250.0 | 88 | 3139 | 14.5 | 15.0 | 0 | 0 | 1 | . 1 9.0 | 304.0 | 193 | 4732 | 18.5 | 20.0 | 0 | 0 | 1 | . 2 36.1 | 91.0 | 60 | 1800 | 16.4 | 10.0 | 1 | 0 | 0 | . 3 18.5 | 250.0 | 98 | 3525 | 19.0 | 15.0 | 0 | 0 | 1 | . 4 34.3 | 97.0 | 78 | 2188 | 15.8 | 10.0 | 0 | 1 | 0 | . autos_origin = autos_origin.drop(&quot;origin_Asia&quot;, axis=1) autos_origin.head() . mpg displ hp weight accel size origin_Europe origin_US . 0 18.0 | 250.0 | 88 | 3139 | 14.5 | 15.0 | 0 | 1 | . 1 9.0 | 304.0 | 193 | 4732 | 18.5 | 20.0 | 0 | 1 | . 2 36.1 | 91.0 | 60 | 1800 | 16.4 | 10.0 | 0 | 0 | . 3 18.5 | 250.0 | 98 | 3525 | 19.0 | 15.0 | 0 | 1 | . 4 34.3 | 97.0 | 78 | 2188 | 15.8 | 10.0 | 1 | 0 | . Linear regression with dummy variables . X_autos_origin = autos_origin[[&quot;origin_Europe&quot;, &quot;origin_US&quot;]].values y_autos_origin = autos_origin[&#39;mpg&#39;].values . X_train_autos_origin, X_test_autos_origin, y_train_autos_origin, y_test_autos_origin, = train_test_split(X_autos_origin, y_autos_origin, test_size=.3, random_state=42) ridge_autos_origin = Ridge(alpha=.5, normalize=True).fit(X_train_autos_origin, y_train_autos_origin) ridge_autos_origin.score(X_test_autos_origin, y_test_autos_origin) . 0.3241789154336545 . Exploring categorical features . The Gapminder dataset that we worked with in previous section also contained a categorical &#39;Region&#39; feature, which we dropped since we did not have the tools to deal with it. Now however, we do, so we have added it back in! . We will explore this feature. Boxplots are particularly useful for visualizing categorical features such as this. . gapminder.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | . gapminder_2 = pd.read_csv(&quot;datasets/gapminder_2.csv&quot;) gapminder_2.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | Middle East &amp; North Africa | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | Sub-Saharan Africa | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | America | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | Europe &amp; Central Asia | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | East Asia &amp; Pacific | . # Create a boxplot of life expectancy per region gapminder_2.boxplot(&quot;life&quot;, &quot;Region&quot;, rot=60) # Show the plot plt.show() . . Important: Exploratory data analysis should always be the precursor to model building. . Creating dummy variables . scikit-learn does not accept non-numerical features. The &#39;Region&#39; feature contains very useful information that can predict life expectancy. For example, Sub-Saharan Africa has a lower life expectancy compared to Europe and Central Asia. Therefore, if we are trying to predict life expectancy, it would be preferable to retain the &#39;Region&#39; feature. To do this, we need to binarize it by creating dummy variables, which is what we will do. . # Create dummy variables with drop_first=True: df_region gapminder_region = pd.get_dummies(gapminder_2, drop_first=True) # Print the new columns of df_region print(gapminder_region.columns) . Index([&#39;population&#39;, &#39;fertility&#39;, &#39;HIV&#39;, &#39;CO2&#39;, &#39;BMI_male&#39;, &#39;GDP&#39;, &#39;BMI_female&#39;, &#39;life&#39;, &#39;child_mortality&#39;, &#39;Region_East Asia &amp; Pacific&#39;, &#39;Region_Europe &amp; Central Asia&#39;, &#39;Region_Middle East &amp; North Africa&#39;, &#39;Region_South Asia&#39;, &#39;Region_Sub-Saharan Africa&#39;], dtype=&#39;object&#39;) . gapminder_region.head() . population fertility HIV CO2 BMI_male GDP BMI_female life child_mortality Region_East Asia &amp; Pacific Region_Europe &amp; Central Asia Region_Middle East &amp; North Africa Region_South Asia Region_Sub-Saharan Africa . 0 34811059.0 | 2.73 | 0.1 | 3.328945 | 24.59620 | 12314.0 | 129.9049 | 75.3 | 29.5 | 0 | 0 | 1 | 0 | 0 | . 1 19842251.0 | 6.43 | 2.0 | 1.474353 | 22.25083 | 7103.0 | 130.1247 | 58.3 | 192.0 | 0 | 0 | 0 | 0 | 1 | . 2 40381860.0 | 2.24 | 0.5 | 4.785170 | 27.50170 | 14646.0 | 118.8915 | 75.5 | 15.4 | 0 | 0 | 0 | 0 | 0 | . 3 2975029.0 | 1.40 | 0.1 | 1.804106 | 25.35542 | 7383.0 | 132.8108 | 72.5 | 20.0 | 0 | 1 | 0 | 0 | 0 | . 4 21370348.0 | 1.96 | 0.1 | 18.016313 | 27.56373 | 41312.0 | 117.3755 | 81.5 | 5.2 | 1 | 0 | 0 | 0 | 0 | . Now that we have created the dummy variables, we can use the &#39;Region&#39; feature to predict life expectancy! . Regression with categorical features . We&#39;ll use ridge regression to perform 5-fold cross-validation. . X_gapminder_region = gapminder_region.drop(&quot;life&quot;, axis=1).values y_gapminder_region = gapminder_region.life.values . # Instantiate a ridge regressor: ridge ridge_gapminder_region = Ridge(alpha=.5, normalize=True) # Perform 5-fold cross-validation: ridge_cv ridge_cv_gapminder_region = cross_val_score(ridge_gapminder_region, X_gapminder_region, y_gapminder_region, cv=5) # Print the cross-validated scores print(ridge_cv_gapminder_region) . [0.86808336 0.80623545 0.84004203 0.7754344 0.87503712] . We now know how to build models using data that includes categorical features. . Handling missing data . Imputing missing data . Making an educated guess about the missing values | Example:Using the mean of the non-missing entries | . PIMA Indians dataset . pidd.head() . pregnancies glucose diastolic triceps insulin bmi dpf age diabetes . 0 6 | 148 | 72 | 35.00000 | 155.548223 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29.00000 | 155.548223 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 29.15342 | 155.548223 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23.00000 | 94.000000 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35.00000 | 168.000000 | 43.1 | 2.288 | 33 | 1 | . pidd.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 pregnancies 768 non-null int64 1 glucose 768 non-null int64 2 diastolic 768 non-null int64 3 triceps 768 non-null float64 4 insulin 768 non-null float64 5 bmi 768 non-null float64 6 dpf 768 non-null float64 7 age 768 non-null int64 8 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB . pidd.insulin.replace(0, np.nan, inplace=True) pidd.bmi.replace(0, np.nan, inplace=True) pidd.triceps.replace(0, np.nan, inplace=True) pidd.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 pregnancies 768 non-null int64 1 glucose 768 non-null int64 2 diastolic 768 non-null int64 3 triceps 768 non-null float64 4 insulin 768 non-null float64 5 bmi 768 non-null float64 6 dpf 768 non-null float64 7 age 768 non-null int64 8 diabetes 768 non-null int64 dtypes: float64(4), int64(5) memory usage: 54.1 KB . pidd.head() . pregnancies glucose diastolic triceps insulin bmi dpf age diabetes . 0 6 | 148 | 72 | 35.00000 | 155.548223 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29.00000 | 155.548223 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 29.15342 | 155.548223 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23.00000 | 94.000000 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35.00000 | 168.000000 | 43.1 | 2.288 | 33 | 1 | . Dropping missing data . The voting dataset1 contained a bunch of missing values that we dealt with for you behind the scenes. . votes2 = pd.read_csv(&quot;datasets/votes2.csv&quot;) votes2.head() . party infants water budget physician salvador religious satellite aid missile immigration synfuels education superfund crime duty_free_exports eaa_rsa . 0 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | ? | 1 | 1 | 1 | 0 | 1 | . 1 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | ? | . 2 democrat | ? | 1 | 1 | ? | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | . 3 democrat | 0 | 1 | 1 | 0 | ? | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 4 democrat | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | ? | 1 | 1 | 1 | 1 | . there are certain data points labeled with a &#39;?&#39;. These denote missing values. We will convert the &#39;?&#39;s to NaNs, and then drop the rows that contain them from the DataFrame. . # Convert &#39;?&#39; to NaN votes2[votes2 == &quot;?&quot;] = np.nan # Print the number of NaNs display(votes2.isnull().sum()) # Print shape of original DataFrame print(&quot;Shape of Original DataFrame: {}&quot;.format(votes2.shape)) # Print shape of new DataFrame print(&quot;Shape of DataFrame After Dropping All Rows with Missing Values: {}&quot;.format(votes2.dropna().shape)) . party 0 infants 12 water 48 budget 11 physician 11 salvador 15 religious 11 satellite 14 aid 15 missile 22 immigration 7 synfuels 21 education 31 superfund 25 crime 17 duty_free_exports 28 eaa_rsa 104 dtype: int64 . Shape of Original DataFrame: (435, 17) Shape of DataFrame After Dropping All Rows with Missing Values: (232, 17) . When many values in a dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It&#39;s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in. . Imputing missing data in a ML Pipeline I . there are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. Imputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow. . We will be setting up a pipeline with two steps: the imputation step, followed by the instantiation of a classifier. We&#39;ve seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree. Here we will be using the SVM (Support Vector Machine) . votes2.head() . party infants water budget physician salvador religious satellite aid missile immigration synfuels education superfund crime duty_free_exports eaa_rsa . 0 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | NaN | 1 | 1 | 1 | 0 | 1 | . 1 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | NaN | . 2 democrat | NaN | 1 | 1 | NaN | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | . 3 democrat | 0 | 1 | 1 | 0 | NaN | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 4 democrat | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | NaN | 1 | 1 | 1 | 1 | . votes2.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 435 entries, 0 to 434 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 party 435 non-null object 1 infants 423 non-null object 2 water 387 non-null object 3 budget 424 non-null object 4 physician 424 non-null object 5 salvador 420 non-null object 6 religious 424 non-null object 7 satellite 421 non-null object 8 aid 420 non-null object 9 missile 413 non-null object 10 immigration 428 non-null object 11 synfuels 414 non-null object 12 education 404 non-null object 13 superfund 410 non-null object 14 crime 418 non-null object 15 duty_free_exports 407 non-null object 16 eaa_rsa 331 non-null object dtypes: object(17) memory usage: 57.9+ KB . # Setup the Imputation transformer: imp imp_votes = SimpleImputer(missing_values=np.nan, strategy=&quot;most_frequent&quot;) # Instantiate the SVC classifier: clf clf_votes = SVC() # Setup the pipeline with the required steps: steps steps_votes = [(&#39;imputation&#39;, imp_votes), (&#39;SVM&#39;, clf_votes)] . Having set up the pipeline steps, we can now use it for classification. . Imputing missing data in a ML Pipeline II . Having setup the steps of the pipeline we will now use it on the voting dataset to classify a Congressman&#39;s party affiliation. What makes pipelines so incredibly useful is the simple interface that they provide. . X_votes[:5] . array([[0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1], [0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1], [0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0], [0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], [1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64) . votes.head() . party infants water budget physician salvador religious satellite aid missile immigration synfuels education superfund crime duty_free_exports eaa_rsa . 0 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | . 1 republican | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 | . 2 democrat | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | . 3 democrat | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | . 4 democrat | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 1 | . X_votes = votes.drop(&quot;party&quot;, axis=1) y_votes = votes.party . X_train_votes, X_test_votes, y_train_votes, y_test_votes = train_test_split(X_votes, y_votes, test_size=.3, random_state=42) . # Create the pipeline: pipeline pipeline_votes = Pipeline(steps_votes) # Fit the pipeline to the train set pipeline_votes.fit(X_train_votes, y_train_votes) # Predict the labels of the test set y_pred_votes = pipeline_votes.predict(X_test_votes) # Compute metrics print(classification_report(y_test_votes, y_pred_votes)) . precision recall f1-score support democrat 0.98 0.96 0.97 85 republican 0.94 0.96 0.95 46 accuracy 0.96 131 macro avg 0.96 0.96 0.96 131 weighted avg 0.96 0.96 0.96 131 . Centering and scaling . Why scale your data? . Many models use some form of distance to inform them | Features on larger scales can unduly influence the model | Example:k-NN uses distance explicitly when making predictions- We want features to be on a similar scale | Normalizing (or scaling and centering) ### Ways to normalize your data | Standardization:Subtract the mean and divide by variance- All features are centered around zero and have variance one | Can also subtract the minimum and divide by the range | Minimum zero and maximum one | Can also normalize so the data ranges from -1 to +1 | . Centering and scaling your data . the performance of a model can improve if the features are scaled. Note that this is not always the case: In the Congressional voting records dataset, for example, all of the features are binary. In such a situation, scaling will have minimal impact. We will explore scalling on White Wine Quality. . wwq = pd.read_csv(&quot;datasets/white_wine_quality.csv&quot;) wwq.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . X_wwq = pd.read_csv(&quot;datasets/X_wwq.csv&quot;).values X_wwq[:5] . array([[7.000e+00, 2.700e-01, 3.600e-01, 2.070e+01, 4.500e-02, 4.500e+01, 1.700e+02, 1.001e+00, 3.000e+00, 4.500e-01, 8.800e+00], [6.300e+00, 3.000e-01, 3.400e-01, 1.600e+00, 4.900e-02, 1.400e+01, 1.320e+02, 9.940e-01, 3.300e+00, 4.900e-01, 9.500e+00], [8.100e+00, 2.800e-01, 4.000e-01, 6.900e+00, 5.000e-02, 3.000e+01, 9.700e+01, 9.951e-01, 3.260e+00, 4.400e-01, 1.010e+01], [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01, 1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00], [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01, 1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00]]) . y_wwq = np.array([False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, True, False, True, True, False, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, True, False, True, True, False, False, True, False, False, True, True, False, False, True, False, True, False, False, False, True, False, False, True, False, False, False, False, False, False, True, False, True, True, True, True, True, False, True, False, False, True, False, True, True, True, True, True, False, False, True, True, True, True, True, False, False, False, True, False, False, False, True, False, True, True, True, True, False, True, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, True, True, False, False, True, True, False, False, True, False, True, False, True, True, True, False, False, True, True, False, True, True, False, True, False, True, False, True, False, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, False, False, True, True, True, True, True, True, False, False, False, False, True, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, True, True, True, True, True, False, False, False, False, False, True, False, True, True, False, False, True, False, True, False, False, False, True, True, True, True, False, False, True, True, False, False, False, True, True, True, True, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, False, True, True, False, False, False, True, False, True, False, True, True, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, True, True, True, False, True, False, False, True, True, True, False, True, False, True, False, True, False, False, True, True, False, False, False, True, False, False, False, False, False, False, False, False, False, True, True, True, True, True, False, True, False, False, True, False, False, True, False, False, False, False, False, True, True, False, False, False, True, True, False, False, False, False, False, True, False, True, True, True, True, False, True, True, False, False, True, True, False, True, False, False, False, True, False, False, False, False, True, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, True, False, True, True, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, True, True, True, False, False, True, False, True, False, False, False, False, True, False, False, False, True, True, False, True, False, True, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, False, True, True, True, False, True, False, False, False, False, False, True, True, False, False, True, True, True, False, False, False, True, True, True, True, False, False, False, False, True, True, False, True, True, False, True, False, False, False, True, True, False, True, False, False, False, True, True, True, False, True, False, True, True, True, True, False, True, False, False, False, False, False, False, False, False, True, True, True, True, False, True, True, False, True, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True, True, False, True, True, True, False, True, False, False, True, False, True, True, False, True, False, True, True, True, True, False, True, False, False, True, True, False, False, True, True, False, False, True, False, False, False, True, False, False, True, True, False, False, False, True, False, True, True, True, False, False, False, False, True, False, False, True, False, False, True, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, True, False, True, True, False, False, False, False, False, True, True, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, True, True, False, False, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, False, True, False, False, True, True, False, True, False, False, False, False, True, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, True, True, False, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, True, True, False, True, True, False, True, False, True, True, True, True, True, False, False, True, False, False, False, False, False, True, True, True, True, True, False, False, True, False, True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, True, False, False, False, True, False, False, True, True, True, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, True, False, True, True, False, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, False, True, False, True, False, True, True, True, False, True, False, False, False, False, False, True, False, True, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, True, False, False, False, False, True, False, False, False, True, True, False, True, True, True, True, False, False, False, True, True, False, False, False, True, False, False, False, True, True, True, False, False, False, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, True, False, True, False, True, False, False, True, True, True, False, True, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, True, False, True, False, True, False, False, False, False, True, True, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, True, True, True, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, True, False, True, False, False, True, False, False, True, False, False, False, False, True, True, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, True, False, False, False, True, True, True, False, False, False, False, False, False, False, True, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, True, True, False, True, False, True, True, False, False, False, True, False, True, True, True, False, False, True, True, True, False, True, True, True, True, True, False, False, False, False, False, False, False, False, True, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, True, True, False, False, True, False, True, False, True, False, False, False, False, False, False, True, True, False, False, False, False, True, False, True, True, False, False, False, True, False, False, False, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, False, True, True, False, False, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, True, False, True, True, False, True, True, False, False, True, True, False, False, True, True, True, True, False, False, True, False, True, True, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, True, False, False, False, False, False, True, False, True, True, True, False, False, False, False, False, False, False, True, True, False, False, True, False, True, True, True, False, True, True, False, False, False, False, True, False, True, False, False, False, False, False, True, True, False, True, False, False, False, True, True, False, False, False, False, False, True, False, True, False, True, False, False, True, False, False, False, False, False, False, False, True, True, True, True, False, False, True, True, True, True, True, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, True, True, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, True, True, True, True, False, False, False, False, True, True, True, True, True, False, False, False, False, False, False, False, True, False, False, False, True, True, False, False, False, False, False, False, True, False, True, True, False, True, True, False, True, False, False, False, False, True, False, False, False, True, True, False, True, False, False, True, False, False, False, False, False, True, False, False, False, True, False, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, True, True, True, True, False, False, False, False, True, False, False, True, True, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, True, False, False, False, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, True, False, True, False, False, True, False, False, True, True, False, False, False, False, False, True, False, False, False, False, True, True, True, True, False, False, True, False, True, False, True, False, True, False, True, False, False, True, True, True, True, True, True, True, False, False, True, False, True, True, True, False, False, True, True, True, False, False, True, True, True, True, False, True, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, True, True, False, True, False, False, True, False, False, False, False, False, False, True, True, True, True, False, False, False, True, False, True, True, True, False, True, True, True, False, True, False, False, False, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, False, True, True, False, True, True, True, False, False, False, False, True, True, False, True, True, False, False, True, True, True, False, True, False, False, True, False, False, False, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, False, False, False, True, True, True, True, False, True, True, True, False, True, True, False, True, True, False, True, True, True, False, True, True, False, False, False, False, True, False, True, True, True, True, True, True, False, False, False, False, True, True, False, False, True, False, False, True, True, True, False, False, True, False, False, False, False, True, True, True, False, False, False, False, False, True, True, False, False, True, False, True, False, False, False, True, True, False, False, True, True, False, False, False, True, False, False, False, True, False, False, True, False, True, False, True, True, True, True, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, False, True, True, False, True, True, False, False, True, True, True, True, True, False, True, False, False, True, False, False, True, True, False, False, True, False, True, False, True, False, False, True, False, True, True, False, False, False, False, False, False, True, False, True, False, False, False, True, False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, True, True, True, False, True, False, True, False, False, False, False, True, True, False, False, True, False, False, True, False, True, False, True, True, False, True, False, False, True, True, True, True, True, False, False, True, False, True, False, False, True, False, False, True, True, False, False, False, False, False, False, True, True, True, False, True, False, True, True, True, True, False, False, False, False, True, True, True, False, False, False, False, True, True, True, False, False, False, False, True, True, True, True, False, False, False, False, False, True, True, True, False, False, False, True, True, False, False, True, True, False, False, True, True, False, False, True, True, True, True, False, True, True, True, False, True, True, False, True, False, False, False, False, False, True, True, True, False, False, True, True, False, True, False, True, False, True, True, True, True, True, False, False, False, True, True, False, False, True, False, False, False, False, True, True, True, True, False, True, True, True, False, False, True, True, False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, True, False, True, True, True, True, True, True, False, True, True, True, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, True, False, False, True, True, False, False, False, False, True, False, True, True, True, False, False, False, False, False, True, True, False, True, False, False, False, True, True, False, True, True, False, True, False, False, False, True, False, False, True, False, True, False, True, False, True, False, True, False, False, True, False, True, True, True, False, True, True, False, False, True, False, False, False, False, False, False, False, False, True, False, False, True, True, False, True, True, False, True, False, False, True, False, False, False, False, False, False, False, False, True, True, False, True, True, False, False, False, False, False, True, True, False, False, True, True, False, False, False, True, False, False, True, True, True, False, True, True, False, False, False, True, True, True, False, False, False, True, False, False, True, True, False, True, True, True, True, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, True, True, False, False, False, True, False, True, True, True, True, False, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, True, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, True, False, True, True, True, False, True, True, False, True, False, False, True, False, False, True, True, False, False, False, False, True, True, False, False, False, True, False, False, False, True, True, False, True, False, True, True, True, False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, True, False, True, False, False, False, True, True, True, False, False, False, True, False, False, True, True, True, True, True, False, False, True, False, False, True, True, False, False, False, True, True, False, True, False, False, False, False, False, True, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, True, False, False, False, True, True, False, False, False, False, False, False, False, True, True, True, True, False, True, False, False, True, False, False, False, True, False, True, False, False, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, True, True, True, False, True, False, False, True, True, True, False, True, True, False, True, True, False, True, True, False, False, False, False, False, True, True, False, True, True, True, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, True, False, False, True, False, False, False, False, False, False, True, False, True, False, True, False, True, False, True, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, True, False, False, False, True, False, False, True, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, True, True, False, True, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, True, False, False, False, True, True, False, False, True, False, True, False, False, False, False, False, False, False, False, True, True, False, True, False, False, True, True, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, True, False, True, False, False, False, True, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, True, False, False, False, False, False, True, False, False, True, False, False, False, False, True, True, True, True, True, False, False, True, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, True, False, True, True, False, False, False, False, True, False, True, True, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, True, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, True, False, False, False, True, False, True, True, False, False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, True, True, True, False, False, False, False, False, False, True, True, True, False, True, True, True, False, True, False, True, True, False, True, True, False, False, False, False, False, False, True, False, False, True, False, True, False, True, True, False, False, False, False, False, False, False, False, True, False, True, False, False, True, True, True, False, False, True, True, True, False, False, False, True, True, False, True, True, True, False, True, True, False, False, True, True, False, True, False, False, False, True, False, False, True, False, False, True, True, True, False, True, True, False, False, True, False, False, True, True, True, True, False, True, False, True, False, True, True, False, False, True, True, True, True, True, False, True, False, False, False, True, False, False, True, False, False, False, True, False, False, False, True, True, False, True, True, False, False, True, True, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, True, True, True, False, True, True, True, True, False, False, True, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, True, True, True, False, True, True, True, False, False, False, True, False, True, False, True, False, False, True, True, False, False, False, False, False, False, True, True, True, True, False, False, True, False, True, True, False, False, False, True, False, False, True, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, True, True, True, True, False, False, False, False, True, False, False, True, True, True, False, True, False, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, True, False, True, True, False, False, True, False, False, True, False, False, True, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, True, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, True, True, True, False, False, True, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, True, True, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, True, True, False, False, True, False, True, False, False, False, False, False, True, True, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, True, True, True, True, False, True, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, True, True, True, True, False, False, False, False, False, True, True, False, False, False, False, False, False, False, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, True, True, False, False, True, True, False, False, False, False, True, False, False, True, True, False, True, True, True, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, False, True, False, True, False, True, False, False, True, False, True, False, True, False, False, True, True, False, False, True, False, True, False, True, True, False, False, False, False, True, True, False, False, False, True, True, True, True, False, False, True, True, True, True, True, True, False, False, True, True, True, False, False, True, False, True, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, False, False, False, False, True, True, True, False, True, True, False, False, False, False, True, False, False, True, True, False, False, True, True, True, False, True, False, False, False, False, True, True, True, True, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, True, True, False, True, True, True, False, False, False, False, True, False, False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, True, False, False, True, True, False, False, False, False, True, True, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, True, True, True, True, False, True, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, True, False, True, True, False, False, False, False, False, False, True, True, False, False, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, False, False, False, True, False, False, True, True, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, True, True, True, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, True, False, False, False, True, False, True, False, False, False, True, False, True, True, True, False, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, True, True, False, False, False, True, False, True, False, False, False, True, False, False, False, True, False, True, False, False, False]) . y_wwq = pd.read_csv(&quot;datasets/y_wwq.csv&quot;).values y_wwq[:5] . array([[False], [False], [False], [False], [False]]) . some features seem to have different units of measurement. &#39;density&#39;, for instance, takes values between 0.98 and 1.04, while &#39;total sulfur dioxide&#39; ranges from 9 to 440. As a result, it may be worth scaling the features here. We will scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features. . # Scale the features: X_scaled X_scaled_wwq = scale(X_wwq) # Print the mean and standard deviation of the unscaled features print(&quot;Mean of Unscaled Features: {}&quot;.format(np.mean(X_wwq))) print(&quot;Standard Deviation of Unscaled Features: {}&quot;.format(np.std(X_wwq))) . Mean of Unscaled Features: 18.43268707245592 Standard Deviation of Unscaled Features: 41.544947640946354 . # Print the mean and standard deviation of the scaled features print(&quot;Mean of Scaled Features: {}&quot;.format(np.mean(X_scaled_wwq))) print(&quot;Standard Deviation of Scaled Features: {}&quot;.format(np.std(X_scaled_wwq))) . Mean of Scaled Features: 2.660809650821445e-15 Standard Deviation of Scaled Features: 1.0 . Centering and scaling in a pipeline . With regard to whether or not scaling is effective, the proof is in the pudding! We will See for ourselves whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. We will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison . # Setup the pipeline steps: steps steps_wwq = [(&#39;scaler&#39;, StandardScaler()), (&#39;knn&#39;, KNeighborsClassifier())] # Create the pipeline: pipeline pipeline_wwq = Pipeline(steps_wwq) # Create train and test sets X_train_wwq, X_test_wwq, y_train_wwq, y_test_wwq = train_test_split(X_wwq,y_wwq, test_size=.3, random_state=42) # Fit the pipeline to the training set: knn_scaled knn_scaled_wwq = pipeline_wwq.fit(X_train_wwq, y_train_wwq) # Instantiate and fit a k-NN classifier to the unscaled data knn_unscaled_wwq = KNeighborsClassifier().fit(X_train_wwq, y_train_wwq) # Compute and print metrics print(&#39;Accuracy with Scaling: {}&#39;.format(knn_scaled_wwq.score(X_test_wwq, y_test_wwq))) print(&#39;Accuracy without Scaling: {}&#39;.format(knn_unscaled_wwq.score(X_test_wwq, y_test_wwq))) . Accuracy with Scaling: 0.7700680272108843 Accuracy without Scaling: 0.6979591836734694 . It looks like scaling has significantly improved model performance! . Bringing it all together I: Pipeline for classification . We will build a pipeline that includes scaling and hyperparameter tuning to classify wine quality. . X_wwq[:5] . array([[7.000e+00, 2.700e-01, 3.600e-01, 2.070e+01, 4.500e-02, 4.500e+01, 1.700e+02, 1.001e+00, 3.000e+00, 4.500e-01, 8.800e+00], [6.300e+00, 3.000e-01, 3.400e-01, 1.600e+00, 4.900e-02, 1.400e+01, 1.320e+02, 9.940e-01, 3.300e+00, 4.900e-01, 9.500e+00], [8.100e+00, 2.800e-01, 4.000e-01, 6.900e+00, 5.000e-02, 3.000e+01, 9.700e+01, 9.951e-01, 3.260e+00, 4.400e-01, 1.010e+01], [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01, 1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00], [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01, 1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00]]) . # Setup the pipeline steps_wwq = [(&#39;scaler&#39;, StandardScaler()), (&#39;SVM&#39;, SVC())] pipeline_wwq = Pipeline(steps_wwq) # Specify the hyperparameter space parameters_wwq = {&#39;SVM__C&#39;:[1, 10, 100], &#39;SVM__gamma&#39;:[0.1, 0.01]} # Create train and test sets X_train_wwq, X_test_wwq, y_train_wwq, y_test_wwq = train_test_split(X_wwq,y_wwq, test_size=.2, random_state=21) # Instantiate the GridSearchCV object: cv cv_wwq = GridSearchCV(pipeline_wwq, param_grid=parameters_wwq, cv=3) # Fit to the training set cv_wwq.fit(X_train_wwq, y_train_wwq) # Predict the labels of the test set: y_pred y_pred_wwq = cv_wwq.predict(X_test_wwq) # Compute and print metrics print(&quot;Accuracy: {}&quot;.format(cv_wwq.score(X_test_wwq, y_test_wwq))) print(classification_report(y_test_wwq, y_pred_wwq)) print(&quot;Tuned Model Parameters: {}&quot;.format(cv_wwq.best_params_)) . Accuracy: 0.7795918367346939 precision recall f1-score support False 0.83 0.85 0.84 662 True 0.67 0.63 0.65 318 accuracy 0.78 980 macro avg 0.75 0.74 0.74 980 weighted avg 0.78 0.78 0.78 980 Tuned Model Parameters: {&#39;SVM__C&#39;: 10, &#39;SVM__gamma&#39;: 0.1} . Bringing it all together II: Pipeline for regression . We will return to the gapminder dataset, it had a lot of missing data. We will build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. We will then tune the l1_ratio of the ElasticNet using GridSearchCV. . X_gapminder = np.array([[3.48110590e+07, 2.73000000e+00, 1.00000000e-01, 3.32894466e+00, 2.45962000e+01, 1.23140000e+04, 1.29904900e+02, 2.95000000e+01], [1.98422510e+07, 6.43000000e+00, 2.00000000e+00, 1.47435339e+00, 2.22508300e+01, 7.10300000e+03, 1.30124700e+02, 1.92000000e+02], [4.03818600e+07, 2.24000000e+00, 5.00000000e-01, 4.78516998e+00, 2.75017000e+01, 1.46460000e+04, 1.18891500e+02, 1.54000000e+01], [2.97502900e+06, 1.40000000e+00, 1.00000000e-01, 1.80410622e+00, 2.53554200e+01, 7.38300000e+03, 1.32810800e+02, 2.00000000e+01], [2.13703480e+07, 1.96000000e+00, 1.00000000e-01, 1.80163133e+01, 2.75637300e+01, 4.13120000e+04, 1.17375500e+02, 5.20000000e+00], [8.33146500e+06, 1.41000000e+00, 3.00000000e-01, 8.18316002e+00, 2.64674100e+01, 4.39520000e+04, 1.24139400e+02, 4.60000000e+00], [8.86871300e+06, 1.99000000e+00, 1.00000000e-01, 5.10953829e+00, 2.56511700e+01, 1.43650000e+04, 1.28602400e+02, 4.33000000e+01], [3.48587000e+05, 1.89000000e+00, 3.10000000e+00, 3.13192132e+00, 2.72459400e+01, 2.43730000e+04, 1.24386200e+02, 1.45000000e+01], [1.48252473e+08, 2.38000000e+00, 6.00000000e-02, 3.19161002e-01, 2.03974200e+01, 2.26500000e+03, 1.25030700e+02, 5.59000000e+01], [2.77315000e+05, 1.83000000e+00, 1.30000000e+00, 6.00827884e+00, 2.63843900e+01, 1.60750000e+04, 1.26394000e+02, 1.54000000e+01], [9.52645300e+06, 1.42000000e+00, 2.00000000e-01, 6.48817388e+00, 2.61644300e+01, 1.44880000e+04, 1.29796800e+02, 7.20000000e+00], [1.07791550e+07, 1.82000000e+00, 2.00000000e-01, 9.79733671e+00, 2.67591500e+01, 4.16410000e+04, 1.21822700e+02, 4.70000000e+00], [3.06165000e+05, 2.91000000e+00, 2.40000000e+00, 1.36012592e+00, 2.70225500e+01, 8.29300000e+03, 1.20922400e+02, 2.01000000e+01], [8.97352500e+06, 5.27000000e+00, 1.20000000e+00, 5.37539184e-01, 2.24183500e+01, 1.64600000e+03, 1.30272300e+02, 1.16300000e+02], [6.94990000e+05, 2.51000000e+00, 2.00000000e-01, 6.01210310e-01, 2.28218000e+01, 5.66300000e+03, 1.25125800e+02, 4.81000000e+01], [9.59991600e+06, 3.48000000e+00, 2.00000000e-01, 1.43182915e+00, 2.44333500e+01, 5.06600000e+03, 1.22415500e+02, 5.20000000e+01], [1.96786600e+06, 2.86000000e+00, 2.49000000e+01, 2.54720549e+00, 2.21298400e+01, 1.38580000e+04, 1.33130700e+02, 6.38000000e+01], [1.94769696e+08, 1.90000000e+00, 4.50000000e-01, 2.02377284e+00, 2.57862300e+01, 1.39060000e+04, 1.24874500e+02, 1.86000000e+01], [7.51364600e+06, 1.43000000e+00, 1.00000000e-01, 6.69013908e+00, 2.65428600e+01, 1.53680000e+04, 1.28472100e+02, 1.37000000e+01], [1.47090110e+07, 6.04000000e+00, 1.20000000e+00, 1.09419171e-01, 2.12715700e+01, 1.35800000e+03, 1.30665100e+02, 1.30400000e+02], [8.82179500e+06, 6.48000000e+00, 3.50000000e+00, 3.13888013e-02, 2.15029100e+01, 7.23000000e+02, 1.34195500e+02, 1.08600000e+02], [1.39336600e+07, 3.05000000e+00, 6.00000000e-01, 2.87547496e-01, 2.08049600e+01, 2.44200000e+03, 1.17552800e+02, 5.15000000e+01], [1.95704180e+07, 5.17000000e+00, 5.30000000e+00, 2.95541639e-01, 2.36817300e+01, 2.57100000e+03, 1.27282300e+02, 1.13800000e+02], [3.33632560e+07, 1.68000000e+00, 2.00000000e-01, 1.63503986e+01, 2.74521000e+01, 4.14680000e+04, 1.18057100e+02, 5.80000000e+00], [1.11397400e+07, 6.81000000e+00, 3.40000000e+00, 4.78391264e-02, 2.14856900e+01, 1.75300000e+03, 1.27864000e+02, 1.68000000e+02], [1.66459400e+07, 1.89000000e+00, 4.00000000e-01, 4.24025914e+00, 2.70154200e+01, 1.86980000e+04, 1.25541700e+02, 8.90000000e+00], [4.49016600e+07, 2.43000000e+00, 5.00000000e-01, 1.47609182e+00, 2.49404100e+01, 1.04890000e+04, 1.24023500e+02, 1.97000000e+01], [6.65414000e+05, 5.05000000e+00, 6.00000000e-02, 1.78853064e-01, 2.20613100e+01, 1.44000000e+03, 1.32135400e+02, 9.12000000e+01], [3.83277100e+06, 5.10000000e+00, 3.50000000e+00, 3.84220477e-01, 2.18713400e+01, 5.02200000e+03, 1.31693500e+02, 7.26000000e+01], [4.42950600e+06, 1.91000000e+00, 3.00000000e-01, 1.91193342e+00, 2.64789700e+01, 1.22190000e+04, 1.21350000e+02, 1.03000000e+01], [1.92616470e+07, 4.91000000e+00, 3.70000000e+00, 3.61896603e-01, 2.25646900e+01, 2.85400000e+03, 1.31523700e+02, 1.16900000e+02], [4.34415100e+06, 1.43000000e+00, 6.00000000e-02, 5.28790258e+00, 2.65962900e+01, 2.18730000e+04, 1.30392100e+02, 5.90000000e+00], [1.12902390e+07, 1.50000000e+00, 1.00000000e-01, 2.70177717e+00, 2.50686700e+01, 1.77650000e+04, 1.26059400e+02, 6.30000000e+00], [5.49530200e+06, 1.89000000e+00, 2.00000000e-01, 8.54150780e+00, 2.61328700e+01, 4.50170000e+04, 1.19581500e+02, 4.30000000e+00], [8.09639000e+05, 3.76000000e+00, 2.60000000e+00, 6.12799524e-01, 2.33840300e+01, 2.50200000e+03, 1.29337600e+02, 8.10000000e+01], [1.44476000e+07, 2.73000000e+00, 4.00000000e-01, 2.11051780e+00, 2.55884100e+01, 9.24400000e+03, 1.22986400e+02, 2.68000000e+01], [7.89761220e+07, 2.95000000e+00, 6.00000000e-02, 2.51239420e+00, 2.67324300e+01, 9.97400000e+03, 1.25093100e+02, 3.14000000e+01], [6.00419900e+06, 2.32000000e+00, 8.00000000e-01, 1.06776463e+00, 2.63675100e+01, 7.45000000e+03, 1.19932100e+02, 2.16000000e+01], [6.86223000e+05, 5.31000000e+00, 4.70000000e+00, 6.79825323e+00, 2.37664000e+01, 4.01430000e+04, 1.32039200e+02, 1.18400000e+02], [4.50063800e+06, 5.16000000e+00, 8.00000000e-01, 8.37456442e-02, 2.08850900e+01, 1.08800000e+03, 1.25794800e+02, 6.04000000e+01], [1.33994100e+06, 1.62000000e+00, 1.20000000e+00, 1.30313789e+01, 2.62644600e+01, 2.47430000e+04, 1.29516100e+02, 5.50000000e+00], [8.43206000e+05, 2.74000000e+00, 1.00000000e-01, 1.27777956e+00, 2.65307800e+01, 7.12900000e+03, 1.27476800e+02, 2.40000000e+01], [5.31417000e+06, 1.85000000e+00, 1.00000000e-01, 1.06441143e+01, 2.67333900e+01, 4.21220000e+04, 1.26564500e+02, 3.30000000e+00], [6.23095290e+07, 1.97000000e+00, 4.00000000e-01, 5.99902073e+00, 2.58532900e+01, 3.75050000e+04, 1.20014600e+02, 4.30000000e+00], [1.47374100e+06, 4.28000000e+00, 5.30000000e+00, 1.07953932e+00, 2.40762000e+01, 1.58000000e+04, 1.30362500e+02, 6.80000000e+01], [1.58674900e+06, 5.80000000e+00, 1.70000000e+00, 2.51002328e-01, 2.16502900e+01, 1.56600000e+03, 1.30208000e+02, 8.74000000e+01], [4.34329000e+06, 1.79000000e+00, 1.00000000e-01, 1.41942978e+00, 2.55494200e+01, 5.90000000e+03, 1.30578900e+02, 1.93000000e+01], [8.06659060e+07, 1.37000000e+00, 1.00000000e-01, 9.49724676e+00, 2.71650900e+01, 4.11990000e+04, 1.24904400e+02, 4.40000000e+00], [2.31159190e+07, 4.19000000e+00, 1.80000000e+00, 3.66600849e-01, 2.28424700e+01, 2.90700000e+03, 1.28295300e+02, 7.99000000e+01], [1.11617550e+07, 1.46000000e+00, 1.00000000e-01, 8.66123553e+00, 2.63378600e+01, 3.21970000e+04, 1.22934200e+02, 4.90000000e+00], [1.41066870e+07, 4.12000000e+00, 8.00000000e-01, 8.35594820e-01, 2.52994700e+01, 6.96000000e+03, 1.20959600e+02, 3.69000000e+01], [1.04273560e+07, 5.34000000e+00, 1.40000000e+00, 1.26964400e-01, 2.25244900e+01, 1.23000000e+03, 1.32276500e+02, 1.21000000e+02], [1.56129300e+06, 5.25000000e+00, 2.50000000e+00, 1.56376432e-01, 2.16433800e+01, 1.32600000e+03, 1.30762700e+02, 1.27600000e+02], [7.48096000e+05, 2.74000000e+00, 1.20000000e+00, 2.07341531e+00, 2.36846500e+01, 5.20800000e+03, 1.25151200e+02, 4.19000000e+01], [9.70513000e+06, 3.50000000e+00, 2.00000000e+00, 2.49306755e-01, 2.36630200e+01, 1.60000000e+03, 1.25346100e+02, 8.33000000e+01], [7.25947000e+06, 3.27000000e+00, 8.00000000e-01, 1.18745352e+00, 2.51087200e+01, 4.39100000e+03, 1.22962100e+02, 2.65000000e+01], [1.00506990e+07, 1.33000000e+00, 6.00000000e-02, 5.45323172e+00, 2.71156800e+01, 2.33340000e+04, 1.28696800e+02, 7.20000000e+00], [3.10033000e+05, 2.12000000e+00, 3.00000000e-01, 6.82190305e+00, 2.72068700e+01, 4.22940000e+04, 1.18738100e+02, 2.70000000e+00], [1.19707011e+09, 2.64000000e+00, 3.20000000e-01, 1.52084942e+00, 2.09595600e+01, 3.90100000e+03, 1.23127400e+02, 6.56000000e+01], [2.35360765e+08, 2.48000000e+00, 2.00000000e-01, 1.75504422e+00, 2.18557600e+01, 7.85600000e+03, 1.26421600e+02, 3.62000000e+01], [7.25306930e+07, 1.88000000e+00, 2.00000000e-01, 7.89221094e+00, 2.53100300e+01, 1.59550000e+04, 1.25185900e+02, 2.14000000e+01], [4.48014500e+06, 2.00000000e+00, 2.00000000e-01, 9.88253103e+00, 2.76532500e+01, 4.77130000e+04, 1.24780100e+02, 4.50000000e+00], [7.09380800e+06, 2.92000000e+00, 2.00000000e-01, 1.00011881e+01, 2.71315100e+01, 2.85620000e+04, 1.21083800e+02, 4.90000000e+00], [5.93192340e+07, 1.39000000e+00, 3.00000000e-01, 7.46594241e+00, 2.64802000e+01, 3.74750000e+04, 1.23703000e+02, 4.10000000e+00], [2.71734400e+06, 2.39000000e+00, 1.70000000e+00, 4.39145647e+00, 2.40042100e+01, 8.95100000e+03, 1.25368500e+02, 1.89000000e+01], [1.27317900e+08, 1.34000000e+00, 6.00000000e-02, 9.53660569e+00, 2.35000400e+01, 3.48000000e+04, 1.21965100e+02, 3.40000000e+00], [1.59159660e+07, 2.51000000e+00, 1.00000000e-01, 1.47181043e+01, 2.62907800e+01, 1.87970000e+04, 1.28851700e+02, 2.59000000e+01], [3.82444420e+07, 4.76000000e+00, 6.30000000e+00, 2.66308378e-01, 2.15925800e+01, 2.35800000e+03, 1.29934100e+02, 7.10000000e+01], [2.14421500e+06, 1.50000000e+00, 6.00000000e-01, 3.34184866e+00, 2.64569300e+01, 2.09770000e+04, 1.29574600e+02, 1.05000000e+01], [4.10938900e+06, 1.57000000e+00, 1.00000000e-01, 3.99672180e+00, 2.72011700e+01, 1.41580000e+04, 1.27503700e+02, 1.13000000e+01], [1.97219400e+06, 3.34000000e+00, 2.36000000e+01, 8.61766942e-03, 2.19015700e+01, 2.04100000e+03, 1.31136100e+02, 1.14200000e+02], [3.67278200e+06, 5.19000000e+00, 1.60000000e+00, 1.57352183e-01, 2.18953700e+01, 5.88000000e+02, 1.31255500e+02, 1.00900000e+02], [3.21980200e+06, 1.42000000e+00, 1.00000000e-01, 4.49848339e+00, 2.68610200e+01, 2.32230000e+04, 1.30822600e+02, 8.20000000e+00], [4.85079000e+05, 1.63000000e+00, 3.00000000e-01, 2.21680797e+01, 2.74340400e+01, 9.50010000e+04, 1.22370500e+02, 2.80000000e+00], [1.99267980e+07, 4.79000000e+00, 2.00000000e-01, 9.94221476e-02, 2.14034700e+01, 1.52800000e+03, 1.32837100e+02, 6.67000000e+01], [1.39046710e+07, 5.78000000e+00, 1.12000000e+01, 8.24698808e-02, 2.20346800e+01, 6.74000000e+02, 1.33939000e+02, 1.01100000e+02], [2.71974190e+07, 2.05000000e+00, 5.00000000e-01, 7.75223395e+00, 2.47306900e+01, 1.99680000e+04, 1.23859300e+02, 8.00000000e+00], [3.21026000e+05, 2.38000000e+00, 6.00000000e-02, 3.27772577e+00, 2.32199100e+01, 1.20290000e+04, 1.23322300e+02, 1.60000000e+01], [1.42234030e+07, 6.82000000e+00, 1.00000000e+00, 4.10788666e-02, 2.17888100e+01, 1.60200000e+03, 1.28030800e+02, 1.48300000e+02], [4.06392000e+05, 1.38000000e+00, 1.00000000e-01, 6.18277102e+00, 2.76836100e+01, 2.78720000e+04, 1.24157100e+02, 6.60000000e+00], [3.41455200e+06, 4.94000000e+00, 7.00000000e-01, 6.13103977e-01, 2.26229500e+01, 3.35600000e+03, 1.29987500e+02, 1.03000000e+02], [1.23801300e+06, 1.58000000e+00, 9.00000000e-01, 3.07876290e+00, 2.51566900e+01, 1.46150000e+04, 1.30878600e+02, 1.58000000e+01], [1.14972821e+08, 2.35000000e+00, 3.00000000e-01, 4.26117187e+00, 2.74246800e+01, 1.58260000e+04, 1.22121600e+02, 1.79000000e+01], [4.11116800e+06, 1.49000000e+00, 4.00000000e-01, 1.31332119e+00, 2.42369000e+01, 3.89000000e+03, 1.29942400e+02, 1.76000000e+01], [2.62966600e+06, 2.37000000e+00, 6.00000000e-02, 3.75948682e+00, 2.48838500e+01, 7.56300000e+03, 1.29750400e+02, 3.48000000e+01], [3.13505440e+07, 2.44000000e+00, 1.00000000e-01, 1.59408314e+00, 2.56318200e+01, 6.09100000e+03, 1.26528400e+02, 3.58000000e+01], [2.29948670e+07, 5.54000000e+00, 1.14000000e+01, 1.04748301e-01, 2.19353600e+01, 8.64000000e+02, 1.35394900e+02, 1.14400000e+02], [5.10300060e+07, 2.05000000e+00, 6.00000000e-01, 1.91053400e-01, 2.14493200e+01, 2.89100000e+03, 1.23142100e+02, 8.72000000e+01], [2.63251830e+07, 2.90000000e+00, 4.00000000e-01, 1.05412983e-01, 2.07634400e+01, 1.86600000e+03, 1.25556100e+02, 5.07000000e+01], [1.65198620e+07, 1.77000000e+00, 2.00000000e-01, 1.05330281e+01, 2.60154100e+01, 4.73880000e+04, 1.21695000e+02, 4.80000000e+00], [4.28538000e+06, 2.12000000e+00, 1.00000000e-01, 8.00908440e+00, 2.77689300e+01, 3.21220000e+04, 1.18742100e+02, 6.40000000e+00], [5.59452400e+06, 2.72000000e+00, 2.00000000e-01, 7.78151613e-01, 2.57729100e+01, 4.06000000e+03, 1.23479200e+02, 2.81000000e+01], [1.50851300e+07, 7.59000000e+00, 8.00000000e-01, 6.34371088e-02, 2.12195800e+01, 8.43000000e+02, 1.35102100e+02, 1.41300000e+02], [1.51115683e+08, 6.02000000e+00, 3.60000000e+00, 6.14689662e-01, 2.30332200e+01, 4.68400000e+03, 1.35492000e+02, 1.40900000e+02], [4.77163300e+06, 1.96000000e+00, 1.00000000e-01, 1.05297688e+01, 2.69342400e+01, 6.52160000e+04, 1.26026600e+02, 3.60000000e+00], [2.65228100e+06, 2.89000000e+00, 1.00000000e-01, 1.55720805e+01, 2.62410900e+01, 4.77990000e+04, 1.26887000e+02, 1.19000000e+01], [1.63096985e+08, 3.58000000e+00, 1.00000000e-01, 9.35618056e-01, 2.22991400e+01, 4.18700000e+03, 1.26519600e+02, 9.55000000e+01], [3.49867900e+06, 2.61000000e+00, 9.00000000e-01, 2.22379634e+00, 2.62695900e+01, 1.40330000e+04, 1.22682900e+02, 2.10000000e+01], [6.54026700e+06, 4.07000000e+00, 9.00000000e-01, 5.30746337e-01, 2.50150600e+01, 1.98200000e+03, 1.20052400e+02, 6.97000000e+01], [6.04713100e+06, 3.06000000e+00, 3.00000000e-01, 6.98581746e-01, 2.55422300e+01, 6.68400000e+03, 1.23615000e+02, 2.57000000e+01], [2.86420480e+07, 2.58000000e+00, 4.00000000e-01, 1.45013444e+00, 2.47704100e+01, 9.24900000e+03, 1.19636800e+02, 2.32000000e+01], [9.02971150e+07, 3.26000000e+00, 6.00000000e-02, 8.42120697e-01, 2.28726300e+01, 5.33200000e+03, 1.22345900e+02, 3.34000000e+01], [3.85257520e+07, 1.33000000e+00, 1.00000000e-01, 8.27076715e+00, 2.66738000e+01, 1.99960000e+04, 1.29676500e+02, 6.70000000e+00], [1.05774580e+07, 1.36000000e+00, 5.00000000e-01, 5.48692640e+00, 2.66844500e+01, 2.77470000e+04, 1.27263100e+02, 4.10000000e+00], [1.38896200e+06, 2.20000000e+00, 6.00000000e-02, 4.87020615e+01, 2.81313800e+01, 1.26076000e+05, 1.26315300e+02, 9.50000000e+00], [2.07416690e+07, 1.34000000e+00, 1.00000000e-01, 4.38344907e+00, 2.54106900e+01, 1.80320000e+04, 1.28755300e+02, 1.61000000e+01], [1.43123163e+08, 1.49000000e+00, 1.00000000e+00, 1.19827176e+01, 2.60113100e+01, 2.25060000e+04, 1.28490300e+02, 1.35000000e+01], [9.75031400e+06, 5.06000000e+00, 2.90000000e+00, 5.42444698e-02, 2.25545300e+01, 1.17300000e+03, 1.35100500e+02, 7.83000000e+01], [1.22297030e+07, 5.11000000e+00, 8.00000000e-01, 4.61633711e-01, 2.19274300e+01, 2.16200000e+03, 1.30279500e+02, 7.58000000e+01], [9.10953500e+06, 1.41000000e+00, 1.00000000e-01, 5.27122268e+00, 2.65149500e+01, 1.25220000e+04, 1.30375500e+02, 8.00000000e+00], [5.52183800e+06, 5.13000000e+00, 1.60000000e+00, 1.18255775e-01, 2.25313900e+01, 1.28900000e+03, 1.34716000e+02, 1.79100000e+02], [4.84964100e+06, 1.28000000e+00, 1.00000000e-01, 4.11444075e+00, 2.38399600e+01, 6.59910000e+04, 1.21173600e+02, 2.80000000e+00], [5.39671000e+06, 1.31000000e+00, 6.00000000e-02, 6.90165446e+00, 2.69271700e+01, 2.46700000e+04, 1.29528000e+02, 8.80000000e+00], [2.03059900e+06, 1.43000000e+00, 6.00000000e-02, 8.51182820e+00, 2.74398300e+01, 3.08160000e+04, 1.29923100e+02, 3.70000000e+00], [9.13258900e+06, 7.06000000e+00, 6.00000000e-01, 6.82188892e-02, 2.19691700e+01, 6.15000000e+02, 1.31531800e+02, 1.68500000e+02], [5.03488110e+07, 2.54000000e+00, 1.79000000e+01, 9.42796037e+00, 2.68553800e+01, 1.22630000e+04, 1.30994900e+02, 6.61000000e+01], [4.58170160e+07, 1.42000000e+00, 4.00000000e-01, 7.29308876e+00, 2.74997500e+01, 3.46760000e+04, 1.22045300e+02, 5.00000000e+00], [1.99495530e+07, 2.32000000e+00, 6.00000000e-02, 5.80791088e-01, 2.19667100e+01, 6.90700000e+03, 1.24861500e+02, 1.17000000e+01], [3.44701380e+07, 4.79000000e+00, 1.00000000e+00, 3.82117945e-01, 2.24048400e+01, 3.24600000e+03, 1.29719900e+02, 8.47000000e+01], [5.06657000e+05, 2.41000000e+00, 1.00000000e+00, 4.74113997e+00, 2.54988700e+01, 1.34700000e+04, 1.24635800e+02, 2.64000000e+01], [1.15375000e+06, 3.70000000e+00, 2.59000000e+01, 9.49860795e-01, 2.31696900e+01, 5.88700000e+03, 1.31879300e+02, 1.12200000e+02], [9.22633300e+06, 1.92000000e+00, 1.00000000e-01, 5.31568840e+00, 2.63762900e+01, 4.34210000e+04, 1.22947300e+02, 3.20000000e+00], [7.64654200e+06, 1.47000000e+00, 4.00000000e-01, 5.33305762e+00, 2.62019500e+01, 5.50200000e+04, 1.19646500e+02, 4.70000000e+00], [7.25407200e+06, 3.70000000e+00, 2.00000000e-01, 4.53167660e-01, 2.37796600e+01, 2.00100000e+03, 1.29965700e+02, 5.62000000e+01], [4.28447440e+07, 5.54000000e+00, 5.80000000e+00, 1.54672995e-01, 2.24779200e+01, 2.03000000e+03, 1.30832800e+02, 7.24000000e+01], [6.64532550e+07, 1.48000000e+00, 1.30000000e+00, 3.83510189e+00, 2.30080300e+01, 1.22160000e+04, 1.20496900e+02, 1.56000000e+01], [6.05293700e+06, 4.88000000e+00, 3.20000000e+00, 2.51983337e-01, 2.18787500e+01, 1.21900000e+03, 1.31024800e+02, 9.64000000e+01], [1.31537200e+06, 1.80000000e+00, 1.50000000e+00, 3.19577173e+01, 2.63966900e+01, 3.08750000e+04, 1.24993900e+02, 2.49000000e+01], [1.04080910e+07, 2.04000000e+00, 6.00000000e-02, 2.44066948e+00, 2.51569900e+01, 9.93800000e+03, 1.28629100e+02, 1.94000000e+01], [7.03443570e+07, 2.15000000e+00, 6.00000000e-02, 4.02190259e+00, 2.67037100e+01, 1.64540000e+04, 1.24067500e+02, 2.22000000e+01], [3.10144270e+07, 6.34000000e+00, 6.40000000e+00, 1.00852839e-01, 2.23583300e+01, 1.43700000e+03, 1.34520400e+02, 8.93000000e+01], [4.60284760e+07, 1.38000000e+00, 1.10000000e+00, 7.03235908e+00, 2.54237900e+01, 8.76200000e+03, 1.31496200e+02, 1.29000000e+01], [6.16896200e+07, 1.87000000e+00, 2.00000000e-01, 8.52646682e+00, 2.73924900e+01, 3.77390000e+04, 1.24084500e+02, 5.60000000e+00], [3.04473143e+08, 2.07000000e+00, 6.00000000e-01, 1.85459917e+01, 2.84569800e+01, 5.03840000e+04, 1.18477700e+02, 7.70000000e+00], [3.35083200e+06, 2.11000000e+00, 5.00000000e-01, 2.48976355e+00, 2.63912300e+01, 1.53170000e+04, 1.24260400e+02, 1.30000000e+01], [2.69527190e+07, 2.46000000e+00, 1.00000000e-01, 4.47666902e+00, 2.53205400e+01, 3.73300000e+03, 1.24346200e+02, 4.92000000e+01], [8.65893420e+07, 1.86000000e+00, 4.00000000e-01, 1.47934658e+00, 2.09163000e+01, 4.08500000e+03, 1.21936700e+02, 2.62000000e+01], [1.31145790e+07, 5.88000000e+00, 1.36000000e+01, 1.48981514e-01, 2.06832100e+01, 3.03900000e+03, 1.32449300e+02, 9.49000000e+01], [1.34954620e+07, 3.85000000e+00, 1.51000000e+01, 6.54323190e-01, 2.20266000e+01, 1.28600000e+03, 1.31974500e+02, 9.83000000e+01]]) . X_gapminder = pd.read_csv(&quot;datasets/X_gapminder.csv&quot;).values X_gapminder[:5] . array([[3.48110590e+07, 2.73000000e+00, 1.00000000e-01, 3.32894466e+00, 2.45962000e+01, 1.23140000e+04, 1.29904900e+02, 2.95000000e+01], [1.98422510e+07, 6.43000000e+00, 2.00000000e+00, 1.47435339e+00, 2.22508300e+01, 7.10300000e+03, 1.30124700e+02, 1.92000000e+02], [4.03818600e+07, 2.24000000e+00, 5.00000000e-01, 4.78516998e+00, 2.75017000e+01, 1.46460000e+04, 1.18891500e+02, 1.54000000e+01], [2.97502900e+06, 1.40000000e+00, 1.00000000e-01, 1.80410622e+00, 2.53554200e+01, 7.38300000e+03, 1.32810800e+02, 2.00000000e+01], [2.13703480e+07, 1.96000000e+00, 1.00000000e-01, 1.80163133e+01, 2.75637300e+01, 4.13120000e+04, 1.17375500e+02, 5.20000000e+00]]) . y_gapminder = np.array([75.3, 58.3, 75.5, 72.5, 81.5, 80.4, 70.6, 72.2, 68.4, 75.3, 70.1, 79.4, 70.7, 63.2, 67.6, 70.9, 61.2, 73.9, 73.2, 59.4, 57.4, 66.2, 56.6, 80.7, 54.8, 78.9, 75.1, 62.6, 58.6, 79.7, 55.9, 76.5, 77.8, 78.7, 61. , 74. , 70.1, 74.1, 56.7, 60.4, 74. , 65.7, 79.4, 81. , 57.5, 62.2, 72.1, 80. , 62.7, 79.5, 70.8, 58.3, 51.3, 63. , 61.7, 70.9, 73.8, 82. , 64.4, 69.5, 76.9, 79.4, 80.9, 81.4, 75.5, 82.6, 66.1, 61.5, 72.3, 77.6, 45.2, 61. , 72. , 80.7, 63.4, 51.4, 74.5, 78.2, 55.8, 81.4, 63.6, 72.1, 75.7, 69.6, 63.2, 73.3, 55. , 60.8, 68.6, 80.3, 80.2, 75.2, 59.7, 58. , 80.7, 74.6, 64.1, 77.1, 58.2, 73.6, 76.8, 69.4, 75.3, 79.2, 80.4, 73.4, 67.6, 62.2, 64.3, 76.4, 55.9, 80.9, 74.8, 78.5, 56.7, 55. , 81.1, 74.3, 67.4, 69.1, 46.1, 81.1, 81.9, 69.5, 59.7, 74.1, 60. , 71.3, 76.5, 75.1, 57.2, 68.2, 79.5, 78.2, 76. , 68.7, 75.4, 52. , 49. ]) . y_gapminder = pd.read_csv(&quot;datasets/y_gapminder.csv&quot;).values y_gapminder[:5] . array([[75.3], [58.3], [75.5], [72.5], [81.5]]) . # Setup the pipeline steps: steps steps_gapminder = [(&#39;imputation&#39;, SimpleImputer(missing_values=np.nan, strategy=&quot;mean&quot;)), (&quot;scaler&quot;, StandardScaler()), (&quot;elasticnet&quot;, ElasticNet())] # Create the pipeline: pipeline pipeline_gapminder = Pipeline(steps_gapminder) # Specify the hyperparameter space parameters_gapminder = {&quot;elasticnet__l1_ratio&quot;:np.linspace(0,1,30)} # Create train and test sets X_train_gapminder, X_test_gapminder, y_train_gapminder, y_test_gapminder = train_test_split(X_gapminder, y_gapminder, test_size=.4, random_state=42) # Create the GridSearchCV object: gm_cv gm_cv_gapminder = GridSearchCV(pipeline_gapminder, param_grid=parameters_gapminder, cv=3) # Fit to the training set gm_cv_gapminder.fit(X_train_gapminder, y_train_gapminder) # Compute and print the metrics r2_gapminder = gm_cv_gapminder.score(X_test_gapminder, y_test_gapminder) print(&quot;Tuned ElasticNet Alpha: {}&quot;.format(gm_cv_gapminder.best_params_)) print(&quot;Tuned ElasticNet R squared: {}&quot;.format(r2_gapminder)) . Tuned ElasticNet Alpha: {&#39;elasticnet__l1_ratio&#39;: 1.0} Tuned ElasticNet R squared: 0.8862016570888216 .",
            "url": "https://victoromondi1997.github.io/blog/machine-learning/supervised-learning/scikit-learn/2020/07/10/Supervised-Learning-with-scikit-learn.html",
            "relUrl": "/machine-learning/supervised-learning/scikit-learn/2020/07/10/Supervised-Learning-with-scikit-learn.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Statistical Thinking in Python (Part 2)",
            "content": "Libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() %matplotlib inline . Parameter estimation by optimization . When doing statistical inference, we speak the language of probability. A probability distribution that describes your data has parameters. So, a major goal of statistical inference is to estimate the values of these parameters, which allows us to concisely and unambiguously describe our data and draw conclusions from it. We will find the optimal parameters, those that best describe your data. . Optimal parameters . Optimal parameters . Parameter values that bring the model in closest agreement with the data | . Packages to do statistical inference . package . | scipy.stats | | | statsmodels | . | hacker stats with numpy | . How often do we get no-hitters? . The number of games played between each no-hitter in the modern era ($1901-2015$) of Major League Baseball is stored in the array nohitter_times. . nohitter_times = np.array([ 843, 1613, 1101, 215, 684, 814, 278, 324, 161, 219, 545, 715, 966, 624, 29, 450, 107, 20, 91, 1325, 124, 1468, 104, 1309, 429, 62, 1878, 1104, 123, 251, 93, 188, 983, 166, 96, 702, 23, 524, 26, 299, 59, 39, 12, 2, 308, 1114, 813, 887, 645, 2088, 42, 2090, 11, 886, 1665, 1084, 2900, 2432, 750, 4021, 1070, 1765, 1322, 26, 548, 1525, 77, 2181, 2752, 127, 2147, 211, 41, 1575, 151, 479, 697, 557, 2267, 542, 392, 73, 603, 233, 255, 528, 397, 1529, 1023, 1194, 462, 583, 37, 943, 996, 480, 1497, 717, 224, 219, 1531, 498, 44, 288, 267, 600, 52, 269, 1086, 386, 176, 2199, 216, 54, 675, 1243, 463, 650, 171, 327, 110, 774, 509, 8, 197, 136, 12, 1124, 64, 380, 811, 232, 192, 731, 715, 226, 605, 539, 1491, 323, 240, 179, 702, 156, 82, 1397, 354, 778, 603, 1001, 385, 986, 203, 149, 576, 445, 180, 1403, 252, 675, 1351, 2983, 1568, 45, 899, 3260, 1025, 31, 100, 2055, 4043, 79, 238, 3931, 2351, 595, 110, 215, 0, 563, 206, 660, 242, 577, 179, 157, 192, 192, 1848, 792, 1693, 55, 388, 225, 1134, 1172, 1555, 31, 1582, 1044, 378, 1687, 2915, 280, 765, 2819, 511, 1521, 745, 2491, 580, 2072, 6450, 578, 745, 1075, 1103, 1549, 1520, 138, 1202, 296, 277, 351, 391, 950, 459, 62, 1056, 1128, 139, 420, 87, 71, 814, 603, 1349, 162, 1027, 783, 326, 101, 876, 381, 905, 156, 419, 239, 119, 129, 467]) . If we assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As we have seen, the Exponential distribution has a single parameter, which we will call τ, the typical interval time. The value of the parameter τ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. . We will compute the value of this parameter from the data. Then, use np.random.exponential() to &quot;repeat&quot; the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the τ we found and plot the histogram as an approximation to the PDF. . # Seed random number generator np.random.seed(42) # Compute mean no-hitter time: tau tau = np.mean(nohitter_times) # Draw out of an exponential distribution with parameter tau: inter_nohitter_time inter_nohitter_time = np.random.exponential(tau, 100000) # Plot the PDF and label axes _ = plt.hist(inter_nohitter_time, bins=50, density=True, histtype=&quot;step&quot;) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . Note: We see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right. . Do the data follow our story? . We have modeled no-hitters using an Exponential distribution. Let&#39;s create an ECDF of the real data. Overlay the theoretical CDF with the ECDF from the data. This helps us to verify that the Exponential distribution describes the observed data. . def ecdf(data): return np.sort(data), np.arange(1, len(data)+1) / len(data) . # Create an ECDF from real data: x, y x, y = ecdf(nohitter_times) # Create a CDF from theoretical samples: x_theor, y_theor x_theor, y_theor = ecdf(inter_nohitter_time) # Overlay the plots plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Margins and axis labels plt.margins(.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was. . How is this parameter optimal? . We will now sample out of an exponential distribution with $ tau$ being twice as large as the optimal $ tau$. Do it again for $ tau$ half as large. Make CDFs of these samples and overlay them with our data. . # Plot the theoretical CDFs plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Take samples with half tau: samples_half samples_half = np.random.exponential(tau/2, size=10000) # Take samples with double tau: samples_double samples_double = np.random.exponential(tau*2, size=10000) # Generate CDFs from these samples x_half, y_half = ecdf(samples_half) x_double, y_double = ecdf(samples_double) # Plot these CDFs as lines _ = plt.plot(x_half, y_half) _ = plt.plot(x_double, y_double) # Show the plot plt.show() . . Note: Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter. We can see that they do not reproduce the data as well. Thus, the $ tau$ we computed from the mean inter-no-hitter times is optimal in that it best reproduces the data. . Linear regression by least squares . Least squares . The process of nding the parameters for which the sum ofthe squares ofthe residuals is minimal | . EDA of literacy/fertility data . we will look at the correlation between female literacy and fertility (defined as the average number of children born per woman) throughout the world. For ease of analysis and interpretation, we will work with the illiteracy rate. . illiteracy= np.array([ 9.5, 49.2, 1. , 11.2, 9.8, 60. , 50.2, 51.2, 0.6, 1. , 8.5, 6.1, 9.8, 1. , 42.2, 77.2, 18.7, 22.8, 8.5, 43.9, 1. , 1. , 1.5, 10.8, 11.9, 3.4, 0.4, 3.1, 6.6, 33.7, 40.4, 2.3, 17.2, 0.7, 36.1, 1. , 33.2, 55.9, 30.8, 87.4, 15.4, 54.6, 5.1, 1.1, 10.2, 19.8, 0. , 40.7, 57.2, 59.9, 3.1, 55.7, 22.8, 10.9, 34.7, 32.2, 43. , 1.3, 1. , 0.5, 78.4, 34.2, 84.9, 29.1, 31.3, 18.3, 81.8, 39. , 11.2, 67. , 4.1, 0.2, 78.1, 1. , 7.1, 1. , 29. , 1.1, 11.7, 73.6, 33.9, 14. , 0.3, 1. , 0.8, 71.9, 40.1, 1. , 2.1, 3.8, 16.5, 4.1, 0.5, 44.4, 46.3, 18.7, 6.5, 36.8, 18.6, 11.1, 22.1, 71.1, 1. , 0. , 0.9, 0.7, 45.5, 8.4, 0. , 3.8, 8.5, 2. , 1. , 58.9, 0.3, 1. , 14. , 47. , 4.1, 2.2, 7.2, 0.3, 1.5, 50.5, 1.3, 0.6, 19.1, 6.9, 9.2, 2.2, 0.2, 12.3, 4.9, 4.6, 0.3, 16.5, 65.7, 63.5, 16.8, 0.2, 1.8, 9.6, 15.2, 14.4, 3.3, 10.6, 61.3, 10.9, 32.2, 9.3, 11.6, 20.7, 6.5, 6.7, 3.5, 1. , 1.6, 20.5, 1.5, 16.7, 2. , 0.9]) . fertility = np.array([1.769, 2.682, 2.077, 2.132, 1.827, 3.872, 2.288, 5.173, 1.393, 1.262, 2.156, 3.026, 2.033, 1.324, 2.816, 5.211, 2.1 , 1.781, 1.822, 5.908, 1.881, 1.852, 1.39 , 2.281, 2.505, 1.224, 1.361, 1.468, 2.404, 5.52 , 4.058, 2.223, 4.859, 1.267, 2.342, 1.579, 6.254, 2.334, 3.961, 6.505, 2.53 , 2.823, 2.498, 2.248, 2.508, 3.04 , 1.854, 4.22 , 5.1 , 4.967, 1.325, 4.514, 3.173, 2.308, 4.62 , 4.541, 5.637, 1.926, 1.747, 2.294, 5.841, 5.455, 7.069, 2.859, 4.018, 2.513, 5.405, 5.737, 3.363, 4.89 , 1.385, 1.505, 6.081, 1.784, 1.378, 1.45 , 1.841, 1.37 , 2.612, 5.329, 5.33 , 3.371, 1.281, 1.871, 2.153, 5.378, 4.45 , 1.46 , 1.436, 1.612, 3.19 , 2.752, 3.35 , 4.01 , 4.166, 2.642, 2.977, 3.415, 2.295, 3.019, 2.683, 5.165, 1.849, 1.836, 2.518, 2.43 , 4.528, 1.263, 1.885, 1.943, 1.899, 1.442, 1.953, 4.697, 1.582, 2.025, 1.841, 5.011, 1.212, 1.502, 2.516, 1.367, 2.089, 4.388, 1.854, 1.748, 2.978, 2.152, 2.362, 1.988, 1.426, 3.29 , 3.264, 1.436, 1.393, 2.822, 4.969, 5.659, 3.24 , 1.693, 1.647, 2.36 , 1.792, 3.45 , 1.516, 2.233, 2.563, 5.283, 3.885, 0.966, 2.373, 2.663, 1.251, 2.052, 3.371, 2.093, 2. , 3.883, 3.852, 3.718, 1.732, 3.928]) . It is always a good idea to do some EDA ahead of our analysis. To this end, we will plot the fertility versus illiteracy and compute the Pearson correlation coefficient. . Note: The Numpy array illiteracy has the illiteracy rate among females for most of the world&#8217;s nations. . Note: The array fertility has the corresponding fertility data. . def pearson_r(x, y): return np.corrcoef(x, y)[0,1] . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins and label axes plt.margins(.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Show the plot plt.show() # Show the Pearson correlation coefficient pearson_r(illiteracy, fertility) . 0.8041324026815346 . We can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman. . Linear regression . We will assume that fertility is a linear function of the female illiteracy rate. That is, $f=ai+b$, where $a$ is the slope and $b$ is the intercept. . Note: We can think of the intercept as the minimal fertility rate, probably somewhere between one and two. . The slope tells us how the fertility rate varies with illiteracy. We can find the best fit line using np.polyfit(). . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Perform a linear regression using np.polyfit(): a, b a, b = np.polyfit(illiteracy, fertility, deg=1) # Print the results to the screen print(&#39;slope =&#39;, a, &#39;children per woman / percent illiterate&#39;) print(&#39;intercept =&#39;, b, &#39;children per woman&#39;) # Make theoretical line to plot x = np.array([0, 100]) y = a * x + b # Add regression line to your plot _ = plt.plot(x, y) # Draw the plot plt.show() . slope = 0.04979854809063423 children per woman / percent illiterate intercept = 1.8880506106365562 children per woman . How is it optimal? . The function np.polyfit() that we used to get your regression parameters finds the optimal slope and intercept. It is optimizing the sum of the squares of the residuals, also known as RSS ( for residual sum of squares ). . We will plot the function that is being optimized, the RSS, versus the slope parameter $a$. To do this, we will fix the intercept to be what we found in the optimization. Then, plot the RSS vs. the slope. . # Specify slopes to consider: a_vals a_vals = np.linspace(0,0.1, 200) # Initialize sum of square of residuals: rss rss = np.empty_like(a_vals) # Compute sum of square of residuals for each value of a_vals for i, a in enumerate(a_vals): rss[i] = np.sum((fertility - a*illiteracy - b)**2) # Plot the RSS plt.plot(a_vals, rss, &#39;-&#39;) plt.xlabel(&#39;slope (children per woman / percent illiterate)&#39;) plt.ylabel(&#39;sum of square of residuals&#39;) plt.show() . . Note: that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression. . The importance of EDA: Anscombe&#39;s quartet . Look before you leap! . Do graphical EDA rst | . Linear regression on appropriate Anscombe data . We will perform a linear regression on the data set from Anscombe&#39;s quartet that is most reasonably interpreted with linear regression. . x = np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]) y = np.array([ 8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]) . # Perform linear regression: a, b a, b = np.polyfit(x,y, deg=1) # Print the slope and intercept print(a, b) # Generate theoretical x and y data: x_theor, y_theor x_theor = np.array([3, 15]) y_theor = a * x_theor + b # Plot the Anscombe data and theoretical line _ = plt.plot(x, y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_theor, y_theor) # Label the axes plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) # Show the plot plt.show() . 0.5000909090909091 3.000090909090908 . Linear regression on all Anscombe data . Now, to verify that all four of the Anscombe data sets have the same slope and intercept from a linear regression, we will compute the slope and intercept for each set. . anscombe_x = [np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]), np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]), np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]), np.array([ 8., 8., 8., 8., 8., 8., 8., 19., 8., 8., 8.])] . anscombe_y = [np.array([ 8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]), np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.1 , 6.13, 3.1 , 9.13, 7.26, 4.74]), np.array([ 7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]), np.array([ 6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5 , 5.56, 7.91, 6.89])] . The data are stored in lists; . # Iterate through x,y pairs for x, y in zip(anscombe_x, anscombe_y): # Compute the slope and intercept: a, b a, b = np.polyfit(x,y, deg=1) # Print the result print(&#39;slope:&#39;, a, &#39;intercept:&#39;, b) . slope: 0.5000909090909091 intercept: 3.000090909090908 slope: 0.5 intercept: 3.0009090909090905 slope: 0.4997272727272729 intercept: 3.002454545454545 slope: 0.4999090909090908 intercept: 3.001727272727274 . . Important: Indeed, they all have the same slope and intercept. . Bootstrap confidence intervals . To &quot;pull yourself up by your bootstraps&quot; is a classic idiom meaning that you achieve a difficult task by yourself with no help at all. In statistical inference, we want to know what would happen if we could repeat our data acquisition an infinite number of times. This task is impossible, but can we use only the data we actually have to get close to the same result as an infinitude of experiments? The answer is yes! The technique to do it is aptly called bootstrapping. . Generating bootstrap replicates . Bootstrapping . The use of resampled data to perform statistical inference | . Bootstrap sample . A resampled array ofthe data | . Bootstrap replicate . A statistic computed from a resampled array | . Getting the terminology down . If we have a data set with $n$ repeated measurements, a bootstrap sample is an array of length $n$ that was drawn from the original data with replacement. bootstrap replicate is A single value of a statistic computed from a bootstrap sample. . Visualizing bootstrap samples . We will generate bootstrap samples from the set of annual rainfall data measured at the Sheffield Weather Station in the UK from 1883 to 2015. . rainfall = np.array([ 875.5, 648.2, 788.1, 940.3, 491.1, 743.5, 730.1, 686.5, 878.8, 865.6, 654.9, 831.5, 798.1, 681.8, 743.8, 689.1, 752.1, 837.2, 710.6, 749.2, 967.1, 701.2, 619. , 747.6, 803.4, 645.6, 804.1, 787.4, 646.8, 997.1, 774. , 734.5, 835. , 840.7, 659.6, 828.3, 909.7, 856.9, 578.3, 904.2, 883.9, 740.1, 773.9, 741.4, 866.8, 871.1, 712.5, 919.2, 927.9, 809.4, 633.8, 626.8, 871.3, 774.3, 898.8, 789.6, 936.3, 765.4, 882.1, 681.1, 661.3, 847.9, 683.9, 985.7, 771.1, 736.6, 713.2, 774.5, 937.7, 694.5, 598.2, 983.8, 700.2, 901.3, 733.5, 964.4, 609.3, 1035.2, 718. , 688.6, 736.8, 643.3, 1038.5, 969. , 802.7, 876.6, 944.7, 786.6, 770.4, 808.6, 761.3, 774.2, 559.3, 674.2, 883.6, 823.9, 960.4, 877.8, 940.6, 831.8, 906.2, 866.5, 674.1, 998.1, 789.3, 915. , 737.1, 763. , 666.7, 824.5, 913.8, 905.1, 667.8, 747.4, 784.7, 925.4, 880.2, 1086.9, 764.4, 1050.1, 595.2, 855.2, 726.9, 785.2, 948.8, 970.6, 896. , 618.4, 572.4, 1146.4, 728.2, 864.2, 793. ]) . The data are stored in the NumPy array rainfall in units of millimeters (mm). By graphically displaying the bootstrap samples with an ECDF, we can get a feel for how bootstrap sampling allows probabilistic descriptions of data. . for _ in range(50): # Generate bootstrap sample: bs_sample bs_sample = np.random.choice(rainfall, size=len(rainfall)) # Compute and plot ECDF from bootstrap sample x, y = ecdf(bs_sample) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;gray&#39;, alpha=0.1) # Compute and plot ECDF from original data x, y = ecdf(rainfall) _ = plt.plot(x, y, marker=&#39;.&#39;) # Make margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;yearly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Show the plot plt.show() . . Note: The bootstrap samples give an idea of how the distribution of rainfalls is spread. . Bootstrap confidence intervals . Condence interval of a statistic . If we repeated measurements over and over again, $p %$ of the observed values would lie within the $p %$ condence interval. | . Generating many bootstrap replicates . def bootstrap_replicate_1d(data, func): &quot;&quot;&quot;Generate bootstrap replicate of 1D data.&quot;&quot;&quot; return func(np.random.choice(data, size=len(data))) . We&#39;ll write another function, draw_bs_reps(data, func, size=1), which generates many bootstrap replicates from the data set. . def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates . Bootstrap replicates of the mean and the SEM . We will compute a bootstrap estimate of the probability density function of the mean annual rainfall at the Sheffield Weather Station. . Note: we are estimating the mean annual rainfall we would get if the Sheffield Weather Station could repeat all of the measurements from 1883 to 2015 over and over again. This is a probabilistic estimate of the mean. . We will plot the PDF as a histogram, and you will see that it is Normal. . The standard deviation of this distribution, called the standard error of the mean, or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)). . # Take 10,000 bootstrap replicates of the mean: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000) # Compute and print SEM sem = np.std(rainfall) / np.sqrt(len(rainfall)) print(sem) # Compute and print standard deviation of bootstrap replicates bs_std = np.std(bs_replicates) print(bs_std) # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, density=True) _ = plt.xlabel(&#39;mean annual rainfall (mm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . 10.510549150506188 10.358764199574097 . . Note: SEM we got from the known expression and the bootstrap replicates is the same and the distribution of the bootstrap replicates of the mean is Normal. . Confidence intervals of rainfall data . A confidence interval gives upper and lower bounds on the range of parameter values you might expect to get if we repeat our measurements. For named distributions, we can compute them analytically or look them up, but one of the many beautiful properties of the bootstrap method is that we can take percentiles of your bootstrap replicates to get your confidence interval. Conveniently, we can use the np.percentile() function. . np.percentile(bs_replicates, [2.5, 97.5]) . array([779.96900376, 820.62793233]) . it&#39;s simple to get confidence intervals using bootstrap! . Bootstrap replicates of other statistics . We&#39;ll generate bootstrap replicates for the variance of the annual rainfall at the Sheffield Weather Station and plot the histogram of the replicates. . # Generate 10,000 bootstrap replicates of the variance: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.var, size=10000) # Put the variance in units of square centimeters bs_replicates = bs_replicates/100 # Make a histogram of the results _ = plt.hist(bs_replicates, density=True, bins=50) _ = plt.xlabel(&#39;variance of annual rainfall (sq. cm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . Note: This is not normally distributed, as it has a longer tail to the right. . Confidence interval on the rate of no-hitters . # Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000) # Compute the 95% confidence interval: conf_int conf_int = np.percentile(bs_replicates, [2.5, 97.5]) # Print the confidence interval print(&#39;95% confidence interval =&#39;, conf_int, &#39;games&#39;) # Plot the histogram of the replicates _ = plt.hist(bs_replicates, bins=50, density=True) _ = plt.xlabel(r&#39;$ tau$ (games)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . 95% confidence interval = [663.65229084 869.79741036] games . . Note: This gives us an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games. . Pairs bootstrap . Nonparametric inference . Make no assumptions about the model or probability distribution underlying the data | . Pairs bootstrap for linear regression . Resample data in pairs | Compute slope and intercept from resampled data | Each slope and intercept is a bootstrap replicate | Compute condence intervals from percentiles of bootstrap replicates | . A function to do pairs bootstrap . pairs bootstrap involves resampling pairs of data. Each collection of pairs fit with a line, in this case using np.polyfit(). We do this again and again, getting bootstrap replicates of the parameter values. To have a useful tool for doing pairs bootstrap, we will write a function to perform pairs bootstrap on a set of x,y data. . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps . Pairs bootstrap of literacy/fertility data . Using the function we just wrote, we&#39;ll perform pairs bootstrap to plot a histogram describing the estimate of the slope from the illiteracy/fertility data. Also reporting the 95% confidence interval of the slope. . # Generate replicates of slope and intercept using pairs bootstrap bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000) # Compute and print 95% CI for slope print(np.percentile(bs_slope_reps, [2.5, 97.5])) # Plot the histogram _ = plt.hist(bs_slope_reps, bins=50, density=True) _ = plt.xlabel(&#39;slope&#39;) _ = plt.ylabel(&#39;PDF&#39;) plt.show() . [0.04389859 0.05528877] . Plotting bootstrap regressions . A nice way to visualize the variability we might expect in a linear regression is to plot the line we would get from each bootstrap replicate of the slope and intercept. We&#39;ll do this for the first 100 of our bootstrap replicates of the slope and intercept . # Generate array of x-values for bootstrap lines: x x = np.array([0,100]) # Plot the bootstrap lines for i in range(100): _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Plot the data _ = plt.plot(illiteracy, fertility, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Label axes, set the margins, and show the plot _ = plt.xlabel(&#39;illiteracy&#39;) _ = plt.ylabel(&#39;fertility&#39;) plt.margins(0.02) plt.show() . Introduction to hypothesis testing . how reasonable is it to observe our data if a model is true? This question is addressed by hypothesis tests. They are the icing on the inference cake. We carefully construct and test hypotheses using hacker statistics. . Formulating and simulating a hypothesis . Hypothesis testing . Assessment of how reasonable the observed data are assuming a hypothesis is true | . Null hypothesis . Another name for the hypothesis you are testing | . Permutation . Random reordering of entries in an array | . Generating a permutation sample . permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so we will write a function to generate a permutation sample from two data sets. . . Note: a permutation sample of two arrays having respectively n1 and n2 entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first n1 entries as the permutation sample of the first array and the last n2 entries as the permutation sample of the second array. . def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1,data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 . Visualizing permutation sampling . To help see how permutation sampling works, we will generate permutation samples and look at them graphically. . We will use the Sheffield Weather Station data again, this time considering the monthly rainfall in June (a dry month) and November (a wet month). We expect these might be differently distributed, so we will take permutation samples to see how their ECDFs would look if they were identically distributed. . rain_june = np.array([ 66.2, 39.7, 76.4, 26.5, 11.2, 61.8, 6.1, 48.4, 89.2, 104. , 34. , 60.6, 57.1, 79.1, 90.9, 32.3, 63.8, 78.2, 27.5, 43.4, 30.1, 17.3, 77.5, 44.9, 92.2, 39.6, 79.4, 66.1, 53.5, 98.5, 20.8, 55.5, 39.6, 56. , 65.1, 14.8, 13.2, 88.1, 8.4, 32.1, 19.6, 40.4, 2.2, 77.5, 105.4, 77.2, 38. , 27.1, 111.8, 17.2, 26.7, 23.3, 77.2, 87.2, 27.7, 50.6, 60.3, 15.1, 6. , 29.4, 39.3, 56.3, 80.4, 85.3, 68.4, 72.5, 13.3, 28.4, 14.7, 37.4, 49.5, 57.2, 85.9, 82.1, 31.8, 126.6, 30.7, 41.4, 33.9, 13.5, 99.1, 70.2, 91.8, 61.3, 13.7, 54.9, 62.5, 24.2, 69.4, 83.1, 44. , 48.5, 11.9, 16.6, 66.4, 90. , 34.9, 132.8, 33.4, 225. , 7.6, 40.9, 76.5, 48. , 140. , 55.9, 54.1, 46.4, 68.6, 52.2, 108.3, 14.6, 11.3, 29.8, 130.9, 152.4, 61. , 46.6, 43.9, 30.9, 111.1, 68.5, 42.2, 9.8, 285.6, 56.7, 168.2, 41.2, 47.8, 166.6, 37.8, 45.4, 43.2]) . rain_november = np.array([ 83.6, 30.9, 62.2, 37. , 41. , 160.2, 18.2, 122.4, 71.3, 44.2, 49.1, 37.6, 114.5, 28.8, 82.5, 71.9, 50.7, 67.7, 112. , 63.6, 42.8, 57.2, 99.1, 86.4, 84.4, 38.1, 17.7, 102.2, 101.3, 58. , 82. , 101.4, 81.4, 100.1, 54.6, 39.6, 57.5, 29.2, 48.8, 37.3, 115.4, 55.6, 62. , 95. , 84.2, 118.1, 153.2, 83.4, 104.7, 59. , 46.4, 50. , 147.6, 76.8, 59.9, 101.8, 136.6, 173. , 92.5, 37. , 59.8, 142.1, 9.9, 158.2, 72.6, 28. , 112.9, 119.3, 199.2, 50.7, 44. , 170.7, 67.2, 21.4, 61.3, 15.6, 106. , 116.2, 42.3, 38.5, 132.5, 40.8, 147.5, 93.9, 71.4, 87.3, 163.7, 141.4, 62.6, 84.9, 28.8, 121.1, 28.6, 32.4, 112. , 50. , 96.9, 81.8, 70.4, 117.5, 41.2, 124.9, 78.2, 93. , 53.5, 50.5, 42.6, 47.9, 73.1, 129.1, 56.9, 103.3, 60.5, 134.3, 93.1, 49.5, 48.2, 167.9, 27. , 111.1, 55.4, 36.2, 57.4, 66.8, 58.3, 60. , 161.6, 112.7, 37.4, 110.6, 56.6, 95.8, 126.8]) . for _ in range(50): # Generate permutation samples perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november) # Compute ECDFs x_1, y_1 = ecdf(perm_sample_1) x_2, y_2 = ecdf(perm_sample_2) # Plot ECDFs of permutation sample _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.02) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.02) # Create and plot ECDFs from original data x_1, y_1 = ecdf(rain_june) x_2, y_2 = ecdf(rain_november) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;monthly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . Note: The permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed. . Test statistics and p-values . Hypothesis testing . Assessment of how reasonable the observed data are assuming a hypothesis is true | . Test statistic . A single number that can be computed from observed data and from data you simulate under the null hypothesis | It serves as a basis of comparison between the two | . p-value . The probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true | NOT the probability that the null hypothesis is true | . Statistical signicance . Determined by the smallness of a p-value | . Null hypothesis signicance testing (NHST) . Another name for Hypothesis testing | . Test statistics . When performing hypothesis tests, the choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test. . Important: The most important thing to consider is: What are you asking? . p-value . The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true. . Generating permutation replicates . a permutation replicate is a single value of a statistic computed from a permutation sample. . As the draw_bs_reps() function is useful for generating bootstrap replicates, it is useful to have a similar function, draw_perm_reps(), to generate permutation replicates. . def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1,perm_sample_2) return perm_replicates . Look before you leap: EDA before hypothesis testing . Kleinteich and Gorb (Sci. Rep., 4, 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog&#39;s tongue when it struck the target. . Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. we will test the hypothesis that the two frogs have the same distribution of impact forces. it is important to do EDA first! Let&#39;s make a bee swarm plot for the data. . They are stored in a Pandas data frame, frogs, where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N). . frogs = pd.read_csv(&quot;datasets/frogs.csv&quot;) frogs.head() . ID impact_force . 0 A | 1.612 | . 1 A | 0.605 | . 2 A | 0.327 | . 3 A | 0.946 | . 4 A | 0.541 | . # Make bee swarm plot _ = sns.swarmplot(data=frogs, x=&quot;ID&quot;, y=&quot;impact_force&quot;) # Label axes _ = plt.xlabel(&#39;frog&#39;) _ = plt.ylabel(&#39;impact force (N)&#39;) # Show the plot plt.show() . Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test. . Permutation test on frog data . frogs[frogs.ID==&quot;A&quot;].impact_force.mean() . 0.70735 . frogs[frogs.ID==&quot;B&quot;].impact_force.mean() . 0.4191000000000001 . frogs[frogs.ID==&quot;A&quot;].impact_force.mean() - frogs[frogs.ID==&quot;B&quot;].impact_force.mean() . 0.28824999999999995 . The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. . force_a = np.array([1.612, 0.605, 0.327, 0.946, 0.541, 1.539, 0.529, 0.628, 1.453, 0.297, 0.703, 0.269, 0.751, 0.245, 1.182, 0.515, 0.435, 0.383, 0.457, 0.73 ]) force_b = np.array([0.172, 0.142, 0.037, 0.453, 0.355, 0.022, 0.502, 0.273, 0.72 , 0.582, 0.198, 0.198, 0.597, 0.516, 0.815, 0.402, 0.605, 0.711, 0.614, 0.468]) . We will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis. . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff . # Compute difference of mean impact force from experiment: empirical_diff_means empirical_diff_means = diff_of_means(force_a, force_b) empirical_diff_means . 0.28825000000000006 . # Draw 10,000 permutation replicates: perm_replicates perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000) # Compute p-value: p p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates) # Print the result print(&#39;p-value =&#39;, p) . p-value = 0.0058 . The p-value tells us that there is about a 0.6% chance that we would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be &quot;statistically significant,&quot; but: . Warning: Warning! warning! warning! We have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be &quot;statistically significant,&quot; but they are definitely not the same! . Bootstrap hypothesis tests . Pipeline for hypothesis testing . Clearly state the null hypothesis | Define your test statistic | Generate many sets of simulated data assuming the null hypothesis is true | Compute the test statistic for each simulated data set | The p-value is the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data | . A one-sample bootstrap hypothesis test . Another juvenile frog was studied, Frog C, and we want to see if Frog B and Frog C have similar impact forces. Unfortunately, we do not have Frog C&#39;s impact forces available, but we know they have a mean of 0.55 N. Because we don&#39;t have the original data, we cannot do a permutation test, and we cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. We will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C. . To set up the bootstrap hypothesis test, we will take the mean as our test statistic. Our goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B&#39;s impact forces is equal to that of Frog C is true. We will first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B&#39;s distribution, such as the variance, unchanged. . # Make an array of translated impact forces: translated_force_b translated_force_b = force_b - np.mean(force_b) + 0.55 # Take bootstrap replicates of Frog B&#39;s translated impact forces: bs_replicates bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000) # Compute fraction of replicates that are less than the observed Frog B force: p p = np.sum(bs_replicates &lt;= np.mean(force_b)) / 10000 # Print the p-value print(&#39;p = &#39;, p) . p = 0.0046 . The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false. . A two-sample bootstrap hypothesis test for difference of means . We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test. . To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed. . forces_concat = np.array([1.612, 0.605, 0.327, 0.946, 0.541, 1.539, 0.529, 0.628, 1.453, 0.297, 0.703, 0.269, 0.751, 0.245, 1.182, 0.515, 0.435, 0.383, 0.457, 0.73 , 0.172, 0.142, 0.037, 0.453, 0.355, 0.022, 0.502, 0.273, 0.72 , 0.582, 0.198, 0.198, 0.597, 0.516, 0.815, 0.402, 0.605, 0.711, 0.614, 0.468]) . empirical_diff_means = 0.28825000000000006 . Hypothesis test examples . Hypothesis testing can be a bit tricky. We need to define the null hypothesis, figure out how to simulate it, and define clearly what it means to be &quot;more extreme&quot; in order to compute the p-value. Like any skill, practice makes perfect. . A/B testing . A/B test . Used by organizations to see if a strategy change gives a better result | . Null hypothesis of an A/B test . The test statistic is impervious to the change | . The vote for the Civil Rights Act in 1964 . The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding &quot;present&quot; and &quot;abstain&quot; votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote? . To answer this question, we will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. We will use the fraction of Democrats voting in favor as our test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That&#39;s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, we will permute the party labels of the House voters and then arbitrarily divide them into &quot;Democrats&quot; and &quot;Republicans&quot; and compute the fraction of Democrats voting yea. . # Construct arrays of data: dems, reps dems = np.array([True] * 153 + [False] * 91) reps = np.array([True]*136 + [False]*35) def frac_yea_dems(dems, reps): &quot;&quot;&quot;Compute fraction of Democrat yea votes.&quot;&quot;&quot; frac = np.sum(dems) / len(dems) return frac # Acquire permutation samples: perm_replicates perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000) # Compute and print p-value: p p = np.sum(perm_replicates &lt;= 153/244) / len(perm_replicates) print(&#39;p-value =&#39;, p) . p-value = 0.0002 . This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias. . A time-on-website analog . It turns out that we already did a hypothesis test analogous to an A/B test where we are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. &quot;Before&quot; = Frog A and &quot;after&quot; = Frog B. Let&#39;s practice this again with something that actually is a before/after scenario. . We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem we will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as our test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live, where &quot;nht&quot; is meant to stand for &quot;no-hitter time.&quot; . nht_dead = np.array([ -1, 894, 10, 130, 1, 934, 29, 6, 485, 254, 372, 81, 191, 355, 180, 286, 47, 269, 361, 173, 246, 492, 462, 1319, 58, 297, 31, 2970, 640, 237, 434, 570, 77, 271, 563, 3365, 89, 0, 379, 221, 479, 367, 628, 843, 1613, 1101, 215, 684, 814, 278, 324, 161, 219, 545, 715, 966, 624, 29, 450, 107, 20, 91, 1325, 124, 1468, 104, 1309, 429, 62, 1878, 1104, 123, 251, 93, 188, 983, 166, 96, 702, 23, 524, 26, 299, 59, 39, 12, 2, 308, 1114, 813, 887]) . nht_live = np.array([ 645, 2088, 42, 2090, 11, 886, 1665, 1084, 2900, 2432, 750, 4021, 1070, 1765, 1322, 26, 548, 1525, 77, 2181, 2752, 127, 2147, 211, 41, 1575, 151, 479, 697, 557, 2267, 542, 392, 73, 603, 233, 255, 528, 397, 1529, 1023, 1194, 462, 583, 37, 943, 996, 480, 1497, 717, 224, 219, 1531, 498, 44, 288, 267, 600, 52, 269, 1086, 386, 176, 2199, 216, 54, 675, 1243, 463, 650, 171, 327, 110, 774, 509, 8, 197, 136, 12, 1124, 64, 380, 811, 232, 192, 731, 715, 226, 605, 539, 1491, 323, 240, 179, 702, 156, 82, 1397, 354, 778, 603, 1001, 385, 986, 203, 149, 576, 445, 180, 1403, 252, 675, 1351, 2983, 1568, 45, 899, 3260, 1025, 31, 100, 2055, 4043, 79, 238, 3931, 2351, 595, 110, 215, 0, 563, 206, 660, 242, 577, 179, 157, 192, 192, 1848, 792, 1693, 55, 388, 225, 1134, 1172, 1555, 31, 1582, 1044, 378, 1687, 2915, 280, 765, 2819, 511, 1521, 745, 2491, 580, 2072, 6450, 578, 745, 1075, 1103, 1549, 1520, 138, 1202, 296, 277, 351, 391, 950, 459, 62, 1056, 1128, 139, 420, 87, 71, 814, 603, 1349, 162, 1027, 783, 326, 101, 876, 381, 905, 156, 419, 239, 119, 129, 467]) . # Compute the observed difference in mean inter-no-hitter times: nht_diff_obs nht_diff_obs = diff_of_means(nht_dead, nht_live) # Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000) # Compute and print the p-value: p p = sum(perm_replicates &lt;= nht_diff_obs)/len(perm_replicates) print(&#39;p-val =&#39;, p) . p-val = 0.0002 . our p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. . Warning: Watch out, though, we could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001. . x_dead, y_dead = ecdf(nht_dead) x_live, y_live = ecdf(nht_live) _ = plt.plot(x_dead, y_dead) _ = plt.plot(x_live, y_live) _ = plt.xlabel(&quot;non hitter times&quot;) _ = plt.legend([&quot;nht dead&quot;, &quot;nht live&quot;]) _ = plt.ylabel(&quot;CDF&quot;) plt.show() . Test of correlation . Hypothesis test of correlation . Posit null hypothesis:the two variables are completely uncorrelated- Simulate data assuming null hypothesis is true | Use Pearson correlation, $ rho$, as test statistic | Compute p-value as fraction of replicates that have ρ at least as large as observed. | . Simulating a null hypothesis concerning correlation . The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. We will test this null hypothesis. . To do the test, we need to simulate the data assuming the null hypothesis is true. The best way to it is to Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data. . Hypothesis test on Pearson correlation . The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. We will test this hypothesis. To do so, we&#39;ll permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, we&#39;ll compute the Pearson correlation coefficient and assess how many of the permutation replicates have a Pearson correlation coefficient greater than the observed one. . # Compute observed correlation: r_obs r_obs = pearson_r(illiteracy, fertility) # Initialize permutation replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute illiteracy measurments: illiteracy_permuted illiteracy_permuted = np.random.permutation(illiteracy) # Compute Pearson correlation perm_replicates[i] = pearson_r(illiteracy_permuted, fertility) # Compute p-value: p p = np.sum(perm_replicates &gt;= r_obs) / len(perm_replicates) print(&#39;p-val =&#39;, p) . p-val = 0.0 . We got a p-value of zero. In hacker statistics, this means that the p-value is very low, since we never got a single replicate in the 10,000 we took that had a Pearson correlation greater than the observed one. we could try increasing the number of replicates you take to continue to move the upper bound on the p-value lower and lower. . Do neonicotinoid insecticides have unintended consequences? . We will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants. . In a recent study, Straub, et al. (Proc. Roy. Soc. B, 2016) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen. . First, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control) and bees treated with pesticide (stored in the Numpy array treated). . control = np.array([ 4.159234, 4.408002, 0.172812, 3.498278, 3.104912, 5.164174, 6.615262, 4.633066, 0.170408, 2.65 , 0.0875 , 1.997148, 6.92668 , 4.574932, 3.896466, 5.209814, 3.70625 , 0. , 4.62545 , 3.01444 , 0.732652, 0.4 , 6.518382, 5.225 , 6.218742, 6.840358, 1.211308, 0.368252, 3.59937 , 4.212158, 6.052364, 2.115532, 6.60413 , 5.26074 , 6.05695 , 6.481172, 3.171522, 3.057228, 0.218808, 5.215112, 4.465168, 2.28909 , 3.732572, 2.17087 , 1.834326, 6.074862, 5.841978, 8.524892, 4.698492, 2.965624, 2.324206, 3.409412, 4.830726, 0.1 , 0. , 4.101432, 3.478162, 1.009688, 4.999296, 4.32196 , 0.299592, 3.606032, 7.54026 , 4.284024, 0.057494, 6.036668, 2.924084, 4.150144, 1.256926, 4.666502, 4.806594, 2.52478 , 2.027654, 2.52283 , 4.735598, 2.033236, 0. , 6.177294, 2.601834, 3.544408, 3.6045 , 5.520346, 4.80698 , 3.002478, 3.559816, 7.075844, 10. , 0.139772, 6.17171 , 3.201232, 8.459546, 0.17857 , 7.088276, 5.496662, 5.415086, 1.932282, 3.02838 , 7.47996 , 1.86259 , 7.838498, 2.242718, 3.292958, 6.363644, 4.386898, 8.47533 , 4.156304, 1.463956, 4.533628, 5.573922, 1.29454 , 7.547504, 3.92466 , 5.820258, 4.118522, 4.125 , 2.286698, 0.591882, 1.273124, 0. , 0. , 0. , 12.22502 , 7.601604, 5.56798 , 1.679914, 8.77096 , 5.823942, 0.258374, 0. , 5.899236, 5.486354, 2.053148, 3.25541 , 2.72564 , 3.364066, 2.43427 , 5.282548, 3.963666, 0.24851 , 0.347916, 4.046862, 5.461436, 4.066104, 0. , 0.065 ]) . treated = np.array([1.342686, 1.058476, 3.793784, 0.40428 , 4.528388, 2.142966, 3.937742, 0.1375 , 6.919164, 0. , 3.597812, 5.196538, 2.78955 , 2.3229 , 1.090636, 5.323916, 1.021618, 0.931836, 2.78 , 0.412202, 1.180934, 2.8674 , 0. , 0.064354, 3.008348, 0.876634, 0. , 4.971712, 7.280658, 4.79732 , 2.084956, 3.251514, 1.9405 , 1.566192, 0.58894 , 5.219658, 0.977976, 3.124584, 1.297564, 1.433328, 4.24337 , 0.880964, 2.376566, 3.763658, 1.918426, 3.74 , 3.841726, 4.69964 , 4.386876, 0. , 1.127432, 1.845452, 0.690314, 4.185602, 2.284732, 7.237594, 2.185148, 2.799124, 3.43218 , 0.63354 , 1.142496, 0.586 , 2.372858, 1.80032 , 3.329306, 4.028804, 3.474156, 7.508752, 2.032824, 1.336556, 1.906496, 1.396046, 2.488104, 4.759114, 1.07853 , 3.19927 , 3.814252, 4.275962, 2.817056, 0.552198, 3.27194 , 5.11525 , 2.064628, 0. , 3.34101 , 6.177322, 0. , 3.66415 , 2.352582, 1.531696]) . # Compute x,y values for ECDFs x_control, y_control = ecdf(control) x_treated, y_treated = ecdf(treated) # Plot the ECDFs plt.plot(x_control, y_control, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(x_treated, y_treated, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins plt.margins(0.02) # Add a legend plt.legend((&#39;control&#39;, &#39;treated&#39;), loc=&#39;lower right&#39;) # Label axes and show plot plt.xlabel(&#39;millions of alive sperm per mL&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let&#39;s now do a hypothesis test. . Bootstrap hypothesis test on bee sperm counts . We will test the following hypothesis: On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. We will use the difference of means as the test statistic. . # Compute the difference in mean sperm count: diff_means diff_means = np.mean(control) - np.mean(treated) # Compute mean of pooled data: mean_count mean_count = np.mean(np.concatenate((control, treated))) # Generate shifted data sets control_shifted = control - np.mean(control) + mean_count treated_shifted = treated - np.mean(treated) + mean_count # Generate bootstrap replicates bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000) bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_reps_control - bs_reps_treated # Compute and print p-value: p p = np.sum(bs_replicates &gt;= np.mean(control) - np.mean(treated)) / len(bs_replicates) print(&#39;p-value =&#39;, p) . p-value = 0.0 . The p-value is small, most likely less than 0.0001, since we never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. . a case study . Every year for the past 40-plus years, Peter and Rosemary Grant have gone to the Galápagos island of Daphne Major and collected data on Darwin&#39;s finches. Using skills in statistical inference, we will explorer the data, and witness first hand, through data, evolution in action. . EDA of beak depths of Darwin&#39;s finches . For our first foray into the Darwin finch data, we will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis. These effects can lead to changes in the species over time. . We will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let&#39;s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot. . dfd = pd.read_csv(&quot;datasets/darwin_finch_data.csv&quot;) dfd.head() . beak_depth year . 0 8.4 | 1975 | . 1 8.8 | 1975 | . 2 8.4 | 1975 | . 3 8.0 | 1975 | . 4 7.9 | 1975 | . dfd.tail() . beak_depth year . 209 9.3 | 2012 | . 210 9.8 | 2012 | . 211 8.9 | 2012 | . 212 9.8 | 2012 | . 213 9.1 | 2012 | . dfd.dtypes . beak_depth float64 year int64 dtype: object . dfd.describe().T . count mean std min 25% 50% 75% max . beak_depth 214.0 | 9.094252 | 0.637941 | 7.7 | 8.6625 | 9.075 | 9.5 | 11.0 | . year 214.0 | 1996.957944 | 18.216566 | 1975.0 | 1975.0000 | 2012.000 | 2012.0 | 2012.0 | . dfd.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 beak_depth 214 non-null float64 1 year 214 non-null int64 dtypes: float64(1), int64(1) memory usage: 3.5 KB . The units of beak depth are millimeters (mm). . # Create bee swarm plot _ = sns.swarmplot(data=dfd, x=&quot;year&quot;, y=&quot;beak_depth&quot;) # Label the axes _ = plt.xlabel(&#39;year&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) # Show the plot plt.show() . It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance. . ECDFs of beak depths . While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot. . bd_1975 = dfd.beak_depth[dfd.year==1975] bd_2012 = dfd.beak_depth[dfd.year==2012] . # Compute ECDFs x_1975, y_1975 = ecdf(bd_1975) x_2012, y_2012 = ecdf(bd_2012) # Plot the ECDFs _ = plt.plot(x_1975, y_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_2012, y_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set margins plt.margins(.02) # Add axis labels and legend _ = plt.xlabel(&#39;beak depth (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;lower right&#39;) # Show the plot plt.show() . The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well. . Parameter estimates of beak depths . Let&#39;s estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. . # Compute the difference of the sample means: mean_diff mean_diff = np.mean(bd_2012) - np.mean(bd_1975) # Get bootstrap replicates of means bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000) # Compute samples of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5]) # Print the results print(&#39;difference of means =&#39;, mean_diff, &#39;mm&#39;) print(&#39;95% confidence interval =&#39;, conf_int, &#39;mm&#39;) . difference of means = 0.22622047244094645 mm 95% confidence interval = [0.05967226 0.38944932] mm . Hypothesis test: Are beaks deeper in 2012? . Our plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same? . Warning: The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means. . # Compute mean of combined data set: combined_mean combined_mean = np.mean(np.concatenate((bd_1975, bd_2012))) # Shift the samples bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean # Get bootstrap replicates of shifted data sets bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000) # Compute replicates of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute the p-value p = np.sum(bs_diff_replicates &gt;= mean_diff) / len(bs_diff_replicates) # Print p-value print(&#39;p =&#39;, p) . p = 0.0046 . We get a p-value of 0.0038, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! We got a difference of 0.2 mm between the means. We should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years. . Variation in beak shapes . EDA of beak length and depth . bl_1975 = np.array([13.9 , 14. , 12.9 , 13.5 , 12.9 , 14.6 , 13. , 14.2 , 14. , 14.2 , 13.1 , 15.1 , 13.5 , 14.4 , 14.9 , 12.9 , 13. , 14.9 , 14. , 13.8 , 13. , 14.75, 13.7 , 13.8 , 14. , 14.6 , 15.2 , 13.5 , 15.1 , 15. , 12.8 , 14.9 , 15.3 , 13.4 , 14.2 , 15.1 , 15.1 , 14. , 13.6 , 14. , 14. , 13.9 , 14. , 14.9 , 15.6 , 13.8 , 14.4 , 12.8 , 14.2 , 13.4 , 14. , 14.8 , 14.2 , 13.5 , 13.4 , 14.6 , 13.5 , 13.7 , 13.9 , 13.1 , 13.4 , 13.8 , 13.6 , 14. , 13.5 , 12.8 , 14. , 13.4 , 14.9 , 15.54, 14.63, 14.73, 15.73, 14.83, 15.94, 15.14, 14.23, 14.15, 14.35, 14.95, 13.95, 14.05, 14.55, 14.05, 14.45, 15.05, 13.25]) . bl_2012 = np.array([14.3 , 12.5 , 13.7 , 13.8 , 12. , 13. , 13. , 13.6 , 12.8 , 13.6 , 12.95, 13.1 , 13.4 , 13.9 , 12.3 , 14. , 12.5 , 12.3 , 13.9 , 13.1 , 12.5 , 13.9 , 13.7 , 12. , 14.4 , 13.5 , 13.8 , 13. , 14.9 , 12.5 , 12.3 , 12.8 , 13.4 , 13.8 , 13.5 , 13.5 , 13.4 , 12.3 , 14.35, 13.2 , 13.8 , 14.6 , 14.3 , 13.8 , 13.6 , 12.9 , 13. , 13.5 , 13.2 , 13.7 , 13.1 , 13.2 , 12.6 , 13. , 13.9 , 13.2 , 15. , 13.37, 11.4 , 13.8 , 13. , 13. , 13.1 , 12.8 , 13.3 , 13.5 , 12.4 , 13.1 , 14. , 13.5 , 11.8 , 13.7 , 13.2 , 12.2 , 13. , 13.1 , 14.7 , 13.7 , 13.5 , 13.3 , 14.1 , 12.5 , 13.7 , 14.6 , 14.1 , 12.9 , 13.9 , 13.4 , 13. , 12.7 , 12.1 , 14. , 14.9 , 13.9 , 12.9 , 14.6 , 14. , 13. , 12.7 , 14. , 14.1 , 14.1 , 13. , 13.5 , 13.4 , 13.9 , 13.1 , 12.9 , 14. , 14. , 14.1 , 14.7 , 13.4 , 13.8 , 13.4 , 13.8 , 12.4 , 14.1 , 12.9 , 13.9 , 14.3 , 13.2 , 14.2 , 13. , 14.6 , 13.1 , 15.2 ]) . The beak length data are stored as bl_1975 and bl_2012, again with units of millimeters (mm). We still have the beak depth data stored in bd_1975 and bd_2012. We will make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens. . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;None&#39;, alpha=0.5, color=&quot;blue&quot;) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;None&#39;, alpha=0.5, color=&quot;red&quot;) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Show the plot plt.show() . In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper. . Linear regressions . We perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line. . # Compute the linear regressions slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975.values, deg=1) slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012.values, deg=1) # Perform pairs bootstrap for the linear regressions bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975.values, size=1000) bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012.values, size=1000) # Compute confidence intervals of slopes slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5]) slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5]) intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5]) intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5]) # Print the results print(&#39;1975: slope =&#39;, slope_1975, &#39;conf int =&#39;, slope_conf_int_1975) print(&#39;1975: intercept =&#39;, intercept_1975, &#39;conf int =&#39;, intercept_conf_int_1975) print(&#39;2012: slope =&#39;, slope_2012, &#39;conf int =&#39;, slope_conf_int_2012) print(&#39;2012: intercept =&#39;, intercept_2012, &#39;conf int =&#39;, intercept_conf_int_2012) . 1975: slope = 0.46520516916059357 conf int = [0.3254032 0.58811989] 1975: intercept = 2.3908752365842285 conf int = [0.67000784 4.40072329] 2012: slope = 0.4626303588353126 conf int = [0.33989291 0.60535488] 2012: intercept = 2.9772474982360184 conf int = [1.03792858 4.62029259] . It looks like they have the same slope, but different intercepts. . Displaying the linear regression results . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Generate x-values for bootstrap lines: x x = np.array([10, 17]) # Plot the bootstrap lines for i in range(100): plt.plot(x, x* bs_slope_reps_1975[i] + bs_intercept_reps_1975[i], linewidth=0.5, alpha=0.2, color=&quot;blue&quot;) plt.plot(x, x*bs_slope_reps_2012[i] + bs_intercept_reps_2012[i], linewidth=0.5, alpha=0.2, color=&quot;red&quot;) # Draw the plot again plt.show() . Beak length to depth ratio . The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let&#39;s make that comparison. . # Compute length-to-depth ratios ratio_1975 = bl_1975/bd_1975 ratio_2012 = bl_2012/bd_2012 # Compute means mean_ratio_1975 = np.mean(ratio_1975) mean_ratio_2012 = np.mean(ratio_2012) # Generate bootstrap replicates of the means bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000) # Compute the 99% confidence intervals conf_int_1975 = np.percentile(bs_replicates_1975, [.5, 99.5]) conf_int_2012 = np.percentile(bs_replicates_2012, [.5, 99.5]) # Print the results print(&#39;1975: mean ratio =&#39;, mean_ratio_1975, &#39;conf int =&#39;, conf_int_1975) print(&#39;2012: mean ratio =&#39;, mean_ratio_2012, &#39;conf int =&#39;, conf_int_2012) . 1975: mean ratio = 1.5788823771858533 conf int = [1.55672661 1.60112763] 2012: mean ratio = 1.4658342276847767 conf int = [1.44471693 1.48753163] . _ = sns.pointplot([np.median(conf_int_1975), np.median(conf_int_2012)], [1975, 2012]) plt.show() . Calculation of heritability . Heredity . The tendency for parental traits to be inherited by offspring | . EDA of heritability . The array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens . bd_parent_scandens = np.array([ 8.3318, 8.4035, 8.5317, 8.7202, 8.7089, 8.7541, 8.773 , 8.8107, 8.7919, 8.8069, 8.6523, 8.6146, 8.6938, 8.7127, 8.7466, 8.7504, 8.7805, 8.7428, 8.7164, 8.8032, 8.8258, 8.856 , 8.9012, 8.9125, 8.8635, 8.8258, 8.8522, 8.8974, 8.9427, 8.9879, 8.9615, 8.9238, 8.9351, 9.0143, 9.0558, 9.0596, 8.9917, 8.905 , 8.9314, 8.9465, 8.9879, 8.9804, 9.0219, 9.052 , 9.0407, 9.0407, 8.9955, 8.9992, 8.9992, 9.0747, 9.0747, 9.5385, 9.4781, 9.4517, 9.3537, 9.2707, 9.1199, 9.1689, 9.1425, 9.135 , 9.1011, 9.1727, 9.2217, 9.2255, 9.2821, 9.3235, 9.3198, 9.3198, 9.3198, 9.3273, 9.3725, 9.3989, 9.4253, 9.4593, 9.4442, 9.4291, 9.2632, 9.2293, 9.1878, 9.1425, 9.1275, 9.1802, 9.1765, 9.2481, 9.2481, 9.1991, 9.1689, 9.1765, 9.2406, 9.3198, 9.3235, 9.1991, 9.2971, 9.2443, 9.316 , 9.2934, 9.3914, 9.3989, 9.5121, 9.6176, 9.5535, 9.4668, 9.3725, 9.3348, 9.3763, 9.3839, 9.4216, 9.4065, 9.3348, 9.4442, 9.4367, 9.5083, 9.448 , 9.4781, 9.595 , 9.6101, 9.5686, 9.6365, 9.7119, 9.8213, 9.825 , 9.7609, 9.6516, 9.5988, 9.546 , 9.6516, 9.7572, 9.8854, 10.0023, 9.3914]) . The array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents. . bd_offspring_scandens = np.array([ 8.419 , 9.2468, 8.1532, 8.0089, 8.2215, 8.3734, 8.5025, 8.6392, 8.7684, 8.8139, 8.7911, 8.9051, 8.9203, 8.8747, 8.943 , 9.0038, 8.981 , 9.0949, 9.2696, 9.1633, 9.1785, 9.1937, 9.2772, 9.0722, 8.9658, 8.9658, 8.5025, 8.4949, 8.4949, 8.5633, 8.6013, 8.6468, 8.1532, 8.3734, 8.662 , 8.6924, 8.7456, 8.8367, 8.8595, 8.9658, 8.9582, 8.8671, 8.8671, 8.943 , 9.0646, 9.1405, 9.2089, 9.2848, 9.3759, 9.4899, 9.4519, 8.1228, 8.2595, 8.3127, 8.4949, 8.6013, 8.4646, 8.5329, 8.7532, 8.8823, 9.0342, 8.6392, 8.6772, 8.6316, 8.7532, 8.8291, 8.8975, 8.9734, 9.0494, 9.1253, 9.1253, 9.1253, 9.1785, 9.2848, 9.4595, 9.3608, 9.2089, 9.2544, 9.3684, 9.3684, 9.2316, 9.1709, 9.2316, 9.0342, 8.8899, 8.8291, 8.981 , 8.8975, 10.4089, 10.1886, 9.7633, 9.7329, 9.6114, 9.5051, 9.5127, 9.3684, 9.6266, 9.5354, 10.0215, 10.0215, 9.6266, 9.6038, 9.4063, 9.2316, 9.338 , 9.262 , 9.262 , 9.4063, 9.4367, 9.0342, 8.943 , 8.9203, 8.7835, 8.7835, 9.057 , 8.9354, 8.8975, 8.8139, 8.8671, 9.0873, 9.2848, 9.2392, 9.2924, 9.4063, 9.3152, 9.4899, 9.5962, 9.6873, 9.5203, 9.6646]) . The arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds. . bd_parent_fortis = np.array([10.1 , 9.55 , 9.4 , 10.25 , 10.125, 9.7 , 9.05 , 7.4 , 9. , 8.65 , 9.625, 9.9 , 9.55 , 9.05 , 8.35 , 10.1 , 10.1 , 9.9 , 10.225, 10. , 10.55 , 10.45 , 9.2 , 10.2 , 8.95 , 10.05 , 10.2 , 9.5 , 9.925, 9.95 , 10.05 , 8.75 , 9.2 , 10.15 , 9.8 , 10.7 , 10.5 , 9.55 , 10.55 , 10.475, 8.65 , 10.7 , 9.1 , 9.4 , 10.3 , 9.65 , 9.5 , 9.7 , 10.525, 9.95 , 10.1 , 9.75 , 10.05 , 9.9 , 10. , 9.1 , 9.45 , 9.25 , 9.5 , 10. , 10.525, 9.9 , 10.4 , 8.95 , 9.4 , 10.95 , 10.75 , 10.1 , 8.05 , 9.1 , 9.55 , 9.05 , 10.2 , 10. , 10.55 , 10.75 , 8.175, 9.7 , 8.8 , 10.75 , 9.3 , 9.7 , 9.6 , 9.75 , 9.6 , 10.45 , 11. , 10.85 , 10.15 , 10.35 , 10.4 , 9.95 , 9.1 , 10.1 , 9.85 , 9.625, 9.475, 9. , 9.25 , 9.1 , 9.25 , 9.2 , 9.95 , 8.65 , 9.8 , 9.4 , 9. , 8.55 , 8.75 , 9.65 , 8.95 , 9.15 , 9.85 , 10.225, 9.825, 10. , 9.425, 10.4 , 9.875, 8.95 , 8.9 , 9.35 , 10.425, 10. , 10.175, 9.875, 9.875, 9.15 , 9.45 , 9.025, 9.7 , 9.7 , 10.05 , 10.3 , 9.6 , 10. , 9.8 , 10.05 , 8.75 , 10.55 , 9.7 , 10. , 9.85 , 9.8 , 9.175, 9.65 , 9.55 , 9.9 , 11.55 , 11.3 , 10.4 , 10.8 , 9.8 , 10.45 , 10. , 10.75 , 9.35 , 10.75 , 9.175, 9.65 , 8.8 , 10.55 , 10.675, 9.95 , 9.55 , 8.825, 9.7 , 9.85 , 9.8 , 9.55 , 9.275, 10.325, 9.15 , 9.35 , 9.15 , 9.65 , 10.575, 9.975, 9.55 , 9.2 , 9.925, 9.2 , 9.3 , 8.775, 9.325, 9.175, 9.325, 8.975, 9.7 , 9.5 , 10.225, 10.025, 8.2 , 8.2 , 9.55 , 9.05 , 9.6 , 9.6 , 10.15 , 9.875, 10.485, 11.485, 10.985, 9.7 , 9.65 , 9.35 , 10.05 , 10.1 , 9.9 , 8.95 , 9.3 , 9.95 , 9.45 , 9.5 , 8.45 , 8.8 , 8.525, 9.375, 10.2 , 7.625, 8.375, 9.25 , 9.4 , 10.55 , 8.9 , 8.8 , 9. , 8.575, 8.575, 9.6 , 9.375, 9.6 , 9.95 , 9.6 , 10.2 , 9.85 , 9.625, 9.025, 10.375, 10.25 , 9.3 , 9.5 , 9.55 , 8.55 , 9.05 , 9.9 , 9.8 , 9.75 , 10.25 , 9.1 , 9.65 , 10.3 , 8.9 , 9.95 , 9.5 , 9.775, 9.425, 7.75 , 7.55 , 9.1 , 9.6 , 9.575, 8.95 , 9.65 , 9.65 , 9.65 , 9.525, 9.85 , 9.05 , 9.3 , 8.9 , 9.45 , 10. , 9.85 , 9.25 , 10.1 , 9.125, 9.65 , 9.1 , 8.05 , 7.4 , 8.85 , 9.075, 9. , 9.7 , 8.7 , 9.45 , 9.7 , 8.35 , 8.85 , 9.7 , 9.45 , 10.3 , 10. , 10.45 , 9.45 , 8.5 , 8.3 , 10. , 9.225, 9.75 , 9.15 , 9.55 , 9. , 9.275, 9.35 , 8.95 , 9.875, 8.45 , 8.6 , 9.7 , 8.55 , 9.05 , 9.6 , 8.65 , 9.2 , 8.95 , 9.6 , 9.15 , 9.4 , 8.95 , 9.95 , 10.55 , 9.7 , 8.85 , 8.8 , 10. , 9.05 , 8.2 , 8.1 , 7.25 , 8.3 , 9.15 , 8.6 , 9.5 , 8.05 , 9.425, 9.3 , 9.8 , 9.3 , 9.85 , 9.5 , 8.65 , 9.825, 9. , 10.45 , 9.1 , 9.55 , 9.05 , 10. , 9.35 , 8.375, 8.3 , 8.8 , 10.1 , 9.5 , 9.75 , 10.1 , 9.575, 9.425, 9.65 , 8.725, 9.025, 8.5 , 8.95 , 9.3 , 8.85 , 8.95 , 9.8 , 9.5 , 8.65 , 9.1 , 9.4 , 8.475, 9.35 , 7.95 , 9.35 , 8.575, 9.05 , 8.175, 9.85 , 7.85 , 9.85 , 10.1 , 9.35 , 8.85 , 8.75 , 9.625, 9.25 , 9.55 , 10.325, 8.55 , 9.675, 9.15 , 9. , 9.65 , 8.6 , 8.8 , 9. , 9.95 , 8.4 , 9.35 , 10.3 , 9.05 , 9.975, 9.975, 8.65 , 8.725, 8.2 , 7.85 , 8.775, 8.5 , 9.4 ]) . bd_offspring_fortis = np.array([10.7 , 9.78, 9.48, 9.6 , 10.27, 9.5 , 9. , 7.46, 7.65, 8.63, 9.81, 9.4 , 9.48, 8.75, 7.6 , 10. , 10.09, 9.74, 9.64, 8.49, 10.15, 10.28, 9.2 , 10.01, 9.03, 9.94, 10.5 , 9.7 , 10.02, 10.04, 9.43, 8.1 , 9.5 , 9.9 , 9.48, 10.18, 10.16, 9.08, 10.39, 9.9 , 8.4 , 10.6 , 8.75, 9.46, 9.6 , 9.6 , 9.95, 10.05, 10.16, 10.1 , 9.83, 9.46, 9.7 , 9.82, 10.34, 8.02, 9.65, 9.87, 9. , 11.14, 9.25, 8.14, 10.23, 8.7 , 9.8 , 10.54, 11.19, 9.85, 8.1 , 9.3 , 9.34, 9.19, 9.52, 9.36, 8.8 , 8.6 , 8. , 8.5 , 8.3 , 10.38, 8.54, 8.94, 10. , 9.76, 9.45, 9.89, 10.9 , 9.91, 9.39, 9.86, 9.74, 9.9 , 9.09, 9.69, 10.24, 8.9 , 9.67, 8.93, 9.3 , 8.67, 9.15, 9.23, 9.59, 9.03, 9.58, 8.97, 8.57, 8.47, 8.71, 9.21, 9.13, 8.5 , 9.58, 9.21, 9.6 , 9.32, 8.7 , 10.46, 9.29, 9.24, 9.45, 9.35, 10.19, 9.91, 9.18, 9.89, 9.6 , 10.3 , 9.45, 8.79, 9.2 , 8.8 , 9.69, 10.61, 9.6 , 9.9 , 9.26, 10.2 , 8.79, 9.28, 8.83, 9.76, 10.2 , 9.43, 9.4 , 9.9 , 9.5 , 8.95, 9.98, 9.72, 9.86, 11.1 , 9.14, 10.49, 9.75, 10.35, 9.73, 9.83, 8.69, 9.58, 8.42, 9.25, 10.12, 9.31, 9.99, 8.59, 8.74, 8.79, 9.6 , 9.52, 8.93, 10.23, 9.35, 9.35, 9.09, 9.04, 9.75, 10.5 , 9.09, 9.05, 9.54, 9.3 , 9.06, 8.7 , 9.32, 8.4 , 8.67, 8.6 , 9.53, 9.77, 9.65, 9.43, 8.35, 8.26, 9.5 , 8.6 , 9.57, 9.14, 10.79, 8.91, 9.93, 10.7 , 9.3 , 9.93, 9.51, 9.44, 10.05, 10.13, 9.24, 8.21, 8.9 , 9.34, 8.77, 9.4 , 8.82, 8.83, 8.6 , 9.5 , 10.2 , 8.09, 9.07, 9.29, 9.1 , 10.19, 9.25, 8.98, 9.02, 8.6 , 8.25, 8.7 , 9.9 , 9.65, 9.45, 9.38, 10.4 , 9.96, 9.46, 8.26, 10.05, 8.92, 9.5 , 9.43, 8.97, 8.44, 8.92, 10.3 , 8.4 , 9.37, 9.91, 10. , 9.21, 9.95, 8.84, 9.82, 9.5 , 10.29, 8.4 , 8.31, 9.29, 8.86, 9.4 , 9.62, 8.62, 8.3 , 9.8 , 8.48, 9.61, 9.5 , 9.37, 8.74, 9.31, 9.5 , 9.49, 9.74, 9.2 , 9.24, 9.7 , 9.64, 9.2 , 7.5 , 7.5 , 8.7 , 8.31, 9. , 9.74, 9.31, 10.5 , 9.3 , 8.12, 9.34, 9.72, 9. , 9.65, 9.9 , 10. , 10.1 , 8. , 9.07, 9.75, 9.33, 8.11, 9.36, 9.74, 9.9 , 9.23, 9.7 , 8.2 , 9.35, 9.49, 9.34, 8.87, 9.03, 9.07, 9.43, 8.2 , 9.19, 9. , 9.2 , 9.06, 9.81, 8.89, 9.4 , 10.45, 9.64, 9.03, 8.71, 9.91, 8.33, 8.2 , 7.83, 7.14, 8.91, 9.18, 8.8 , 9.9 , 7.73, 9.25, 8.7 , 9.5 , 9.3 , 9.05, 10.18, 8.85, 9.24, 9.15, 9.98, 8.77, 9.8 , 8.65, 10. , 8.81, 8.01, 7.9 , 9.41, 10.18, 9.55, 9.08, 8.4 , 9.75, 8.9 , 9.07, 9.35, 8.9 , 8.19, 8.65, 9.19, 8.9 , 9.28, 10.58, 9. , 9.4 , 8.91, 9.93, 10. , 9.37, 7.4 , 9. , 8.8 , 9.18, 8.3 , 10.08, 7.9 , 9.96, 10.4 , 9.65, 8.8 , 8.65, 9.7 , 9.23, 9.43, 9.93, 8.47, 9.55, 9.28, 8.85, 8.9 , 8.75, 8.63, 9. , 9.43, 8.28, 9.23, 10.4 , 9. , 9.8 , 9.77, 8.97, 8.37, 7.7 , 7.9 , 9.5 , 8.2 , 8.8 ]) . We&#39;ll make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. . # Make scatter plots _ = plt.plot(bd_parent_fortis, bd_offspring_fortis, marker=&quot;.&quot;, linestyle=&quot;none&quot;, color=&quot;blue&quot;, alpha=.5) _ = plt.plot(bd_parent_scandens, bd_offspring_scandens, marker=&quot;.&quot;, linestyle=&quot;none&quot;, color=&quot;red&quot;, alpha=.5) # Label axes _ = plt.xlabel(&#39;parental beak depth (mm)&#39;) _ = plt.ylabel(&#39;offspring beak depth (mm)&#39;) # Add legend _ = plt.legend((&#39;G. fortis&#39;, &#39;G. scandens&#39;), loc=&#39;lower right&#39;) # Show plot plt.show() . It appears as though there is a stronger correlation in G. fortis than in G. scandens. This suggests that beak depth is more strongly inherited in G. fortis. We&#39;ll quantify this correlation . Correlation of offspring and parental data . In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap. . def draw_bs_pairs(x, y, func, size=1): &quot;&quot;&quot;Perform pairs bootstrap for a single statistic.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_replicates[i] = func(bs_x, bs_y) return bs_replicates . Pearson correlation of offspring and parental data . The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. We&#39;ll compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens. And Do the same for G. fortis. Then, use the function draw_bs_pairs to compute a 95% confidence interval using pairs bootstrap. . # Compute the Pearson correlation coefficients r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens) r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of Pearson r bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000) bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, r_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, r_fortis, conf_int_fortis) . G. scandens: 0.41170636294012586 [0.28114522 0.55186026] G. fortis: 0.7283412395518486 [0.66716345 0.7791389 ] . It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts. . Measuring heritability . . Note: Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. . This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone. We will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval. . . Warning: Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient. . def heritability(parents, offspring): &quot;&quot;&quot;Compute the heritability from parent and offspring samples.&quot;&quot;&quot; covariance_matrix = np.cov(parents, offspring) return covariance_matrix[0,1] / covariance_matrix[0,0] # Compute the heritability heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens) heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of heritability replicates_scandens = draw_bs_pairs( bd_parent_scandens, bd_offspring_scandens, heritability, size=1000) replicates_fortis = draw_bs_pairs( bd_parent_fortis, bd_offspring_fortis, heritability, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, heritability_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, heritability_fortis, conf_int_fortis) . G. scandens: 0.5485340868685983 [0.35159687 0.74984943] G. fortis: 0.7229051911438156 [0.64286124 0.78727894] . Here again, we see that G. fortis has stronger heritability than G. scandens. This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization. . Is beak depth heritable at all in G. scandens? . The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species. We will test that hypothesis here. To do this, We will do a pairs permutation test. . # Initialize array of replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute parent beak depths bd_parent_permuted = np.random.permutation(bd_parent_scandens) perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens) # Compute p-value: p p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates) # Print the p-value print(&#39;p-val =&#39;, p) . p-val = 0.0 . We get a p-value of zero, which means that none of the 10,000 permutation pairs replicates we drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens, just not as much as in G. fortis. If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance. . _ = plt.hist(perm_replicates, bins = int(np.sqrt(len(perm_replicates)))) _ = plt.ylabel(&quot;counts&quot;) _ = plt.xlabel(&quot;Heritability replicates&quot;) plt.show() .",
            "url": "https://victoromondi1997.github.io/blog/statistical-thinking/hypothesis-testing/data-science/2020/07/08/Statistical-Thinking-in-Python-(Part-2).html",
            "relUrl": "/statistical-thinking/hypothesis-testing/data-science/2020/07/08/Statistical-Thinking-in-Python-(Part-2).html",
            "date": " • Jul 8, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Statistical Thinking in Python (Part 1)",
            "content": "Graphical exploratory data analysis . Before diving into sophisticated statistical inference techniques, we should first explore our data by plotting them and computing simple summary statistics. This process, called exploratory data analysis, is a crucial first step in statistical analysis of data. . Introduction to Exploratory Data Analysis . Exploratory Data Analysis is the process of organizing, plo!ing, and summarizing a data set . “Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone. ” &gt; ~ John Tukey . Tukey&#39;s comments on EDA . Exploratory data analysis is detective work. | There is no excuse for failing to plot and look. | The greatest value of a picture is that it forces us to notice what we never expected to see. | It is important to understand what you can do before you learn how to measure how well you seem to have done it. | . If you don&#39;t have time to do EDA, you really don&#39;t have time to do hypothesis tests. And you should always do EDA first. . Advantages of graphical EDA . It often involves converting tabular data into graphical form. | If done well, graphical representations can allow for more rapid interpretation of data. | There is no excuse for neglecting to do graphical EDA. | . While a good, informative plot can sometimes be the end point of an analysis, it is more like a beginning:it helps guide you in the quantitative statistical analyses that come next. . Plotting a histogram . Plotting a histogram of iris data . We will use a classic data set collected by botanist Edward Anderson and made famous by Ronald Fisher, one of the most prolific statisticians in history. Anderson carefully measured the anatomical properties of samples of three different species of iris, Iris setosa, Iris versicolor, and Iris virginica. The full data set is available as part of scikit-learn. Here, you will work with his measurements of petal length. . We will plot a histogram of the petal lengths of his 50 samples of Iris versicolor using matplotlib/seaborn&#39;s default settings. . The subset of the data set containing the Iris versicolor petal lengths in units of centimeters (cm) is stored in the NumPy array versicolor_petal_length. . Libraries . # Import plotting modules import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np # Set default Seaborn style sns.set() %matplotlib inline . versicolor_petal_length = np.array([4.7, 4.5, 4.9, 4. , 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4. , 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4. , 4.9, 4.7, 4.3, 4.4, 4.8, 5. , 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4. , 4.4, 4.6, 4. , 3.3, 4.2, 4.2, 4.2, 4.3, 3. , 4.1]) . # Plot histogram of versicolor petal lengths plt.hist(versicolor_petal_length) plt.ylabel(&quot;count&quot;) plt.xlabel(&quot;petal length (cm)&quot;) plt.show() . Adjusting the number of bins in a histogram . The histogram we just made had ten bins. This is the default of matplotlib. . Tip: The &quot;square root rule&quot; is a commonly-used rule of thumb for choosing number of bins: choose the number of bins to be the square root of the number of samples. We will plot the histogram of Iris versicolor petal lengths again, this time using the square root rule for the number of bins. You specify the number of bins using the bins keyword argument of plt.hist(). . # Compute number of data points: n_data n_data = len(versicolor_petal_length) # Number of bins is the square root of number of data points: n_bins n_bins = np.sqrt(n_data) # Convert number of bins to integer: n_bins n_bins = int(n_bins) # Plot the histogram _ = plt.hist(versicolor_petal_length, bins=n_bins) # Label axes _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;count&#39;) # Show histogram plt.show() . Plot all data: Bee swarm plots . Bee swarm plot . We will make a bee swarm plot of the iris petal lengths. The x-axis will contain each of the three species, and the y-axis the petal lengths. . iris_petal_lengths = pd.read_csv(&quot;../datasets/iris_petal_lengths.csv&quot;) iris_petal_lengths.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . iris_petal_lengths.shape . (150, 5) . iris_petal_lengths.tail() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species . 145 6.7 | 3.0 | 5.2 | 2.3 | virginica | . 146 6.3 | 2.5 | 5.0 | 1.9 | virginica | . 147 6.5 | 3.0 | 5.2 | 2.0 | virginica | . 148 6.2 | 3.4 | 5.4 | 2.3 | virginica | . 149 5.9 | 3.0 | 5.1 | 1.8 | virginica | . # Create bee swarm plot with Seaborn&#39;s default settings _ = sns.swarmplot(data=iris_petal_lengths, x=&quot;species&quot;, y=&quot;petal length (cm)&quot;) # Label the axes _ = plt.xlabel(&quot;species&quot;) _ = plt.ylabel(&quot;petal length (cm)&quot;) # Show the plot plt.show() . Interpreting a bee swarm plot . I. virginica petals tend to be the longest, and I. setosa petals tend to be the shortest of the three species. . Note: Notice that we said &quot;tend to be.&quot; Some individual I. virginica flowers may be shorter than individual I. versicolor flowers. It is also possible that an individual I. setosa flower may have longer petals than in individual I. versicolor flower, though this is highly unlikely, and was not observed by Anderson. | . Plot all data: ECDFs . . Note: Empirical cumulative distribution function (ECDF) . Computing the ECDF . We will write a function that takes as input a 1D array of data and then returns the x and y values of the ECDF. . Important: ECDFs are among the most important plots in statistical analysis. . def ecdf(data): &quot;&quot;&quot;Compute ECDF for a one-dimensional array of measurements.&quot;&quot;&quot; # Number of data points: n n = len(data) # x-data for the ECDF: x x = np.sort(data) # y-data for the ECDF: y y = np.arange(1, n+1) / n return x, y . Plotting the ECDF . We will now use ecdf() function to compute the ECDF for the petal lengths of Anderson&#39;s Iris versicolor flowers. We will then plot the ECDF. . Warning: ecdf() function returns two arrays so we will need to unpack them. An example of such unpacking is x, y = foo(data), for some function foo(). . # Compute ECDF for versicolor data: x_vers, y_vers x_vers, y_vers = ecdf(versicolor_petal_length) # Generate plot _ = plt.plot(x_vers, y_vers, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Label the axes _ = plt.xlabel(&quot;versicolor petal length, (cm)&quot;) _ = plt.ylabel(&quot;ECDF&quot;) # Display the plot plt.show() . Comparison of ECDFs . ECDFs also allow us to compare two or more distributions (though plots get cluttered if you have too many). Here, we will plot ECDFs for the petal lengths of all three iris species. . Important: we already wrote a function to generate ECDFs so we can put it to good use! . setosa_petal_length = iris_petal_lengths[&quot;petal length (cm)&quot;][iris_petal_lengths.species == &quot;setosa&quot;] versicolor_petal_length = iris_petal_lengths[&quot;petal length (cm)&quot;][iris_petal_lengths.species == &quot;versicolor&quot;] virginica_petal_length = iris_petal_lengths[&quot;petal length (cm)&quot;][iris_petal_lengths.species == &quot;virginica&quot;] setosa_petal_length.head() . 0 1.4 1 1.4 2 1.3 3 1.5 4 1.4 Name: petal length (cm), dtype: float64 . # Compute ECDFs x_set, y_set = ecdf(setosa_petal_length) x_vers, y_vers = ecdf(versicolor_petal_length) x_virg, y_virg = ecdf(virginica_petal_length) # Plot all ECDFs on the same plot _ = plt.plot(x_set, y_set, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_vers, y_vers, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_virg, y_virg, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Annotate the plot plt.legend((&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;), loc=&#39;lower right&#39;) _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Display the plot plt.show() . . Note: The ECDFs expose clear differences among the species. Setosa is much shorter, also with less absolute variability in petal length than versicolor and virginica. . Onward toward the whole story! . . Important: “Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone.” —John Tukey . Quantitative exploratory data analysis . We will compute useful summary statistics, which serve to concisely describe salient features of a dataset with a few numbers. . Introduction to summary statistics: The sample mean and median . $$ mean = bar{x} = frac{1}{n} sum_{i=1}^{n} x_i $$Outliers . ● Data points whose value is far greater or less than most of the rest of the data . The median . ● The middle value of a data set . Note:An outlier can significantly affect the value of the mean, but not the median . Computing means . The mean of all measurements gives an indication of the typical magnitude of a measurement. It is computed using np.mean(). . # Compute the mean: mean_length_vers mean_length_vers = np.mean(versicolor_petal_length) # Print the result with some nice formatting print(&#39;I. versicolor:&#39;, mean_length_vers, &#39;cm&#39;) . I. versicolor: 4.26 cm . Percentiles, outliers, and box plots . Computing percentiles . We will compute the percentiles of petal length of Iris versicolor. . # Specify array of percentiles: percentiles percentiles = np.array([2.5, 25, 50, 75, 97.5]) # Compute percentiles: ptiles_vers ptiles_vers = np.percentile(versicolor_petal_length, percentiles) # Print the result ptiles_vers . array([3.3 , 4. , 4.35 , 4.6 , 4.9775]) . Comparing percentiles to ECDF . To see how the percentiles relate to the ECDF, we will plot the percentiles of Iris versicolor petal lengths on the ECDF plot. . # Plot the ECDF _ = plt.plot(x_vers, y_vers, &#39;.&#39;) _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Overlay percentiles as red diamonds. _ = plt.plot(ptiles_vers, percentiles/100, marker=&#39;D&#39;, color=&#39;red&#39;, linestyle=&quot;none&quot;) # Show the plot plt.show() . Box-and-whisker plot . . Warning: Making a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine. We will Make a box plot of the iris petal lengths. . # Create box plot with Seaborn&#39;s default settings _ = sns.boxplot(data=iris_petal_lengths, x=&quot;species&quot;, y=&quot;petal length (cm)&quot;) # Label the axes _ = plt.xlabel(&quot;species&quot;) _ = plt.ylabel(&quot;petal length (cm)&quot;) # Show the plot plt.show() . Variance and standard deviation . Variance . ● The mean squared distance of the data from their mean . Tip:Variance; nformally, a measure of the spread of data&gt; $$ variance = frac{1}{n} sum_{i=1}^{n} (x_i - bar{x})^2 $$ . standard Deviation . $$ std = sqrt { frac{1}{n} sum_{i=1}^{n} (x_i - bar{x})^2} $$ Computing the variance . we will explicitly compute the variance of the petal length of Iris veriscolor, we will then use np.var() to compute it. . # Array of differences to mean: differences differences = versicolor_petal_length-np.mean(versicolor_petal_length) # Square the differences: diff_sq diff_sq = differences**2 # Compute the mean square difference: variance_explicit variance_explicit = np.mean(diff_sq) # Compute the variance using NumPy: variance_np variance_np = np.var(versicolor_petal_length) # Print the results print(variance_explicit, variance_np) . 0.21640000000000004 0.21640000000000004 . The standard deviation and the variance . the standard deviation is the square root of the variance. . # Compute the variance: variance variance = np.var(versicolor_petal_length) # Print the square root of the variance print(np.sqrt(variance)) # Print the standard deviation print(np.std(versicolor_petal_length)) . 0.4651881339845203 0.4651881339845203 . Covariance and the Pearson correlation coefficient . Covariance . ● A measure of how two quantities vary together $$ covariance = frac{1}{n} sum_{i=1}^{n} (x_i bar{x}) (y_i - bar{y}) $$ . Pearson correlation coefficient . $$ rho = Pearson correlation = frac{covariance}{(std of x) (std of y)} = frac{variability due to codependence}{independent variability} $$ Scatter plots . When we made bee swarm plots, box plots, and ECDF plots in previous exercises, we compared the petal lengths of different species of iris. But what if we want to compare two properties of a single species? This is exactly what we will do, we will make a scatter plot of the petal length and width measurements of Anderson&#39;s Iris versicolor flowers. . Important:If the flower scales (that is, it preserves its proportion as it grows), we would expect the length and width to be correlated. . versicolor_petal_width = np.array([1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1. , 1.3, 1.4, 1. , 1.5, 1. , 1.4, 1.3, 1.4, 1.5, 1. , 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1. , 1.1, 1. , 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1. , 1.3, 1.2, 1.3, 1.3, 1.1, 1.3]) . # Make a scatter plot _ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Label the axes _ = plt.xlabel(&quot;petal length, (cm)&quot;) _ = plt.ylabel(&quot;petal length, (cm)&quot;) # Show the result plt.show() . . Tip: we see some correlation. Longer petals also tend to be wider. . Computing the covariance . The covariance may be computed using the Numpy function np.cov(). For example, we have two sets of data $x$ and $y$, np.cov(x, y) returns a 2D array where entries [0,1] and [1,0] are the covariances. Entry [0,0] is the variance of the data in x, and entry [1,1] is the variance of the data in y. This 2D output array is called the covariance matrix, since it organizes the self- and covariance. . # Compute the covariance matrix: covariance_matrix covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width) # Print covariance matrix print(covariance_matrix) # Extract covariance of length and width of petals: petal_cov petal_cov = covariance_matrix[0,1] # Print the length/width covariance print(petal_cov) . [[0.22081633 0.07310204] [0.07310204 0.03910612]] 0.07310204081632653 . Computing the Pearson correlation coefficient . the Pearson correlation coefficient, also called the Pearson r, is often easier to interpret than the covariance. It is computed using the np.corrcoef() function. Like np.cov(), it takes two arrays as arguments and returns a 2D array. Entries [0,0] and [1,1] are necessarily equal to 1, and the value we are after is entry [0,1]. . We will write a function, pearson_r(x, y) that takes in two arrays and returns the Pearson correlation coefficient. We will then use this function to compute it for the petal lengths and widths of $I. versicolor$. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x,y) # Return entry [0,1] return corr_mat[0,1] # Compute Pearson correlation coefficient for I. versicolor: r r = pearson_r(versicolor_petal_length, versicolor_petal_width) # Print the result print(r) . 0.7866680885228169 . Thinking probabilistically-- Discrete variables . Statistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data. We will think probabilistically about discrete quantities: those that can only take certain values, like integers. . Probabilistic logic and statistical inference . the goal of statistical inference . To draw probabilistic conclusions about what we might expect if we collected the same data again. | To draw actionable conclusions from data. | To draw more general conclusions from relatively few data or observations. . Note: Statistical inference involves taking your data to probabilistic conclusions about what you would expect if you took even more data, and you can make decisions based on these conclusions. | . Why we use the probabilistic language in statistical inference . Probability provides a measure of uncertainty and this is crucial because we can quantify what we might expect if the data were acquired again. | Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary. We need probability to say how data might vary if acquired again. . Note: Probabilistic language is in fact very precise. It precisely describes uncertainty. | . Random number generators and hacker statistics . Hacker statistics . Uses simulated repeated measurements to compute probabilities. | . The np.random module . Suite of functions based on random number generation | np.random.random():draw a number between $0$ and $1$ ### Bernoulli trial ● An experiment that has two options, &quot;success&quot; (True) and &quot;failure&quot; (False). | . Random number seed . Integer fed into random number generating algorithm | Manually seed random number generator if you need reproducibility | Specified using np.random.seed() | . Hacker stats probabilities . Determine how to simulate data | Simulate many many times | Probability is approximately fraction of trials with the outcome of interest | . Generating random numbers using the np.random module . we&#39;ll generate lots of random numbers between zero and one, and then plot a histogram of the results. If the numbers are truly random, all bars in the histogram should be of (close to) equal height. . # Seed the random number generator np.random.seed(42) # Initialize random numbers: random_numbers random_numbers = np.empty(100000) # Generate random numbers by looping over range(100000) for i in range(100000): random_numbers[i] = np.random.random() # Plot a histogram _ = plt.hist(random_numbers, bins=316, histtype=&quot;step&quot;, density=True) _ = plt.xlabel(&quot;random numbers&quot;) _ = plt.ylabel(&quot;counts&quot;) # Show the plot plt.show() . . Note: The histogram is almost exactly flat across the top, indicating that there is equal chance that a randomly-generated number is in any of the bins of the histogram. . The np.random module and Bernoulli trials . . Tip: You can think of a Bernoulli trial as a flip of a possibly biased coin. Each coin flip has a probability $p$ of landing heads (success) and probability $1−p$ of landing tails (failure). We will write a function to perform n Bernoulli trials, perform_bernoulli_trials(n, p), which returns the number of successes out of n Bernoulli trials, each of which has probability $p$ of success. To perform each Bernoulli trial, we will use the np.random.random() function, which returns a random number between zero and one. . def perform_bernoulli_trials(n, p): &quot;&quot;&quot;Perform n Bernoulli trials with success probability p and return number of successes.&quot;&quot;&quot; # Initialize number of successes: n_success n_success = False # Perform trials for i in range(n): # Choose random number between zero and one: random_number random_number = np.random.random() # If less than p, it&#39;s a success so add one to n_success if random_number &lt; p: n_success += 1 return n_success . How many defaults might we expect? . Let&#39;s say a bank made 100 mortgage loans. It is possible that anywhere between $0$ and $100$ of the loans will be defaulted upon. We would like to know the probability of getting a given number of defaults, given that the probability of a default is $p = 0.05$. To investigate this, we will do a simulation. We will perform 100 Bernoulli trials using the perform_bernoulli_trials() function and record how many defaults we get. Here, a success is a default. . Important: Remember that the word &quot;success&quot; just means that the Bernoulli trial evaluates to True, i.e., did the loan recipient default? You will do this for another $100$ Bernoulli trials. And again and again until we have tried it $1000$ times. Then, we will plot a histogram describing the probability of the number of defaults. . # Seed random number generator np.random.seed(42) # Initialize the number of defaults: n_defaults n_defaults = np.empty(1000) # Compute the number of defaults for i in range(1000): n_defaults[i] = perform_bernoulli_trials(100, 0.05) # Plot the histogram with default number of bins; label your axes _ = plt.hist(n_defaults, density=True) _ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;) _ = plt.ylabel(&#39;probability&#39;) # Show the plot plt.show() . . Warning: This is actually not an optimal way to plot a histogram when the results are known to be integers. We will revisit this . Will the bank fail? . If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money? . # Compute ECDF: x, y x,y = ecdf(n_defaults) # Plot the ECDF with labeled axes _ = plt.plot(x,y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.xlabel(&quot;number of defaults&quot;) _ = plt.ylabel(&quot;ECDF&quot;) # Show the plot plt.show() # Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money n_lose_money = np.sum(n_defaults &gt;= 10) # Compute and print probability of losing money print(&#39;Probability of losing money =&#39;, n_lose_money / len(n_defaults)) . Probability of losing money = 0.022 . . Note: we most likely get 5/100 defaults. But we still have about a 2% chance of getting 10 or more defaults out of 100 loans. . Probability distributions and stories: The Binomial distribution . Probability mass function (PMF) . The set of probabilities of discrete outcomes | . Probability distribution . A mathematical description of outcomes | . Discrete Uniform distribution:the story- The outcome of rolling a single fair die is Discrete Uniformly distributed. . Binomial distribution:the story- The number $r$ of successes in $n$ Bernoulli trials with . probability $p$ of success, is Binomially distributed . The number $r$ of heads in $4$ coin flips with probability $0.5$ of heads, is Binomially distributed | . Sampling out of the Binomial distribution . We will compute the probability mass function for the number of defaults we would expect for $100$ loans as in the last section, but instead of simulating all of the Bernoulli trials, we will perform the sampling using np.random.binomial()1. . Note: This is identical to the calculation we did in the last set of exercises using our custom-written perform_bernoulli_trials() function, but far more computationally efficient. Given this extra efficiency, we will take $10,000$ samples instead of $1000$. After taking the samples, we will plot the CDF. This CDF that we are plotting is that of the Binomial distribution. . # Take 10,000 samples out of the binomial distribution: n_defaults n_defaults = np.random.binomial(100, 0.05, size=10000) # Compute CDF: x, y x,y = ecdf(n_defaults) # Plot the CDF with axis labels _ = plt.plot(x,y, marker=&quot;.&quot;, linestyle=&quot;-&quot;) _ = plt.xlabel(&quot;number of defaults out of 100 loans&quot;) _ = plt.ylabel(&quot;CDF&quot;) # Show the plot plt.show() . . Tip: If you know the story, using built-in algorithms to directly sample out of the distribution is much faster. . Plotting the Binomial PMF . . Warning: plotting a nice looking PMF requires a bit of matplotlib trickery that we will not go into here. we will plot the PMF of the Binomial distribution as a histogram. The trick is setting up the edges of the bins to pass to plt.hist() via the bins keyword argument. We want the bins centered on the integers. So, the edges of the bins should be $-0.5, 0.5, 1.5, 2.5, ...$ up to max(n_defaults) + 1.5. We can generate an array like this using np.arange()and then subtracting 0.5 from the array. . # Compute bin edges: bins bins = np.arange(0, max(n_defaults) + 1.5) - 0.5 # Generate histogram _ = plt.hist(n_defaults, density=True, bins=bins) # Label axes _ = plt.xlabel(&quot;number of defaults out of 100 loans&quot;) _ = plt.ylabel(&quot;probability&quot;) # Show the plot plt.show() . Poisson processes and the Poisson distribution . Poisson process . The timing of the next event is completely independent of when the previous event happened | . Examples of Poisson processes . Natural births in a given hospital | Hit on a website during a given hour | Meteor strikes | Molecular collisions in a gas | Aviation incidents | Buses in Poissonville | . Poisson distribution . The number $r$ of arrivals of a Poisson process in a given time interval with average rate of $λ$ arrivals per interval is Poisson distributed. | The number r of hits on a website in one hour with an average hit rate of 6 hits per hour is Poisson distributed. | . Poisson Distribution . Limit of the Binomial distribution for low probability of success and large number of trials. | That is, for rare events. | . Relationship between Binomial and Poisson distributions . Important:Poisson distribution is a limit of the Binomial distribution for rare events. . Tip: Poisson distribution with arrival rate equal to $np$ approximates a Binomial distribution for $n$ Bernoulli trials with probability $p$ of success (with $n$ large and $p$ small). Importantly, the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution. Let&#39;s explore these two distributions computationally. We will compute the mean and standard deviation of samples from a Poisson distribution with an arrival rate of $10$. Then, we will compute the mean and standard deviation of samples from a Binomial distribution with parameters $n$ and $p$ such that $np = 10$. . # Draw 10,000 samples out of Poisson distribution: samples_poisson samples_poisson = np.random.poisson(10, size=10000) # Print the mean and standard deviation print(&#39;Poisson: &#39;, np.mean(samples_poisson), np.std(samples_poisson)) # Specify values of n and p to consider for Binomial: n, p n = [20, 100, 1000] p = [.5, .1, .01] # Draw 10,000 samples for each n,p pair: samples_binomial for i in range(3): samples_binomial = np.random.binomial(n[i],p[i], size=10000) # Print results print(&#39;n =&#39;, n[i], &#39;Binom:&#39;, np.mean(samples_binomial), np.std(samples_binomial)) . Poisson: 10.0145 3.1713545607516043 n = 20 Binom: 10.0592 2.23523944131272 n = 100 Binom: 10.0441 2.9942536949964675 n = 1000 Binom: 10.0129 3.139639085946026 . . Note: The means are all about the same, which can be shown to be true by doing some pen-and-paper work. The standard deviation of the Binomial distribution gets closer and closer to that of the Poisson distribution as the probability $p$ gets lower and lower. . Was 2015 anomalous? . In baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. The ECDF of the number of no-hitters in a season is shown to the right. The probability distribution that would be appropriate to describe the number of no-hitters we would expect in a given season? is Both Binomial and Poisson, though Poisson is easier to model and compute. . Important: When we have rare events (low $p$, high $n$), the Binomial distribution is Poisson. This has a single parameter, the mean number of successes per time interval, in our case the mean number of no-hitters per season. 1990 and 2015 featured the most no-hitters of any season of baseball (there were seven). Given that there are on average $ frac{251}{115}$ no-hitters per season, what is the probability of having seven or more in a season? Let&#39;s find out . # Draw 10,000 samples out of Poisson distribution: n_nohitters n_nohitters = np.random.poisson(251/115, size=10000) # Compute number of samples that are seven or greater: n_large n_large = np.sum(n_nohitters &gt;= 7) # Compute probability of getting seven or more: p_large p_large = n_large/10000 # Print the result print(&#39;Probability of seven or more no-hitters:&#39;, p_large) . Probability of seven or more no-hitters: 0.0072 . . Note: The result is about $0.007$. This means that it is not that improbable to see a 7-or-more no-hitter season in a century. We have seen two in a century and a half, so it is not unreasonable. . Thinking probabilistically-- Continuous variables . It’s time to move onto continuous variables, such as those that can take on any fractional value. Many of the principles are the same, but there are some subtleties. We will be speaking the probabilistic language needed to launch into the inference techniques. . Probability density functions . Continuous variables . Quantities that can take any value, not just discrete values | . Probability density function (PDF) . Continuous analog to the PMF | Mathematical description of the relative likelihood of observing a value of a continuous variable | . Introduction to the Normal distribution . Normal distribution . Describes a continuous variable whose PDF has a single symmetric peak. | . Parameter Calculated from data . mean of a Normal distribution | ≠ | mean computed from data | . st. dev. of a Normal distribution | ≠ | standard deviation computed from data | . The Normal PDF . # Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10 samples_std1 = np.random.normal(20,1,size=100000) samples_std3 = np.random.normal(20, 3, size=100000) samples_std10 = np.random.normal(20, 10, size=100000) # Make histograms _ = plt.hist(samples_std1, density=True, histtype=&quot;step&quot;, bins=100) _ = plt.hist(samples_std3, density=True, histtype=&quot;step&quot;, bins=100) _ = plt.hist(samples_std10, density=True, histtype=&quot;step&quot;, bins=100) # Make a legend, set limits and show plot _ = plt.legend((&#39;std = 1&#39;, &#39;std = 3&#39;, &#39;std = 10&#39;)) plt.ylim(-0.01, 0.42) plt.show() . . Note: You can see how the different standard deviations result in PDFs of different widths. The peaks are all centered at the mean of 20. . The Normal CDF . # Generate CDFs x_std1, y_std1 = ecdf(samples_std1) x_std3, y_std3 = ecdf(samples_std3) x_std10, y_std10 = ecdf(samples_std10) # Plot CDFs _ = plt.plot(x_std1, y_std1, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_std3, y_std3, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_std10, y_std10, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Make a legend and show the plot _ = plt.legend((&#39;std = 1&#39;, &#39;std = 3&#39;, &#39;std = 10&#39;), loc=&#39;lower right&#39;) plt.show() . . Note: The CDFs all pass through the mean at the 50th percentile; the mean and median of a Normal distribution are equal. The width of the CDF varies with the standard deviation. . The Normal distribution: Properties and warnings . Are the Belmont Stakes results Normally distributed? . Since 1926, the Belmont Stakes is a $1.5$ mile-long race of 3-year old thoroughbred horses. Secretariat ran the fastest Belmont Stakes in history in $1973$. While that was the fastest year, 1970 was the slowest because of unusually wet and sloppy conditions. With these two outliers removed from the data set, we will compute the mean and standard deviation of the Belmont winners&#39; times. We will sample out of a Normal distribution with this mean and standard deviation using the np.random.normal() function and plot a CDF. Overlay the ECDF from the winning Belmont times 2. . belmont_no_outliers = np.array([148.51, 146.65, 148.52, 150.7 , 150.42, 150.88, 151.57, 147.54, 149.65, 148.74, 147.86, 148.75, 147.5 , 148.26, 149.71, 146.56, 151.19, 147.88, 149.16, 148.82, 148.96, 152.02, 146.82, 149.97, 146.13, 148.1 , 147.2 , 146. , 146.4 , 148.2 , 149.8 , 147. , 147.2 , 147.8 , 148.2 , 149. , 149.8 , 148.6 , 146.8 , 149.6 , 149. , 148.2 , 149.2 , 148. , 150.4 , 148.8 , 147.2 , 148.8 , 149.6 , 148.4 , 148.4 , 150.2 , 148.8 , 149.2 , 149.2 , 148.4 , 150.2 , 146.6 , 149.8 , 149. , 150.8 , 148.6 , 150.2 , 149. , 148.6 , 150.2 , 148.2 , 149.4 , 150.8 , 150.2 , 152.2 , 148.2 , 149.2 , 151. , 149.6 , 149.6 , 149.4 , 148.6 , 150. , 150.6 , 149.2 , 152.6 , 152.8 , 149.6 , 151.6 , 152.8 , 153.2 , 152.4 , 152.2 ]) . # Compute mean and standard deviation: mu, sigma mu = np.mean(belmont_no_outliers) sigma = np.std(belmont_no_outliers) # Sample out of a normal distribution with this mu and sigma: samples samples = np.random.normal(mu, sigma, size=10000) # Get the CDF of the samples and of the data x_theor, y_theor = ecdf(samples) x,y = ecdf(belmont_no_outliers) # Plot the CDFs and show the plot _ = plt.plot(x_theor, y_theor) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.xlabel(&#39;Belmont winning time (sec.)&#39;) _ = plt.ylabel(&#39;CDF&#39;) plt.show() . . Note: The theoretical CDF and the ECDF of the data suggest that the winning Belmont times are, indeed, Normally distributed. This also suggests that in the last 100 years or so, there have not been major technological or training advances that have significantly affected the speed at which horses can run this race. . What are the chances of a horse matching or beating Secretariat&#39;s record? . The probability that the winner of a given Belmont Stakes will run it as fast or faster than Secretariat assuming that the Belmont winners&#39; times are Normally distributed (with the 1970 and 1973 years removed) . # Take a million samples out of the Normal distribution: samples samples = np.random.normal(mu, sigma, size=1000000) # Compute the fraction that are faster than 144 seconds: prob prob = np.sum(samples&lt;=144)/len(samples) # Print the result print(&#39;Probability of besting Secretariat:&#39;, prob) . Probability of besting Secretariat: 0.000614 . . Note: We had to take a million samples because the probability of a fast time is very low and we had to be sure to sample enough. We get that there is only a 0.06% chance of a horse running the Belmont as fast as Secretariat. . The Exponential distribution . The waiting time between arrivals of a Poisson process is Exponentially distributed . Possible Poisson process . Nuclear incidents:- Timing of one is independent of all others $f(x; frac{1}{ beta}) = frac{1}{ beta} exp(- frac{x}{ beta})$ | . If you have a story, you can simulate it! . Sometimes, the story describing our probability distribution does not have a named distribution to go along with it. In these cases, fear not! You can always simulate it. . we looked at the rare event of no-hitters in Major League Baseball. Hitting the cycle is another rare baseball event. When a batter hits the cycle, he gets all four kinds of hits, a single, double, triple, and home run, in a single game. Like no-hitters, this can be modeled as a Poisson process, so the time between hits of the cycle are also Exponentially distributed. . How long must we wait to see both a no-hitter and then a batter hit the cycle? The idea is that we have to wait some time for the no-hitter, and then after the no-hitter, we have to wait for hitting the cycle. Stated another way, what is the total waiting time for the arrival of two different Poisson processes? The total waiting time is the time waited for the no-hitter, plus the time waited for the hitting the cycle. . Important: We will write a function to sample out of the distribution described by this story. . def successive_poisson(tau1, tau2, size=1): &quot;&quot;&quot;Compute time for arrival of 2 successive Poisson processes.&quot;&quot;&quot; # Draw samples out of first exponential distribution: t1 t1 = np.random.exponential(tau1, size=size) # Draw samples out of second exponential distribution: t2 t2 = np.random.exponential(tau2, size=size) return t1 + t2 . Distribution of no-hitters and cycles . We&#39;ll use the sampling function to compute the waiting time to observe a no-hitter and hitting of the cycle. The mean waiting time for a no-hitter is $764$ games, and the mean waiting time for hitting the cycle is $715$ games. . # Draw samples of waiting times: waiting_times waiting_times = successive_poisson(764, 715, size=100000) # Make the histogram _ = plt.hist(waiting_times, bins=100, density=True, histtype=&quot;step&quot;) # Label axes _ = plt.xlabel(&quot;Waiting times&quot;) _ = plt.ylabel(&quot;probability&quot;) # Show the plot plt.show() . Notice that the PDF is peaked, unlike the waiting time for a single Poisson process. For fun (and enlightenment), Let&#39;s also plot the CDF. . x,y = ecdf(waiting_times) _ = plt.plot(x,y) _ = plt.plot(x,y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.xlabel(&quot;Waiting times&quot;) _ = plt.ylabel(&quot;CDF&quot;) plt.show() . 1. For this exercise and all going forward, the random number generator is pre-seeded for you (with np.random.seed(42)) to save you typing that each time.↩ . 2. we scraped the data concerning the Belmont Stakes from the Belmont Wikipedia page.↩ .",
            "url": "https://victoromondi1997.github.io/blog/statistical-thinking/eda/data-science/2020/07/03/Statistical-Thinking-in-Python-(Part-1).html",
            "relUrl": "/statistical-thinking/eda/data-science/2020/07/03/Statistical-Thinking-in-Python-(Part-1).html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Write Markdown and LaTeX Math Equation in The Jupyter Notebook",
            "content": "Markdown . Headings . # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 . Heading 1 . Heading 2 . Heading 3 . Heading 4 . Heading 5 . Heading 6 . Paragraph . This is a paragraph of text. This is another paragraph of text. . This is a paragraph of text. . This is another paragraph of text. . Line breaks . This is a text. This is another text. . This is a text. This is another text. . Mark emphasis . Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes ~ . ~~Scratch this.~~ . Emphasis, aka italics, with asterisks or underscores. . Strong emphasis, aka bold, with asterisks or underscores. . Combined emphasis with asterisks and underscores. . Strikethrough uses two tildes ~ . Scratch this. . Lists . 1. Item 1 2. Item 2 ( we can type 1. and the markdown will automatically numerate them) * First Item * Nested item 1 * Nested item 2 1. Keep going 1. Yes * Second Item - First Item - Second Item . Item 1 | Item 2 ( we can type 1. and the markdown will automatically numerate them) | First Item . Nested item 1 | Nested item 2 Keep going | Yes | | . | Second Item . | First Item | Second Item | . Links and&#160;Images . &lt;!-- [Text](link) --&gt; [Link Text](https://medium.com/@ahmedazizkhelifi &quot;Optional Title&quot;) &lt;!-- ![Alt Text](image path &quot;title&quot;) --&gt; ![Alt Text](https://miro.medium.com/max/80/0*PRNVc7bjff0Jj1pm.png &quot;Optional Title&quot;) &lt;!-- [![Alt Text](image path &quot;title&quot;)](link) --&gt; [![Alt Text](https://miro.medium.com/max/80/0*PRNVc7bjff0Jj1pm.png &quot;Optional Title&quot;)](https://medium.com/@ahmedazizkhelifi) . Link Text . Horizontal Rule . Reading articles on Medium is awesome. Sure !! . Reading articles on Medium is awesome. . . Sure !! . Table . | Id | Label | Price | | |-| | | 01 | Markdown | $1600 | | 02 | is | $12 | | 03 | AWESOME | $999 | . Id Label Price . 01 | Markdown | $1600 | . 02 | is | $12 | . 03 | AWESOME | $999 | . Code and Syntax Highlighting . python def staySafe(Coronavirus) if not home: return home . python def staySafe(Coronavirus) if not home: return home . Blockquotes . &gt; This is a blockquote. &gt; &gt; This is part of the same blockquote. Quote break &gt; This is a new blockquote. . This is a blockquote. This is part of the same blockquote. . Quote break . This is a new blockquote. . LaTeX . To insert a mathematical formula we use the dollar symbol $, as follows: . Euler&#39;s identity: $ e^{i pi} + 1 = 0 $ To isolate and center the formulas and enter in math display mode, we use 2 dollars symbol: $$ ... $$ Euler&#39;s identity: $$ e^{i pi} + 1 = 0 $$ . Euler&#39;s identity: $ e^{i pi} + 1 = 0 $ . To isolate and center the formulas and enter in math display mode, we use 2 dollars symbol: $$ ... $$ . Euler&#39;s identity: $$ e^{i pi} + 1 = 0 $$ . Important Note . $$ frac{arg 1}{arg 2} x^2 e^{i pi} A_i B_{ij} sqrt[n]{arg} $$ . $$ frac{arg 1}{arg 2} x^2 e^{i pi} A_i B_{ij} sqrt[n]{arg} $$ Greek Letters: . Given : $ pi = 3.14$ , $ alpha = frac{3 pi}{4} , rad$ $$ omega = 2 pi f f = frac{c}{ lambda} lambda_0= theta^2+ delta Delta lambda = frac{1}{ lambda^2} $$ . Given : $ pi = 3.14$ , $ alpha = frac{3 pi}{4} , rad$ $$ omega = 2 pi f f = frac{c}{ lambda} lambda_0= theta^2+ delta Delta lambda = frac{1}{ lambda^2} $$ . Important Note: . |Uppercase| LaTeX |Lowercase| LaTeX | ||-||-| |$ Delta$ | Delta|$ delta$ | delta| |$ Omega$ | Omega|$ omega$ | omega| . Uppercase LaTeX Lowercase LaTeX . $ Delta$ | Delta | $ delta$ | delta | . $ Omega$ | Omega | $ omega$ | omega | . Roman Names: . $$ sin(- alpha)=- sin( alpha) arccos(x)= arcsin(u) log_n(n)=1 tan(x) = frac{ sin(x)}{ cos(x)} $$ . $$ sin(- alpha)=- sin( alpha) arccos(x)= arcsin(u) log_n(n)=1 tan(x) = frac{ sin(x)}{ cos(x)} $$ Other Symbols . Angles: . Left angle : $ langle$ Right angle : $ rangle$ Angle between two vectors u and v : $ langle vec{u}, vec{v} rangle$ $$ vec{AB} , cdot , vec{CD} =0 Rightarrow vec{AB} , perp , vec{CD}$$ ##Sets and logic $$ mathbb{N} subset mathbb{Z} subset mathbb{D} subset mathbb{Q} subset mathbb{R} subset mathbb{C}$$ . Left angle : $ langle$ . Right angle : $ rangle$ . Angle between two vectors u and v : $ langle vec{u}, vec{v} rangle$ . $$ vec{AB} , cdot , vec{CD} =0 Rightarrow vec{AB} , perp , vec{CD}$$ . Sets and&#160;logic . $$ mathbb{N} subset mathbb{Z} subset mathbb{D} subset mathbb{Q} subset mathbb{R} subset mathbb{C}$$ . Vertical curly&#160;braces: . $$ sign(x) = left { begin{array} 1 &amp; mbox{if } x in mathbf{N}^* 0 &amp; mbox{if } x = 0 -1 &amp; mbox{else.} end{array} right. $$ $$ left. begin{array} alpha^2 = sqrt5 alpha geq 0 end{array} right } alpha = 5 $$ . $$ sign(x) = left { begin{array} 1 &amp; mbox{if } x in mathbf{N}^* 0 &amp; mbox{if } x = 0 -1 &amp; mbox{else.} end{array} right. $$ . $$ left. begin{array} alpha^2 = sqrt5 alpha geq 0 end{array} right } alpha = 5 $$ Horizontal curly&#160;braces $ underbrace{}$ . Horizontal curly&#160;braces $ underbrace{}$ . $$ underbrace{ ln left( frac{5}{6} right)}_{ simeq -0.1823} &lt; overbrace{ exp (2)}^{ simeq 7.3890} $$ . Derivate . First order derivative : $$f&#39;(x)$$ K-th order derivative : $$f^{(k)}(x)$$ Partial firt order derivative : $$ frac{ partial f}{ partial x}$$ Partial k-th order derivative : $$ frac{ partial^{k} f}{ partial x^k}$$ . First order derivative : $$f&#39;(x)$$ K-th order derivative : $$f^{(k)}(x)$$ Partial firt order derivative : $$ frac{ partial f}{ partial x}$$ Partial k-th order derivative : $$ frac{ partial^{k} f}{ partial x^k}$$ . Limit $ lim$ . Limit $ lim$ . Limit at plus infinity : $$ lim_{x to + infty} f(x)$$ Limit at minus infinity : $$ lim_{x to - infty} f(x)$$ Limit at $ alpha$ : $$ lim_{x to alpha} f(x)$$ Max : $$ max_{x in [a,b]}f(x)$$ Min : $$ min_{x in [ alpha, beta]}f(x)$$ Sup : $$ sup_{x in mathbb{R}}f(x)$$ Inf : $$ inf_{x &gt; s}f(x)$$ . Limit at plus infinity : $$ lim_{x to + infty} f(x)$$ Limit at minus infinity : $$ lim_{x to - infty} f(x)$$ Limit at $ alpha$ : $$ lim_{x to alpha} f(x)$$ . Max : $$ max_{x in [a,b]}f(x)$$ Min : $$ min_{x in [ alpha, beta]}f(x)$$ Sup : $$ sup_{x in mathbb{R}}f(x)$$ Inf : $$ inf_{x &gt; s}f(x)$$ . Sum $ sum$ . Sum $ sum$ . Sum from 0 to +inf: $$ sum_{j=0}^{+ infty} A_{j}$$ Double sum: $$ sum^k_{i=1} sum^{l+1}_{j=1} ,A_i A_j$$ Taylor expansion of $e^x$: $$ e^x = sum_{k=0}^{n} , frac{x^k}{k!} + o(x^n) $$ . Sum from 0 to +inf: . $$ sum_{j=0}^{+ infty} A_{j}$$ . Double sum: $$ sum^k_{i=1} sum^{l+1}_{j=1} ,A_i A_j$$ . Taylor expansion of $e^x$: $$ e^x = sum_{k=0}^{n} , frac{x^k}{k!} + o(x^n) $$ . Product $ prod$ . Product $ prod$ . Product: $$ prod_{j=1}^k A_{ alpha_j}$$ Double product: $$ prod^k_{i=1} prod^l_{j=1} ,A_i A_j$$ . Product: $$ prod_{j=1}^k A_{ alpha_j}$$ Double product: $$ prod^k_{i=1} prod^l_{j=1} ,A_i A_j$$ . Integral : $ int$ . Integral : $ int$ . Simple integral: $$ int_{a}^b f(x)dx$$ Double integral: $$ int_{a}^b int_{c}^d f(x,y) ,dxdy$$ Triple integral: $$ iiint$$ Quadruple integral: $$ iiiint$$ Multiple integral : $$ idotsint$$ Contour integral: $$ oint$$ . Simple integral: $$ int_{a}^b f(x)dx$$ . Double integral: $$ int_{a}^b int_{c}^d f(x,y) ,dxdy$$ . Triple integral: $$ iiint$$ . Quadruple integral: $$ iiiint$$ . Multiple integral : $$ idotsint$$ . Contour integral: $$ oint$$ . Matrix . Plain: begin{matrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{matrix} Round brackets: begin{pmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{pmatrix} Curly brackets: begin{Bmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{Bmatrix} Pipes: begin{vmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{vmatrix} Double pipes begin{Vmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{Vmatrix} . Plain: . begin{matrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{matrix}Round brackets: begin{pmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{pmatrix} . Curly brackets: begin{Bmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{Bmatrix} . Pipes: begin{vmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{vmatrix} .",
            "url": "https://victoromondi1997.github.io/blog/latex/markdown/2020/07/03/Markdown-LaTeX.html",
            "relUrl": "/latex/markdown/2020/07/03/Markdown-LaTeX.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Dr. Semmelweis and the Discovery of Handwashing",
            "content": "1. Meet Dr. Ignaz Semmelweis . This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it&#39;s probably because he&#39;s thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It&#39;s the contaminated hands of the doctors delivering the babies. And they won&#39;t listen to him and wash their hands! . In this notebook, we&#39;re going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let&#39;s start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital. . # importing modules import pandas as pd import matplotlib.pyplot as plt # Read datasets/yearly_deaths_by_clinic.csv into yearly yearly = pd.read_csv(&quot;datasets/yearly_deaths_by_clinic.csv&quot;) # Print out yearly yearly . year births deaths clinic . 0 1841 | 3036 | 237 | clinic 1 | . 1 1842 | 3287 | 518 | clinic 1 | . 2 1843 | 3060 | 274 | clinic 1 | . 3 1844 | 3157 | 260 | clinic 1 | . 4 1845 | 3492 | 241 | clinic 1 | . 5 1846 | 4010 | 459 | clinic 1 | . 6 1841 | 2442 | 86 | clinic 2 | . 7 1842 | 2659 | 202 | clinic 2 | . 8 1843 | 2739 | 164 | clinic 2 | . 9 1844 | 2956 | 68 | clinic 2 | . 10 1845 | 3241 | 66 | clinic 2 | . 11 1846 | 3754 | 105 | clinic 2 | . 2. The alarming number of deaths . The table above shows the number of women giving birth at the two clinics at the Vienna General Hospital for the years 1841 to 1846. You&#39;ll notice that giving birth was very dangerous; an alarming number of women died as the result of childbirth, most of them from childbed fever. . We see this more clearly if we look at the proportion of deaths out of the number of women giving birth. Let&#39;s zoom in on the proportion of deaths at Clinic 1. . # Calculate proportion of deaths per no. births yearly[&quot;proportion_deaths&quot;] = yearly.deaths/yearly.births # Extract clinic 1 data into yearly1 and clinic 2 data into yearly2 yearly1 = yearly[yearly.clinic==&quot;clinic 1&quot;] yearly2 = yearly[yearly.clinic==&quot;clinic 2&quot;] # Print out yearly1 yearly1 . year births deaths clinic proportion_deaths . 0 1841 | 3036 | 237 | clinic 1 | 0.078063 | . 1 1842 | 3287 | 518 | clinic 1 | 0.157591 | . 2 1843 | 3060 | 274 | clinic 1 | 0.089542 | . 3 1844 | 3157 | 260 | clinic 1 | 0.082357 | . 4 1845 | 3492 | 241 | clinic 1 | 0.069015 | . 5 1846 | 4010 | 459 | clinic 1 | 0.114464 | . 3. Death at the clinics . If we now plot the proportion of deaths at both clinic 1 and clinic 2 we&#39;ll see a curious pattern... . # This makes plots appear in the notebook %matplotlib inline # Plot yearly proportion of deaths at the two clinics ax = yearly1.plot(x=&quot;year&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Clinic 1&quot;) yearly2.plot(x=&quot;year&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Clinic 2&quot;, ax=ax) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7fab04d06d30&gt; . 4. The handwashing begins . Why is the proportion of deaths constantly so much higher in Clinic 1? Semmelweis saw the same pattern and was puzzled and distressed. The only difference between the clinics was that many medical students served at Clinic 1, while mostly midwife students served at Clinic 2. While the midwives only tended to the women giving birth, the medical students also spent time in the autopsy rooms examining corpses. . Semmelweis started to suspect that something on the corpses, spread from the hands of the medical students, caused childbed fever. So in a desperate attempt to stop the high mortality rates, he decreed: Wash your hands! This was an unorthodox and controversial request, nobody in Vienna knew about bacteria at this point in time. . Let&#39;s load in monthly data from Clinic 1 to see if the handwashing had any effect. . # Read datasets/monthly_deaths.csv into monthly monthly = pd.read_csv(&quot;datasets/monthly_deaths.csv&quot;, parse_dates=[&quot;date&quot;]) # Calculate proportion of deaths per no. births monthly[&quot;proportion_deaths&quot;] = monthly.deaths/monthly.births # Print out the first rows in monthly monthly.head() . date births deaths proportion_deaths . 0 1841-01-01 | 254 | 37 | 0.145669 | . 1 1841-02-01 | 239 | 18 | 0.075314 | . 2 1841-03-01 | 277 | 12 | 0.043321 | . 3 1841-04-01 | 255 | 4 | 0.015686 | . 4 1841-05-01 | 255 | 2 | 0.007843 | . 5. The effect of handwashing . With the data loaded we can now look at the proportion of deaths over time. In the plot below we haven&#39;t marked where obligatory handwashing started, but it reduced the proportion of deaths to such a degree that you should be able to spot it! . # Plot monthly proportion of deaths ax = monthly.plot(x=&quot;date&quot;, y=&quot;proportion_deaths&quot;) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7faae5d53b38&gt; . 6. The effect of handwashing highlighted . Starting from the summer of 1847 the proportion of deaths is drastically reduced and, yes, this was when Semmelweis made handwashing obligatory. . The effect of handwashing is made even more clear if we highlight this in the graph. . # Date when handwashing was made mandatory import pandas as pd handwashing_start = pd.to_datetime(&#39;1847-06-01&#39;) # Split monthly into before and after handwashing_start before_washing = monthly[monthly.date&lt;handwashing_start] after_washing = monthly[monthly.date&gt;=handwashing_start] # Plot monthly proportion of deaths before and after handwashing ax = before_washing.plot(x=&quot;date&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Before Washing&quot;) after_washing.plot(ax=ax, x=&quot;date&quot;, y=&quot;proportion_deaths&quot;, label=&quot;After Washing&quot;) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7faae5c44940&gt; . 7. More handwashing, fewer deaths? . Again, the graph shows that handwashing had a huge effect. How much did it reduce the monthly proportion of deaths on average? . # Difference in mean monthly proportion of deaths due to handwashing before_proportion = before_washing.proportion_deaths after_proportion = after_washing.proportion_deaths mean_diff = after_proportion.mean() - before_proportion.mean() mean_diff . -0.08395660751183336 . 8. A Bootstrap analysis of Semmelweis handwashing data . It reduced the proportion of deaths by around 8 percentage points! From 10% on average to just 2% (which is still a high number by modern standards). . To get a feeling for the uncertainty around how much handwashing reduces mortalities we could look at a confidence interval (here calculated using the bootstrap method). . # A bootstrap analysis of the reduction of deaths due to handwashing boot_mean_diff = [] for i in range(3000): boot_before = before_proportion.sample(frac=1, replace=True) boot_after = after_proportion.sample(frac=1, replace=True) boot_mean_diff.append( boot_after.mean() - boot_before.mean() ) # Calculating a 95% confidence interval from boot_mean_diff confidence_interval = pd.Series(boot_mean_diff).quantile([.025, .975]) confidence_interval . 0.025 -0.102262 0.975 -0.067096 dtype: float64 . 9. The fate of Dr. Semmelweis . So handwashing reduced the proportion of deaths by between 6.7 and 10 percentage points, according to a 95% confidence interval. All in all, it would seem that Semmelweis had solid evidence that handwashing was a simple but highly effective procedure that could save many lives. . The tragedy is that, despite the evidence, Semmelweis&#39; theory — that childbed fever was caused by some &quot;substance&quot; (what we today know as bacteria) from autopsy room corpses — was ridiculed by contemporary scientists. The medical community largely rejected his discovery and in 1849 he was forced to leave the Vienna General Hospital for good. . One reason for this was that statistics and statistical arguments were uncommon in medical science in the 1800s. Semmelweis only published his data as long tables of raw data, but he didn&#39;t show any graphs nor confidence intervals. If he would have had access to the analysis we&#39;ve just put together he might have been more successful in getting the Viennese doctors to wash their hands. . # The data Semmelweis collected points to that: doctors_should_wash_their_hands = True .",
            "url": "https://victoromondi1997.github.io/blog/ignaz/data-analysis/handwashing/2020/07/01/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "relUrl": "/ignaz/data-analysis/handwashing/2020/07/01/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Analyzing Police Activity with Pandas",
            "content": "Preparing the data for analysis . Before beginning our analysis, it is critical that we first examine and clean the dataset, to make working with it a more efficient process. We will fixing data types, handle missing values, and dropping columns and rows while exploring the Stanford Open Policing Project dataset. . Stanford Open Policing Project dataset . Examining the dataset . We&#39;ll be analyzing a dataset of traffic stops in Rhode Island that was collected by the Stanford Open Policing Project. . Before beginning our analysis, it&#39;s important that we familiarize yourself with the dataset. We read the dataset into pandas, examine the first few rows, and then count the number of missing values. . Libraries . import pandas as pd import matplotlib.pyplot as plt from pandas.api.types import CategoricalDtype . # Read &#39;police.csv&#39; into a DataFrame named ri ri = pd.read_csv(&quot;../datasets/police.csv&quot;) # Examine the head of the DataFrame display(ri.head()) # Count the number of missing values in each column ri.isnull().sum() . state stop_date stop_time county_name driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district . 0 RI | 2005-01-04 | 12:55 | NaN | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 1 RI | 2005-01-23 | 23:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | . 2 RI | 2005-02-17 | 04:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 3 RI | 2005-02-20 | 17:15 | NaN | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | . 4 RI | 2005-02-24 | 01:20 | NaN | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | . state 0 stop_date 0 stop_time 0 county_name 91741 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . It looks like most of the columns have at least some missing values. We&#39;ll figure out how to handle these values in the next. . Dropping columns . We&#39;ll drop the county_name column because it only contains missing values, and we&#39;ll drop the state column because all of the traffic stops took place in one state (Rhode Island). . # Examine the shape of the DataFrame print(ri.shape) # Drop the &#39;county_name&#39; and &#39;state&#39; columns ri.drop([&quot;county_name&quot;, &quot;state&quot;], axis=&#39;columns&#39;, inplace=True) # Examine the shape of the DataFrame (again) print(ri.shape) . (91741, 15) (91741, 13) . We&#39;ll continue to remove unnecessary data from the DataFrame . Dropping rows . the driver_gender column will be critical to many of our analyses. Because only a small fraction of rows are missing driver_gender, we&#39;ll drop those rows from the dataset. . # Count the number of missing values in each column display(ri.isnull().sum()) # Drop all rows that are missing &#39;driver_gender&#39; ri.dropna(subset=[&quot;driver_gender&quot;], inplace=True) # Count the number of missing values in each column (again) display(ri.isnull().sum()) # Examine the shape of the DataFrame ri.shape . stop_date 0 stop_time 0 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . stop_date 0 stop_time 0 driver_gender 0 driver_race 0 violation_raw 0 violation 0 search_conducted 0 search_type 83229 stop_outcome 0 is_arrested 0 stop_duration 0 drugs_related_stop 0 district 0 dtype: int64 . (86536, 13) . We dropped around 5,000 rows, which is a small fraction of the dataset, and now only one column remains with any missing values. . Using proper data types . Finding an incorrect data type . ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested object stop_duration object drugs_related_stop bool district object dtype: object . stop_date: should be datetime | stop_time: should be datetime | driver_gender: should be category | driver_race: should be category | violation_raw: should be category | violation: should be category | district: should be category | is_arrested: should be bool | . We&#39;ll fix the data type of the is_arrested column . # Examine the head of the &#39;is_arrested&#39; column display(ri.is_arrested.head()) # Change the data type of &#39;is_arrested&#39; to &#39;bool&#39; ri[&#39;is_arrested&#39;] = ri.is_arrested.astype(&#39;bool&#39;) # Check the data type of &#39;is_arrested&#39; ri.is_arrested.dtype . 0 False 1 False 2 False 3 True 4 False Name: is_arrested, dtype: object . dtype(&#39;bool&#39;) . Creating a DatetimeIndex . Combining object columns . Currently, the date and time of each traffic stop are stored in separate object columns: stop_date and stop_time. We&#39;ll combine these two columns into a single column, and then convert it to datetime format. . ri[&#39;stop_date_time&#39;] = pd.to_datetime(ri.stop_date.str.replace(&quot;/&quot;, &quot;-&quot;).str.cat(ri.stop_time, sep=&quot; &quot;)) ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested bool stop_duration object drugs_related_stop bool district object stop_date_time datetime64[ns] dtype: object . Setting the index . # Set &#39;stop_datetime&#39; as the index ri.set_index(&quot;stop_date_time&quot;, inplace=True) # Examine the index display(ri.index) # Examine the columns ri.columns . DatetimeIndex([&#39;2005-01-04 12:55:00&#39;, &#39;2005-01-23 23:15:00&#39;, &#39;2005-02-17 04:15:00&#39;, &#39;2005-02-20 17:15:00&#39;, &#39;2005-02-24 01:20:00&#39;, &#39;2005-03-14 10:00:00&#39;, &#39;2005-03-29 21:55:00&#39;, &#39;2005-04-04 21:25:00&#39;, &#39;2005-07-14 11:20:00&#39;, &#39;2005-07-14 19:55:00&#39;, ... &#39;2015-12-31 13:23:00&#39;, &#39;2015-12-31 18:59:00&#39;, &#39;2015-12-31 19:13:00&#39;, &#39;2015-12-31 20:20:00&#39;, &#39;2015-12-31 20:50:00&#39;, &#39;2015-12-31 21:21:00&#39;, &#39;2015-12-31 21:59:00&#39;, &#39;2015-12-31 22:04:00&#39;, &#39;2015-12-31 22:09:00&#39;, &#39;2015-12-31 22:47:00&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;stop_date_time&#39;, length=86536, freq=None) . Index([&#39;stop_date&#39;, &#39;stop_time&#39;, &#39;driver_gender&#39;, &#39;driver_race&#39;, &#39;violation_raw&#39;, &#39;violation&#39;, &#39;search_conducted&#39;, &#39;search_type&#39;, &#39;stop_outcome&#39;, &#39;is_arrested&#39;, &#39;stop_duration&#39;, &#39;drugs_related_stop&#39;, &#39;district&#39;], dtype=&#39;object&#39;) . Exploring the relationship between gender and policing . Does the gender of a driver have an impact on police behavior during a traffic stop? We will explore that question while doing filtering, grouping, method chaining, Boolean math, string methods, and more! . Do the genders commit different violations? . Examining traffic violations . Before comparing the violations being committed by each gender, we should examine the violations committed by all drivers to get a baseline understanding of the data. . We&#39;ll count the unique values in the violation column, and then separately express those counts as proportions. . # Count the unique values in &#39;violation&#39; display(ri.violation.value_counts()) # Express the counts as proportions ri.violation.value_counts(normalize=True) . Speeding 48423 Moving violation 16224 Equipment 10921 Other 4409 Registration/plates 3703 Seat belt 2856 Name: violation, dtype: int64 . Speeding 0.559571 Moving violation 0.187483 Equipment 0.126202 Other 0.050950 Registration/plates 0.042791 Seat belt 0.033004 Name: violation, dtype: float64 . More than half of all violations are for speeding, followed by other moving violations and equipment violations. . Comparing violations by gender . The question we&#39;re trying to answer is whether male and female drivers tend to commit different types of traffic violations. . We&#39;ll first create a DataFrame for each gender, and then analyze the violations in each DataFrame separately. . # Create a DataFrame of female drivers female = ri[ri.driver_gender==&quot;F&quot;] # Create a DataFrame of male drivers male = ri[ri.driver_gender==&quot;M&quot;] # Compute the violations by female drivers (as proportions) display(female.violation.value_counts(normalize=True)) # Compute the violations by male drivers (as proportions) male.violation.value_counts(normalize=True) . Speeding 0.658114 Moving violation 0.138218 Equipment 0.105199 Registration/plates 0.044418 Other 0.029738 Seat belt 0.024312 Name: violation, dtype: float64 . Speeding 0.522243 Moving violation 0.206144 Equipment 0.134158 Other 0.058985 Registration/plates 0.042175 Seat belt 0.036296 Name: violation, dtype: float64 . About two-thirds of female traffic stops are for speeding, whereas stops of males are more balanced among the six categories. This doesn&#39;t mean that females speed more often than males, however, since we didn&#39;t take into account the number of stops or drivers. . Does gender affect who gets a ticket for speeding? . Comparing speeding outcomes by gender . When a driver is pulled over for speeding, many people believe that gender has an impact on whether the driver will receive a ticket or a warning. Can we find evidence of this in the dataset? . First, we&#39;ll create two DataFrames of drivers who were stopped for speeding: one containing females and the other containing males. . Then, for each gender, we&#39;ll use the stop_outcome column to calculate what percentage of stops resulted in a &quot;Citation&quot; (meaning a ticket) versus a &quot;Warning&quot;. . # Create a DataFrame of female drivers stopped for speeding female_and_speeding = ri[(ri.driver_gender==&quot;F&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Create a DataFrame of male drivers stopped for speeding male_and_speeding = ri[(ri.driver_gender==&quot;M&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Compute the stop outcomes for female drivers (as proportions) display(female_and_speeding.stop_outcome.value_counts(normalize=True)) # Compute the stop outcomes for male drivers (as proportions) male_and_speeding.stop_outcome.value_counts(normalize=True) . Citation 0.952192 Warning 0.040074 Arrest Driver 0.005752 N/D 0.000959 Arrest Passenger 0.000639 No Action 0.000383 Name: stop_outcome, dtype: float64 . Citation 0.944595 Warning 0.036184 Arrest Driver 0.015895 Arrest Passenger 0.001281 No Action 0.001068 N/D 0.000976 Name: stop_outcome, dtype: float64 . The numbers are similar for males and females: about 95% of stops for speeding result in a ticket. Thus, the data fails to show that gender has an impact on who gets a ticket for speeding. . Does gender affect whose vehicle is searched? . Calculating the search rate . During a traffic stop, the police officer sometimes conducts a search of the vehicle. We&#39;ll calculate the percentage of all stops in the ri DataFrame that result in a vehicle search, also known as the search rate. . # Check the data type of &#39;search_conducted&#39; print(ri.search_conducted.dtype) # Calculate the search rate by counting the values display(ri.search_conducted.value_counts(normalize=True)) # Calculate the search rate by taking the mean ri.search_conducted.mean() . bool . False 0.961785 True 0.038215 Name: search_conducted, dtype: float64 . 0.0382153092354627 . It looks like the search rate is about 3.8%. Next, we&#39;ll examine whether the search rate varies by driver gender. . Comparing search rates by gender . We&#39;ll compare the rates at which female and male drivers are searched during a traffic stop. Remember that the vehicle search rate across all stops is about 3.8%. . First, we&#39;ll filter the DataFrame by gender and calculate the search rate for each group separately. Then, we&#39;ll perform the same calculation for both genders at once using a .groupby(). . ri[ri.driver_gender==&quot;F&quot;].search_conducted.mean() . 0.019180617481282074 . ri[ri.driver_gender==&quot;M&quot;].search_conducted.mean() . 0.04542557598546892 . ri.groupby(&quot;driver_gender&quot;).search_conducted.mean() . driver_gender F 0.019181 M 0.045426 Name: search_conducted, dtype: float64 . Male drivers are searched more than twice as often as female drivers. Why might this be? . Adding a second factor to the analysis . Even though the search rate for males is much higher than for females, it&#39;s possible that the difference is mostly due to a second factor. . For example, we might hypothesize that the search rate varies by violation type, and the difference in search rate between males and females is because they tend to commit different violations. . we can test this hypothesis by examining the search rate for each combination of gender and violation. If the hypothesis was true, out would find that males and females are searched at about the same rate for each violation. Let&#39;s find out below if that&#39;s the case! . # Calculate the search rate for each combination of gender and violation ri.groupby([&quot;driver_gender&quot;, &quot;violation&quot;]).search_conducted.mean() . driver_gender violation F Equipment 0.039984 Moving violation 0.039257 Other 0.041018 Registration/plates 0.054924 Seat belt 0.017301 Speeding 0.008309 M Equipment 0.071496 Moving violation 0.061524 Other 0.046191 Registration/plates 0.108802 Seat belt 0.035119 Speeding 0.027885 Name: search_conducted, dtype: float64 . ri.groupby([&quot;violation&quot;, &quot;driver_gender&quot;]).search_conducted.mean() . violation driver_gender Equipment F 0.039984 M 0.071496 Moving violation F 0.039257 M 0.061524 Other F 0.041018 M 0.046191 Registration/plates F 0.054924 M 0.108802 Seat belt F 0.017301 M 0.035119 Speeding F 0.008309 M 0.027885 Name: search_conducted, dtype: float64 . For all types of violations, the search rate is higher for males than for females, disproving our hypothesis. . Does gender affect who is frisked during a search? . Counting protective frisks . During a vehicle search, the police officer may pat down the driver to check if they have a weapon. This is known as a &quot;protective frisk.&quot; . We&#39;ll first check to see how many times &quot;Protective Frisk&quot; was the only search type. Then, we&#39;ll use a string method to locate all instances in which the driver was frisked. . # Count the &#39;search_type&#39; values display(ri.search_type.value_counts()) # Check if &#39;search_type&#39; contains the string &#39;Protective Frisk&#39; ri[&#39;frisk&#39;] = ri.search_type.str.contains(&#39;Protective Frisk&#39;, na=False) # Check the data type of &#39;frisk&#39; print(ri.frisk.dtype) # Take the sum of &#39;frisk&#39; print(ri.frisk.sum()) . Incident to Arrest 1290 Probable Cause 924 Inventory 219 Reasonable Suspicion 214 Protective Frisk 164 Incident to Arrest,Inventory 123 Incident to Arrest,Probable Cause 100 Probable Cause,Reasonable Suspicion 54 Probable Cause,Protective Frisk 35 Incident to Arrest,Inventory,Probable Cause 35 Incident to Arrest,Protective Frisk 33 Inventory,Probable Cause 25 Protective Frisk,Reasonable Suspicion 19 Incident to Arrest,Inventory,Protective Frisk 18 Incident to Arrest,Probable Cause,Protective Frisk 13 Inventory,Protective Frisk 12 Incident to Arrest,Reasonable Suspicion 8 Probable Cause,Protective Frisk,Reasonable Suspicion 5 Incident to Arrest,Probable Cause,Reasonable Suspicion 5 Incident to Arrest,Inventory,Reasonable Suspicion 4 Incident to Arrest,Protective Frisk,Reasonable Suspicion 2 Inventory,Reasonable Suspicion 2 Inventory,Probable Cause,Protective Frisk 1 Inventory,Probable Cause,Reasonable Suspicion 1 Inventory,Protective Frisk,Reasonable Suspicion 1 Name: search_type, dtype: int64 . bool 303 . It looks like there were 303 drivers who were frisked. Next, we&#39;ll examine whether gender affects who is frisked. . Comparing frisk rates by gender . We&#39;ll compare the rates at which female and male drivers are frisked during a search. Are males frisked more often than females, perhaps because police officers consider them to be higher risk? . Before doing any calculations, it&#39;s important to filter the DataFrame to only include the relevant subset of data, namely stops in which a search was conducted. . # Create a DataFrame of stops in which a search was conducted searched = ri[ri.search_conducted == True] # Calculate the overall frisk rate by taking the mean of &#39;frisk&#39; print(searched.frisk.mean()) # Calculate the frisk rate for each gender searched.groupby(&quot;driver_gender&quot;).frisk.mean() . 0.09162382824312065 . driver_gender F 0.074561 M 0.094353 Name: frisk, dtype: float64 . The frisk rate is higher for males than for females, though we can&#39;t conclude that this difference is caused by the driver&#39;s gender. . Visual exploratory data analysis . Are you more likely to get arrested at a certain time of day? Are drug-related stops on the rise? We will answer these and other questions by analyzing the dataset visually, since plots can help us to understand trends in a way that examining the raw data cannot. . Does time of the day affect arrest rate? . Calculating the hourly arrest rate . When a police officer stops a driver, a small percentage of those stops ends in an arrest. This is known as the arrest rate. We&#39;ll find out whether the arrest rate varies by time of day. . First, we&#39;ll calculate the arrest rate across all stops in the ri DataFrame. Then, we&#39;ll calculate the hourly arrest rate by using the hour attribute of the index. The hour ranges from 0 to 23, in which: . 0 = midnight | 12 = noon | 23 = 11 PM | . # Calculate the overall arrest rate print(ri.is_arrested.mean()) # Calculate the hourly arrest rate # Save the hourly arrest rate hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean() hourly_arrest_rate . 0.0355690117407784 . stop_date_time 0 0.051431 1 0.064932 2 0.060798 3 0.060549 4 0.048000 5 0.042781 6 0.013813 7 0.013032 8 0.021854 9 0.025206 10 0.028213 11 0.028897 12 0.037399 13 0.030776 14 0.030605 15 0.030679 16 0.035281 17 0.040619 18 0.038204 19 0.032245 20 0.038107 21 0.064541 22 0.048666 23 0.047592 Name: is_arrested, dtype: float64 . Next we&#39;ll plot the data so that you can visually examine the arrest rate trends. . Plotting the hourly arrest rate . We&#39;ll create a line plot from the hourly_arrest_rate object. . Important: A line plot is appropriate in this case because you&#8217;re showing how a quantity changes over time. This plot should help us to spot some trends that may not have been obvious when examining the raw numbers! . # Create a line plot of &#39;hourly_arrest_rate&#39; hourly_arrest_rate.plot() # Add the xlabel, ylabel, and title plt.xlabel(&quot;Hour&quot;) plt.ylabel(&quot;Arrest Rate&quot;) plt.title(&quot;Arrest Rate by Time of Day&quot;) # Display the plot plt.show() . The arrest rate has a significant spike overnight, and then dips in the early morning hours. . Are drug-related stops on the rise? . Plotting drug-related stops . In a small portion of traffic stops, drugs are found in the vehicle during a search. In this exercise, you&#39;ll assess whether these drug-related stops are becoming more common over time. . The Boolean column drugs_related_stop indicates whether drugs were found during a given stop. We&#39;ll calculate the annual drug rate by resampling this column, and then we&#39;ll use a line plot to visualize how the rate has changed over time. . # Calculate the annual rate of drug-related stops # Save the annual rate of drug-related stops annual_drug_rate = ri.drugs_related_stop.resample(&quot;A&quot;).mean() display(annual_drug_rate) # Create a line plot of &#39;annual_drug_rate&#39; annual_drug_rate.plot() # Display the plot plt.show() . stop_date_time 2005-12-31 0.006501 2006-12-31 0.007258 2007-12-31 0.007970 2008-12-31 0.007505 2009-12-31 0.009889 2010-12-31 0.010081 2011-12-31 0.009731 2012-12-31 0.009921 2013-12-31 0.013094 2014-12-31 0.013826 2015-12-31 0.012266 Freq: A-DEC, Name: drugs_related_stop, dtype: float64 . The rate of drug-related stops nearly doubled over the course of 10 years. Why might that be the case? . Comparing drug and search rates . The rate of drug-related stops increased significantly between 2005 and 2015. We might hypothesize that the rate of vehicle searches was also increasing, which would have led to an increase in drug-related stops even if more drivers were not carrying drugs. . We can test this hypothesis by calculating the annual search rate, and then plotting it against the annual drug rate. If the hypothesis is true, then we&#39;ll see both rates increasing over time. . # Calculate and save the annual search rate annual_search_rate = ri.search_conducted.resample(&quot;A&quot;).mean() # Concatenate &#39;annual_drug_rate&#39; and &#39;annual_search_rate&#39; annual = pd.concat([annual_drug_rate, annual_search_rate], axis=&quot;columns&quot;) # Create subplots from &#39;annual&#39; annual.plot(subplots=True) # Display the subplots plt.show() . The rate of drug-related stops increased even though the search rate decreased, disproving our hypothesis. . What violations are caught in each district? . Tallying violations by district . The state of Rhode Island is broken into six police districts, also known as zones. How do the zones compare in terms of what violations are caught by police? . We&#39;ll create a frequency table to determine how many violations of each type took place in each of the six zones. Then, we&#39;ll filter the table to focus on the &quot;K&quot; zones, which we&#39;ll examine further. . # Create a frequency table of districts and violations # Save the frequency table as &#39;all_zones&#39; all_zones = pd.crosstab(ri.district, ri.violation) display(all_zones) # Select rows &#39;Zone K1&#39; through &#39;Zone K3&#39; # Save the smaller table as &#39;k_zones&#39; k_zones = all_zones.loc[&quot;Zone K1&quot;:&quot;Zone K3&quot;] k_zones . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . Zone X1 296 | 671 | 143 | 38 | 74 | 1119 | . Zone X3 2049 | 3086 | 769 | 671 | 820 | 8779 | . Zone X4 3541 | 5353 | 1560 | 1411 | 843 | 9795 | . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . We&#39;ll plot the violations so that you can compare these districts. . Plotting violations by district . Now that we&#39;ve created a frequency table focused on the &quot;K&quot; zones, we&#39;ll visualize the data to help us compare what violations are being caught in each zone. . First we&#39;ll create a bar plot, which is an appropriate plot type since we&#39;re comparing categorical data. Then we&#39;ll create a stacked bar plot in order to get a slightly different look at the data. . # Create a bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;) # Display the plot plt.show() . # Create a stacked bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;, stacked=True) # Display the plot plt.show() . The vast majority of traffic stops in Zone K1 are for speeding, and Zones K2 and K3 are remarkably similar to one another in terms of violations. . How long might you be stopped for a violation? . Converting stop durations to numbers . In the traffic stops dataset, the stop_duration column tells us approximately how long the driver was detained by the officer. Unfortunately, the durations are stored as strings, such as &#39;0-15 Min&#39;. How can we make this data easier to analyze? . We&#39;ll convert the stop durations to integers. Because the precise durations are not available, we&#39;ll have to estimate the numbers using reasonable values: . Convert &#39;0-15 Min&#39; to 8 | Convert &#39;16-30 Min&#39; to 23 | Convert &#39;30+ Min&#39; to 45 | . # Create a dictionary that maps strings to integers mapping = {&quot;0-15 Min&quot;:8, &#39;16-30 Min&#39;:23, &#39;30+ Min&#39;:45} # Convert the &#39;stop_duration&#39; strings to integers using the &#39;mapping&#39; ri[&#39;stop_minutes&#39;] = ri.stop_duration.map(mapping) # Print the unique values in &#39;stop_minutes&#39; ri.stop_minutes.unique() . array([ 8, 23, 45], dtype=int64) . Next we&#39;ll analyze the stop length for each type of violation. . Plotting stop length . If you were stopped for a particular violation, how long might you expect to be detained? . We&#39;ll visualize the average length of time drivers are stopped for each type of violation. Rather than using the violation column we&#39;ll use violation_raw since it contains more detailed descriptions of the violations. . # Calculate the mean &#39;stop_minutes&#39; for each value in &#39;violation_raw&#39; # Save the resulting Series as &#39;stop_length&#39; stop_length = ri.groupby(&quot;violation_raw&quot;).stop_minutes.mean() display(stop_length) # Sort &#39;stop_length&#39; by its values and create a horizontal bar plot stop_length.sort_values().plot(kind=&quot;barh&quot;) # Display the plot plt.show() . violation_raw APB 17.967033 Call for Service 22.124371 Equipment/Inspection Violation 11.445655 Motorist Assist/Courtesy 17.741463 Other Traffic Violation 13.844490 Registration Violation 13.736970 Seatbelt Violation 9.662815 Special Detail/Directed Patrol 15.123632 Speeding 10.581562 Suspicious Person 14.910714 Violation of City/Town Ordinance 13.254144 Warrant 24.055556 Name: stop_minutes, dtype: float64 . Analyzing the effect of weather on policing . We will use a second dataset to explore the impact of weather conditions on police behavior during traffic stops. We will be merging and reshaping datasets, assessing whether a data source is trustworthy, working with categorical data, and other advanced skills. . Exploring the weather dataset . Plotting the temperature . We&#39;ll examine the temperature columns from the weather dataset to assess whether the data seems trustworthy. First we&#39;ll print the summary statistics, and then you&#39;ll visualize the data using a box plot. . # Read &#39;weather.csv&#39; into a DataFrame named &#39;weather&#39; weather = pd.read_csv(&quot;../datasets/weather.csv&quot;) display(weather.head()) # Describe the temperature columns display(weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].describe().T) # Create a box plot of the temperature columns weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].plot(kind=&#39;box&#39;) # Display the plot plt.show() . STATION DATE TAVG TMIN TMAX AWND WSF2 WT01 WT02 WT03 ... WT11 WT13 WT14 WT15 WT16 WT17 WT18 WT19 WT21 WT22 . 0 USW00014765 | 2005-01-01 | 44.0 | 35 | 53 | 8.95 | 25.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 USW00014765 | 2005-01-02 | 36.0 | 28 | 44 | 9.40 | 14.1 | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 2 USW00014765 | 2005-01-03 | 49.0 | 44 | 53 | 6.93 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 3 USW00014765 | 2005-01-04 | 42.0 | 39 | 45 | 6.93 | 16.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | 1.0 | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 4 USW00014765 | 2005-01-05 | 36.0 | 28 | 43 | 7.83 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 5 rows × 27 columns . count mean std min 25% 50% 75% max . TMIN 4017.0 | 43.484441 | 17.020298 | -5.0 | 30.0 | 44.0 | 58.0 | 77.0 | . TAVG 1217.0 | 52.493016 | 17.830714 | 6.0 | 39.0 | 54.0 | 68.0 | 86.0 | . TMAX 4017.0 | 61.268608 | 18.199517 | 15.0 | 47.0 | 62.0 | 77.0 | 102.0 | . The temperature data looks good so far: the TAVG values are in between TMIN and TMAX, and the measurements and ranges seem reasonable. . Plotting the temperature difference . We&#39;ll continue to assess whether the dataset seems trustworthy by plotting the difference between the maximum and minimum temperatures. . # Create a &#39;TDIFF&#39; column that represents temperature difference weather[&quot;TDIFF&quot;] = weather.TMAX - weather.TMIN # Describe the &#39;TDIFF&#39; column display(weather.TDIFF.describe()) # Create a histogram with 20 bins to visualize &#39;TDIFF&#39; weather.TDIFF.plot(kind=&quot;hist&quot;, bins=20) # Display the plot plt.show() . count 4017.000000 mean 17.784167 std 6.350720 min 2.000000 25% 14.000000 50% 18.000000 75% 22.000000 max 43.000000 Name: TDIFF, dtype: float64 . The TDIFF column has no negative values and its distribution is approximately normal, both of which are signs that the data is trustworthy. . Categorizing the weather . Counting bad weather conditions . The weather DataFrame contains 20 columns that start with &#39;WT&#39;, each of which represents a bad weather condition. For example: . WT05 indicates &quot;Hail&quot; | WT11 indicates &quot;High or damaging winds&quot; | WT17 indicates &quot;Freezing rain&quot; | . For every row in the dataset, each WT column contains either a 1 (meaning the condition was present that day) or NaN (meaning the condition was not present). . We&#39;ll quantify &quot;how bad&quot; the weather was each day by counting the number of 1 values in each row. . # Copy &#39;WT01&#39; through &#39;WT22&#39; to a new DataFrame WT = weather.loc[:, &quot;WT01&quot;:&quot;WT22&quot;] # Calculate the sum of each row in &#39;WT&#39; weather[&#39;bad_conditions&#39;] = WT.sum(axis=&quot;columns&quot;) # Replace missing values in &#39;bad_conditions&#39; with &#39;0&#39; weather[&#39;bad_conditions&#39;] = weather.bad_conditions.fillna(0).astype(&#39;int&#39;) # Create a histogram to visualize &#39;bad_conditions&#39; weather.bad_conditions.plot(kind=&quot;hist&quot;) # Display the plot plt.show() . It looks like many days didn&#39;t have any bad weather conditions, and only a small portion of days had more than four bad weather conditions. . Rating the weather conditions . We counted the number of bad weather conditions each day. We&#39;ll use the counts to create a rating system for the weather. . The counts range from 0 to 9, and should be converted to ratings as follows: . Convert 0 to &#39;good&#39; | Convert 1 through 4 to &#39;bad&#39; | Convert 5 through 9 to &#39;worse&#39; | . # Count the unique values in &#39;bad_conditions&#39; and sort the index display(weather.bad_conditions.value_counts().sort_index()) # Create a dictionary that maps integers to strings mapping = {0:&#39;good&#39;, 1:&#39;bad&#39;, 2:&#39;bad&#39;, 3:&#39;bad&#39;, 4:&#39;bad&#39;, 5:&#39;worse&#39;, 6:&#39;worse&#39;, 7:&#39;worse&#39;, 8:&#39;worse&#39;, 9:&#39;worse&#39;} # Convert the &#39;bad_conditions&#39; integers to strings using the &#39;mapping&#39; weather[&#39;rating&#39;] = weather.bad_conditions.map(mapping) # Count the unique values in &#39;rating&#39; weather.rating.value_counts() . 0 1749 1 613 2 367 3 380 4 476 5 282 6 101 7 41 8 4 9 4 Name: bad_conditions, dtype: int64 . bad 1836 good 1749 worse 432 Name: rating, dtype: int64 . Changing the data type to category . Since the rating column only has a few possible values, we&#39;ll change its data type to category in order to store the data more efficiently. we&#39;ll also specify a logical order for the categories, which will be useful for future work. . # Create a list of weather ratings in logical order cats = [&#39;good&#39;, &#39;bad&#39;, &#39;worse&#39;] # Change the data type of &#39;rating&#39; to category weather[&#39;rating&#39;] = weather.rating.astype(CategoricalDtype(ordered=True, categories=cats)) # Examine the head of &#39;rating&#39; weather.rating.head() . 0 bad 1 bad 2 bad 3 bad 4 bad Name: rating, dtype: category Categories (3, object): [good &lt; bad &lt; worse] . We&#39;ll use the rating column in future exercises to analyze the effects of weather on police behavior. . Merging datasets . Preparing the DataFrames . We&#39;ll prepare the traffic stop and weather rating DataFrames so that they&#39;re ready to be merged: . With the ri DataFrame, we&#39;ll move the stop_datetime index to a column since the index will be lost during the merge. | With the weather DataFrame, we&#39;ll select the DATE and rating columns and put them in a new DataFrame. | . # Reset the index of &#39;ri&#39; ri.reset_index(inplace=True) # Examine the head of &#39;ri&#39; display(ri.head()) # Create a DataFrame from the &#39;DATE&#39; and &#39;rating&#39; columns weather_rating = weather[[&quot;DATE&quot;, &quot;rating&quot;]] # Examine the head of &#39;weather_rating&#39; weather_rating.head() . stop_date_time stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes . 0 2005-01-04 12:55:00 | 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 1 2005-01-23 23:15:00 | 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | . 2 2005-02-17 04:15:00 | 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 3 2005-02-20 17:15:00 | 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | . 4 2005-02-24 01:20:00 | 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | . DATE rating . 0 2005-01-01 | bad | . 1 2005-01-02 | bad | . 2 2005-01-03 | bad | . 3 2005-01-04 | bad | . 4 2005-01-05 | bad | . The ri and weather_rating DataFrames are now ready to be merged. . Merging the DataFrames . We&#39;ll merge the ri and weather_rating DataFrames into a new DataFrame, ri_weather. . The DataFrames will be joined using the stop_date column from ri and the DATE column from weather_rating. Thankfully the date formatting matches exactly, which is not always the case! . Once the merge is complete, we&#39;ll set stop_datetime as the index . # Examine the shape of &#39;ri&#39; print(ri.shape) # Merge &#39;ri&#39; and &#39;weather_rating&#39; using a left join ri_weather = pd.merge(left=ri, right=weather_rating, left_on=&#39;stop_date&#39;, right_on=&#39;DATE&#39;, how=&#39;left&#39;) # Examine the shape of &#39;ri_weather&#39; print(ri_weather.shape) # Set &#39;stop_datetime&#39; as the index of &#39;ri_weather&#39; ri_weather.set_index(&#39;stop_date_time&#39;, inplace=True) ri_weather.head() . (86536, 16) (86536, 18) . stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes DATE rating . stop_date_time . 2005-01-04 12:55:00 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-01-04 | bad | . 2005-01-23 23:15:00 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | 2005-01-23 | worse | . 2005-02-17 04:15:00 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-02-17 | good | . 2005-02-20 17:15:00 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | 2005-02-20 | bad | . 2005-02-24 01:20:00 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | 2005-02-24 | bad | . We&#39;ll use ri_weather to analyze the relationship between weather conditions and police behavior. . Does weather affect the arrest rate? . Comparing arrest rates by weather rating . Do police officers arrest drivers more often when the weather is bad? Let&#39;s find out below! . First, we&#39;ll calculate the overall arrest rate. | Then, we&#39;ll calculate the arrest rate for each of the weather ratings we previously assigned. | Finally, we&#39;ll add violation type as a second factor in the analysis, to see if that accounts for any differences in the arrest rate. | . Since we previously defined a logical order for the weather categories, good &lt; bad &lt; worse, they will be sorted that way in the results. . # Calculate the overall arrest rate print(ri_weather.is_arrested.mean()) . 0.0355690117407784 . # Calculate the arrest rate for each &#39;rating&#39; ri_weather.groupby(&quot;rating&quot;).is_arrested.mean() . rating good 0.033715 bad 0.036261 worse 0.041667 Name: is_arrested, dtype: float64 . # Calculate the arrest rate for each &#39;violation&#39; and &#39;rating&#39; ri_weather.groupby([&quot;violation&quot;, &#39;rating&#39;]).is_arrested.mean() . violation rating Equipment good 0.059007 bad 0.066311 worse 0.097357 Moving violation good 0.056227 bad 0.058050 worse 0.065860 Other good 0.076966 bad 0.087443 worse 0.062893 Registration/plates good 0.081574 bad 0.098160 worse 0.115625 Seat belt good 0.028587 bad 0.022493 worse 0.000000 Speeding good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . The arrest rate increases as the weather gets worse, and that trend persists across many of the violation types. This doesn&#39;t prove a causal link, but it&#39;s quite an interesting result! . Selecting from a multi-indexed Series . The output of a single .groupby() operation on multiple columns is a Series with a MultiIndex. Working with this type of object is similar to working with a DataFrame: . The outer index level is like the DataFrame rows. | The inner index level is like the DataFrame columns. | . # Save the output of the groupby operation from the last exercise arrest_rate = ri_weather.groupby([&#39;violation&#39;, &#39;rating&#39;]).is_arrested.mean() # Print the arrest rate for moving violations in bad weather display(arrest_rate.loc[&quot;Moving violation&quot;, &quot;bad&quot;]) # Print the arrest rates for speeding violations in all three weather conditions arrest_rate.loc[&quot;Speeding&quot;] . 0.05804964058049641 . rating good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . Reshaping the arrest rate data . We&#39;ll start by reshaping the arrest_rate Series into a DataFrame. This is a useful step when working with any multi-indexed Series, since it enables you to access the full range of DataFrame methods. . Then, we&#39;ll create the exact same DataFrame using a pivot table. This is a great example of how pandas often gives you more than one way to reach the same result! . # Unstack the &#39;arrest_rate&#39; Series into a DataFrame display(arrest_rate.unstack()) # Create the same DataFrame using a pivot table ri_weather.pivot_table(index=&#39;violation&#39;, columns=&#39;rating&#39;, values=&#39;is_arrested&#39;) . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | .",
            "url": "https://victoromondi1997.github.io/blog/pandas/eda/python/data-science/data-analysis/2020/06/28/Analyzing-Police-Activity-with-pandas.html",
            "relUrl": "/pandas/eda/python/data-science/data-analysis/2020/06/28/Analyzing-Police-Activity-with-pandas.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Exploring Boston Weather Data",
            "content": "weather = readRDS(gzcon(url(&#39;https://assets.datacamp.com/production/repositories/34/datasets/b3c1036d9a60a9dfe0f99051d2474a54f76055ea/weather.rds&#39;))) . Libraries . library(readr) library(dplyr) library(lubridate) library(stringr) library(installr) library(tidyr) . Warning message: &#34;package &#39;tidyr&#39; was built under R version 3.6.3&#34; . # Verify that weather is a data.frame class(weather) # Check the dimensions dim(weather) # View the column names names(weather) . &#39;data.frame&#39; &lt;ol class=list-inline&gt; 286 | 35 | &lt;/ol&gt; &lt;ol class=list-inline&gt; &#39;X&#39; | &#39;year&#39; | &#39;month&#39; | &#39;measure&#39; | &#39;X1&#39; | &#39;X2&#39; | &#39;X3&#39; | &#39;X4&#39; | &#39;X5&#39; | &#39;X6&#39; | &#39;X7&#39; | &#39;X8&#39; | &#39;X9&#39; | &#39;X10&#39; | &#39;X11&#39; | &#39;X12&#39; | &#39;X13&#39; | &#39;X14&#39; | &#39;X15&#39; | &#39;X16&#39; | &#39;X17&#39; | &#39;X18&#39; | &#39;X19&#39; | &#39;X20&#39; | &#39;X21&#39; | &#39;X22&#39; | &#39;X23&#39; | &#39;X24&#39; | &#39;X25&#39; | &#39;X26&#39; | &#39;X27&#39; | &#39;X28&#39; | &#39;X29&#39; | &#39;X30&#39; | &#39;X31&#39; | &lt;/ol&gt; We&#39;ve confirmed that the object is a data frame with 286 rows and 35 columns. . Summarize the data . Next up is to look at some summaries of the data. This is where functions like str(), glimpse() from dplyr, and summary() come in handy. . # View the structure of the data str(weather) # Look at the structure using dplyr&#39;s glimpse() glimpse(weather) # View a summary of the data summary(weather) . &#39;data.frame&#39;: 286 obs. of 35 variables: $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... $ month : int 12 12 12 12 12 12 12 12 12 12 ... $ measure: chr &#34;Max.TemperatureF&#34; &#34;Mean.TemperatureF&#34; &#34;Min.TemperatureF&#34; &#34;Max.Dew.PointF&#34; ... $ X1 : chr &#34;64&#34; &#34;52&#34; &#34;39&#34; &#34;46&#34; ... $ X2 : chr &#34;42&#34; &#34;38&#34; &#34;33&#34; &#34;40&#34; ... $ X3 : chr &#34;51&#34; &#34;44&#34; &#34;37&#34; &#34;49&#34; ... $ X4 : chr &#34;43&#34; &#34;37&#34; &#34;30&#34; &#34;24&#34; ... $ X5 : chr &#34;42&#34; &#34;34&#34; &#34;26&#34; &#34;37&#34; ... $ X6 : chr &#34;45&#34; &#34;42&#34; &#34;38&#34; &#34;45&#34; ... $ X7 : chr &#34;38&#34; &#34;30&#34; &#34;21&#34; &#34;36&#34; ... $ X8 : chr &#34;29&#34; &#34;24&#34; &#34;18&#34; &#34;28&#34; ... $ X9 : chr &#34;49&#34; &#34;39&#34; &#34;29&#34; &#34;49&#34; ... $ X10 : chr &#34;48&#34; &#34;43&#34; &#34;38&#34; &#34;45&#34; ... $ X11 : chr &#34;39&#34; &#34;36&#34; &#34;32&#34; &#34;37&#34; ... $ X12 : chr &#34;39&#34; &#34;35&#34; &#34;31&#34; &#34;28&#34; ... $ X13 : chr &#34;42&#34; &#34;37&#34; &#34;32&#34; &#34;28&#34; ... $ X14 : chr &#34;45&#34; &#34;39&#34; &#34;33&#34; &#34;29&#34; ... $ X15 : chr &#34;42&#34; &#34;37&#34; &#34;32&#34; &#34;33&#34; ... $ X16 : chr &#34;44&#34; &#34;40&#34; &#34;35&#34; &#34;42&#34; ... $ X17 : chr &#34;49&#34; &#34;45&#34; &#34;41&#34; &#34;46&#34; ... $ X18 : chr &#34;44&#34; &#34;40&#34; &#34;36&#34; &#34;34&#34; ... $ X19 : chr &#34;37&#34; &#34;33&#34; &#34;29&#34; &#34;25&#34; ... $ X20 : chr &#34;36&#34; &#34;32&#34; &#34;27&#34; &#34;30&#34; ... $ X21 : chr &#34;36&#34; &#34;33&#34; &#34;30&#34; &#34;30&#34; ... $ X22 : chr &#34;44&#34; &#34;39&#34; &#34;33&#34; &#34;39&#34; ... $ X23 : chr &#34;47&#34; &#34;45&#34; &#34;42&#34; &#34;45&#34; ... $ X24 : chr &#34;46&#34; &#34;44&#34; &#34;41&#34; &#34;46&#34; ... $ X25 : chr &#34;59&#34; &#34;52&#34; &#34;44&#34; &#34;58&#34; ... $ X26 : chr &#34;50&#34; &#34;44&#34; &#34;37&#34; &#34;31&#34; ... $ X27 : chr &#34;52&#34; &#34;45&#34; &#34;38&#34; &#34;34&#34; ... $ X28 : chr &#34;52&#34; &#34;46&#34; &#34;40&#34; &#34;42&#34; ... $ X29 : chr &#34;41&#34; &#34;36&#34; &#34;30&#34; &#34;26&#34; ... $ X30 : chr &#34;30&#34; &#34;26&#34; &#34;22&#34; &#34;10&#34; ... $ X31 : chr &#34;30&#34; &#34;25&#34; &#34;20&#34; &#34;8&#34; ... Rows: 286 Columns: 35 $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ... $ year &lt;int&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,... $ month &lt;int&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,... $ measure &lt;chr&gt; &#34;Max.TemperatureF&#34;, &#34;Mean.TemperatureF&#34;, &#34;Min.TemperatureF&#34;... $ X1 &lt;chr&gt; &#34;64&#34;, &#34;52&#34;, &#34;39&#34;, &#34;46&#34;, &#34;40&#34;, &#34;26&#34;, &#34;74&#34;, &#34;63&#34;, &#34;52&#34;, &#34;30.4... $ X2 &lt;chr&gt; &#34;42&#34;, &#34;38&#34;, &#34;33&#34;, &#34;40&#34;, &#34;27&#34;, &#34;17&#34;, &#34;92&#34;, &#34;72&#34;, &#34;51&#34;, &#34;30.7... $ X3 &lt;chr&gt; &#34;51&#34;, &#34;44&#34;, &#34;37&#34;, &#34;49&#34;, &#34;42&#34;, &#34;24&#34;, &#34;100&#34;, &#34;79&#34;, &#34;57&#34;, &#34;30.... $ X4 &lt;chr&gt; &#34;43&#34;, &#34;37&#34;, &#34;30&#34;, &#34;24&#34;, &#34;21&#34;, &#34;13&#34;, &#34;69&#34;, &#34;54&#34;, &#34;39&#34;, &#34;30.5... $ X5 &lt;chr&gt; &#34;42&#34;, &#34;34&#34;, &#34;26&#34;, &#34;37&#34;, &#34;25&#34;, &#34;12&#34;, &#34;85&#34;, &#34;66&#34;, &#34;47&#34;, &#34;30.6... $ X6 &lt;chr&gt; &#34;45&#34;, &#34;42&#34;, &#34;38&#34;, &#34;45&#34;, &#34;40&#34;, &#34;36&#34;, &#34;100&#34;, &#34;93&#34;, &#34;85&#34;, &#34;30.... $ X7 &lt;chr&gt; &#34;38&#34;, &#34;30&#34;, &#34;21&#34;, &#34;36&#34;, &#34;20&#34;, &#34;-3&#34;, &#34;92&#34;, &#34;61&#34;, &#34;29&#34;, &#34;30.6... $ X8 &lt;chr&gt; &#34;29&#34;, &#34;24&#34;, &#34;18&#34;, &#34;28&#34;, &#34;16&#34;, &#34;3&#34;, &#34;92&#34;, &#34;70&#34;, &#34;47&#34;, &#34;30.77... $ X9 &lt;chr&gt; &#34;49&#34;, &#34;39&#34;, &#34;29&#34;, &#34;49&#34;, &#34;41&#34;, &#34;28&#34;, &#34;100&#34;, &#34;93&#34;, &#34;86&#34;, &#34;30.... $ X10 &lt;chr&gt; &#34;48&#34;, &#34;43&#34;, &#34;38&#34;, &#34;45&#34;, &#34;39&#34;, &#34;37&#34;, &#34;100&#34;, &#34;95&#34;, &#34;89&#34;, &#34;29.... $ X11 &lt;chr&gt; &#34;39&#34;, &#34;36&#34;, &#34;32&#34;, &#34;37&#34;, &#34;31&#34;, &#34;27&#34;, &#34;92&#34;, &#34;87&#34;, &#34;82&#34;, &#34;29.8... $ X12 &lt;chr&gt; &#34;39&#34;, &#34;35&#34;, &#34;31&#34;, &#34;28&#34;, &#34;27&#34;, &#34;25&#34;, &#34;85&#34;, &#34;75&#34;, &#34;64&#34;, &#34;29.8... $ X13 &lt;chr&gt; &#34;42&#34;, &#34;37&#34;, &#34;32&#34;, &#34;28&#34;, &#34;26&#34;, &#34;24&#34;, &#34;75&#34;, &#34;65&#34;, &#34;55&#34;, &#34;29.8... $ X14 &lt;chr&gt; &#34;45&#34;, &#34;39&#34;, &#34;33&#34;, &#34;29&#34;, &#34;27&#34;, &#34;25&#34;, &#34;82&#34;, &#34;68&#34;, &#34;53&#34;, &#34;29.9... $ X15 &lt;chr&gt; &#34;42&#34;, &#34;37&#34;, &#34;32&#34;, &#34;33&#34;, &#34;29&#34;, &#34;27&#34;, &#34;89&#34;, &#34;75&#34;, &#34;60&#34;, &#34;30.1... $ X16 &lt;chr&gt; &#34;44&#34;, &#34;40&#34;, &#34;35&#34;, &#34;42&#34;, &#34;36&#34;, &#34;30&#34;, &#34;96&#34;, &#34;85&#34;, &#34;73&#34;, &#34;30.1... $ X17 &lt;chr&gt; &#34;49&#34;, &#34;45&#34;, &#34;41&#34;, &#34;46&#34;, &#34;41&#34;, &#34;32&#34;, &#34;100&#34;, &#34;85&#34;, &#34;70&#34;, &#34;29.... $ X18 &lt;chr&gt; &#34;44&#34;, &#34;40&#34;, &#34;36&#34;, &#34;34&#34;, &#34;30&#34;, &#34;26&#34;, &#34;89&#34;, &#34;73&#34;, &#34;57&#34;, &#34;29.8... $ X19 &lt;chr&gt; &#34;37&#34;, &#34;33&#34;, &#34;29&#34;, &#34;25&#34;, &#34;22&#34;, &#34;20&#34;, &#34;69&#34;, &#34;63&#34;, &#34;56&#34;, &#34;30.1... $ X20 &lt;chr&gt; &#34;36&#34;, &#34;32&#34;, &#34;27&#34;, &#34;30&#34;, &#34;24&#34;, &#34;20&#34;, &#34;89&#34;, &#34;79&#34;, &#34;69&#34;, &#34;30.3... $ X21 &lt;chr&gt; &#34;36&#34;, &#34;33&#34;, &#34;30&#34;, &#34;30&#34;, &#34;27&#34;, &#34;25&#34;, &#34;85&#34;, &#34;77&#34;, &#34;69&#34;, &#34;30.3... $ X22 &lt;chr&gt; &#34;44&#34;, &#34;39&#34;, &#34;33&#34;, &#34;39&#34;, &#34;34&#34;, &#34;25&#34;, &#34;89&#34;, &#34;79&#34;, &#34;69&#34;, &#34;30.4... $ X23 &lt;chr&gt; &#34;47&#34;, &#34;45&#34;, &#34;42&#34;, &#34;45&#34;, &#34;42&#34;, &#34;37&#34;, &#34;100&#34;, &#34;91&#34;, &#34;82&#34;, &#34;30.... $ X24 &lt;chr&gt; &#34;46&#34;, &#34;44&#34;, &#34;41&#34;, &#34;46&#34;, &#34;44&#34;, &#34;41&#34;, &#34;100&#34;, &#34;98&#34;, &#34;96&#34;, &#34;30.... $ X25 &lt;chr&gt; &#34;59&#34;, &#34;52&#34;, &#34;44&#34;, &#34;58&#34;, &#34;43&#34;, &#34;29&#34;, &#34;100&#34;, &#34;75&#34;, &#34;49&#34;, &#34;29.... $ X26 &lt;chr&gt; &#34;50&#34;, &#34;44&#34;, &#34;37&#34;, &#34;31&#34;, &#34;29&#34;, &#34;28&#34;, &#34;70&#34;, &#34;60&#34;, &#34;49&#34;, &#34;30.1... $ X27 &lt;chr&gt; &#34;52&#34;, &#34;45&#34;, &#34;38&#34;, &#34;34&#34;, &#34;31&#34;, &#34;29&#34;, &#34;70&#34;, &#34;60&#34;, &#34;50&#34;, &#34;30.2... $ X28 &lt;chr&gt; &#34;52&#34;, &#34;46&#34;, &#34;40&#34;, &#34;42&#34;, &#34;35&#34;, &#34;27&#34;, &#34;76&#34;, &#34;65&#34;, &#34;53&#34;, &#34;29.9... $ X29 &lt;chr&gt; &#34;41&#34;, &#34;36&#34;, &#34;30&#34;, &#34;26&#34;, &#34;20&#34;, &#34;10&#34;, &#34;64&#34;, &#34;51&#34;, &#34;37&#34;, &#34;30.2... $ X30 &lt;chr&gt; &#34;30&#34;, &#34;26&#34;, &#34;22&#34;, &#34;10&#34;, &#34;4&#34;, &#34;-6&#34;, &#34;50&#34;, &#34;38&#34;, &#34;26&#34;, &#34;30.36... $ X31 &lt;chr&gt; &#34;30&#34;, &#34;25&#34;, &#34;20&#34;, &#34;8&#34;, &#34;5&#34;, &#34;1&#34;, &#34;57&#34;, &#34;44&#34;, &#34;31&#34;, &#34;30.32&#34;,... . X year month measure Min. : 1.00 Min. :2014 Min. : 1.000 Length:286 1st Qu.: 72.25 1st Qu.:2015 1st Qu.: 4.000 Class :character Median :143.50 Median :2015 Median : 7.000 Mode :character Mean :143.50 Mean :2015 Mean : 6.923 3rd Qu.:214.75 3rd Qu.:2015 3rd Qu.:10.000 Max. :286.00 Max. :2015 Max. :12.000 X1 X2 X3 X4 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X5 X6 X7 X8 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X9 X10 X11 X12 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X13 X14 X15 X16 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X17 X18 X19 X20 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X21 X22 X23 X24 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X25 X26 X27 X28 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X29 X30 X31 Length:286 Length:286 Length:286 Class :character Class :character Class :character Mode :character Mode :character Mode :character . Now that we have a pretty good feel for how the table is structured, we&#39;ll take a look at some real observations! . Take a closer look . After understanding the structure of the data and looking at some brief summaries, it often helps to preview the actual data. The functions head() and tail() allow us to view the top and bottom rows of the data, respectively. . # View first 6 rows head(weather) # View first 15 rows head(weather, n=15) # View the last 6 rows tail(weather) # View the last 10 rows tail(weather, n=10) . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 1 | 2014 | 12 | Max.TemperatureF | 64 | 42 | 51 | 43 | 42 | 45 | ... | 44 | 47 | 46 | 59 | 50 | 52 | 52 | 41 | 30 | 30 | . 2 | 2014 | 12 | Mean.TemperatureF | 52 | 38 | 44 | 37 | 34 | 42 | ... | 39 | 45 | 44 | 52 | 44 | 45 | 46 | 36 | 26 | 25 | . 3 | 2014 | 12 | Min.TemperatureF | 39 | 33 | 37 | 30 | 26 | 38 | ... | 33 | 42 | 41 | 44 | 37 | 38 | 40 | 30 | 22 | 20 | . 4 | 2014 | 12 | Max.Dew.PointF | 46 | 40 | 49 | 24 | 37 | 45 | ... | 39 | 45 | 46 | 58 | 31 | 34 | 42 | 26 | 10 | 8 | . 5 | 2014 | 12 | MeanDew.PointF | 40 | 27 | 42 | 21 | 25 | 40 | ... | 34 | 42 | 44 | 43 | 29 | 31 | 35 | 20 | 4 | 5 | . 6 | 2014 | 12 | Min.DewpointF | 26 | 17 | 24 | 13 | 12 | 36 | ... | 25 | 37 | 41 | 29 | 28 | 29 | 27 | 10 | -6 | 1 | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 1 | 2014 | 12 | Max.TemperatureF | 64 | 42 | 51 | 43 | 42 | 45 | ... | 44 | 47 | 46 | 59 | 50 | 52 | 52 | 41 | 30 | 30 | . 2 | 2014 | 12 | Mean.TemperatureF | 52 | 38 | 44 | 37 | 34 | 42 | ... | 39 | 45 | 44 | 52 | 44 | 45 | 46 | 36 | 26 | 25 | . 3 | 2014 | 12 | Min.TemperatureF | 39 | 33 | 37 | 30 | 26 | 38 | ... | 33 | 42 | 41 | 44 | 37 | 38 | 40 | 30 | 22 | 20 | . 4 | 2014 | 12 | Max.Dew.PointF | 46 | 40 | 49 | 24 | 37 | 45 | ... | 39 | 45 | 46 | 58 | 31 | 34 | 42 | 26 | 10 | 8 | . 5 | 2014 | 12 | MeanDew.PointF | 40 | 27 | 42 | 21 | 25 | 40 | ... | 34 | 42 | 44 | 43 | 29 | 31 | 35 | 20 | 4 | 5 | . 6 | 2014 | 12 | Min.DewpointF | 26 | 17 | 24 | 13 | 12 | 36 | ... | 25 | 37 | 41 | 29 | 28 | 29 | 27 | 10 | -6 | 1 | . 7 | 2014 | 12 | Max.Humidity | 74 | 92 | 100 | 69 | 85 | 100 | ... | 89 | 100 | 100 | 100 | 70 | 70 | 76 | 64 | 50 | 57 | . 8 | 2014 | 12 | Mean.Humidity | 63 | 72 | 79 | 54 | 66 | 93 | ... | 79 | 91 | 98 | 75 | 60 | 60 | 65 | 51 | 38 | 44 | . 9 | 2014 | 12 | Min.Humidity | 52 | 51 | 57 | 39 | 47 | 85 | ... | 69 | 82 | 96 | 49 | 49 | 50 | 53 | 37 | 26 | 31 | . 10 | 2014 | 12 | Max.Sea.Level.PressureIn | 30.45 | 30.71 | 30.4 | 30.56 | 30.68 | 30.42 | ... | 30.4 | 30.31 | 30.13 | 29.96 | 30.16 | 30.22 | 29.99 | 30.22 | 30.36 | 30.32 | . 11 | 2014 | 12 | Mean.Sea.Level.PressureIn | 30.13 | 30.59 | 30.07 | 30.33 | 30.59 | 30.24 | ... | 30.35 | 30.23 | 29.9 | 29.63 | 30.11 | 30.14 | 29.87 | 30.12 | 30.32 | 30.25 | . 12 | 2014 | 12 | Min.Sea.Level.PressureIn | 30.01 | 30.4 | 29.87 | 30.09 | 30.45 | 30.16 | ... | 30.3 | 30.16 | 29.55 | 29.47 | 29.99 | 30.03 | 29.77 | 30 | 30.23 | 30.13 | . 13 | 2014 | 12 | Max.VisibilityMiles | 10 | 10 | 10 | 10 | 10 | 10 | ... | 10 | 10 | 2 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | . 14 | 2014 | 12 | Mean.VisibilityMiles | 10 | 8 | 5 | 10 | 10 | 4 | ... | 10 | 5 | 1 | 8 | 10 | 10 | 10 | 10 | 10 | 10 | . 15 | 2014 | 12 | Min.VisibilityMiles | 10 | 2 | 1 | 10 | 5 | 0 | ... | 4 | 1 | 0 | 1 | 10 | 10 | 10 | 10 | 10 | 10 | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 281281 | 2015 | 12 | Mean.Wind.SpeedMPH | 6 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 282282 | 2015 | 12 | Max.Gust.SpeedMPH | 17 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 283283 | 2015 | 12 | PrecipitationIn | 0.14 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 284284 | 2015 | 12 | CloudCover | 7 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 285285 | 2015 | 12 | Events | Rain | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 286286 | 2015 | 12 | WindDirDegrees | 109 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 277277 | 2015 | 12 | Max.VisibilityMiles | 10 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 278278 | 2015 | 12 | Mean.VisibilityMiles | 8 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 279279 | 2015 | 12 | Min.VisibilityMiles | 1 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 280280 | 2015 | 12 | Max.Wind.SpeedMPH | 15 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 281281 | 2015 | 12 | Mean.Wind.SpeedMPH | 6 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 282282 | 2015 | 12 | Max.Gust.SpeedMPH | 17 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 283283 | 2015 | 12 | PrecipitationIn | 0.14 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 284284 | 2015 | 12 | CloudCover | 7 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 285285 | 2015 | 12 | Events | Rain | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 286286 | 2015 | 12 | WindDirDegrees | 109 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . Let&#39;s tidy the data . Column names are values . The weather dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. . The tidyr package provides the gather() function for exactly this scenario. . gather(df, time, val, t1:t3) . gather() allows us to select multiple columns to be gathered by using the : operator. . # Gather the columns weather2 &lt;- gather(weather, day, value, X1:X31, na.rm = TRUE) # View the head head(weather2) . Xyearmonthmeasuredayvalue . 1 | 2014 | 12 | Max.TemperatureF | X1 | 64 | . 2 | 2014 | 12 | Mean.TemperatureF | X1 | 52 | . 3 | 2014 | 12 | Min.TemperatureF | X1 | 39 | . 4 | 2014 | 12 | Max.Dew.PointF | X1 | 46 | . 5 | 2014 | 12 | MeanDew.PointF | X1 | 40 | . 6 | 2014 | 12 | Min.DewpointF | X1 | 26 | . Values are variable names . Our data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. . The spread() function from tidyr is designed to help with this. . spread(df2, time, val) . # First remove column of row names without_x &lt;- weather2[, -1] # Spread the data weather3 &lt;- spread(without_x, measure, value) # View the head head(weather3) . yearmonthdayCloudCoverEventsMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureF...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014 | 12 | X1 | 6 | Rain | 46 | 29 | 74 | 30.45 | 64 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014 | 12 | X10 | 8 | Rain | 45 | 29 | 100 | 29.58 | 48 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014 | 12 | X11 | 8 | Rain-Snow | 37 | 28 | 92 | 29.81 | 39 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014 | 12 | X12 | 7 | Snow | 28 | 21 | 85 | 29.88 | 39 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014 | 12 | X13 | 5 | | 28 | 23 | 75 | 29.86 | 42 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014 | 12 | X14 | 4 | | 29 | 20 | 82 | 29.91 | 45 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . This dataset is looking much better already! . Prepare the data for analysis . Clean up dates . Now that the weather dataset adheres to tidy data principles, the next step is to prepare it for analysis. We&#39;ll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task. . # Remove X&#39;s from day column weather3$day &lt;- str_replace(weather3$day, &#39;X&#39;, &#39;&#39;) # Unite the year, month, and day columns weather4 &lt;- unite(weather3, date, year, month, day, sep = &quot;-&quot;) # Convert date column to proper date format using lubridates&#39;s ymd() weather4$date &lt;- ymd(weather4$date) # Rearrange columns using dplyr&#39;s select() weather5 &lt;- select(weather4, date, Events, CloudCover:WindDirDegrees) # View the head of weather5 head(weather5) . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014-12-13 | | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014-12-14 | | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . A closer look at column types . It&#39;s important for analysis that variables are coded appropriately. This is not yet the case with our weather data. . # View the structure of weather5 str(weather5) # Examine the first 20 rows of weather5. Are most of the characters numeric? head(weather5, 20) # See what happens if we try to convert PrecipitationIn to numeric as.numeric(weather5$PrecipitationIn) . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ Events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ CloudCover : chr &#34;6&#34; &#34;8&#34; &#34;8&#34; &#34;7&#34; ... $ Max.Dew.PointF : chr &#34;46&#34; &#34;45&#34; &#34;37&#34; &#34;28&#34; ... $ Max.Gust.SpeedMPH : chr &#34;29&#34; &#34;29&#34; &#34;28&#34; &#34;21&#34; ... $ Max.Humidity : chr &#34;74&#34; &#34;100&#34; &#34;92&#34; &#34;85&#34; ... $ Max.Sea.Level.PressureIn : chr &#34;30.45&#34; &#34;29.58&#34; &#34;29.81&#34; &#34;29.88&#34; ... $ Max.TemperatureF : chr &#34;64&#34; &#34;48&#34; &#34;39&#34; &#34;39&#34; ... $ Max.VisibilityMiles : chr &#34;10&#34; &#34;10&#34; &#34;10&#34; &#34;10&#34; ... $ Max.Wind.SpeedMPH : chr &#34;22&#34; &#34;23&#34; &#34;21&#34; &#34;16&#34; ... $ Mean.Humidity : chr &#34;63&#34; &#34;95&#34; &#34;87&#34; &#34;75&#34; ... $ Mean.Sea.Level.PressureIn: chr &#34;30.13&#34; &#34;29.5&#34; &#34;29.61&#34; &#34;29.85&#34; ... $ Mean.TemperatureF : chr &#34;52&#34; &#34;43&#34; &#34;36&#34; &#34;35&#34; ... $ Mean.VisibilityMiles : chr &#34;10&#34; &#34;3&#34; &#34;7&#34; &#34;10&#34; ... $ Mean.Wind.SpeedMPH : chr &#34;13&#34; &#34;13&#34; &#34;13&#34; &#34;11&#34; ... $ MeanDew.PointF : chr &#34;40&#34; &#34;39&#34; &#34;31&#34; &#34;27&#34; ... $ Min.DewpointF : chr &#34;26&#34; &#34;37&#34; &#34;27&#34; &#34;25&#34; ... $ Min.Humidity : chr &#34;52&#34; &#34;89&#34; &#34;82&#34; &#34;64&#34; ... $ Min.Sea.Level.PressureIn : chr &#34;30.01&#34; &#34;29.43&#34; &#34;29.44&#34; &#34;29.81&#34; ... $ Min.TemperatureF : chr &#34;39&#34; &#34;38&#34; &#34;32&#34; &#34;31&#34; ... $ Min.VisibilityMiles : chr &#34;10&#34; &#34;1&#34; &#34;1&#34; &#34;7&#34; ... $ PrecipitationIn : chr &#34;0.01&#34; &#34;0.28&#34; &#34;0.02&#34; &#34;T&#34; ... $ WindDirDegrees : chr &#34;268&#34; &#34;357&#34; &#34;230&#34; &#34;286&#34; ... . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014-12-13 | | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014-12-14 | | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . 2014-12-15 | | 2 | 33 | 21 | 89 | 30.15 | 42 | 10 | 15 | ... | 10 | 6 | 29 | 27 | 60 | 29.91 | 32 | 10 | 0.00 | 324 | . 2014-12-16 | Rain | 8 | 42 | 10 | 96 | 30.17 | 44 | 10 | 8 | ... | 9 | 4 | 36 | 30 | 73 | 29.92 | 35 | 5 | T | 79 | . 2014-12-17 | Rain | 8 | 46 | 26 | 100 | 29.91 | 49 | 10 | 20 | ... | 6 | 11 | 41 | 32 | 70 | 29.69 | 41 | 1 | 0.43 | 311 | . 2014-12-18 | Rain | 7 | 34 | 30 | 89 | 29.87 | 44 | 10 | 23 | ... | 10 | 14 | 30 | 26 | 57 | 29.71 | 36 | 10 | 0.01 | 281 | . 2014-12-19 | | 4 | 25 | 23 | 69 | 30.15 | 37 | 10 | 17 | ... | 10 | 11 | 22 | 20 | 56 | 29.86 | 29 | 10 | 0.00 | 305 | . 2014-12-02 | Rain-Snow | 7 | 40 | 29 | 92 | 30.71 | 42 | 10 | 24 | ... | 8 | 15 | 27 | 17 | 51 | 30.4 | 33 | 2 | 0.10 | 62 | . 2014-12-20 | Snow | 6 | 30 | 26 | 89 | 30.31 | 36 | 10 | 21 | ... | 10 | 10 | 24 | 20 | 69 | 30.17 | 27 | 7 | T | 350 | . 2014-12-21 | Snow | 8 | 30 | 20 | 85 | 30.37 | 36 | 10 | 16 | ... | 9 | 9 | 27 | 25 | 69 | 30.28 | 30 | 6 | T | 2 | . 2014-12-22 | Rain | 7 | 39 | 22 | 89 | 30.4 | 44 | 10 | 18 | ... | 10 | 8 | 34 | 25 | 69 | 30.3 | 33 | 4 | 0.05 | 24 | . 2014-12-23 | Rain | 8 | 45 | 25 | 100 | 30.31 | 47 | 10 | 20 | ... | 5 | 13 | 42 | 37 | 82 | 30.16 | 42 | 1 | 0.25 | 63 | . 2014-12-24 | Fog-Rain | 8 | 46 | 15 | 100 | 30.13 | 46 | 2 | 13 | ... | 1 | 6 | 44 | 41 | 96 | 29.55 | 41 | 0 | 0.56 | 12 | . 2014-12-25 | Rain | 6 | 58 | 40 | 100 | 29.96 | 59 | 10 | 28 | ... | 8 | 14 | 43 | 29 | 49 | 29.47 | 44 | 1 | 0.14 | 250 | . 2014-12-26 | | 1 | 31 | 25 | 70 | 30.16 | 50 | 10 | 18 | ... | 10 | 11 | 29 | 28 | 49 | 29.99 | 37 | 10 | 0.00 | 255 | . 2014-12-27 | | 3 | 34 | 21 | 70 | 30.22 | 52 | 10 | 17 | ... | 10 | 9 | 31 | 29 | 50 | 30.03 | 38 | 10 | 0.00 | 251 | . Warning message in eval(expr, envir, enclos): &#34;NAs introduced by coercion&#34; . &lt;ol class=list-inline&gt; 0.01 | 0.28 | 0.02 | &lt;NA&gt; | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0.43 | 0.01 | 0 | 0.1 | &lt;NA&gt; | &lt;NA&gt; | 0.05 | 0.25 | 0.56 | 0.14 | 0 | 0 | 0.01 | 0 | 0.44 | 0 | 0 | 0 | 0.11 | 1.09 | 0.13 | 0.03 | 2.9 | 0 | 0 | 0 | 0.2 | 0 | &lt;NA&gt; | 0.12 | 0 | 0 | 0.15 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.71 | 0 | 0.1 | 0.95 | 0.01 | &lt;NA&gt; | 0.62 | 0.06 | 0.05 | 0.57 | 0 | 0.02 | &lt;NA&gt; | 0 | 0.01 | 0 | 0.05 | 0.01 | 0.03 | 0 | 0.23 | 0.39 | 0 | 0.02 | 0.01 | 0.06 | 0.78 | 0 | 0.17 | 0.11 | 0 | &lt;NA&gt; | 0.07 | 0.02 | 0 | 0 | 0 | 0 | 0.09 | &lt;NA&gt; | 0.07 | 0.37 | 0.88 | 0.17 | 0.06 | 0.01 | 0 | 0 | 0.8 | 0.27 | 0 | 0.14 | 0 | 0 | 0.01 | 0.05 | 0.09 | 0 | 0 | 0 | 0.04 | 0.8 | 0.21 | 0.12 | 0 | 0.26 | &lt;NA&gt; | 0 | 0.02 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0.09 | 0 | 0 | 0 | 0.01 | 0 | 0 | 0.06 | 0 | 0 | 0 | 0.61 | 0.54 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0 | 0 | 0.1 | 0.07 | 0 | 0.03 | 0 | 0.39 | 0 | 0 | 0.03 | 0.26 | 0.09 | 0 | 0 | 0 | 0.02 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0.27 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0.91 | 0 | 0.02 | 0 | 0 | 0 | 0 | 0.38 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.4 | &lt;NA&gt; | 0 | 0 | 0 | 0.74 | 0.04 | 1.72 | 0 | 0.01 | 0 | 0 | &lt;NA&gt; | 0.2 | 1.43 | &lt;NA&gt; | 0 | 0 | 0 | &lt;NA&gt; | 0.09 | 0 | &lt;NA&gt; | &lt;NA&gt; | 0.5 | 1.12 | 0 | 0 | 0 | 0.03 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0.14 | &lt;NA&gt; | 0 | &lt;NA&gt; | &lt;NA&gt; | 0 | 0 | 0.01 | 0 | &lt;NA&gt; | 0.06 | 0 | 0 | 0 | 0.02 | 0 | &lt;NA&gt; | 0 | 0 | 0.02 | &lt;NA&gt; | 0.15 | &lt;NA&gt; | 0 | 0.83 | 0 | 0 | 0 | 0.08 | 0 | 0 | 0.14 | 0 | 0 | 0 | 0.63 | &lt;NA&gt; | 0.02 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0 | 0 | 0 | 0.49 | 0 | 0 | 0 | 0 | 0 | 0 | 0.17 | 0.66 | 0.01 | 0.38 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.04 | 0.01 | 2.46 | &lt;NA&gt; | 0 | 0 | 0 | 0.2 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0.12 | 0 | 0 | &lt;NA&gt; | &lt;NA&gt; | &lt;NA&gt; | 0 | 0.08 | &lt;NA&gt; | 0.07 | &lt;NA&gt; | 0 | 0 | 0.03 | 0 | 0 | 0.36 | 0.73 | 0.01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.34 | &lt;NA&gt; | 0.07 | 0.54 | 0.04 | 0.01 | 0 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.86 | 0 | 0.3 | 0.04 | 0 | 0 | 0 | 0 | 0.21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.14 | &lt;/ol&gt; Column type conversions . &quot;T&quot; was used to denote a trace amount (i.e. too small to be accurately measured) of precipitation in the PrecipitationIn column. In order to coerce this column to numeric, wwe&#39;ll need to deal with this somehow. To keep things simple, we will just replace &quot;T&quot; with zero, as a string (&quot;0&quot;). . # Replace &quot;T&quot; with &quot;0&quot; (T = trace) weather5$PrecipitationIn &lt;- str_replace(weather5$PrecipitationIn, &quot;T&quot;, &quot;0&quot;) # Convert characters to numerics weather6 &lt;- mutate_at(weather5, vars(CloudCover:WindDirDegrees), funs(as.numeric)) # Look at result str(weather6) . Warning message: &#34;`funs()` is deprecated as of dplyr 0.8.0. Please use a list of either functions or lambdas: # Simple named list: list(mean = mean, median = median) # Auto named with `tibble::lst()`: tibble::lst(mean, median) # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) This warning is displayed once every 8 hours. Call `lifecycle::last_warnings()` to see where this warning was generated.&#34; . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ Events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ CloudCover : num 6 8 8 7 5 4 2 8 8 7 ... $ Max.Dew.PointF : num 46 45 37 28 28 29 33 42 46 34 ... $ Max.Gust.SpeedMPH : num 29 29 28 21 23 20 21 10 26 30 ... $ Max.Humidity : num 74 100 92 85 75 82 89 96 100 89 ... $ Max.Sea.Level.PressureIn : num 30.4 29.6 29.8 29.9 29.9 ... $ Max.TemperatureF : num 64 48 39 39 42 45 42 44 49 44 ... $ Max.VisibilityMiles : num 10 10 10 10 10 10 10 10 10 10 ... $ Max.Wind.SpeedMPH : num 22 23 21 16 17 15 15 8 20 23 ... $ Mean.Humidity : num 63 95 87 75 65 68 75 85 85 73 ... $ Mean.Sea.Level.PressureIn: num 30.1 29.5 29.6 29.9 29.8 ... $ Mean.TemperatureF : num 52 43 36 35 37 39 37 40 45 40 ... $ Mean.VisibilityMiles : num 10 3 7 10 10 10 10 9 6 10 ... $ Mean.Wind.SpeedMPH : num 13 13 13 11 12 10 6 4 11 14 ... $ MeanDew.PointF : num 40 39 31 27 26 27 29 36 41 30 ... $ Min.DewpointF : num 26 37 27 25 24 25 27 30 32 26 ... $ Min.Humidity : num 52 89 82 64 55 53 60 73 70 57 ... $ Min.Sea.Level.PressureIn : num 30 29.4 29.4 29.8 29.8 ... $ Min.TemperatureF : num 39 38 32 31 32 33 32 35 41 36 ... $ Min.VisibilityMiles : num 10 1 1 7 10 10 10 5 1 10 ... $ PrecipitationIn : num 0.01 0.28 0.02 0 0 0 0 0 0.43 0.01 ... $ WindDirDegrees : num 268 357 230 286 298 306 324 79 311 281 ... . It looks like our data are finally in the correct formats and organized in a logical manner! Now that our data are in the right form, we can begin the analysis. . Missing, extreme, and unexpected values . Find missing values . Before dealing with missing values in the data, it&#39;s important to find them and figure out why they exist in the first place. . If the dataset is too big to look at all at once, like it is here, we will use sum() and is.na() to quickly size up the situation by counting the number of NA values. . The summary() function also come in handy for identifying which variables contain the missing values. Finally, the which() function is useful for locating the missing values within a particular column. . # Count missing values sum(is.na(weather6)) # Find missing values summary(weather6) # Find indices of NAs in Max.Gust.SpeedMPH ind &lt;- which(is.na(weather6$Max.Gust.SpeedMPH)) # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind, ] . 6 date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 85.69 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :1000.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. :-1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.861 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1612015-05-18 | Fog | 6 | 52 | NA | 100 | 30.30 | 58 | 10 | 16 | ... | 8 | 10 | 48 | 43 | 57 | 30.12 | 49 | 0 | 0 | 72 | . 2052015-06-03 | | 7 | 48 | NA | 93 | 30.31 | 56 | 10 | 14 | ... | 10 | 7 | 45 | 43 | 71 | 30.19 | 47 | 10 | 0 | 90 | . 2732015-08-08 | | 4 | 61 | NA | 87 | 30.02 | 76 | 10 | 14 | ... | 10 | 6 | 57 | 54 | 49 | 29.95 | 61 | 10 | 0 | 45 | . 2752015-09-01 | | 1 | 63 | NA | 78 | 30.06 | 79 | 10 | 15 | ... | 10 | 9 | 62 | 59 | 52 | 29.96 | 69 | 10 | 0 | 54 | . 3082015-10-12 | | 0 | 56 | NA | 89 | 29.86 | 76 | 10 | 15 | ... | 10 | 8 | 51 | 48 | 41 | 29.74 | 51 | 10 | 0 | 199 | . 3582015-11-03 | | 1 | 44 | NA | 82 | 30.25 | 73 | 10 | 16 | ... | 10 | 8 | 42 | 40 | 31 | 30.06 | 47 | 10 | 0 | 281 | . In this situation it&#39;s unclear why these values are missing and there doesn&#39;t appear to be any obvious pattern to their missingness, so we&#39;ll leave them alone for now. . An obvious error . Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with summary(). . Once implausible values are identified, they must be dealt with in an intelligent and informed way. . Sometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data. . # Review distributions for all variables summary(weather6) # Find row with Max.Humidity of 1000 ind &lt;- which(weather6$Max.Humidity==1000) # Look at the data for that day weather6[ind, ] # Change 1000 to 100 weather6$Max.Humidity[ind] &lt;- 100 . date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 85.69 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :1000.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. :-1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.861 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1352015-04-21 | Fog-Rain-Thunderstorm | 6 | 57 | 94 | 1000 | 29.75 | 65 | 10 | 20 | ... | 5 | 10 | 49 | 36 | 42 | 29.53 | 46 | 0 | 0.54 | 184 | . Once you find obvious errors, it&#39;s not too hard to fix them if you know which values they should take. . Another obvious error . We&#39;ve discovered and repaired one obvious error in the data, but it appears that there&#39;s another. Sometimes we get lucky and can infer the correct or intended value from the other data. For example, if you know the minimum and maximum values of a particular metric on a given day... . # Look at summary of Mean.VisibilityMiles summary(weather6$Mean.VisibilityMiles) # Get index of row with -1 value ind &lt;- which(weather6$Mean.VisibilityMiles == -1) # Look at full row weather6[ind,] # Set Mean.VisibilityMiles to the appropriate value weather6$Mean.VisibilityMiles[ind] &lt;- 10 . Min. 1st Qu. Median Mean 3rd Qu. Max. -1.000 8.000 10.000 8.861 10.000 10.000 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1922015-06-18 | | 5 | 54 | 23 | 72 | 30.14 | 76 | 10 | 17 | ... | -1 | 10 | 49 | 45 | 46 | 29.93 | 57 | 10 | 0 | 189 | . Our data are looking tidy. Just a quick sanity check left! . Check other extreme values . In addition to dealing with obvious errors in the data, we want to see if there are other extreme values. In addition to the trusty summary() function, hist() is useful for quickly getting a feel for how different variables are distributed. . # Review summary of full data once more summary(weather6) # Look at histogram for MeanDew.PointF hist(weather6$MeanDew.PointF) # Look at histogram for Min.TemperatureF hist(weather6$Min.TemperatureF) # Compare to histogram for Mean.TemperatureF hist(weather6$Mean.TemperatureF) . date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 83.23 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :100.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. : 1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.891 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . It looks like you have sufficiently tidied your data! . Finishing touches . Before officially calling our weather data clean, we want to put a couple of finishing touches on the data. These are a bit more subjective and may not be necessary for analysis, but they will make the data easier for others to interpret, which is generally a good thing. . There are a number of stylistic conventions in the R language. Depending on who you ask, these conventions may vary. Because the period (.) has special meaning in certain situations, we will be using underscores (_) to separate words in variable names. We also prefer all lowercase letters so that no one has to remember which letters are uppercase or lowercase. . Finally, the events column (renamed to be all lowercase in the first instruction) contains an empty string (&quot;&quot;) for any day on which there was no significant weather event such as rain, fog, a thunderstorm, etc. However, if it&#39;s the first time you&#39;re seeing these data, it may not be obvious that this is the case, so it&#39;s best for us to be explicit and replace the empty strings with something more meaningful. . new_colnames = c(&quot;date&quot;, &quot;events&quot;, &quot;cloud_cover&quot;, &quot;max_dew_point_f&quot;, &quot;max_gust_speed_mph&quot;, &quot;max_humidity&quot;, &quot;max_sea_level_pressure_in&quot;, &quot;max_temperature_f&quot;, &quot;max_visibility_miles&quot;, &quot;max_wind_speed_mph&quot;, &quot;mean_humidity&quot;, &quot;mean_sea_level_pressure_in&quot;, &quot;mean_temperature_f&quot;, &quot;mean_visibility_miles&quot;, &quot;mean_wind_speed_mph&quot;, &quot;mean_dew_point_f&quot;, &quot;min_dew_point_f&quot;, &quot;min_humidity&quot;, &quot;min_sea_level_pressure_in&quot;, &quot;min_temperature_f&quot;, &quot;min_visibility_miles&quot;, &quot;precipitation_in&quot;,&quot;wind_dir_degrees&quot;) . # Clean up column names names(weather6) &lt;- new_colnames # Replace empty cells in events column weather6$events[weather6$events == &quot;&quot;] &lt;- &quot;None&quot; # Print the first 6 rows of weather6 head(weather6) . dateeventscloud_covermax_dew_point_fmax_gust_speed_mphmax_humiditymax_sea_level_pressure_inmax_temperature_fmax_visibility_milesmax_wind_speed_mph...mean_visibility_milesmean_wind_speed_mphmean_dew_point_fmin_dew_point_fmin_humiditymin_sea_level_pressure_inmin_temperature_fmin_visibility_milesprecipitation_inwind_dir_degrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | 0.00 | 286 | . 2014-12-13 | None | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | 0.00 | 298 | . 2014-12-14 | None | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . tail(weather6) . dateeventscloud_covermax_dew_point_fmax_gust_speed_mphmax_humiditymax_sea_level_pressure_inmax_temperature_fmax_visibility_milesmax_wind_speed_mph...mean_visibility_milesmean_wind_speed_mphmean_dew_point_fmin_dew_point_fmin_humiditymin_sea_level_pressure_inmin_temperature_fmin_visibility_milesprecipitation_inwind_dir_degrees . 3612015-11-05 | None | 4 | 61 | 31 | 100 | 30.30 | 76 | 10 | 22 | ... | 9 | 12 | 55 | 48 | 53 | 30.09 | 50 | 5 | 0.00 | 224 | . 3622015-11-06 | None | 4 | 62 | 32 | 93 | 30.07 | 73 | 10 | 26 | ... | 10 | 15 | 61 | 54 | 64 | 29.71 | 62 | 10 | 0.00 | 222 | . 3632015-11-07 | None | 6 | 45 | 33 | 57 | 30.02 | 69 | 10 | 25 | ... | 10 | 13 | 38 | 33 | 39 | 29.83 | 50 | 10 | 0.00 | 280 | . 3642015-11-08 | None | 0 | 34 | 25 | 65 | 30.38 | 56 | 10 | 18 | ... | 10 | 12 | 30 | 24 | 30 | 30.04 | 44 | 10 | 0.00 | 283 | . 3652015-11-09 | None | 2 | 36 | 20 | 70 | 30.43 | 60 | 10 | 16 | ... | 10 | 9 | 32 | 30 | 33 | 30.32 | 41 | 10 | 0.00 | 237 | . 3662015-12-01 | Rain | 7 | 43 | 17 | 96 | 30.40 | 45 | 10 | 15 | ... | 8 | 6 | 35 | 25 | 69 | 30.01 | 32 | 1 | 0.14 | 109 | . str(weather6) . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ cloud_cover : num 6 8 8 7 5 4 2 8 8 7 ... $ max_dew_point_f : num 46 45 37 28 28 29 33 42 46 34 ... $ max_gust_speed_mph : num 29 29 28 21 23 20 21 10 26 30 ... $ max_humidity : num 74 100 92 85 75 82 89 96 100 89 ... $ max_sea_level_pressure_in : num 30.4 29.6 29.8 29.9 29.9 ... $ max_temperature_f : num 64 48 39 39 42 45 42 44 49 44 ... $ max_visibility_miles : num 10 10 10 10 10 10 10 10 10 10 ... $ max_wind_speed_mph : num 22 23 21 16 17 15 15 8 20 23 ... $ mean_humidity : num 63 95 87 75 65 68 75 85 85 73 ... $ mean_sea_level_pressure_in: num 30.1 29.5 29.6 29.9 29.8 ... $ mean_temperature_f : num 52 43 36 35 37 39 37 40 45 40 ... $ mean_visibility_miles : num 10 3 7 10 10 10 10 9 6 10 ... $ mean_wind_speed_mph : num 13 13 13 11 12 10 6 4 11 14 ... $ mean_dew_point_f : num 40 39 31 27 26 27 29 36 41 30 ... $ min_dew_point_f : num 26 37 27 25 24 25 27 30 32 26 ... $ min_humidity : num 52 89 82 64 55 53 60 73 70 57 ... $ min_sea_level_pressure_in : num 30 29.4 29.4 29.8 29.8 ... $ min_temperature_f : num 39 38 32 31 32 33 32 35 41 36 ... $ min_visibility_miles : num 10 1 1 7 10 10 10 5 1 10 ... $ precipitation_in : num 0.01 0.28 0.02 0 0 0 0 0 0.43 0.01 ... $ wind_dir_degrees : num 268 357 230 286 298 306 324 79 311 281 ... . glimpse(weather6) . Rows: 366 Columns: 23 $ date &lt;date&gt; 2014-12-01, 2014-12-10, 2014-12-11, 201... $ events &lt;chr&gt; &#34;Rain&#34;, &#34;Rain&#34;, &#34;Rain-Snow&#34;, &#34;Snow&#34;, &#34;No... $ cloud_cover &lt;dbl&gt; 6, 8, 8, 7, 5, 4, 2, 8, 8, 7, 4, 7, 6, 8... $ max_dew_point_f &lt;dbl&gt; 46, 45, 37, 28, 28, 29, 33, 42, 46, 34, ... $ max_gust_speed_mph &lt;dbl&gt; 29, 29, 28, 21, 23, 20, 21, 10, 26, 30, ... $ max_humidity &lt;dbl&gt; 74, 100, 92, 85, 75, 82, 89, 96, 100, 89... $ max_sea_level_pressure_in &lt;dbl&gt; 30.45, 29.58, 29.81, 29.88, 29.86, 29.91... $ max_temperature_f &lt;dbl&gt; 64, 48, 39, 39, 42, 45, 42, 44, 49, 44, ... $ max_visibility_miles &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, ... $ max_wind_speed_mph &lt;dbl&gt; 22, 23, 21, 16, 17, 15, 15, 8, 20, 23, 1... $ mean_humidity &lt;dbl&gt; 63, 95, 87, 75, 65, 68, 75, 85, 85, 73, ... $ mean_sea_level_pressure_in &lt;dbl&gt; 30.13, 29.50, 29.61, 29.85, 29.82, 29.83... $ mean_temperature_f &lt;dbl&gt; 52, 43, 36, 35, 37, 39, 37, 40, 45, 40, ... $ mean_visibility_miles &lt;dbl&gt; 10, 3, 7, 10, 10, 10, 10, 9, 6, 10, 10, ... $ mean_wind_speed_mph &lt;dbl&gt; 13, 13, 13, 11, 12, 10, 6, 4, 11, 14, 11... $ mean_dew_point_f &lt;dbl&gt; 40, 39, 31, 27, 26, 27, 29, 36, 41, 30, ... $ min_dew_point_f &lt;dbl&gt; 26, 37, 27, 25, 24, 25, 27, 30, 32, 26, ... $ min_humidity &lt;dbl&gt; 52, 89, 82, 64, 55, 53, 60, 73, 70, 57, ... $ min_sea_level_pressure_in &lt;dbl&gt; 30.01, 29.43, 29.44, 29.81, 29.78, 29.78... $ min_temperature_f &lt;dbl&gt; 39, 38, 32, 31, 32, 33, 32, 35, 41, 36, ... $ min_visibility_miles &lt;dbl&gt; 10, 1, 1, 7, 10, 10, 10, 5, 1, 10, 10, 2... $ precipitation_in &lt;dbl&gt; 0.01, 0.28, 0.02, 0.00, 0.00, 0.00, 0.00... $ wind_dir_degrees &lt;dbl&gt; 268, 357, 230, 286, 298, 306, 324, 79, 3... . summary(weather6) . date events cloud_cover max_dew_point_f Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 max_gust_speed_mph max_humidity max_sea_level_pressure_in Min. : 0.00 Min. : 39.00 Min. :29.58 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 Median :25.50 Median : 86.00 Median :30.14 Mean :26.99 Mean : 83.23 Mean :30.16 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 Max. :94.00 Max. :100.00 Max. :30.88 NA&#39;s :6 max_temperature_f max_visibility_miles max_wind_speed_mph mean_humidity Min. :18.00 Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:42.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :60.00 Median :10.000 Median :20.00 Median :66.00 Mean :58.93 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:76.00 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :96.00 Max. :10.000 Max. :38.00 Max. :98.00 mean_sea_level_pressure_in mean_temperature_f mean_visibility_miles Min. :29.49 Min. : 8.00 Min. : 1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.891 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 mean_wind_speed_mph mean_dew_point_f min_dew_point_f min_humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 min_sea_level_pressure_in min_temperature_f min_visibility_miles Min. :29.16 Min. :-3.00 Min. : 0.000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 Median :29.94 Median :46.00 Median :10.000 Mean :29.93 Mean :43.33 Mean : 6.716 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 Max. :30.64 Max. :74.00 Max. :10.000 precipitation_in wind_dir_degrees Min. :0.0000 Min. : 1.0 1st Qu.:0.0000 1st Qu.:113.0 Median :0.0000 Median :222.0 Mean :0.1016 Mean :200.1 3rd Qu.:0.0400 3rd Qu.:275.0 Max. :2.9000 Max. :360.0 .",
            "url": "https://victoromondi1997.github.io/blog/2020/06/15/Exploring-Boston-Weather-Data.html",
            "relUrl": "/2020/06/15/Exploring-Boston-Weather-Data.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://victoromondi1997.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://victoromondi1997.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "“Hello! Thanks for visiting my blog, I’m Victor Omondi, Data Scientist with industry experience analyzing, visualizing and modeling data. I specialize in Python &amp; R and have professional experience working with C++ and JavaScript|, currently am a student at Multimedia University of Kenya pursuing Bachelor of Science (Computer Technology)” . Twitter . Tweets by VictorOmondi197 This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://victoromondi1997.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://victoromondi1997.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}