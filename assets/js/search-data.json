{
  
    
        "post0": {
            "title": "Statistical Thinking in Python (Part 2)",
            "content": "Libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() %matplotlib inline . Parameter estimation by optimization . When doing statistical inference, we speak the language of probability. A probability distribution that describes your data has parameters. So, a major goal of statistical inference is to estimate the values of these parameters, which allows us to concisely and unambiguously describe our data and draw conclusions from it. We will find the optimal parameters, those that best describe your data. . Optimal parameters . Optimal parameters . Parameter values that bring the model in closest agreement with the data | . Packages to do statistical inference . package . | scipy.stats | | | statsmodels | . | hacker stats with numpy | . How often do we get no-hitters? . The number of games played between each no-hitter in the modern era ($1901-2015$) of Major League Baseball is stored in the array nohitter_times. . nohitter_times = np.array([ 843, 1613, 1101, 215, 684, 814, 278, 324, 161, 219, 545, 715, 966, 624, 29, 450, 107, 20, 91, 1325, 124, 1468, 104, 1309, 429, 62, 1878, 1104, 123, 251, 93, 188, 983, 166, 96, 702, 23, 524, 26, 299, 59, 39, 12, 2, 308, 1114, 813, 887, 645, 2088, 42, 2090, 11, 886, 1665, 1084, 2900, 2432, 750, 4021, 1070, 1765, 1322, 26, 548, 1525, 77, 2181, 2752, 127, 2147, 211, 41, 1575, 151, 479, 697, 557, 2267, 542, 392, 73, 603, 233, 255, 528, 397, 1529, 1023, 1194, 462, 583, 37, 943, 996, 480, 1497, 717, 224, 219, 1531, 498, 44, 288, 267, 600, 52, 269, 1086, 386, 176, 2199, 216, 54, 675, 1243, 463, 650, 171, 327, 110, 774, 509, 8, 197, 136, 12, 1124, 64, 380, 811, 232, 192, 731, 715, 226, 605, 539, 1491, 323, 240, 179, 702, 156, 82, 1397, 354, 778, 603, 1001, 385, 986, 203, 149, 576, 445, 180, 1403, 252, 675, 1351, 2983, 1568, 45, 899, 3260, 1025, 31, 100, 2055, 4043, 79, 238, 3931, 2351, 595, 110, 215, 0, 563, 206, 660, 242, 577, 179, 157, 192, 192, 1848, 792, 1693, 55, 388, 225, 1134, 1172, 1555, 31, 1582, 1044, 378, 1687, 2915, 280, 765, 2819, 511, 1521, 745, 2491, 580, 2072, 6450, 578, 745, 1075, 1103, 1549, 1520, 138, 1202, 296, 277, 351, 391, 950, 459, 62, 1056, 1128, 139, 420, 87, 71, 814, 603, 1349, 162, 1027, 783, 326, 101, 876, 381, 905, 156, 419, 239, 119, 129, 467]) . If we assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As we have seen, the Exponential distribution has a single parameter, which we will call τ, the typical interval time. The value of the parameter τ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. . We will compute the value of this parameter from the data. Then, use np.random.exponential() to &quot;repeat&quot; the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the τ we found and plot the histogram as an approximation to the PDF. . # Seed random number generator np.random.seed(42) # Compute mean no-hitter time: tau tau = np.mean(nohitter_times) # Draw out of an exponential distribution with parameter tau: inter_nohitter_time inter_nohitter_time = np.random.exponential(tau, 100000) # Plot the PDF and label axes _ = plt.hist(inter_nohitter_time, bins=50, density=True, histtype=&quot;step&quot;) _ = plt.xlabel(&#39;Games between no-hitters&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . Note: We see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right. . Do the data follow our story? . We have modeled no-hitters using an Exponential distribution. Let&#39;s create an ECDF of the real data. Overlay the theoretical CDF with the ECDF from the data. This helps us to verify that the Exponential distribution describes the observed data. . def ecdf(data): return np.sort(data), np.arange(1, len(data)+1) / len(data) . # Create an ECDF from real data: x, y x, y = ecdf(nohitter_times) # Create a CDF from theoretical samples: x_theor, y_theor x_theor, y_theor = ecdf(inter_nohitter_time) # Overlay the plots plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Margins and axis labels plt.margins(.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Show the plot plt.show() . It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was. . How is this parameter optimal? . We will now sample out of an exponential distribution with $ tau$ being twice as large as the optimal $ tau$. Do it again for $ tau$ half as large. Make CDFs of these samples and overlay them with our data. . # Plot the theoretical CDFs plt.plot(x_theor, y_theor) plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) plt.xlabel(&#39;Games between no-hitters&#39;) plt.ylabel(&#39;CDF&#39;) # Take samples with half tau: samples_half samples_half = np.random.exponential(tau/2, size=10000) # Take samples with double tau: samples_double samples_double = np.random.exponential(tau*2, size=10000) # Generate CDFs from these samples x_half, y_half = ecdf(samples_half) x_double, y_double = ecdf(samples_double) # Plot these CDFs as lines _ = plt.plot(x_half, y_half) _ = plt.plot(x_double, y_double) # Show the plot plt.show() . . Note: Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter. We can see that they do not reproduce the data as well. Thus, the $ tau$ we computed from the mean inter-no-hitter times is optimal in that it best reproduces the data. . Linear regression by least squares . Least squares . The process of nding the parameters for which the sum ofthe squares ofthe residuals is minimal | . EDA of literacy/fertility data . we will look at the correlation between female literacy and fertility (defined as the average number of children born per woman) throughout the world. For ease of analysis and interpretation, we will work with the illiteracy rate. . illiteracy= np.array([ 9.5, 49.2, 1. , 11.2, 9.8, 60. , 50.2, 51.2, 0.6, 1. , 8.5, 6.1, 9.8, 1. , 42.2, 77.2, 18.7, 22.8, 8.5, 43.9, 1. , 1. , 1.5, 10.8, 11.9, 3.4, 0.4, 3.1, 6.6, 33.7, 40.4, 2.3, 17.2, 0.7, 36.1, 1. , 33.2, 55.9, 30.8, 87.4, 15.4, 54.6, 5.1, 1.1, 10.2, 19.8, 0. , 40.7, 57.2, 59.9, 3.1, 55.7, 22.8, 10.9, 34.7, 32.2, 43. , 1.3, 1. , 0.5, 78.4, 34.2, 84.9, 29.1, 31.3, 18.3, 81.8, 39. , 11.2, 67. , 4.1, 0.2, 78.1, 1. , 7.1, 1. , 29. , 1.1, 11.7, 73.6, 33.9, 14. , 0.3, 1. , 0.8, 71.9, 40.1, 1. , 2.1, 3.8, 16.5, 4.1, 0.5, 44.4, 46.3, 18.7, 6.5, 36.8, 18.6, 11.1, 22.1, 71.1, 1. , 0. , 0.9, 0.7, 45.5, 8.4, 0. , 3.8, 8.5, 2. , 1. , 58.9, 0.3, 1. , 14. , 47. , 4.1, 2.2, 7.2, 0.3, 1.5, 50.5, 1.3, 0.6, 19.1, 6.9, 9.2, 2.2, 0.2, 12.3, 4.9, 4.6, 0.3, 16.5, 65.7, 63.5, 16.8, 0.2, 1.8, 9.6, 15.2, 14.4, 3.3, 10.6, 61.3, 10.9, 32.2, 9.3, 11.6, 20.7, 6.5, 6.7, 3.5, 1. , 1.6, 20.5, 1.5, 16.7, 2. , 0.9]) . fertility = np.array([1.769, 2.682, 2.077, 2.132, 1.827, 3.872, 2.288, 5.173, 1.393, 1.262, 2.156, 3.026, 2.033, 1.324, 2.816, 5.211, 2.1 , 1.781, 1.822, 5.908, 1.881, 1.852, 1.39 , 2.281, 2.505, 1.224, 1.361, 1.468, 2.404, 5.52 , 4.058, 2.223, 4.859, 1.267, 2.342, 1.579, 6.254, 2.334, 3.961, 6.505, 2.53 , 2.823, 2.498, 2.248, 2.508, 3.04 , 1.854, 4.22 , 5.1 , 4.967, 1.325, 4.514, 3.173, 2.308, 4.62 , 4.541, 5.637, 1.926, 1.747, 2.294, 5.841, 5.455, 7.069, 2.859, 4.018, 2.513, 5.405, 5.737, 3.363, 4.89 , 1.385, 1.505, 6.081, 1.784, 1.378, 1.45 , 1.841, 1.37 , 2.612, 5.329, 5.33 , 3.371, 1.281, 1.871, 2.153, 5.378, 4.45 , 1.46 , 1.436, 1.612, 3.19 , 2.752, 3.35 , 4.01 , 4.166, 2.642, 2.977, 3.415, 2.295, 3.019, 2.683, 5.165, 1.849, 1.836, 2.518, 2.43 , 4.528, 1.263, 1.885, 1.943, 1.899, 1.442, 1.953, 4.697, 1.582, 2.025, 1.841, 5.011, 1.212, 1.502, 2.516, 1.367, 2.089, 4.388, 1.854, 1.748, 2.978, 2.152, 2.362, 1.988, 1.426, 3.29 , 3.264, 1.436, 1.393, 2.822, 4.969, 5.659, 3.24 , 1.693, 1.647, 2.36 , 1.792, 3.45 , 1.516, 2.233, 2.563, 5.283, 3.885, 0.966, 2.373, 2.663, 1.251, 2.052, 3.371, 2.093, 2. , 3.883, 3.852, 3.718, 1.732, 3.928]) . It is always a good idea to do some EDA ahead of our analysis. To this end, we will plot the fertility versus illiteracy and compute the Pearson correlation coefficient. . Note: The Numpy array illiteracy has the illiteracy rate among females for most of the world&#8217;s nations. . Note: The array fertility has the corresponding fertility data. . def pearson_r(x, y): return np.corrcoef(x, y)[0,1] . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins and label axes plt.margins(.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Show the plot plt.show() # Show the Pearson correlation coefficient pearson_r(illiteracy, fertility) . 0.8041324026815346 . We can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman. . Linear regression . We will assume that fertility is a linear function of the female illiteracy rate. That is, $f=ai+b$, where $a$ is the slope and $b$ is the intercept. . Note: We can think of the intercept as the minimal fertility rate, probably somewhere between one and two. . The slope tells us how the fertility rate varies with illiteracy. We can find the best fit line using np.polyfit(). . # Plot the illiteracy rate versus fertility _ = plt.plot(illiteracy, fertility, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.margins(0.02) _ = plt.xlabel(&#39;percent illiterate&#39;) _ = plt.ylabel(&#39;fertility&#39;) # Perform a linear regression using np.polyfit(): a, b a, b = np.polyfit(illiteracy, fertility, deg=1) # Print the results to the screen print(&#39;slope =&#39;, a, &#39;children per woman / percent illiterate&#39;) print(&#39;intercept =&#39;, b, &#39;children per woman&#39;) # Make theoretical line to plot x = np.array([0, 100]) y = a * x + b # Add regression line to your plot _ = plt.plot(x, y) # Draw the plot plt.show() . slope = 0.04979854809063423 children per woman / percent illiterate intercept = 1.8880506106365562 children per woman . How is it optimal? . The function np.polyfit() that we used to get your regression parameters finds the optimal slope and intercept. It is optimizing the sum of the squares of the residuals, also known as RSS ( for residual sum of squares ). . We will plot the function that is being optimized, the RSS, versus the slope parameter $a$. To do this, we will fix the intercept to be what we found in the optimization. Then, plot the RSS vs. the slope. . # Specify slopes to consider: a_vals a_vals = np.linspace(0,0.1, 200) # Initialize sum of square of residuals: rss rss = np.empty_like(a_vals) # Compute sum of square of residuals for each value of a_vals for i, a in enumerate(a_vals): rss[i] = np.sum((fertility - a*illiteracy - b)**2) # Plot the RSS plt.plot(a_vals, rss, &#39;-&#39;) plt.xlabel(&#39;slope (children per woman / percent illiterate)&#39;) plt.ylabel(&#39;sum of square of residuals&#39;) plt.show() . . Note: that the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression. . The importance of EDA: Anscombe&#39;s quartet . Look before you leap! . Do graphical EDA rst | . Linear regression on appropriate Anscombe data . We will perform a linear regression on the data set from Anscombe&#39;s quartet that is most reasonably interpreted with linear regression. . x = np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]) y = np.array([ 8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]) . # Perform linear regression: a, b a, b = np.polyfit(x,y, deg=1) # Print the slope and intercept print(a, b) # Generate theoretical x and y data: x_theor, y_theor x_theor = np.array([3, 15]) y_theor = a * x_theor + b # Plot the Anscombe data and theoretical line _ = plt.plot(x, y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_theor, y_theor) # Label the axes plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) # Show the plot plt.show() . 0.5000909090909091 3.000090909090908 . Linear regression on all Anscombe data . Now, to verify that all four of the Anscombe data sets have the same slope and intercept from a linear regression, we will compute the slope and intercept for each set. . anscombe_x = [np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]), np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]), np.array([10., 8., 13., 9., 11., 14., 6., 4., 12., 7., 5.]), np.array([ 8., 8., 8., 8., 8., 8., 8., 19., 8., 8., 8.])] . anscombe_y = [np.array([ 8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]), np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.1 , 6.13, 3.1 , 9.13, 7.26, 4.74]), np.array([ 7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]), np.array([ 6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5 , 5.56, 7.91, 6.89])] . The data are stored in lists; . # Iterate through x,y pairs for x, y in zip(anscombe_x, anscombe_y): # Compute the slope and intercept: a, b a, b = np.polyfit(x,y, deg=1) # Print the result print(&#39;slope:&#39;, a, &#39;intercept:&#39;, b) . slope: 0.5000909090909091 intercept: 3.000090909090908 slope: 0.5 intercept: 3.0009090909090905 slope: 0.4997272727272729 intercept: 3.002454545454545 slope: 0.4999090909090908 intercept: 3.001727272727274 . . Important: Indeed, they all have the same slope and intercept. . Bootstrap confidence intervals . To &quot;pull yourself up by your bootstraps&quot; is a classic idiom meaning that you achieve a difficult task by yourself with no help at all. In statistical inference, we want to know what would happen if we could repeat our data acquisition an infinite number of times. This task is impossible, but can we use only the data we actually have to get close to the same result as an infinitude of experiments? The answer is yes! The technique to do it is aptly called bootstrapping. . Generating bootstrap replicates . Bootstrapping . The use of resampled data to perform statistical inference | . Bootstrap sample . A resampled array ofthe data | . Bootstrap replicate . A statistic computed from a resampled array | . Getting the terminology down . If we have a data set with $n$ repeated measurements, a bootstrap sample is an array of length $n$ that was drawn from the original data with replacement. bootstrap replicate is A single value of a statistic computed from a bootstrap sample. . Visualizing bootstrap samples . We will generate bootstrap samples from the set of annual rainfall data measured at the Sheffield Weather Station in the UK from 1883 to 2015. . rainfall = np.array([ 875.5, 648.2, 788.1, 940.3, 491.1, 743.5, 730.1, 686.5, 878.8, 865.6, 654.9, 831.5, 798.1, 681.8, 743.8, 689.1, 752.1, 837.2, 710.6, 749.2, 967.1, 701.2, 619. , 747.6, 803.4, 645.6, 804.1, 787.4, 646.8, 997.1, 774. , 734.5, 835. , 840.7, 659.6, 828.3, 909.7, 856.9, 578.3, 904.2, 883.9, 740.1, 773.9, 741.4, 866.8, 871.1, 712.5, 919.2, 927.9, 809.4, 633.8, 626.8, 871.3, 774.3, 898.8, 789.6, 936.3, 765.4, 882.1, 681.1, 661.3, 847.9, 683.9, 985.7, 771.1, 736.6, 713.2, 774.5, 937.7, 694.5, 598.2, 983.8, 700.2, 901.3, 733.5, 964.4, 609.3, 1035.2, 718. , 688.6, 736.8, 643.3, 1038.5, 969. , 802.7, 876.6, 944.7, 786.6, 770.4, 808.6, 761.3, 774.2, 559.3, 674.2, 883.6, 823.9, 960.4, 877.8, 940.6, 831.8, 906.2, 866.5, 674.1, 998.1, 789.3, 915. , 737.1, 763. , 666.7, 824.5, 913.8, 905.1, 667.8, 747.4, 784.7, 925.4, 880.2, 1086.9, 764.4, 1050.1, 595.2, 855.2, 726.9, 785.2, 948.8, 970.6, 896. , 618.4, 572.4, 1146.4, 728.2, 864.2, 793. ]) . The data are stored in the NumPy array rainfall in units of millimeters (mm). By graphically displaying the bootstrap samples with an ECDF, we can get a feel for how bootstrap sampling allows probabilistic descriptions of data. . for _ in range(50): # Generate bootstrap sample: bs_sample bs_sample = np.random.choice(rainfall, size=len(rainfall)) # Compute and plot ECDF from bootstrap sample x, y = ecdf(bs_sample) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;gray&#39;, alpha=0.1) # Compute and plot ECDF from original data x, y = ecdf(rainfall) _ = plt.plot(x, y, marker=&#39;.&#39;) # Make margins and label axes plt.margins(0.02) _ = plt.xlabel(&#39;yearly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Show the plot plt.show() . . Note: The bootstrap samples give an idea of how the distribution of rainfalls is spread. . Bootstrap confidence intervals . Condence interval of a statistic . If we repeated measurements over and over again, $p %$ of the observed values would lie within the $p %$ condence interval. | . Generating many bootstrap replicates . def bootstrap_replicate_1d(data, func): &quot;&quot;&quot;Generate bootstrap replicate of 1D data.&quot;&quot;&quot; return func(np.random.choice(data, size=len(data))) . We&#39;ll write another function, draw_bs_reps(data, func, size=1), which generates many bootstrap replicates from the data set. . def draw_bs_reps(data, func, size=1): &quot;&quot;&quot;Draw bootstrap replicates.&quot;&quot;&quot; # Initialize array of replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_replicates[i] = bootstrap_replicate_1d(data, func) return bs_replicates . Bootstrap replicates of the mean and the SEM . We will compute a bootstrap estimate of the probability density function of the mean annual rainfall at the Sheffield Weather Station. . Note: we are estimating the mean annual rainfall we would get if the Sheffield Weather Station could repeat all of the measurements from 1883 to 2015 over and over again. This is a probabilistic estimate of the mean. . We will plot the PDF as a histogram, and you will see that it is Normal. . The standard deviation of this distribution, called the standard error of the mean, or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)). . # Take 10,000 bootstrap replicates of the mean: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000) # Compute and print SEM sem = np.std(rainfall) / np.sqrt(len(rainfall)) print(sem) # Compute and print standard deviation of bootstrap replicates bs_std = np.std(bs_replicates) print(bs_std) # Make a histogram of the results _ = plt.hist(bs_replicates, bins=50, density=True) _ = plt.xlabel(&#39;mean annual rainfall (mm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . 10.510549150506188 10.358764199574097 . . Note: SEM we got from the known expression and the bootstrap replicates is the same and the distribution of the bootstrap replicates of the mean is Normal. . Confidence intervals of rainfall data . A confidence interval gives upper and lower bounds on the range of parameter values you might expect to get if we repeat our measurements. For named distributions, we can compute them analytically or look them up, but one of the many beautiful properties of the bootstrap method is that we can take percentiles of your bootstrap replicates to get your confidence interval. Conveniently, we can use the np.percentile() function. . np.percentile(bs_replicates, [2.5, 97.5]) . array([779.96900376, 820.62793233]) . it&#39;s simple to get confidence intervals using bootstrap! . Bootstrap replicates of other statistics . We&#39;ll generate bootstrap replicates for the variance of the annual rainfall at the Sheffield Weather Station and plot the histogram of the replicates. . # Generate 10,000 bootstrap replicates of the variance: bs_replicates bs_replicates = draw_bs_reps(rainfall, np.var, size=10000) # Put the variance in units of square centimeters bs_replicates = bs_replicates/100 # Make a histogram of the results _ = plt.hist(bs_replicates, density=True, bins=50) _ = plt.xlabel(&#39;variance of annual rainfall (sq. cm)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . . Note: This is not normally distributed, as it has a longer tail to the right. . Confidence interval on the rate of no-hitters . # Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000) # Compute the 95% confidence interval: conf_int conf_int = np.percentile(bs_replicates, [2.5, 97.5]) # Print the confidence interval print(&#39;95% confidence interval =&#39;, conf_int, &#39;games&#39;) # Plot the histogram of the replicates _ = plt.hist(bs_replicates, bins=50, density=True) _ = plt.xlabel(r&#39;$ tau$ (games)&#39;) _ = plt.ylabel(&#39;PDF&#39;) # Show the plot plt.show() . 95% confidence interval = [663.65229084 869.79741036] games . . Note: This gives us an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games. . Pairs bootstrap . Nonparametric inference . Make no assumptions about the model or probability distribution underlying the data | . Pairs bootstrap for linear regression . Resample data in pairs | Compute slope and intercept from resampled data | Each slope and intercept is a bootstrap replicate | Compute condence intervals from percentiles of bootstrap replicates | . A function to do pairs bootstrap . pairs bootstrap involves resampling pairs of data. Each collection of pairs fit with a line, in this case using np.polyfit(). We do this again and again, getting bootstrap replicates of the parameter values. To have a useful tool for doing pairs bootstrap, we will write a function to perform pairs bootstrap on a set of x,y data. . def draw_bs_pairs_linreg(x, y, size=1): &quot;&quot;&quot;Perform pairs bootstrap for linear regression.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_slope_reps, bs_intercept_reps bs_slope_reps = np.empty(size) bs_intercept_reps = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, size=len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1) return bs_slope_reps, bs_intercept_reps . Pairs bootstrap of literacy/fertility data . Using the function we just wrote, we&#39;ll perform pairs bootstrap to plot a histogram describing the estimate of the slope from the illiteracy/fertility data. Also reporting the 95% confidence interval of the slope. . # Generate replicates of slope and intercept using pairs bootstrap bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000) # Compute and print 95% CI for slope print(np.percentile(bs_slope_reps, [2.5, 97.5])) # Plot the histogram _ = plt.hist(bs_slope_reps, bins=50, density=True) _ = plt.xlabel(&#39;slope&#39;) _ = plt.ylabel(&#39;PDF&#39;) plt.show() . [0.04389859 0.05528877] . Plotting bootstrap regressions . A nice way to visualize the variability we might expect in a linear regression is to plot the line we would get from each bootstrap replicate of the slope and intercept. We&#39;ll do this for the first 100 of our bootstrap replicates of the slope and intercept . # Generate array of x-values for bootstrap lines: x x = np.array([0,100]) # Plot the bootstrap lines for i in range(100): _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color=&#39;red&#39;) # Plot the data _ = plt.plot(illiteracy, fertility, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Label axes, set the margins, and show the plot _ = plt.xlabel(&#39;illiteracy&#39;) _ = plt.ylabel(&#39;fertility&#39;) plt.margins(0.02) plt.show() . Introduction to hypothesis testing . how reasonable is it to observe our data if a model is true? This question is addressed by hypothesis tests. They are the icing on the inference cake. We carefully construct and test hypotheses using hacker statistics. . Formulating and simulating a hypothesis . Hypothesis testing . Assessment of how reasonable the observed data are assuming a hypothesis is true | . Null hypothesis . Another name for the hypothesis you are testing | . Permutation . Random reordering of entries in an array | . Generating a permutation sample . permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so we will write a function to generate a permutation sample from two data sets. . . Note: a permutation sample of two arrays having respectively n1 and n2 entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first n1 entries as the permutation sample of the first array and the last n2 entries as the permutation sample of the second array. . def permutation_sample(data1, data2): &quot;&quot;&quot;Generate a permutation sample from two data sets.&quot;&quot;&quot; # Concatenate the data sets: data data = np.concatenate((data1,data2)) # Permute the concatenated array: permuted_data permuted_data = np.random.permutation(data) # Split the permuted array into two: perm_sample_1, perm_sample_2 perm_sample_1 = permuted_data[:len(data1)] perm_sample_2 = permuted_data[len(data1):] return perm_sample_1, perm_sample_2 . Visualizing permutation sampling . To help see how permutation sampling works, we will generate permutation samples and look at them graphically. . We will use the Sheffield Weather Station data again, this time considering the monthly rainfall in June (a dry month) and November (a wet month). We expect these might be differently distributed, so we will take permutation samples to see how their ECDFs would look if they were identically distributed. . rain_june = np.array([ 66.2, 39.7, 76.4, 26.5, 11.2, 61.8, 6.1, 48.4, 89.2, 104. , 34. , 60.6, 57.1, 79.1, 90.9, 32.3, 63.8, 78.2, 27.5, 43.4, 30.1, 17.3, 77.5, 44.9, 92.2, 39.6, 79.4, 66.1, 53.5, 98.5, 20.8, 55.5, 39.6, 56. , 65.1, 14.8, 13.2, 88.1, 8.4, 32.1, 19.6, 40.4, 2.2, 77.5, 105.4, 77.2, 38. , 27.1, 111.8, 17.2, 26.7, 23.3, 77.2, 87.2, 27.7, 50.6, 60.3, 15.1, 6. , 29.4, 39.3, 56.3, 80.4, 85.3, 68.4, 72.5, 13.3, 28.4, 14.7, 37.4, 49.5, 57.2, 85.9, 82.1, 31.8, 126.6, 30.7, 41.4, 33.9, 13.5, 99.1, 70.2, 91.8, 61.3, 13.7, 54.9, 62.5, 24.2, 69.4, 83.1, 44. , 48.5, 11.9, 16.6, 66.4, 90. , 34.9, 132.8, 33.4, 225. , 7.6, 40.9, 76.5, 48. , 140. , 55.9, 54.1, 46.4, 68.6, 52.2, 108.3, 14.6, 11.3, 29.8, 130.9, 152.4, 61. , 46.6, 43.9, 30.9, 111.1, 68.5, 42.2, 9.8, 285.6, 56.7, 168.2, 41.2, 47.8, 166.6, 37.8, 45.4, 43.2]) . rain_november = np.array([ 83.6, 30.9, 62.2, 37. , 41. , 160.2, 18.2, 122.4, 71.3, 44.2, 49.1, 37.6, 114.5, 28.8, 82.5, 71.9, 50.7, 67.7, 112. , 63.6, 42.8, 57.2, 99.1, 86.4, 84.4, 38.1, 17.7, 102.2, 101.3, 58. , 82. , 101.4, 81.4, 100.1, 54.6, 39.6, 57.5, 29.2, 48.8, 37.3, 115.4, 55.6, 62. , 95. , 84.2, 118.1, 153.2, 83.4, 104.7, 59. , 46.4, 50. , 147.6, 76.8, 59.9, 101.8, 136.6, 173. , 92.5, 37. , 59.8, 142.1, 9.9, 158.2, 72.6, 28. , 112.9, 119.3, 199.2, 50.7, 44. , 170.7, 67.2, 21.4, 61.3, 15.6, 106. , 116.2, 42.3, 38.5, 132.5, 40.8, 147.5, 93.9, 71.4, 87.3, 163.7, 141.4, 62.6, 84.9, 28.8, 121.1, 28.6, 32.4, 112. , 50. , 96.9, 81.8, 70.4, 117.5, 41.2, 124.9, 78.2, 93. , 53.5, 50.5, 42.6, 47.9, 73.1, 129.1, 56.9, 103.3, 60.5, 134.3, 93.1, 49.5, 48.2, 167.9, 27. , 111.1, 55.4, 36.2, 57.4, 66.8, 58.3, 60. , 161.6, 112.7, 37.4, 110.6, 56.6, 95.8, 126.8]) . for _ in range(50): # Generate permutation samples perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november) # Compute ECDFs x_1, y_1 = ecdf(perm_sample_1) x_2, y_2 = ecdf(perm_sample_2) # Plot ECDFs of permutation sample _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.02) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.02) # Create and plot ECDFs from original data x_1, y_1 = ecdf(rain_june) x_2, y_2 = ecdf(rain_november) _ = plt.plot(x_1, y_1, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;) _ = plt.plot(x_2, y_2, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;) # Label axes, set margin, and show plot plt.margins(0.02) _ = plt.xlabel(&#39;monthly rainfall (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) plt.show() . . Note: The permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed. . Test statistics and p-values . Hypothesis testing . Assessment of how reasonable the observed data are assuming a hypothesis is true | . Test statistic . A single number that can be computed from observed data and from data you simulate under the null hypothesis | It serves as a basis of comparison between the two | . p-value . The probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true | NOT the probability that the null hypothesis is true | . Statistical signicance . Determined by the smallness of a p-value | . Null hypothesis signicance testing (NHST) . Another name for Hypothesis testing | . Test statistics . When performing hypothesis tests, the choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test. . Important: The most important thing to consider is: What are you asking? . p-value . The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true. . Generating permutation replicates . a permutation replicate is a single value of a statistic computed from a permutation sample. . As the draw_bs_reps() function is useful for generating bootstrap replicates, it is useful to have a similar function, draw_perm_reps(), to generate permutation replicates. . def draw_perm_reps(data_1, data_2, func, size=1): &quot;&quot;&quot;Generate multiple permutation replicates.&quot;&quot;&quot; # Initialize array of replicates: perm_replicates perm_replicates = np.empty(size) for i in range(size): # Generate permutation sample perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2) # Compute the test statistic perm_replicates[i] = func(perm_sample_1,perm_sample_2) return perm_replicates . Look before you leap: EDA before hypothesis testing . Kleinteich and Gorb (Sci. Rep., 4, 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog&#39;s tongue when it struck the target. . Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. we will test the hypothesis that the two frogs have the same distribution of impact forces. it is important to do EDA first! Let&#39;s make a bee swarm plot for the data. . They are stored in a Pandas data frame, frogs, where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N). . frogs = pd.read_csv(&quot;datasets/frogs.csv&quot;) frogs.head() . ID impact_force . 0 A | 1.612 | . 1 A | 0.605 | . 2 A | 0.327 | . 3 A | 0.946 | . 4 A | 0.541 | . # Make bee swarm plot _ = sns.swarmplot(data=frogs, x=&quot;ID&quot;, y=&quot;impact_force&quot;) # Label axes _ = plt.xlabel(&#39;frog&#39;) _ = plt.ylabel(&#39;impact force (N)&#39;) # Show the plot plt.show() . Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test. . Permutation test on frog data . frogs[frogs.ID==&quot;A&quot;].impact_force.mean() . 0.70735 . frogs[frogs.ID==&quot;B&quot;].impact_force.mean() . 0.4191000000000001 . frogs[frogs.ID==&quot;A&quot;].impact_force.mean() - frogs[frogs.ID==&quot;B&quot;].impact_force.mean() . 0.28824999999999995 . The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. . force_a = np.array([1.612, 0.605, 0.327, 0.946, 0.541, 1.539, 0.529, 0.628, 1.453, 0.297, 0.703, 0.269, 0.751, 0.245, 1.182, 0.515, 0.435, 0.383, 0.457, 0.73 ]) force_b = np.array([0.172, 0.142, 0.037, 0.453, 0.355, 0.022, 0.502, 0.273, 0.72 , 0.582, 0.198, 0.198, 0.597, 0.516, 0.815, 0.402, 0.605, 0.711, 0.614, 0.468]) . We will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis. . def diff_of_means(data_1, data_2): &quot;&quot;&quot;Difference in means of two arrays.&quot;&quot;&quot; # The difference of means of data_1, data_2: diff diff = np.mean(data_1) - np.mean(data_2) return diff . # Compute difference of mean impact force from experiment: empirical_diff_means empirical_diff_means = diff_of_means(force_a, force_b) empirical_diff_means . 0.28825000000000006 . # Draw 10,000 permutation replicates: perm_replicates perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000) # Compute p-value: p p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates) # Print the result print(&#39;p-value =&#39;, p) . p-value = 0.0058 . The p-value tells us that there is about a 0.6% chance that we would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be &quot;statistically significant,&quot; but: . Warning: Warning! warning! warning! We have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be &quot;statistically significant,&quot; but they are definitely not the same! . Bootstrap hypothesis tests . Pipeline for hypothesis testing . Clearly state the null hypothesis | Define your test statistic | Generate many sets of simulated data assuming the null hypothesis is true | Compute the test statistic for each simulated data set | The p-value is the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data | . A one-sample bootstrap hypothesis test . Another juvenile frog was studied, Frog C, and we want to see if Frog B and Frog C have similar impact forces. Unfortunately, we do not have Frog C&#39;s impact forces available, but we know they have a mean of 0.55 N. Because we don&#39;t have the original data, we cannot do a permutation test, and we cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. We will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C. . To set up the bootstrap hypothesis test, we will take the mean as our test statistic. Our goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B&#39;s impact forces is equal to that of Frog C is true. We will first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B&#39;s distribution, such as the variance, unchanged. . # Make an array of translated impact forces: translated_force_b translated_force_b = force_b - np.mean(force_b) + 0.55 # Take bootstrap replicates of Frog B&#39;s translated impact forces: bs_replicates bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000) # Compute fraction of replicates that are less than the observed Frog B force: p p = np.sum(bs_replicates &lt;= np.mean(force_b)) / 10000 # Print the p-value print(&#39;p = &#39;, p) . p = 0.0046 . The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false. . A two-sample bootstrap hypothesis test for difference of means . We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test. . To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed. . forces_concat = np.array([1.612, 0.605, 0.327, 0.946, 0.541, 1.539, 0.529, 0.628, 1.453, 0.297, 0.703, 0.269, 0.751, 0.245, 1.182, 0.515, 0.435, 0.383, 0.457, 0.73 , 0.172, 0.142, 0.037, 0.453, 0.355, 0.022, 0.502, 0.273, 0.72 , 0.582, 0.198, 0.198, 0.597, 0.516, 0.815, 0.402, 0.605, 0.711, 0.614, 0.468]) . empirical_diff_means = 0.28825000000000006 . Hypothesis test examples . Hypothesis testing can be a bit tricky. We need to define the null hypothesis, figure out how to simulate it, and define clearly what it means to be &quot;more extreme&quot; in order to compute the p-value. Like any skill, practice makes perfect. . A/B testing . A/B test . Used by organizations to see if a strategy change gives a better result | . Null hypothesis of an A/B test . The test statistic is impervious to the change | . The vote for the Civil Rights Act in 1964 . The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding &quot;present&quot; and &quot;abstain&quot; votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote? . To answer this question, we will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. We will use the fraction of Democrats voting in favor as our test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That&#39;s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, we will permute the party labels of the House voters and then arbitrarily divide them into &quot;Democrats&quot; and &quot;Republicans&quot; and compute the fraction of Democrats voting yea. . # Construct arrays of data: dems, reps dems = np.array([True] * 153 + [False] * 91) reps = np.array([True]*136 + [False]*35) def frac_yea_dems(dems, reps): &quot;&quot;&quot;Compute fraction of Democrat yea votes.&quot;&quot;&quot; frac = np.sum(dems) / len(dems) return frac # Acquire permutation samples: perm_replicates perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000) # Compute and print p-value: p p = np.sum(perm_replicates &lt;= 153/244) / len(perm_replicates) print(&#39;p-value =&#39;, p) . p-value = 0.0002 . This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias. . A time-on-website analog . It turns out that we already did a hypothesis test analogous to an A/B test where we are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. &quot;Before&quot; = Frog A and &quot;after&quot; = Frog B. Let&#39;s practice this again with something that actually is a before/after scenario. . We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem we will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as our test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live, where &quot;nht&quot; is meant to stand for &quot;no-hitter time.&quot; . nht_dead = np.array([ -1, 894, 10, 130, 1, 934, 29, 6, 485, 254, 372, 81, 191, 355, 180, 286, 47, 269, 361, 173, 246, 492, 462, 1319, 58, 297, 31, 2970, 640, 237, 434, 570, 77, 271, 563, 3365, 89, 0, 379, 221, 479, 367, 628, 843, 1613, 1101, 215, 684, 814, 278, 324, 161, 219, 545, 715, 966, 624, 29, 450, 107, 20, 91, 1325, 124, 1468, 104, 1309, 429, 62, 1878, 1104, 123, 251, 93, 188, 983, 166, 96, 702, 23, 524, 26, 299, 59, 39, 12, 2, 308, 1114, 813, 887]) . nht_live = np.array([ 645, 2088, 42, 2090, 11, 886, 1665, 1084, 2900, 2432, 750, 4021, 1070, 1765, 1322, 26, 548, 1525, 77, 2181, 2752, 127, 2147, 211, 41, 1575, 151, 479, 697, 557, 2267, 542, 392, 73, 603, 233, 255, 528, 397, 1529, 1023, 1194, 462, 583, 37, 943, 996, 480, 1497, 717, 224, 219, 1531, 498, 44, 288, 267, 600, 52, 269, 1086, 386, 176, 2199, 216, 54, 675, 1243, 463, 650, 171, 327, 110, 774, 509, 8, 197, 136, 12, 1124, 64, 380, 811, 232, 192, 731, 715, 226, 605, 539, 1491, 323, 240, 179, 702, 156, 82, 1397, 354, 778, 603, 1001, 385, 986, 203, 149, 576, 445, 180, 1403, 252, 675, 1351, 2983, 1568, 45, 899, 3260, 1025, 31, 100, 2055, 4043, 79, 238, 3931, 2351, 595, 110, 215, 0, 563, 206, 660, 242, 577, 179, 157, 192, 192, 1848, 792, 1693, 55, 388, 225, 1134, 1172, 1555, 31, 1582, 1044, 378, 1687, 2915, 280, 765, 2819, 511, 1521, 745, 2491, 580, 2072, 6450, 578, 745, 1075, 1103, 1549, 1520, 138, 1202, 296, 277, 351, 391, 950, 459, 62, 1056, 1128, 139, 420, 87, 71, 814, 603, 1349, 162, 1027, 783, 326, 101, 876, 381, 905, 156, 419, 239, 119, 129, 467]) . # Compute the observed difference in mean inter-no-hitter times: nht_diff_obs nht_diff_obs = diff_of_means(nht_dead, nht_live) # Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000) # Compute and print the p-value: p p = sum(perm_replicates &lt;= nht_diff_obs)/len(perm_replicates) print(&#39;p-val =&#39;, p) . p-val = 0.0002 . our p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. . Warning: Watch out, though, we could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001. . x_dead, y_dead = ecdf(nht_dead) x_live, y_live = ecdf(nht_live) _ = plt.plot(x_dead, y_dead) _ = plt.plot(x_live, y_live) _ = plt.xlabel(&quot;non hitter times&quot;) _ = plt.legend([&quot;nht dead&quot;, &quot;nht live&quot;]) _ = plt.ylabel(&quot;CDF&quot;) plt.show() . Test of correlation . Hypothesis test of correlation . Posit null hypothesis:the two variables are completely uncorrelated- Simulate data assuming null hypothesis is true | Use Pearson correlation, $ rho$, as test statistic | Compute p-value as fraction of replicates that have ρ at least as large as observed. | . Simulating a null hypothesis concerning correlation . The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. We will test this null hypothesis. . To do the test, we need to simulate the data assuming the null hypothesis is true. The best way to it is to Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data. . Hypothesis test on Pearson correlation . The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. We will test this hypothesis. To do so, we&#39;ll permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, we&#39;ll compute the Pearson correlation coefficient and assess how many of the permutation replicates have a Pearson correlation coefficient greater than the observed one. . # Compute observed correlation: r_obs r_obs = pearson_r(illiteracy, fertility) # Initialize permutation replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute illiteracy measurments: illiteracy_permuted illiteracy_permuted = np.random.permutation(illiteracy) # Compute Pearson correlation perm_replicates[i] = pearson_r(illiteracy_permuted, fertility) # Compute p-value: p p = np.sum(perm_replicates &gt;= r_obs) / len(perm_replicates) print(&#39;p-val =&#39;, p) . p-val = 0.0 . We got a p-value of zero. In hacker statistics, this means that the p-value is very low, since we never got a single replicate in the 10,000 we took that had a Pearson correlation greater than the observed one. we could try increasing the number of replicates you take to continue to move the upper bound on the p-value lower and lower. . Do neonicotinoid insecticides have unintended consequences? . We will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants. . In a recent study, Straub, et al. (Proc. Roy. Soc. B, 2016) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen. . First, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control) and bees treated with pesticide (stored in the Numpy array treated). . control = np.array([ 4.159234, 4.408002, 0.172812, 3.498278, 3.104912, 5.164174, 6.615262, 4.633066, 0.170408, 2.65 , 0.0875 , 1.997148, 6.92668 , 4.574932, 3.896466, 5.209814, 3.70625 , 0. , 4.62545 , 3.01444 , 0.732652, 0.4 , 6.518382, 5.225 , 6.218742, 6.840358, 1.211308, 0.368252, 3.59937 , 4.212158, 6.052364, 2.115532, 6.60413 , 5.26074 , 6.05695 , 6.481172, 3.171522, 3.057228, 0.218808, 5.215112, 4.465168, 2.28909 , 3.732572, 2.17087 , 1.834326, 6.074862, 5.841978, 8.524892, 4.698492, 2.965624, 2.324206, 3.409412, 4.830726, 0.1 , 0. , 4.101432, 3.478162, 1.009688, 4.999296, 4.32196 , 0.299592, 3.606032, 7.54026 , 4.284024, 0.057494, 6.036668, 2.924084, 4.150144, 1.256926, 4.666502, 4.806594, 2.52478 , 2.027654, 2.52283 , 4.735598, 2.033236, 0. , 6.177294, 2.601834, 3.544408, 3.6045 , 5.520346, 4.80698 , 3.002478, 3.559816, 7.075844, 10. , 0.139772, 6.17171 , 3.201232, 8.459546, 0.17857 , 7.088276, 5.496662, 5.415086, 1.932282, 3.02838 , 7.47996 , 1.86259 , 7.838498, 2.242718, 3.292958, 6.363644, 4.386898, 8.47533 , 4.156304, 1.463956, 4.533628, 5.573922, 1.29454 , 7.547504, 3.92466 , 5.820258, 4.118522, 4.125 , 2.286698, 0.591882, 1.273124, 0. , 0. , 0. , 12.22502 , 7.601604, 5.56798 , 1.679914, 8.77096 , 5.823942, 0.258374, 0. , 5.899236, 5.486354, 2.053148, 3.25541 , 2.72564 , 3.364066, 2.43427 , 5.282548, 3.963666, 0.24851 , 0.347916, 4.046862, 5.461436, 4.066104, 0. , 0.065 ]) . treated = np.array([1.342686, 1.058476, 3.793784, 0.40428 , 4.528388, 2.142966, 3.937742, 0.1375 , 6.919164, 0. , 3.597812, 5.196538, 2.78955 , 2.3229 , 1.090636, 5.323916, 1.021618, 0.931836, 2.78 , 0.412202, 1.180934, 2.8674 , 0. , 0.064354, 3.008348, 0.876634, 0. , 4.971712, 7.280658, 4.79732 , 2.084956, 3.251514, 1.9405 , 1.566192, 0.58894 , 5.219658, 0.977976, 3.124584, 1.297564, 1.433328, 4.24337 , 0.880964, 2.376566, 3.763658, 1.918426, 3.74 , 3.841726, 4.69964 , 4.386876, 0. , 1.127432, 1.845452, 0.690314, 4.185602, 2.284732, 7.237594, 2.185148, 2.799124, 3.43218 , 0.63354 , 1.142496, 0.586 , 2.372858, 1.80032 , 3.329306, 4.028804, 3.474156, 7.508752, 2.032824, 1.336556, 1.906496, 1.396046, 2.488104, 4.759114, 1.07853 , 3.19927 , 3.814252, 4.275962, 2.817056, 0.552198, 3.27194 , 5.11525 , 2.064628, 0. , 3.34101 , 6.177322, 0. , 3.66415 , 2.352582, 1.531696]) . # Compute x,y values for ECDFs x_control, y_control = ecdf(control) x_treated, y_treated = ecdf(treated) # Plot the ECDFs plt.plot(x_control, y_control, marker=&#39;.&#39;, linestyle=&#39;none&#39;) plt.plot(x_treated, y_treated, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set the margins plt.margins(0.02) # Add a legend plt.legend((&#39;control&#39;, &#39;treated&#39;), loc=&#39;lower right&#39;) # Label axes and show plot plt.xlabel(&#39;millions of alive sperm per mL&#39;) plt.ylabel(&#39;ECDF&#39;) plt.show() . The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let&#39;s now do a hypothesis test. . Bootstrap hypothesis test on bee sperm counts . We will test the following hypothesis: On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. We will use the difference of means as the test statistic. . # Compute the difference in mean sperm count: diff_means diff_means = np.mean(control) - np.mean(treated) # Compute mean of pooled data: mean_count mean_count = np.mean(np.concatenate((control, treated))) # Generate shifted data sets control_shifted = control - np.mean(control) + mean_count treated_shifted = treated - np.mean(treated) + mean_count # Generate bootstrap replicates bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000) bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000) # Get replicates of difference of means: bs_replicates bs_replicates = bs_reps_control - bs_reps_treated # Compute and print p-value: p p = np.sum(bs_replicates &gt;= np.mean(control) - np.mean(treated)) / len(bs_replicates) print(&#39;p-value =&#39;, p) . p-value = 0.0 . The p-value is small, most likely less than 0.0001, since we never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. . a case study . Every year for the past 40-plus years, Peter and Rosemary Grant have gone to the Galápagos island of Daphne Major and collected data on Darwin&#39;s finches. Using skills in statistical inference, we will explorer the data, and witness first hand, through data, evolution in action. . EDA of beak depths of Darwin&#39;s finches . For our first foray into the Darwin finch data, we will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis. These effects can lead to changes in the species over time. . We will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let&#39;s plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot. . dfd = pd.read_csv(&quot;datasets/darwin_finch_data.csv&quot;) dfd.head() . beak_depth year . 0 8.4 | 1975 | . 1 8.8 | 1975 | . 2 8.4 | 1975 | . 3 8.0 | 1975 | . 4 7.9 | 1975 | . dfd.tail() . beak_depth year . 209 9.3 | 2012 | . 210 9.8 | 2012 | . 211 8.9 | 2012 | . 212 9.8 | 2012 | . 213 9.1 | 2012 | . dfd.dtypes . beak_depth float64 year int64 dtype: object . dfd.describe().T . count mean std min 25% 50% 75% max . beak_depth 214.0 | 9.094252 | 0.637941 | 7.7 | 8.6625 | 9.075 | 9.5 | 11.0 | . year 214.0 | 1996.957944 | 18.216566 | 1975.0 | 1975.0000 | 2012.000 | 2012.0 | 2012.0 | . dfd.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 beak_depth 214 non-null float64 1 year 214 non-null int64 dtypes: float64(1), int64(1) memory usage: 3.5 KB . The units of beak depth are millimeters (mm). . # Create bee swarm plot _ = sns.swarmplot(data=dfd, x=&quot;year&quot;, y=&quot;beak_depth&quot;) # Label the axes _ = plt.xlabel(&#39;year&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) # Show the plot plt.show() . It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance. . ECDFs of beak depths . While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot. . bd_1975 = dfd.beak_depth[dfd.year==1975] bd_2012 = dfd.beak_depth[dfd.year==2012] . # Compute ECDFs x_1975, y_1975 = ecdf(bd_1975) x_2012, y_2012 = ecdf(bd_2012) # Plot the ECDFs _ = plt.plot(x_1975, y_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.plot(x_2012, y_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;) # Set margins plt.margins(.02) # Add axis labels and legend _ = plt.xlabel(&#39;beak depth (mm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;lower right&#39;) # Show the plot plt.show() . The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well. . Parameter estimates of beak depths . Let&#39;s estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. . # Compute the difference of the sample means: mean_diff mean_diff = np.mean(bd_2012) - np.mean(bd_1975) # Get bootstrap replicates of means bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000) # Compute samples of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute 95% confidence interval: conf_int conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5]) # Print the results print(&#39;difference of means =&#39;, mean_diff, &#39;mm&#39;) print(&#39;95% confidence interval =&#39;, conf_int, &#39;mm&#39;) . difference of means = 0.22622047244094645 mm 95% confidence interval = [0.05967226 0.38944932] mm . Hypothesis test: Are beaks deeper in 2012? . Our plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of G. scandens on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same? . Warning: The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means. . # Compute mean of combined data set: combined_mean combined_mean = np.mean(np.concatenate((bd_1975, bd_2012))) # Shift the samples bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean # Get bootstrap replicates of shifted data sets bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000) # Compute replicates of difference of means: bs_diff_replicates bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975 # Compute the p-value p = np.sum(bs_diff_replicates &gt;= mean_diff) / len(bs_diff_replicates) # Print p-value print(&#39;p =&#39;, p) . p = 0.0046 . We get a p-value of 0.0038, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! We got a difference of 0.2 mm between the means. We should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years. . Variation in beak shapes . EDA of beak length and depth . bl_1975 = np.array([13.9 , 14. , 12.9 , 13.5 , 12.9 , 14.6 , 13. , 14.2 , 14. , 14.2 , 13.1 , 15.1 , 13.5 , 14.4 , 14.9 , 12.9 , 13. , 14.9 , 14. , 13.8 , 13. , 14.75, 13.7 , 13.8 , 14. , 14.6 , 15.2 , 13.5 , 15.1 , 15. , 12.8 , 14.9 , 15.3 , 13.4 , 14.2 , 15.1 , 15.1 , 14. , 13.6 , 14. , 14. , 13.9 , 14. , 14.9 , 15.6 , 13.8 , 14.4 , 12.8 , 14.2 , 13.4 , 14. , 14.8 , 14.2 , 13.5 , 13.4 , 14.6 , 13.5 , 13.7 , 13.9 , 13.1 , 13.4 , 13.8 , 13.6 , 14. , 13.5 , 12.8 , 14. , 13.4 , 14.9 , 15.54, 14.63, 14.73, 15.73, 14.83, 15.94, 15.14, 14.23, 14.15, 14.35, 14.95, 13.95, 14.05, 14.55, 14.05, 14.45, 15.05, 13.25]) . bl_2012 = np.array([14.3 , 12.5 , 13.7 , 13.8 , 12. , 13. , 13. , 13.6 , 12.8 , 13.6 , 12.95, 13.1 , 13.4 , 13.9 , 12.3 , 14. , 12.5 , 12.3 , 13.9 , 13.1 , 12.5 , 13.9 , 13.7 , 12. , 14.4 , 13.5 , 13.8 , 13. , 14.9 , 12.5 , 12.3 , 12.8 , 13.4 , 13.8 , 13.5 , 13.5 , 13.4 , 12.3 , 14.35, 13.2 , 13.8 , 14.6 , 14.3 , 13.8 , 13.6 , 12.9 , 13. , 13.5 , 13.2 , 13.7 , 13.1 , 13.2 , 12.6 , 13. , 13.9 , 13.2 , 15. , 13.37, 11.4 , 13.8 , 13. , 13. , 13.1 , 12.8 , 13.3 , 13.5 , 12.4 , 13.1 , 14. , 13.5 , 11.8 , 13.7 , 13.2 , 12.2 , 13. , 13.1 , 14.7 , 13.7 , 13.5 , 13.3 , 14.1 , 12.5 , 13.7 , 14.6 , 14.1 , 12.9 , 13.9 , 13.4 , 13. , 12.7 , 12.1 , 14. , 14.9 , 13.9 , 12.9 , 14.6 , 14. , 13. , 12.7 , 14. , 14.1 , 14.1 , 13. , 13.5 , 13.4 , 13.9 , 13.1 , 12.9 , 14. , 14. , 14.1 , 14.7 , 13.4 , 13.8 , 13.4 , 13.8 , 12.4 , 14.1 , 12.9 , 13.9 , 14.3 , 13.2 , 14.2 , 13. , 14.6 , 13.1 , 15.2 ]) . The beak length data are stored as bl_1975 and bl_2012, again with units of millimeters (mm). We still have the beak depth data stored in bd_1975 and bd_2012. We will make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens. . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;None&#39;, alpha=0.5, color=&quot;blue&quot;) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;None&#39;, alpha=0.5, color=&quot;red&quot;) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Show the plot plt.show() . In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper. . Linear regressions . We perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line. . # Compute the linear regressions slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975.values, deg=1) slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012.values, deg=1) # Perform pairs bootstrap for the linear regressions bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975.values, size=1000) bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012.values, size=1000) # Compute confidence intervals of slopes slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5]) slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5]) intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5]) intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5]) # Print the results print(&#39;1975: slope =&#39;, slope_1975, &#39;conf int =&#39;, slope_conf_int_1975) print(&#39;1975: intercept =&#39;, intercept_1975, &#39;conf int =&#39;, intercept_conf_int_1975) print(&#39;2012: slope =&#39;, slope_2012, &#39;conf int =&#39;, slope_conf_int_2012) print(&#39;2012: intercept =&#39;, intercept_2012, &#39;conf int =&#39;, intercept_conf_int_2012) . 1975: slope = 0.46520516916059357 conf int = [0.3254032 0.58811989] 1975: intercept = 2.3908752365842285 conf int = [0.67000784 4.40072329] 2012: slope = 0.4626303588353126 conf int = [0.33989291 0.60535488] 2012: intercept = 2.9772474982360184 conf int = [1.03792858 4.62029259] . It looks like they have the same slope, but different intercepts. . Displaying the linear regression results . # Make scatter plot of 1975 data _ = plt.plot(bl_1975, bd_1975, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;blue&#39;, alpha=0.5) # Make scatter plot of 2012 data _ = plt.plot(bl_2012, bd_2012, marker=&#39;.&#39;, linestyle=&#39;none&#39;, color=&#39;red&#39;, alpha=0.5) # Label axes and make legend _ = plt.xlabel(&#39;beak length (mm)&#39;) _ = plt.ylabel(&#39;beak depth (mm)&#39;) _ = plt.legend((&#39;1975&#39;, &#39;2012&#39;), loc=&#39;upper left&#39;) # Generate x-values for bootstrap lines: x x = np.array([10, 17]) # Plot the bootstrap lines for i in range(100): plt.plot(x, x* bs_slope_reps_1975[i] + bs_intercept_reps_1975[i], linewidth=0.5, alpha=0.2, color=&quot;blue&quot;) plt.plot(x, x*bs_slope_reps_2012[i] + bs_intercept_reps_2012[i], linewidth=0.5, alpha=0.2, color=&quot;red&quot;) # Draw the plot again plt.show() . Beak length to depth ratio . The linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let&#39;s make that comparison. . # Compute length-to-depth ratios ratio_1975 = bl_1975/bd_1975 ratio_2012 = bl_2012/bd_2012 # Compute means mean_ratio_1975 = np.mean(ratio_1975) mean_ratio_2012 = np.mean(ratio_2012) # Generate bootstrap replicates of the means bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000) bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000) # Compute the 99% confidence intervals conf_int_1975 = np.percentile(bs_replicates_1975, [.5, 99.5]) conf_int_2012 = np.percentile(bs_replicates_2012, [.5, 99.5]) # Print the results print(&#39;1975: mean ratio =&#39;, mean_ratio_1975, &#39;conf int =&#39;, conf_int_1975) print(&#39;2012: mean ratio =&#39;, mean_ratio_2012, &#39;conf int =&#39;, conf_int_2012) . 1975: mean ratio = 1.5788823771858533 conf int = [1.55672661 1.60112763] 2012: mean ratio = 1.4658342276847767 conf int = [1.44471693 1.48753163] . _ = sns.pointplot([np.median(conf_int_1975), np.median(conf_int_2012)], [1975, 2012]) plt.show() . Calculation of heritability . Heredity . The tendency for parental traits to be inherited by offspring | . EDA of heritability . The array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens . bd_parent_scandens = np.array([ 8.3318, 8.4035, 8.5317, 8.7202, 8.7089, 8.7541, 8.773 , 8.8107, 8.7919, 8.8069, 8.6523, 8.6146, 8.6938, 8.7127, 8.7466, 8.7504, 8.7805, 8.7428, 8.7164, 8.8032, 8.8258, 8.856 , 8.9012, 8.9125, 8.8635, 8.8258, 8.8522, 8.8974, 8.9427, 8.9879, 8.9615, 8.9238, 8.9351, 9.0143, 9.0558, 9.0596, 8.9917, 8.905 , 8.9314, 8.9465, 8.9879, 8.9804, 9.0219, 9.052 , 9.0407, 9.0407, 8.9955, 8.9992, 8.9992, 9.0747, 9.0747, 9.5385, 9.4781, 9.4517, 9.3537, 9.2707, 9.1199, 9.1689, 9.1425, 9.135 , 9.1011, 9.1727, 9.2217, 9.2255, 9.2821, 9.3235, 9.3198, 9.3198, 9.3198, 9.3273, 9.3725, 9.3989, 9.4253, 9.4593, 9.4442, 9.4291, 9.2632, 9.2293, 9.1878, 9.1425, 9.1275, 9.1802, 9.1765, 9.2481, 9.2481, 9.1991, 9.1689, 9.1765, 9.2406, 9.3198, 9.3235, 9.1991, 9.2971, 9.2443, 9.316 , 9.2934, 9.3914, 9.3989, 9.5121, 9.6176, 9.5535, 9.4668, 9.3725, 9.3348, 9.3763, 9.3839, 9.4216, 9.4065, 9.3348, 9.4442, 9.4367, 9.5083, 9.448 , 9.4781, 9.595 , 9.6101, 9.5686, 9.6365, 9.7119, 9.8213, 9.825 , 9.7609, 9.6516, 9.5988, 9.546 , 9.6516, 9.7572, 9.8854, 10.0023, 9.3914]) . The array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents. . bd_offspring_scandens = np.array([ 8.419 , 9.2468, 8.1532, 8.0089, 8.2215, 8.3734, 8.5025, 8.6392, 8.7684, 8.8139, 8.7911, 8.9051, 8.9203, 8.8747, 8.943 , 9.0038, 8.981 , 9.0949, 9.2696, 9.1633, 9.1785, 9.1937, 9.2772, 9.0722, 8.9658, 8.9658, 8.5025, 8.4949, 8.4949, 8.5633, 8.6013, 8.6468, 8.1532, 8.3734, 8.662 , 8.6924, 8.7456, 8.8367, 8.8595, 8.9658, 8.9582, 8.8671, 8.8671, 8.943 , 9.0646, 9.1405, 9.2089, 9.2848, 9.3759, 9.4899, 9.4519, 8.1228, 8.2595, 8.3127, 8.4949, 8.6013, 8.4646, 8.5329, 8.7532, 8.8823, 9.0342, 8.6392, 8.6772, 8.6316, 8.7532, 8.8291, 8.8975, 8.9734, 9.0494, 9.1253, 9.1253, 9.1253, 9.1785, 9.2848, 9.4595, 9.3608, 9.2089, 9.2544, 9.3684, 9.3684, 9.2316, 9.1709, 9.2316, 9.0342, 8.8899, 8.8291, 8.981 , 8.8975, 10.4089, 10.1886, 9.7633, 9.7329, 9.6114, 9.5051, 9.5127, 9.3684, 9.6266, 9.5354, 10.0215, 10.0215, 9.6266, 9.6038, 9.4063, 9.2316, 9.338 , 9.262 , 9.262 , 9.4063, 9.4367, 9.0342, 8.943 , 8.9203, 8.7835, 8.7835, 9.057 , 8.9354, 8.8975, 8.8139, 8.8671, 9.0873, 9.2848, 9.2392, 9.2924, 9.4063, 9.3152, 9.4899, 9.5962, 9.6873, 9.5203, 9.6646]) . The arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds. . bd_parent_fortis = np.array([10.1 , 9.55 , 9.4 , 10.25 , 10.125, 9.7 , 9.05 , 7.4 , 9. , 8.65 , 9.625, 9.9 , 9.55 , 9.05 , 8.35 , 10.1 , 10.1 , 9.9 , 10.225, 10. , 10.55 , 10.45 , 9.2 , 10.2 , 8.95 , 10.05 , 10.2 , 9.5 , 9.925, 9.95 , 10.05 , 8.75 , 9.2 , 10.15 , 9.8 , 10.7 , 10.5 , 9.55 , 10.55 , 10.475, 8.65 , 10.7 , 9.1 , 9.4 , 10.3 , 9.65 , 9.5 , 9.7 , 10.525, 9.95 , 10.1 , 9.75 , 10.05 , 9.9 , 10. , 9.1 , 9.45 , 9.25 , 9.5 , 10. , 10.525, 9.9 , 10.4 , 8.95 , 9.4 , 10.95 , 10.75 , 10.1 , 8.05 , 9.1 , 9.55 , 9.05 , 10.2 , 10. , 10.55 , 10.75 , 8.175, 9.7 , 8.8 , 10.75 , 9.3 , 9.7 , 9.6 , 9.75 , 9.6 , 10.45 , 11. , 10.85 , 10.15 , 10.35 , 10.4 , 9.95 , 9.1 , 10.1 , 9.85 , 9.625, 9.475, 9. , 9.25 , 9.1 , 9.25 , 9.2 , 9.95 , 8.65 , 9.8 , 9.4 , 9. , 8.55 , 8.75 , 9.65 , 8.95 , 9.15 , 9.85 , 10.225, 9.825, 10. , 9.425, 10.4 , 9.875, 8.95 , 8.9 , 9.35 , 10.425, 10. , 10.175, 9.875, 9.875, 9.15 , 9.45 , 9.025, 9.7 , 9.7 , 10.05 , 10.3 , 9.6 , 10. , 9.8 , 10.05 , 8.75 , 10.55 , 9.7 , 10. , 9.85 , 9.8 , 9.175, 9.65 , 9.55 , 9.9 , 11.55 , 11.3 , 10.4 , 10.8 , 9.8 , 10.45 , 10. , 10.75 , 9.35 , 10.75 , 9.175, 9.65 , 8.8 , 10.55 , 10.675, 9.95 , 9.55 , 8.825, 9.7 , 9.85 , 9.8 , 9.55 , 9.275, 10.325, 9.15 , 9.35 , 9.15 , 9.65 , 10.575, 9.975, 9.55 , 9.2 , 9.925, 9.2 , 9.3 , 8.775, 9.325, 9.175, 9.325, 8.975, 9.7 , 9.5 , 10.225, 10.025, 8.2 , 8.2 , 9.55 , 9.05 , 9.6 , 9.6 , 10.15 , 9.875, 10.485, 11.485, 10.985, 9.7 , 9.65 , 9.35 , 10.05 , 10.1 , 9.9 , 8.95 , 9.3 , 9.95 , 9.45 , 9.5 , 8.45 , 8.8 , 8.525, 9.375, 10.2 , 7.625, 8.375, 9.25 , 9.4 , 10.55 , 8.9 , 8.8 , 9. , 8.575, 8.575, 9.6 , 9.375, 9.6 , 9.95 , 9.6 , 10.2 , 9.85 , 9.625, 9.025, 10.375, 10.25 , 9.3 , 9.5 , 9.55 , 8.55 , 9.05 , 9.9 , 9.8 , 9.75 , 10.25 , 9.1 , 9.65 , 10.3 , 8.9 , 9.95 , 9.5 , 9.775, 9.425, 7.75 , 7.55 , 9.1 , 9.6 , 9.575, 8.95 , 9.65 , 9.65 , 9.65 , 9.525, 9.85 , 9.05 , 9.3 , 8.9 , 9.45 , 10. , 9.85 , 9.25 , 10.1 , 9.125, 9.65 , 9.1 , 8.05 , 7.4 , 8.85 , 9.075, 9. , 9.7 , 8.7 , 9.45 , 9.7 , 8.35 , 8.85 , 9.7 , 9.45 , 10.3 , 10. , 10.45 , 9.45 , 8.5 , 8.3 , 10. , 9.225, 9.75 , 9.15 , 9.55 , 9. , 9.275, 9.35 , 8.95 , 9.875, 8.45 , 8.6 , 9.7 , 8.55 , 9.05 , 9.6 , 8.65 , 9.2 , 8.95 , 9.6 , 9.15 , 9.4 , 8.95 , 9.95 , 10.55 , 9.7 , 8.85 , 8.8 , 10. , 9.05 , 8.2 , 8.1 , 7.25 , 8.3 , 9.15 , 8.6 , 9.5 , 8.05 , 9.425, 9.3 , 9.8 , 9.3 , 9.85 , 9.5 , 8.65 , 9.825, 9. , 10.45 , 9.1 , 9.55 , 9.05 , 10. , 9.35 , 8.375, 8.3 , 8.8 , 10.1 , 9.5 , 9.75 , 10.1 , 9.575, 9.425, 9.65 , 8.725, 9.025, 8.5 , 8.95 , 9.3 , 8.85 , 8.95 , 9.8 , 9.5 , 8.65 , 9.1 , 9.4 , 8.475, 9.35 , 7.95 , 9.35 , 8.575, 9.05 , 8.175, 9.85 , 7.85 , 9.85 , 10.1 , 9.35 , 8.85 , 8.75 , 9.625, 9.25 , 9.55 , 10.325, 8.55 , 9.675, 9.15 , 9. , 9.65 , 8.6 , 8.8 , 9. , 9.95 , 8.4 , 9.35 , 10.3 , 9.05 , 9.975, 9.975, 8.65 , 8.725, 8.2 , 7.85 , 8.775, 8.5 , 9.4 ]) . bd_offspring_fortis = np.array([10.7 , 9.78, 9.48, 9.6 , 10.27, 9.5 , 9. , 7.46, 7.65, 8.63, 9.81, 9.4 , 9.48, 8.75, 7.6 , 10. , 10.09, 9.74, 9.64, 8.49, 10.15, 10.28, 9.2 , 10.01, 9.03, 9.94, 10.5 , 9.7 , 10.02, 10.04, 9.43, 8.1 , 9.5 , 9.9 , 9.48, 10.18, 10.16, 9.08, 10.39, 9.9 , 8.4 , 10.6 , 8.75, 9.46, 9.6 , 9.6 , 9.95, 10.05, 10.16, 10.1 , 9.83, 9.46, 9.7 , 9.82, 10.34, 8.02, 9.65, 9.87, 9. , 11.14, 9.25, 8.14, 10.23, 8.7 , 9.8 , 10.54, 11.19, 9.85, 8.1 , 9.3 , 9.34, 9.19, 9.52, 9.36, 8.8 , 8.6 , 8. , 8.5 , 8.3 , 10.38, 8.54, 8.94, 10. , 9.76, 9.45, 9.89, 10.9 , 9.91, 9.39, 9.86, 9.74, 9.9 , 9.09, 9.69, 10.24, 8.9 , 9.67, 8.93, 9.3 , 8.67, 9.15, 9.23, 9.59, 9.03, 9.58, 8.97, 8.57, 8.47, 8.71, 9.21, 9.13, 8.5 , 9.58, 9.21, 9.6 , 9.32, 8.7 , 10.46, 9.29, 9.24, 9.45, 9.35, 10.19, 9.91, 9.18, 9.89, 9.6 , 10.3 , 9.45, 8.79, 9.2 , 8.8 , 9.69, 10.61, 9.6 , 9.9 , 9.26, 10.2 , 8.79, 9.28, 8.83, 9.76, 10.2 , 9.43, 9.4 , 9.9 , 9.5 , 8.95, 9.98, 9.72, 9.86, 11.1 , 9.14, 10.49, 9.75, 10.35, 9.73, 9.83, 8.69, 9.58, 8.42, 9.25, 10.12, 9.31, 9.99, 8.59, 8.74, 8.79, 9.6 , 9.52, 8.93, 10.23, 9.35, 9.35, 9.09, 9.04, 9.75, 10.5 , 9.09, 9.05, 9.54, 9.3 , 9.06, 8.7 , 9.32, 8.4 , 8.67, 8.6 , 9.53, 9.77, 9.65, 9.43, 8.35, 8.26, 9.5 , 8.6 , 9.57, 9.14, 10.79, 8.91, 9.93, 10.7 , 9.3 , 9.93, 9.51, 9.44, 10.05, 10.13, 9.24, 8.21, 8.9 , 9.34, 8.77, 9.4 , 8.82, 8.83, 8.6 , 9.5 , 10.2 , 8.09, 9.07, 9.29, 9.1 , 10.19, 9.25, 8.98, 9.02, 8.6 , 8.25, 8.7 , 9.9 , 9.65, 9.45, 9.38, 10.4 , 9.96, 9.46, 8.26, 10.05, 8.92, 9.5 , 9.43, 8.97, 8.44, 8.92, 10.3 , 8.4 , 9.37, 9.91, 10. , 9.21, 9.95, 8.84, 9.82, 9.5 , 10.29, 8.4 , 8.31, 9.29, 8.86, 9.4 , 9.62, 8.62, 8.3 , 9.8 , 8.48, 9.61, 9.5 , 9.37, 8.74, 9.31, 9.5 , 9.49, 9.74, 9.2 , 9.24, 9.7 , 9.64, 9.2 , 7.5 , 7.5 , 8.7 , 8.31, 9. , 9.74, 9.31, 10.5 , 9.3 , 8.12, 9.34, 9.72, 9. , 9.65, 9.9 , 10. , 10.1 , 8. , 9.07, 9.75, 9.33, 8.11, 9.36, 9.74, 9.9 , 9.23, 9.7 , 8.2 , 9.35, 9.49, 9.34, 8.87, 9.03, 9.07, 9.43, 8.2 , 9.19, 9. , 9.2 , 9.06, 9.81, 8.89, 9.4 , 10.45, 9.64, 9.03, 8.71, 9.91, 8.33, 8.2 , 7.83, 7.14, 8.91, 9.18, 8.8 , 9.9 , 7.73, 9.25, 8.7 , 9.5 , 9.3 , 9.05, 10.18, 8.85, 9.24, 9.15, 9.98, 8.77, 9.8 , 8.65, 10. , 8.81, 8.01, 7.9 , 9.41, 10.18, 9.55, 9.08, 8.4 , 9.75, 8.9 , 9.07, 9.35, 8.9 , 8.19, 8.65, 9.19, 8.9 , 9.28, 10.58, 9. , 9.4 , 8.91, 9.93, 10. , 9.37, 7.4 , 9. , 8.8 , 9.18, 8.3 , 10.08, 7.9 , 9.96, 10.4 , 9.65, 8.8 , 8.65, 9.7 , 9.23, 9.43, 9.93, 8.47, 9.55, 9.28, 8.85, 8.9 , 8.75, 8.63, 9. , 9.43, 8.28, 9.23, 10.4 , 9. , 9.8 , 9.77, 8.97, 8.37, 7.7 , 7.9 , 9.5 , 8.2 , 8.8 ]) . We&#39;ll make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species. . # Make scatter plots _ = plt.plot(bd_parent_fortis, bd_offspring_fortis, marker=&quot;.&quot;, linestyle=&quot;none&quot;, color=&quot;blue&quot;, alpha=.5) _ = plt.plot(bd_parent_scandens, bd_offspring_scandens, marker=&quot;.&quot;, linestyle=&quot;none&quot;, color=&quot;red&quot;, alpha=.5) # Label axes _ = plt.xlabel(&#39;parental beak depth (mm)&#39;) _ = plt.ylabel(&#39;offspring beak depth (mm)&#39;) # Add legend _ = plt.legend((&#39;G. fortis&#39;, &#39;G. scandens&#39;), loc=&#39;lower right&#39;) # Show plot plt.show() . It appears as though there is a stronger correlation in G. fortis than in G. scandens. This suggests that beak depth is more strongly inherited in G. fortis. We&#39;ll quantify this correlation . Correlation of offspring and parental data . In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap. . def draw_bs_pairs(x, y, func, size=1): &quot;&quot;&quot;Perform pairs bootstrap for a single statistic.&quot;&quot;&quot; # Set up array of indices to sample from: inds inds = np.arange(len(x)) # Initialize replicates: bs_replicates bs_replicates = np.empty(size) # Generate replicates for i in range(size): bs_inds = np.random.choice(inds, len(inds)) bs_x, bs_y = x[bs_inds], y[bs_inds] bs_replicates[i] = func(bs_x, bs_y) return bs_replicates . Pearson correlation of offspring and parental data . The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. We&#39;ll compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens. And Do the same for G. fortis. Then, use the function draw_bs_pairs to compute a 95% confidence interval using pairs bootstrap. . # Compute the Pearson correlation coefficients r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens) r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of Pearson r bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000) bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, r_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, r_fortis, conf_int_fortis) . G. scandens: 0.41170636294012586 [0.28114522 0.55186026] G. fortis: 0.7283412395518486 [0.66716345 0.7791389 ] . It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts. . Measuring heritability . . Note: Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. . This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone. We will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval. . . Warning: Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient. . def heritability(parents, offspring): &quot;&quot;&quot;Compute the heritability from parent and offspring samples.&quot;&quot;&quot; covariance_matrix = np.cov(parents, offspring) return covariance_matrix[0,1] / covariance_matrix[0,0] # Compute the heritability heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens) heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis) # Acquire 1000 bootstrap replicates of heritability replicates_scandens = draw_bs_pairs( bd_parent_scandens, bd_offspring_scandens, heritability, size=1000) replicates_fortis = draw_bs_pairs( bd_parent_fortis, bd_offspring_fortis, heritability, size=1000) # Compute 95% confidence intervals conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5]) conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5]) # Print results print(&#39;G. scandens:&#39;, heritability_scandens, conf_int_scandens) print(&#39;G. fortis:&#39;, heritability_fortis, conf_int_fortis) . G. scandens: 0.5485340868685983 [0.35159687 0.74984943] G. fortis: 0.7229051911438156 [0.64286124 0.78727894] . Here again, we see that G. fortis has stronger heritability than G. scandens. This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization. . Is beak depth heritable at all in G. scandens? . The heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species. We will test that hypothesis here. To do this, We will do a pairs permutation test. . # Initialize array of replicates: perm_replicates perm_replicates = np.empty(10000) # Draw replicates for i in range(10000): # Permute parent beak depths bd_parent_permuted = np.random.permutation(bd_parent_scandens) perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens) # Compute p-value: p p = np.sum(perm_replicates &gt;= heritability_scandens) / len(perm_replicates) # Print the p-value print(&#39;p-val =&#39;, p) . p-val = 0.0 . We get a p-value of zero, which means that none of the 10,000 permutation pairs replicates we drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens, just not as much as in G. fortis. If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance. . _ = plt.hist(perm_replicates, bins = int(np.sqrt(len(perm_replicates)))) _ = plt.ylabel(&quot;counts&quot;) _ = plt.xlabel(&quot;Heritability replicates&quot;) plt.show() .",
            "url": "https://victoromondi1997.github.io/blog/statistical-thinking/hypothesis-testing/data-science/2020/07/08/Statistical-Thinking-in-Python-(Part-2).html",
            "relUrl": "/statistical-thinking/hypothesis-testing/data-science/2020/07/08/Statistical-Thinking-in-Python-(Part-2).html",
            "date": " • Jul 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Statistical Thinking in Python (Part 1)",
            "content": "Graphical exploratory data analysis . Before diving into sophisticated statistical inference techniques, we should first explore our data by plotting them and computing simple summary statistics. This process, called exploratory data analysis, is a crucial first step in statistical analysis of data. . Introduction to Exploratory Data Analysis . Exploratory Data Analysis is the process of organizing, plo!ing, and summarizing a data set . “Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone. ” &gt; ~ John Tukey . Tukey&#39;s comments on EDA . Exploratory data analysis is detective work. | There is no excuse for failing to plot and look. | The greatest value of a picture is that it forces us to notice what we never expected to see. | It is important to understand what you can do before you learn how to measure how well you seem to have done it. | . If you don&#39;t have time to do EDA, you really don&#39;t have time to do hypothesis tests. And you should always do EDA first. . Advantages of graphical EDA . It often involves converting tabular data into graphical form. | If done well, graphical representations can allow for more rapid interpretation of data. | There is no excuse for neglecting to do graphical EDA. | . While a good, informative plot can sometimes be the end point of an analysis, it is more like a beginning:it helps guide you in the quantitative statistical analyses that come next. . Plotting a histogram . Plotting a histogram of iris data . We will use a classic data set collected by botanist Edward Anderson and made famous by Ronald Fisher, one of the most prolific statisticians in history. Anderson carefully measured the anatomical properties of samples of three different species of iris, Iris setosa, Iris versicolor, and Iris virginica. The full data set is available as part of scikit-learn. Here, you will work with his measurements of petal length. . We will plot a histogram of the petal lengths of his 50 samples of Iris versicolor using matplotlib/seaborn&#39;s default settings. . The subset of the data set containing the Iris versicolor petal lengths in units of centimeters (cm) is stored in the NumPy array versicolor_petal_length. . Libraries . # Import plotting modules import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np # Set default Seaborn style sns.set() %matplotlib inline . versicolor_petal_length = np.array([4.7, 4.5, 4.9, 4. , 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4. , 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4. , 4.9, 4.7, 4.3, 4.4, 4.8, 5. , 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4. , 4.4, 4.6, 4. , 3.3, 4.2, 4.2, 4.2, 4.3, 3. , 4.1]) . # Plot histogram of versicolor petal lengths plt.hist(versicolor_petal_length) plt.ylabel(&quot;count&quot;) plt.xlabel(&quot;petal length (cm)&quot;) plt.show() . Adjusting the number of bins in a histogram . The histogram we just made had ten bins. This is the default of matplotlib. . Tip: The &quot;square root rule&quot; is a commonly-used rule of thumb for choosing number of bins: choose the number of bins to be the square root of the number of samples. We will plot the histogram of Iris versicolor petal lengths again, this time using the square root rule for the number of bins. You specify the number of bins using the bins keyword argument of plt.hist(). . # Compute number of data points: n_data n_data = len(versicolor_petal_length) # Number of bins is the square root of number of data points: n_bins n_bins = np.sqrt(n_data) # Convert number of bins to integer: n_bins n_bins = int(n_bins) # Plot the histogram _ = plt.hist(versicolor_petal_length, bins=n_bins) # Label axes _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;count&#39;) # Show histogram plt.show() . Plot all data: Bee swarm plots . Bee swarm plot . We will make a bee swarm plot of the iris petal lengths. The x-axis will contain each of the three species, and the y-axis the petal lengths. . iris_petal_lengths = pd.read_csv(&quot;../datasets/iris_petal_lengths.csv&quot;) iris_petal_lengths.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . iris_petal_lengths.shape . (150, 5) . iris_petal_lengths.tail() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species . 145 6.7 | 3.0 | 5.2 | 2.3 | virginica | . 146 6.3 | 2.5 | 5.0 | 1.9 | virginica | . 147 6.5 | 3.0 | 5.2 | 2.0 | virginica | . 148 6.2 | 3.4 | 5.4 | 2.3 | virginica | . 149 5.9 | 3.0 | 5.1 | 1.8 | virginica | . # Create bee swarm plot with Seaborn&#39;s default settings _ = sns.swarmplot(data=iris_petal_lengths, x=&quot;species&quot;, y=&quot;petal length (cm)&quot;) # Label the axes _ = plt.xlabel(&quot;species&quot;) _ = plt.ylabel(&quot;petal length (cm)&quot;) # Show the plot plt.show() . Interpreting a bee swarm plot . I. virginica petals tend to be the longest, and I. setosa petals tend to be the shortest of the three species. . Note: Notice that we said &quot;tend to be.&quot; Some individual I. virginica flowers may be shorter than individual I. versicolor flowers. It is also possible that an individual I. setosa flower may have longer petals than in individual I. versicolor flower, though this is highly unlikely, and was not observed by Anderson. | . Plot all data: ECDFs . . Note: Empirical cumulative distribution function (ECDF) . Computing the ECDF . We will write a function that takes as input a 1D array of data and then returns the x and y values of the ECDF. . Important: ECDFs are among the most important plots in statistical analysis. . def ecdf(data): &quot;&quot;&quot;Compute ECDF for a one-dimensional array of measurements.&quot;&quot;&quot; # Number of data points: n n = len(data) # x-data for the ECDF: x x = np.sort(data) # y-data for the ECDF: y y = np.arange(1, n+1) / n return x, y . Plotting the ECDF . We will now use ecdf() function to compute the ECDF for the petal lengths of Anderson&#39;s Iris versicolor flowers. We will then plot the ECDF. . Warning: ecdf() function returns two arrays so we will need to unpack them. An example of such unpacking is x, y = foo(data), for some function foo(). . # Compute ECDF for versicolor data: x_vers, y_vers x_vers, y_vers = ecdf(versicolor_petal_length) # Generate plot _ = plt.plot(x_vers, y_vers, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Label the axes _ = plt.xlabel(&quot;versicolor petal length, (cm)&quot;) _ = plt.ylabel(&quot;ECDF&quot;) # Display the plot plt.show() . Comparison of ECDFs . ECDFs also allow us to compare two or more distributions (though plots get cluttered if you have too many). Here, we will plot ECDFs for the petal lengths of all three iris species. . Important: we already wrote a function to generate ECDFs so we can put it to good use! . setosa_petal_length = iris_petal_lengths[&quot;petal length (cm)&quot;][iris_petal_lengths.species == &quot;setosa&quot;] versicolor_petal_length = iris_petal_lengths[&quot;petal length (cm)&quot;][iris_petal_lengths.species == &quot;versicolor&quot;] virginica_petal_length = iris_petal_lengths[&quot;petal length (cm)&quot;][iris_petal_lengths.species == &quot;virginica&quot;] setosa_petal_length.head() . 0 1.4 1 1.4 2 1.3 3 1.5 4 1.4 Name: petal length (cm), dtype: float64 . # Compute ECDFs x_set, y_set = ecdf(setosa_petal_length) x_vers, y_vers = ecdf(versicolor_petal_length) x_virg, y_virg = ecdf(virginica_petal_length) # Plot all ECDFs on the same plot _ = plt.plot(x_set, y_set, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_vers, y_vers, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_virg, y_virg, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Annotate the plot plt.legend((&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;), loc=&#39;lower right&#39;) _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Display the plot plt.show() . . Note: The ECDFs expose clear differences among the species. Setosa is much shorter, also with less absolute variability in petal length than versicolor and virginica. . Onward toward the whole story! . . Important: “Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone.” —John Tukey . Quantitative exploratory data analysis . We will compute useful summary statistics, which serve to concisely describe salient features of a dataset with a few numbers. . Introduction to summary statistics: The sample mean and median . $$ mean = bar{x} = frac{1}{n} sum_{i=1}^{n} x_i $$Outliers . ● Data points whose value is far greater or less than most of the rest of the data . The median . ● The middle value of a data set . Note:An outlier can significantly affect the value of the mean, but not the median . Computing means . The mean of all measurements gives an indication of the typical magnitude of a measurement. It is computed using np.mean(). . # Compute the mean: mean_length_vers mean_length_vers = np.mean(versicolor_petal_length) # Print the result with some nice formatting print(&#39;I. versicolor:&#39;, mean_length_vers, &#39;cm&#39;) . I. versicolor: 4.26 cm . Percentiles, outliers, and box plots . Computing percentiles . We will compute the percentiles of petal length of Iris versicolor. . # Specify array of percentiles: percentiles percentiles = np.array([2.5, 25, 50, 75, 97.5]) # Compute percentiles: ptiles_vers ptiles_vers = np.percentile(versicolor_petal_length, percentiles) # Print the result ptiles_vers . array([3.3 , 4. , 4.35 , 4.6 , 4.9775]) . Comparing percentiles to ECDF . To see how the percentiles relate to the ECDF, we will plot the percentiles of Iris versicolor petal lengths on the ECDF plot. . # Plot the ECDF _ = plt.plot(x_vers, y_vers, &#39;.&#39;) _ = plt.xlabel(&#39;petal length (cm)&#39;) _ = plt.ylabel(&#39;ECDF&#39;) # Overlay percentiles as red diamonds. _ = plt.plot(ptiles_vers, percentiles/100, marker=&#39;D&#39;, color=&#39;red&#39;, linestyle=&quot;none&quot;) # Show the plot plt.show() . Box-and-whisker plot . . Warning: Making a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine. We will Make a box plot of the iris petal lengths. . # Create box plot with Seaborn&#39;s default settings _ = sns.boxplot(data=iris_petal_lengths, x=&quot;species&quot;, y=&quot;petal length (cm)&quot;) # Label the axes _ = plt.xlabel(&quot;species&quot;) _ = plt.ylabel(&quot;petal length (cm)&quot;) # Show the plot plt.show() . Variance and standard deviation . Variance . ● The mean squared distance of the data from their mean . Tip:Variance; nformally, a measure of the spread of data&gt; $$ variance = frac{1}{n} sum_{i=1}^{n} (x_i - bar{x})^2 $$ . standard Deviation . $$ std = sqrt { frac{1}{n} sum_{i=1}^{n} (x_i - bar{x})^2} $$ Computing the variance . we will explicitly compute the variance of the petal length of Iris veriscolor, we will then use np.var() to compute it. . # Array of differences to mean: differences differences = versicolor_petal_length-np.mean(versicolor_petal_length) # Square the differences: diff_sq diff_sq = differences**2 # Compute the mean square difference: variance_explicit variance_explicit = np.mean(diff_sq) # Compute the variance using NumPy: variance_np variance_np = np.var(versicolor_petal_length) # Print the results print(variance_explicit, variance_np) . 0.21640000000000004 0.21640000000000004 . The standard deviation and the variance . the standard deviation is the square root of the variance. . # Compute the variance: variance variance = np.var(versicolor_petal_length) # Print the square root of the variance print(np.sqrt(variance)) # Print the standard deviation print(np.std(versicolor_petal_length)) . 0.4651881339845203 0.4651881339845203 . Covariance and the Pearson correlation coefficient . Covariance . ● A measure of how two quantities vary together $$ covariance = frac{1}{n} sum_{i=1}^{n} (x_i bar{x}) (y_i - bar{y}) $$ . Pearson correlation coefficient . $$ rho = Pearson correlation = frac{covariance}{(std of x) (std of y)} = frac{variability due to codependence}{independent variability} $$ Scatter plots . When we made bee swarm plots, box plots, and ECDF plots in previous exercises, we compared the petal lengths of different species of iris. But what if we want to compare two properties of a single species? This is exactly what we will do, we will make a scatter plot of the petal length and width measurements of Anderson&#39;s Iris versicolor flowers. . Important:If the flower scales (that is, it preserves its proportion as it grows), we would expect the length and width to be correlated. . versicolor_petal_width = np.array([1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1. , 1.3, 1.4, 1. , 1.5, 1. , 1.4, 1.3, 1.4, 1.5, 1. , 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1. , 1.1, 1. , 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1. , 1.3, 1.2, 1.3, 1.3, 1.1, 1.3]) . # Make a scatter plot _ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Label the axes _ = plt.xlabel(&quot;petal length, (cm)&quot;) _ = plt.ylabel(&quot;petal length, (cm)&quot;) # Show the result plt.show() . . Tip: we see some correlation. Longer petals also tend to be wider. . Computing the covariance . The covariance may be computed using the Numpy function np.cov(). For example, we have two sets of data $x$ and $y$, np.cov(x, y) returns a 2D array where entries [0,1] and [1,0] are the covariances. Entry [0,0] is the variance of the data in x, and entry [1,1] is the variance of the data in y. This 2D output array is called the covariance matrix, since it organizes the self- and covariance. . # Compute the covariance matrix: covariance_matrix covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width) # Print covariance matrix print(covariance_matrix) # Extract covariance of length and width of petals: petal_cov petal_cov = covariance_matrix[0,1] # Print the length/width covariance print(petal_cov) . [[0.22081633 0.07310204] [0.07310204 0.03910612]] 0.07310204081632653 . Computing the Pearson correlation coefficient . the Pearson correlation coefficient, also called the Pearson r, is often easier to interpret than the covariance. It is computed using the np.corrcoef() function. Like np.cov(), it takes two arrays as arguments and returns a 2D array. Entries [0,0] and [1,1] are necessarily equal to 1, and the value we are after is entry [0,1]. . We will write a function, pearson_r(x, y) that takes in two arrays and returns the Pearson correlation coefficient. We will then use this function to compute it for the petal lengths and widths of $I. versicolor$. . def pearson_r(x, y): &quot;&quot;&quot;Compute Pearson correlation coefficient between two arrays.&quot;&quot;&quot; # Compute correlation matrix: corr_mat corr_mat = np.corrcoef(x,y) # Return entry [0,1] return corr_mat[0,1] # Compute Pearson correlation coefficient for I. versicolor: r r = pearson_r(versicolor_petal_length, versicolor_petal_width) # Print the result print(r) . 0.7866680885228169 . Thinking probabilistically-- Discrete variables . Statistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data. We will think probabilistically about discrete quantities: those that can only take certain values, like integers. . Probabilistic logic and statistical inference . the goal of statistical inference . To draw probabilistic conclusions about what we might expect if we collected the same data again. | To draw actionable conclusions from data. | To draw more general conclusions from relatively few data or observations. . Note: Statistical inference involves taking your data to probabilistic conclusions about what you would expect if you took even more data, and you can make decisions based on these conclusions. | . Why we use the probabilistic language in statistical inference . Probability provides a measure of uncertainty and this is crucial because we can quantify what we might expect if the data were acquired again. | Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary. We need probability to say how data might vary if acquired again. . Note: Probabilistic language is in fact very precise. It precisely describes uncertainty. | . Random number generators and hacker statistics . Hacker statistics . Uses simulated repeated measurements to compute probabilities. | . The np.random module . Suite of functions based on random number generation | np.random.random():draw a number between $0$ and $1$ ### Bernoulli trial ● An experiment that has two options, &quot;success&quot; (True) and &quot;failure&quot; (False). | . Random number seed . Integer fed into random number generating algorithm | Manually seed random number generator if you need reproducibility | Specified using np.random.seed() | . Hacker stats probabilities . Determine how to simulate data | Simulate many many times | Probability is approximately fraction of trials with the outcome of interest | . Generating random numbers using the np.random module . we&#39;ll generate lots of random numbers between zero and one, and then plot a histogram of the results. If the numbers are truly random, all bars in the histogram should be of (close to) equal height. . # Seed the random number generator np.random.seed(42) # Initialize random numbers: random_numbers random_numbers = np.empty(100000) # Generate random numbers by looping over range(100000) for i in range(100000): random_numbers[i] = np.random.random() # Plot a histogram _ = plt.hist(random_numbers, bins=316, histtype=&quot;step&quot;, density=True) _ = plt.xlabel(&quot;random numbers&quot;) _ = plt.ylabel(&quot;counts&quot;) # Show the plot plt.show() . . Note: The histogram is almost exactly flat across the top, indicating that there is equal chance that a randomly-generated number is in any of the bins of the histogram. . The np.random module and Bernoulli trials . . Tip: You can think of a Bernoulli trial as a flip of a possibly biased coin. Each coin flip has a probability $p$ of landing heads (success) and probability $1−p$ of landing tails (failure). We will write a function to perform n Bernoulli trials, perform_bernoulli_trials(n, p), which returns the number of successes out of n Bernoulli trials, each of which has probability $p$ of success. To perform each Bernoulli trial, we will use the np.random.random() function, which returns a random number between zero and one. . def perform_bernoulli_trials(n, p): &quot;&quot;&quot;Perform n Bernoulli trials with success probability p and return number of successes.&quot;&quot;&quot; # Initialize number of successes: n_success n_success = False # Perform trials for i in range(n): # Choose random number between zero and one: random_number random_number = np.random.random() # If less than p, it&#39;s a success so add one to n_success if random_number &lt; p: n_success += 1 return n_success . How many defaults might we expect? . Let&#39;s say a bank made 100 mortgage loans. It is possible that anywhere between $0$ and $100$ of the loans will be defaulted upon. We would like to know the probability of getting a given number of defaults, given that the probability of a default is $p = 0.05$. To investigate this, we will do a simulation. We will perform 100 Bernoulli trials using the perform_bernoulli_trials() function and record how many defaults we get. Here, a success is a default. . Important: Remember that the word &quot;success&quot; just means that the Bernoulli trial evaluates to True, i.e., did the loan recipient default? You will do this for another $100$ Bernoulli trials. And again and again until we have tried it $1000$ times. Then, we will plot a histogram describing the probability of the number of defaults. . # Seed random number generator np.random.seed(42) # Initialize the number of defaults: n_defaults n_defaults = np.empty(1000) # Compute the number of defaults for i in range(1000): n_defaults[i] = perform_bernoulli_trials(100, 0.05) # Plot the histogram with default number of bins; label your axes _ = plt.hist(n_defaults, density=True) _ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;) _ = plt.ylabel(&#39;probability&#39;) # Show the plot plt.show() . . Warning: This is actually not an optimal way to plot a histogram when the results are known to be integers. We will revisit this . Will the bank fail? . If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money? . # Compute ECDF: x, y x,y = ecdf(n_defaults) # Plot the ECDF with labeled axes _ = plt.plot(x,y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.xlabel(&quot;number of defaults&quot;) _ = plt.ylabel(&quot;ECDF&quot;) # Show the plot plt.show() # Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money n_lose_money = np.sum(n_defaults &gt;= 10) # Compute and print probability of losing money print(&#39;Probability of losing money =&#39;, n_lose_money / len(n_defaults)) . Probability of losing money = 0.022 . . Note: we most likely get 5/100 defaults. But we still have about a 2% chance of getting 10 or more defaults out of 100 loans. . Probability distributions and stories: The Binomial distribution . Probability mass function (PMF) . The set of probabilities of discrete outcomes | . Probability distribution . A mathematical description of outcomes | . Discrete Uniform distribution:the story- The outcome of rolling a single fair die is Discrete Uniformly distributed. . Binomial distribution:the story- The number $r$ of successes in $n$ Bernoulli trials with . probability $p$ of success, is Binomially distributed . The number $r$ of heads in $4$ coin flips with probability $0.5$ of heads, is Binomially distributed | . Sampling out of the Binomial distribution . We will compute the probability mass function for the number of defaults we would expect for $100$ loans as in the last section, but instead of simulating all of the Bernoulli trials, we will perform the sampling using np.random.binomial()1. . Note: This is identical to the calculation we did in the last set of exercises using our custom-written perform_bernoulli_trials() function, but far more computationally efficient. Given this extra efficiency, we will take $10,000$ samples instead of $1000$. After taking the samples, we will plot the CDF. This CDF that we are plotting is that of the Binomial distribution. . # Take 10,000 samples out of the binomial distribution: n_defaults n_defaults = np.random.binomial(100, 0.05, size=10000) # Compute CDF: x, y x,y = ecdf(n_defaults) # Plot the CDF with axis labels _ = plt.plot(x,y, marker=&quot;.&quot;, linestyle=&quot;-&quot;) _ = plt.xlabel(&quot;number of defaults out of 100 loans&quot;) _ = plt.ylabel(&quot;CDF&quot;) # Show the plot plt.show() . . Tip: If you know the story, using built-in algorithms to directly sample out of the distribution is much faster. . Plotting the Binomial PMF . . Warning: plotting a nice looking PMF requires a bit of matplotlib trickery that we will not go into here. we will plot the PMF of the Binomial distribution as a histogram. The trick is setting up the edges of the bins to pass to plt.hist() via the bins keyword argument. We want the bins centered on the integers. So, the edges of the bins should be $-0.5, 0.5, 1.5, 2.5, ...$ up to max(n_defaults) + 1.5. We can generate an array like this using np.arange()and then subtracting 0.5 from the array. . # Compute bin edges: bins bins = np.arange(0, max(n_defaults) + 1.5) - 0.5 # Generate histogram _ = plt.hist(n_defaults, density=True, bins=bins) # Label axes _ = plt.xlabel(&quot;number of defaults out of 100 loans&quot;) _ = plt.ylabel(&quot;probability&quot;) # Show the plot plt.show() . Poisson processes and the Poisson distribution . Poisson process . The timing of the next event is completely independent of when the previous event happened | . Examples of Poisson processes . Natural births in a given hospital | Hit on a website during a given hour | Meteor strikes | Molecular collisions in a gas | Aviation incidents | Buses in Poissonville | . Poisson distribution . The number $r$ of arrivals of a Poisson process in a given time interval with average rate of $λ$ arrivals per interval is Poisson distributed. | The number r of hits on a website in one hour with an average hit rate of 6 hits per hour is Poisson distributed. | . Poisson Distribution . Limit of the Binomial distribution for low probability of success and large number of trials. | That is, for rare events. | . Relationship between Binomial and Poisson distributions . Important:Poisson distribution is a limit of the Binomial distribution for rare events. . Tip: Poisson distribution with arrival rate equal to $np$ approximates a Binomial distribution for $n$ Bernoulli trials with probability $p$ of success (with $n$ large and $p$ small). Importantly, the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution. Let&#39;s explore these two distributions computationally. We will compute the mean and standard deviation of samples from a Poisson distribution with an arrival rate of $10$. Then, we will compute the mean and standard deviation of samples from a Binomial distribution with parameters $n$ and $p$ such that $np = 10$. . # Draw 10,000 samples out of Poisson distribution: samples_poisson samples_poisson = np.random.poisson(10, size=10000) # Print the mean and standard deviation print(&#39;Poisson: &#39;, np.mean(samples_poisson), np.std(samples_poisson)) # Specify values of n and p to consider for Binomial: n, p n = [20, 100, 1000] p = [.5, .1, .01] # Draw 10,000 samples for each n,p pair: samples_binomial for i in range(3): samples_binomial = np.random.binomial(n[i],p[i], size=10000) # Print results print(&#39;n =&#39;, n[i], &#39;Binom:&#39;, np.mean(samples_binomial), np.std(samples_binomial)) . Poisson: 10.0145 3.1713545607516043 n = 20 Binom: 10.0592 2.23523944131272 n = 100 Binom: 10.0441 2.9942536949964675 n = 1000 Binom: 10.0129 3.139639085946026 . . Note: The means are all about the same, which can be shown to be true by doing some pen-and-paper work. The standard deviation of the Binomial distribution gets closer and closer to that of the Poisson distribution as the probability $p$ gets lower and lower. . Was 2015 anomalous? . In baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. The ECDF of the number of no-hitters in a season is shown to the right. The probability distribution that would be appropriate to describe the number of no-hitters we would expect in a given season? is Both Binomial and Poisson, though Poisson is easier to model and compute. . Important: When we have rare events (low $p$, high $n$), the Binomial distribution is Poisson. This has a single parameter, the mean number of successes per time interval, in our case the mean number of no-hitters per season. 1990 and 2015 featured the most no-hitters of any season of baseball (there were seven). Given that there are on average $ frac{251}{115}$ no-hitters per season, what is the probability of having seven or more in a season? Let&#39;s find out . # Draw 10,000 samples out of Poisson distribution: n_nohitters n_nohitters = np.random.poisson(251/115, size=10000) # Compute number of samples that are seven or greater: n_large n_large = np.sum(n_nohitters &gt;= 7) # Compute probability of getting seven or more: p_large p_large = n_large/10000 # Print the result print(&#39;Probability of seven or more no-hitters:&#39;, p_large) . Probability of seven or more no-hitters: 0.0072 . . Note: The result is about $0.007$. This means that it is not that improbable to see a 7-or-more no-hitter season in a century. We have seen two in a century and a half, so it is not unreasonable. . Thinking probabilistically-- Continuous variables . It’s time to move onto continuous variables, such as those that can take on any fractional value. Many of the principles are the same, but there are some subtleties. We will be speaking the probabilistic language needed to launch into the inference techniques. . Probability density functions . Continuous variables . Quantities that can take any value, not just discrete values | . Probability density function (PDF) . Continuous analog to the PMF | Mathematical description of the relative likelihood of observing a value of a continuous variable | . Introduction to the Normal distribution . Normal distribution . Describes a continuous variable whose PDF has a single symmetric peak. | . Parameter Calculated from data . mean of a Normal distribution | ≠ | mean computed from data | . st. dev. of a Normal distribution | ≠ | standard deviation computed from data | . The Normal PDF . # Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10 samples_std1 = np.random.normal(20,1,size=100000) samples_std3 = np.random.normal(20, 3, size=100000) samples_std10 = np.random.normal(20, 10, size=100000) # Make histograms _ = plt.hist(samples_std1, density=True, histtype=&quot;step&quot;, bins=100) _ = plt.hist(samples_std3, density=True, histtype=&quot;step&quot;, bins=100) _ = plt.hist(samples_std10, density=True, histtype=&quot;step&quot;, bins=100) # Make a legend, set limits and show plot _ = plt.legend((&#39;std = 1&#39;, &#39;std = 3&#39;, &#39;std = 10&#39;)) plt.ylim(-0.01, 0.42) plt.show() . . Note: You can see how the different standard deviations result in PDFs of different widths. The peaks are all centered at the mean of 20. . The Normal CDF . # Generate CDFs x_std1, y_std1 = ecdf(samples_std1) x_std3, y_std3 = ecdf(samples_std3) x_std10, y_std10 = ecdf(samples_std10) # Plot CDFs _ = plt.plot(x_std1, y_std1, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_std3, y_std3, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.plot(x_std10, y_std10, marker=&quot;.&quot;, linestyle=&quot;none&quot;) # Make a legend and show the plot _ = plt.legend((&#39;std = 1&#39;, &#39;std = 3&#39;, &#39;std = 10&#39;), loc=&#39;lower right&#39;) plt.show() . . Note: The CDFs all pass through the mean at the 50th percentile; the mean and median of a Normal distribution are equal. The width of the CDF varies with the standard deviation. . The Normal distribution: Properties and warnings . Are the Belmont Stakes results Normally distributed? . Since 1926, the Belmont Stakes is a $1.5$ mile-long race of 3-year old thoroughbred horses. Secretariat ran the fastest Belmont Stakes in history in $1973$. While that was the fastest year, 1970 was the slowest because of unusually wet and sloppy conditions. With these two outliers removed from the data set, we will compute the mean and standard deviation of the Belmont winners&#39; times. We will sample out of a Normal distribution with this mean and standard deviation using the np.random.normal() function and plot a CDF. Overlay the ECDF from the winning Belmont times 2. . belmont_no_outliers = np.array([148.51, 146.65, 148.52, 150.7 , 150.42, 150.88, 151.57, 147.54, 149.65, 148.74, 147.86, 148.75, 147.5 , 148.26, 149.71, 146.56, 151.19, 147.88, 149.16, 148.82, 148.96, 152.02, 146.82, 149.97, 146.13, 148.1 , 147.2 , 146. , 146.4 , 148.2 , 149.8 , 147. , 147.2 , 147.8 , 148.2 , 149. , 149.8 , 148.6 , 146.8 , 149.6 , 149. , 148.2 , 149.2 , 148. , 150.4 , 148.8 , 147.2 , 148.8 , 149.6 , 148.4 , 148.4 , 150.2 , 148.8 , 149.2 , 149.2 , 148.4 , 150.2 , 146.6 , 149.8 , 149. , 150.8 , 148.6 , 150.2 , 149. , 148.6 , 150.2 , 148.2 , 149.4 , 150.8 , 150.2 , 152.2 , 148.2 , 149.2 , 151. , 149.6 , 149.6 , 149.4 , 148.6 , 150. , 150.6 , 149.2 , 152.6 , 152.8 , 149.6 , 151.6 , 152.8 , 153.2 , 152.4 , 152.2 ]) . # Compute mean and standard deviation: mu, sigma mu = np.mean(belmont_no_outliers) sigma = np.std(belmont_no_outliers) # Sample out of a normal distribution with this mu and sigma: samples samples = np.random.normal(mu, sigma, size=10000) # Get the CDF of the samples and of the data x_theor, y_theor = ecdf(samples) x,y = ecdf(belmont_no_outliers) # Plot the CDFs and show the plot _ = plt.plot(x_theor, y_theor) _ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;) _ = plt.xlabel(&#39;Belmont winning time (sec.)&#39;) _ = plt.ylabel(&#39;CDF&#39;) plt.show() . . Note: The theoretical CDF and the ECDF of the data suggest that the winning Belmont times are, indeed, Normally distributed. This also suggests that in the last 100 years or so, there have not been major technological or training advances that have significantly affected the speed at which horses can run this race. . What are the chances of a horse matching or beating Secretariat&#39;s record? . The probability that the winner of a given Belmont Stakes will run it as fast or faster than Secretariat assuming that the Belmont winners&#39; times are Normally distributed (with the 1970 and 1973 years removed) . # Take a million samples out of the Normal distribution: samples samples = np.random.normal(mu, sigma, size=1000000) # Compute the fraction that are faster than 144 seconds: prob prob = np.sum(samples&lt;=144)/len(samples) # Print the result print(&#39;Probability of besting Secretariat:&#39;, prob) . Probability of besting Secretariat: 0.000614 . . Note: We had to take a million samples because the probability of a fast time is very low and we had to be sure to sample enough. We get that there is only a 0.06% chance of a horse running the Belmont as fast as Secretariat. . The Exponential distribution . The waiting time between arrivals of a Poisson process is Exponentially distributed . Possible Poisson process . Nuclear incidents:- Timing of one is independent of all others $f(x; frac{1}{ beta}) = frac{1}{ beta} exp(- frac{x}{ beta})$ | . If you have a story, you can simulate it! . Sometimes, the story describing our probability distribution does not have a named distribution to go along with it. In these cases, fear not! You can always simulate it. . we looked at the rare event of no-hitters in Major League Baseball. Hitting the cycle is another rare baseball event. When a batter hits the cycle, he gets all four kinds of hits, a single, double, triple, and home run, in a single game. Like no-hitters, this can be modeled as a Poisson process, so the time between hits of the cycle are also Exponentially distributed. . How long must we wait to see both a no-hitter and then a batter hit the cycle? The idea is that we have to wait some time for the no-hitter, and then after the no-hitter, we have to wait for hitting the cycle. Stated another way, what is the total waiting time for the arrival of two different Poisson processes? The total waiting time is the time waited for the no-hitter, plus the time waited for the hitting the cycle. . Important: We will write a function to sample out of the distribution described by this story. . def successive_poisson(tau1, tau2, size=1): &quot;&quot;&quot;Compute time for arrival of 2 successive Poisson processes.&quot;&quot;&quot; # Draw samples out of first exponential distribution: t1 t1 = np.random.exponential(tau1, size=size) # Draw samples out of second exponential distribution: t2 t2 = np.random.exponential(tau2, size=size) return t1 + t2 . Distribution of no-hitters and cycles . We&#39;ll use the sampling function to compute the waiting time to observe a no-hitter and hitting of the cycle. The mean waiting time for a no-hitter is $764$ games, and the mean waiting time for hitting the cycle is $715$ games. . # Draw samples of waiting times: waiting_times waiting_times = successive_poisson(764, 715, size=100000) # Make the histogram _ = plt.hist(waiting_times, bins=100, density=True, histtype=&quot;step&quot;) # Label axes _ = plt.xlabel(&quot;Waiting times&quot;) _ = plt.ylabel(&quot;probability&quot;) # Show the plot plt.show() . Notice that the PDF is peaked, unlike the waiting time for a single Poisson process. For fun (and enlightenment), Let&#39;s also plot the CDF. . x,y = ecdf(waiting_times) _ = plt.plot(x,y) _ = plt.plot(x,y, marker=&quot;.&quot;, linestyle=&quot;none&quot;) _ = plt.xlabel(&quot;Waiting times&quot;) _ = plt.ylabel(&quot;CDF&quot;) plt.show() . 1. For this exercise and all going forward, the random number generator is pre-seeded for you (with np.random.seed(42)) to save you typing that each time.↩ . 2. we scraped the data concerning the Belmont Stakes from the Belmont Wikipedia page.↩ .",
            "url": "https://victoromondi1997.github.io/blog/statistical-thinking/eda/data-science/2020/07/03/Statistical-Thinking-in-Python-(Part-1).html",
            "relUrl": "/statistical-thinking/eda/data-science/2020/07/03/Statistical-Thinking-in-Python-(Part-1).html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Write Markdown and LaTeX Math Equation in The Jupyter Notebook",
            "content": "Markdown . Headings . # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 . Heading 1 . Heading 2 . Heading 3 . Heading 4 . Heading 5 . Heading 6 . Paragraph . This is a paragraph of text. This is another paragraph of text. . This is a paragraph of text. . This is another paragraph of text. . Line breaks . This is a text. This is another text. . This is a text. This is another text. . Mark emphasis . Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes ~ . ~~Scratch this.~~ . Emphasis, aka italics, with asterisks or underscores. . Strong emphasis, aka bold, with asterisks or underscores. . Combined emphasis with asterisks and underscores. . Strikethrough uses two tildes ~ . Scratch this. . Lists . 1. Item 1 2. Item 2 ( we can type 1. and the markdown will automatically numerate them) * First Item * Nested item 1 * Nested item 2 1. Keep going 1. Yes * Second Item - First Item - Second Item . Item 1 | Item 2 ( we can type 1. and the markdown will automatically numerate them) | First Item . Nested item 1 | Nested item 2 Keep going | Yes | | . | Second Item . | First Item | Second Item | . Links and&#160;Images . &lt;!-- [Text](link) --&gt; [Link Text](https://medium.com/@ahmedazizkhelifi &quot;Optional Title&quot;) &lt;!-- ![Alt Text](image path &quot;title&quot;) --&gt; ![Alt Text](https://miro.medium.com/max/80/0*PRNVc7bjff0Jj1pm.png &quot;Optional Title&quot;) &lt;!-- [![Alt Text](image path &quot;title&quot;)](link) --&gt; [![Alt Text](https://miro.medium.com/max/80/0*PRNVc7bjff0Jj1pm.png &quot;Optional Title&quot;)](https://medium.com/@ahmedazizkhelifi) . Link Text . Horizontal Rule . Reading articles on Medium is awesome. Sure !! . Reading articles on Medium is awesome. . . Sure !! . Table . | Id | Label | Price | | |-| | | 01 | Markdown | $1600 | | 02 | is | $12 | | 03 | AWESOME | $999 | . Id Label Price . 01 | Markdown | $1600 | . 02 | is | $12 | . 03 | AWESOME | $999 | . Code and Syntax Highlighting . python def staySafe(Coronavirus) if not home: return home . python def staySafe(Coronavirus) if not home: return home . Blockquotes . &gt; This is a blockquote. &gt; &gt; This is part of the same blockquote. Quote break &gt; This is a new blockquote. . This is a blockquote. This is part of the same blockquote. . Quote break . This is a new blockquote. . LaTeX . To insert a mathematical formula we use the dollar symbol $, as follows: . Euler&#39;s identity: $ e^{i pi} + 1 = 0 $ To isolate and center the formulas and enter in math display mode, we use 2 dollars symbol: $$ ... $$ Euler&#39;s identity: $$ e^{i pi} + 1 = 0 $$ . Euler&#39;s identity: $ e^{i pi} + 1 = 0 $ . To isolate and center the formulas and enter in math display mode, we use 2 dollars symbol: $$ ... $$ . Euler&#39;s identity: $$ e^{i pi} + 1 = 0 $$ . Important Note . $$ frac{arg 1}{arg 2} x^2 e^{i pi} A_i B_{ij} sqrt[n]{arg} $$ . $$ frac{arg 1}{arg 2} x^2 e^{i pi} A_i B_{ij} sqrt[n]{arg} $$ Greek Letters: . Given : $ pi = 3.14$ , $ alpha = frac{3 pi}{4} , rad$ $$ omega = 2 pi f f = frac{c}{ lambda} lambda_0= theta^2+ delta Delta lambda = frac{1}{ lambda^2} $$ . Given : $ pi = 3.14$ , $ alpha = frac{3 pi}{4} , rad$ $$ omega = 2 pi f f = frac{c}{ lambda} lambda_0= theta^2+ delta Delta lambda = frac{1}{ lambda^2} $$ . Important Note: . |Uppercase| LaTeX |Lowercase| LaTeX | ||-||-| |$ Delta$ | Delta|$ delta$ | delta| |$ Omega$ | Omega|$ omega$ | omega| . Uppercase LaTeX Lowercase LaTeX . $ Delta$ | Delta | $ delta$ | delta | . $ Omega$ | Omega | $ omega$ | omega | . Roman Names: . $$ sin(- alpha)=- sin( alpha) arccos(x)= arcsin(u) log_n(n)=1 tan(x) = frac{ sin(x)}{ cos(x)} $$ . $$ sin(- alpha)=- sin( alpha) arccos(x)= arcsin(u) log_n(n)=1 tan(x) = frac{ sin(x)}{ cos(x)} $$ Other Symbols . Angles: . Left angle : $ langle$ Right angle : $ rangle$ Angle between two vectors u and v : $ langle vec{u}, vec{v} rangle$ $$ vec{AB} , cdot , vec{CD} =0 Rightarrow vec{AB} , perp , vec{CD}$$ ##Sets and logic $$ mathbb{N} subset mathbb{Z} subset mathbb{D} subset mathbb{Q} subset mathbb{R} subset mathbb{C}$$ . Left angle : $ langle$ . Right angle : $ rangle$ . Angle between two vectors u and v : $ langle vec{u}, vec{v} rangle$ . $$ vec{AB} , cdot , vec{CD} =0 Rightarrow vec{AB} , perp , vec{CD}$$ . Sets and&#160;logic . $$ mathbb{N} subset mathbb{Z} subset mathbb{D} subset mathbb{Q} subset mathbb{R} subset mathbb{C}$$ . Vertical curly&#160;braces: . $$ sign(x) = left { begin{array} 1 &amp; mbox{if } x in mathbf{N}^* 0 &amp; mbox{if } x = 0 -1 &amp; mbox{else.} end{array} right. $$ $$ left. begin{array} alpha^2 = sqrt5 alpha geq 0 end{array} right } alpha = 5 $$ . $$ sign(x) = left { begin{array} 1 &amp; mbox{if } x in mathbf{N}^* 0 &amp; mbox{if } x = 0 -1 &amp; mbox{else.} end{array} right. $$ . $$ left. begin{array} alpha^2 = sqrt5 alpha geq 0 end{array} right } alpha = 5 $$ Horizontal curly&#160;braces $ underbrace{}$ . Horizontal curly&#160;braces $ underbrace{}$ . $$ underbrace{ ln left( frac{5}{6} right)}_{ simeq -0.1823} &lt; overbrace{ exp (2)}^{ simeq 7.3890} $$ . Derivate . First order derivative : $$f&#39;(x)$$ K-th order derivative : $$f^{(k)}(x)$$ Partial firt order derivative : $$ frac{ partial f}{ partial x}$$ Partial k-th order derivative : $$ frac{ partial^{k} f}{ partial x^k}$$ . First order derivative : $$f&#39;(x)$$ K-th order derivative : $$f^{(k)}(x)$$ Partial firt order derivative : $$ frac{ partial f}{ partial x}$$ Partial k-th order derivative : $$ frac{ partial^{k} f}{ partial x^k}$$ . Limit $ lim$ . Limit $ lim$ . Limit at plus infinity : $$ lim_{x to + infty} f(x)$$ Limit at minus infinity : $$ lim_{x to - infty} f(x)$$ Limit at $ alpha$ : $$ lim_{x to alpha} f(x)$$ Max : $$ max_{x in [a,b]}f(x)$$ Min : $$ min_{x in [ alpha, beta]}f(x)$$ Sup : $$ sup_{x in mathbb{R}}f(x)$$ Inf : $$ inf_{x &gt; s}f(x)$$ . Limit at plus infinity : $$ lim_{x to + infty} f(x)$$ Limit at minus infinity : $$ lim_{x to - infty} f(x)$$ Limit at $ alpha$ : $$ lim_{x to alpha} f(x)$$ . Max : $$ max_{x in [a,b]}f(x)$$ Min : $$ min_{x in [ alpha, beta]}f(x)$$ Sup : $$ sup_{x in mathbb{R}}f(x)$$ Inf : $$ inf_{x &gt; s}f(x)$$ . Sum $ sum$ . Sum $ sum$ . Sum from 0 to +inf: $$ sum_{j=0}^{+ infty} A_{j}$$ Double sum: $$ sum^k_{i=1} sum^{l+1}_{j=1} ,A_i A_j$$ Taylor expansion of $e^x$: $$ e^x = sum_{k=0}^{n} , frac{x^k}{k!} + o(x^n) $$ . Sum from 0 to +inf: . $$ sum_{j=0}^{+ infty} A_{j}$$ . Double sum: $$ sum^k_{i=1} sum^{l+1}_{j=1} ,A_i A_j$$ . Taylor expansion of $e^x$: $$ e^x = sum_{k=0}^{n} , frac{x^k}{k!} + o(x^n) $$ . Product $ prod$ . Product $ prod$ . Product: $$ prod_{j=1}^k A_{ alpha_j}$$ Double product: $$ prod^k_{i=1} prod^l_{j=1} ,A_i A_j$$ . Product: $$ prod_{j=1}^k A_{ alpha_j}$$ Double product: $$ prod^k_{i=1} prod^l_{j=1} ,A_i A_j$$ . Integral : $ int$ . Integral : $ int$ . Simple integral: $$ int_{a}^b f(x)dx$$ Double integral: $$ int_{a}^b int_{c}^d f(x,y) ,dxdy$$ Triple integral: $$ iiint$$ Quadruple integral: $$ iiiint$$ Multiple integral : $$ idotsint$$ Contour integral: $$ oint$$ . Simple integral: $$ int_{a}^b f(x)dx$$ . Double integral: $$ int_{a}^b int_{c}^d f(x,y) ,dxdy$$ . Triple integral: $$ iiint$$ . Quadruple integral: $$ iiiint$$ . Multiple integral : $$ idotsint$$ . Contour integral: $$ oint$$ . Matrix . Plain: begin{matrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{matrix} Round brackets: begin{pmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{pmatrix} Curly brackets: begin{Bmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{Bmatrix} Pipes: begin{vmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{vmatrix} Double pipes begin{Vmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{Vmatrix} . Plain: . begin{matrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{matrix}Round brackets: begin{pmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{pmatrix} . Curly brackets: begin{Bmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{Bmatrix} . Pipes: begin{vmatrix} 1 &amp; 2 &amp; 3 a &amp; b &amp; c end{vmatrix} .",
            "url": "https://victoromondi1997.github.io/blog/latex/markdown/2020/07/03/Markdown-LaTeX.html",
            "relUrl": "/latex/markdown/2020/07/03/Markdown-LaTeX.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Dr. Semmelweis and the Discovery of Handwashing",
            "content": "1. Meet Dr. Ignaz Semmelweis . This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it&#39;s probably because he&#39;s thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It&#39;s the contaminated hands of the doctors delivering the babies. And they won&#39;t listen to him and wash their hands! . In this notebook, we&#39;re going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let&#39;s start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital. . # importing modules import pandas as pd import matplotlib.pyplot as plt # Read datasets/yearly_deaths_by_clinic.csv into yearly yearly = pd.read_csv(&quot;datasets/yearly_deaths_by_clinic.csv&quot;) # Print out yearly yearly . year births deaths clinic . 0 1841 | 3036 | 237 | clinic 1 | . 1 1842 | 3287 | 518 | clinic 1 | . 2 1843 | 3060 | 274 | clinic 1 | . 3 1844 | 3157 | 260 | clinic 1 | . 4 1845 | 3492 | 241 | clinic 1 | . 5 1846 | 4010 | 459 | clinic 1 | . 6 1841 | 2442 | 86 | clinic 2 | . 7 1842 | 2659 | 202 | clinic 2 | . 8 1843 | 2739 | 164 | clinic 2 | . 9 1844 | 2956 | 68 | clinic 2 | . 10 1845 | 3241 | 66 | clinic 2 | . 11 1846 | 3754 | 105 | clinic 2 | . 2. The alarming number of deaths . The table above shows the number of women giving birth at the two clinics at the Vienna General Hospital for the years 1841 to 1846. You&#39;ll notice that giving birth was very dangerous; an alarming number of women died as the result of childbirth, most of them from childbed fever. . We see this more clearly if we look at the proportion of deaths out of the number of women giving birth. Let&#39;s zoom in on the proportion of deaths at Clinic 1. . # Calculate proportion of deaths per no. births yearly[&quot;proportion_deaths&quot;] = yearly.deaths/yearly.births # Extract clinic 1 data into yearly1 and clinic 2 data into yearly2 yearly1 = yearly[yearly.clinic==&quot;clinic 1&quot;] yearly2 = yearly[yearly.clinic==&quot;clinic 2&quot;] # Print out yearly1 yearly1 . year births deaths clinic proportion_deaths . 0 1841 | 3036 | 237 | clinic 1 | 0.078063 | . 1 1842 | 3287 | 518 | clinic 1 | 0.157591 | . 2 1843 | 3060 | 274 | clinic 1 | 0.089542 | . 3 1844 | 3157 | 260 | clinic 1 | 0.082357 | . 4 1845 | 3492 | 241 | clinic 1 | 0.069015 | . 5 1846 | 4010 | 459 | clinic 1 | 0.114464 | . 3. Death at the clinics . If we now plot the proportion of deaths at both clinic 1 and clinic 2 we&#39;ll see a curious pattern... . # This makes plots appear in the notebook %matplotlib inline # Plot yearly proportion of deaths at the two clinics ax = yearly1.plot(x=&quot;year&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Clinic 1&quot;) yearly2.plot(x=&quot;year&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Clinic 2&quot;, ax=ax) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7fab04d06d30&gt; . 4. The handwashing begins . Why is the proportion of deaths constantly so much higher in Clinic 1? Semmelweis saw the same pattern and was puzzled and distressed. The only difference between the clinics was that many medical students served at Clinic 1, while mostly midwife students served at Clinic 2. While the midwives only tended to the women giving birth, the medical students also spent time in the autopsy rooms examining corpses. . Semmelweis started to suspect that something on the corpses, spread from the hands of the medical students, caused childbed fever. So in a desperate attempt to stop the high mortality rates, he decreed: Wash your hands! This was an unorthodox and controversial request, nobody in Vienna knew about bacteria at this point in time. . Let&#39;s load in monthly data from Clinic 1 to see if the handwashing had any effect. . # Read datasets/monthly_deaths.csv into monthly monthly = pd.read_csv(&quot;datasets/monthly_deaths.csv&quot;, parse_dates=[&quot;date&quot;]) # Calculate proportion of deaths per no. births monthly[&quot;proportion_deaths&quot;] = monthly.deaths/monthly.births # Print out the first rows in monthly monthly.head() . date births deaths proportion_deaths . 0 1841-01-01 | 254 | 37 | 0.145669 | . 1 1841-02-01 | 239 | 18 | 0.075314 | . 2 1841-03-01 | 277 | 12 | 0.043321 | . 3 1841-04-01 | 255 | 4 | 0.015686 | . 4 1841-05-01 | 255 | 2 | 0.007843 | . 5. The effect of handwashing . With the data loaded we can now look at the proportion of deaths over time. In the plot below we haven&#39;t marked where obligatory handwashing started, but it reduced the proportion of deaths to such a degree that you should be able to spot it! . # Plot monthly proportion of deaths ax = monthly.plot(x=&quot;date&quot;, y=&quot;proportion_deaths&quot;) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7faae5d53b38&gt; . 6. The effect of handwashing highlighted . Starting from the summer of 1847 the proportion of deaths is drastically reduced and, yes, this was when Semmelweis made handwashing obligatory. . The effect of handwashing is made even more clear if we highlight this in the graph. . # Date when handwashing was made mandatory import pandas as pd handwashing_start = pd.to_datetime(&#39;1847-06-01&#39;) # Split monthly into before and after handwashing_start before_washing = monthly[monthly.date&lt;handwashing_start] after_washing = monthly[monthly.date&gt;=handwashing_start] # Plot monthly proportion of deaths before and after handwashing ax = before_washing.plot(x=&quot;date&quot;, y=&quot;proportion_deaths&quot;, label=&quot;Before Washing&quot;) after_washing.plot(ax=ax, x=&quot;date&quot;, y=&quot;proportion_deaths&quot;, label=&quot;After Washing&quot;) ax.set_ylabel(&quot;Proportion deaths&quot;) . &lt;matplotlib.text.Text at 0x7faae5c44940&gt; . 7. More handwashing, fewer deaths? . Again, the graph shows that handwashing had a huge effect. How much did it reduce the monthly proportion of deaths on average? . # Difference in mean monthly proportion of deaths due to handwashing before_proportion = before_washing.proportion_deaths after_proportion = after_washing.proportion_deaths mean_diff = after_proportion.mean() - before_proportion.mean() mean_diff . -0.08395660751183336 . 8. A Bootstrap analysis of Semmelweis handwashing data . It reduced the proportion of deaths by around 8 percentage points! From 10% on average to just 2% (which is still a high number by modern standards). . To get a feeling for the uncertainty around how much handwashing reduces mortalities we could look at a confidence interval (here calculated using the bootstrap method). . # A bootstrap analysis of the reduction of deaths due to handwashing boot_mean_diff = [] for i in range(3000): boot_before = before_proportion.sample(frac=1, replace=True) boot_after = after_proportion.sample(frac=1, replace=True) boot_mean_diff.append( boot_after.mean() - boot_before.mean() ) # Calculating a 95% confidence interval from boot_mean_diff confidence_interval = pd.Series(boot_mean_diff).quantile([.025, .975]) confidence_interval . 0.025 -0.102262 0.975 -0.067096 dtype: float64 . 9. The fate of Dr. Semmelweis . So handwashing reduced the proportion of deaths by between 6.7 and 10 percentage points, according to a 95% confidence interval. All in all, it would seem that Semmelweis had solid evidence that handwashing was a simple but highly effective procedure that could save many lives. . The tragedy is that, despite the evidence, Semmelweis&#39; theory — that childbed fever was caused by some &quot;substance&quot; (what we today know as bacteria) from autopsy room corpses — was ridiculed by contemporary scientists. The medical community largely rejected his discovery and in 1849 he was forced to leave the Vienna General Hospital for good. . One reason for this was that statistics and statistical arguments were uncommon in medical science in the 1800s. Semmelweis only published his data as long tables of raw data, but he didn&#39;t show any graphs nor confidence intervals. If he would have had access to the analysis we&#39;ve just put together he might have been more successful in getting the Viennese doctors to wash their hands. . # The data Semmelweis collected points to that: doctors_should_wash_their_hands = True .",
            "url": "https://victoromondi1997.github.io/blog/ignaz/data-analysis/handwashing/2020/07/01/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "relUrl": "/ignaz/data-analysis/handwashing/2020/07/01/Dr.-Semmelweis-and-the-Discovery-of-Handwashing.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Analyzing Police Activity with Pandas",
            "content": "Preparing the data for analysis . Before beginning our analysis, it is critical that we first examine and clean the dataset, to make working with it a more efficient process. We will fixing data types, handle missing values, and dropping columns and rows while exploring the Stanford Open Policing Project dataset. . Stanford Open Policing Project dataset . Examining the dataset . We&#39;ll be analyzing a dataset of traffic stops in Rhode Island that was collected by the Stanford Open Policing Project. . Before beginning our analysis, it&#39;s important that we familiarize yourself with the dataset. We read the dataset into pandas, examine the first few rows, and then count the number of missing values. . Libraries . import pandas as pd import matplotlib.pyplot as plt from pandas.api.types import CategoricalDtype . # Read &#39;police.csv&#39; into a DataFrame named ri ri = pd.read_csv(&quot;../datasets/police.csv&quot;) # Examine the head of the DataFrame display(ri.head()) # Count the number of missing values in each column ri.isnull().sum() . state stop_date stop_time county_name driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district . 0 RI | 2005-01-04 | 12:55 | NaN | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 1 RI | 2005-01-23 | 23:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | . 2 RI | 2005-02-17 | 04:15 | NaN | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | . 3 RI | 2005-02-20 | 17:15 | NaN | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | . 4 RI | 2005-02-24 | 01:20 | NaN | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | . state 0 stop_date 0 stop_time 0 county_name 91741 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . It looks like most of the columns have at least some missing values. We&#39;ll figure out how to handle these values in the next. . Dropping columns . We&#39;ll drop the county_name column because it only contains missing values, and we&#39;ll drop the state column because all of the traffic stops took place in one state (Rhode Island). . # Examine the shape of the DataFrame print(ri.shape) # Drop the &#39;county_name&#39; and &#39;state&#39; columns ri.drop([&quot;county_name&quot;, &quot;state&quot;], axis=&#39;columns&#39;, inplace=True) # Examine the shape of the DataFrame (again) print(ri.shape) . (91741, 15) (91741, 13) . We&#39;ll continue to remove unnecessary data from the DataFrame . Dropping rows . the driver_gender column will be critical to many of our analyses. Because only a small fraction of rows are missing driver_gender, we&#39;ll drop those rows from the dataset. . # Count the number of missing values in each column display(ri.isnull().sum()) # Drop all rows that are missing &#39;driver_gender&#39; ri.dropna(subset=[&quot;driver_gender&quot;], inplace=True) # Count the number of missing values in each column (again) display(ri.isnull().sum()) # Examine the shape of the DataFrame ri.shape . stop_date 0 stop_time 0 driver_gender 5205 driver_race 5202 violation_raw 5202 violation 5202 search_conducted 0 search_type 88434 stop_outcome 5202 is_arrested 5202 stop_duration 5202 drugs_related_stop 0 district 0 dtype: int64 . stop_date 0 stop_time 0 driver_gender 0 driver_race 0 violation_raw 0 violation 0 search_conducted 0 search_type 83229 stop_outcome 0 is_arrested 0 stop_duration 0 drugs_related_stop 0 district 0 dtype: int64 . (86536, 13) . We dropped around 5,000 rows, which is a small fraction of the dataset, and now only one column remains with any missing values. . Using proper data types . Finding an incorrect data type . ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested object stop_duration object drugs_related_stop bool district object dtype: object . stop_date: should be datetime | stop_time: should be datetime | driver_gender: should be category | driver_race: should be category | violation_raw: should be category | violation: should be category | district: should be category | is_arrested: should be bool | . We&#39;ll fix the data type of the is_arrested column . # Examine the head of the &#39;is_arrested&#39; column display(ri.is_arrested.head()) # Change the data type of &#39;is_arrested&#39; to &#39;bool&#39; ri[&#39;is_arrested&#39;] = ri.is_arrested.astype(&#39;bool&#39;) # Check the data type of &#39;is_arrested&#39; ri.is_arrested.dtype . 0 False 1 False 2 False 3 True 4 False Name: is_arrested, dtype: object . dtype(&#39;bool&#39;) . Creating a DatetimeIndex . Combining object columns . Currently, the date and time of each traffic stop are stored in separate object columns: stop_date and stop_time. We&#39;ll combine these two columns into a single column, and then convert it to datetime format. . ri[&#39;stop_date_time&#39;] = pd.to_datetime(ri.stop_date.str.replace(&quot;/&quot;, &quot;-&quot;).str.cat(ri.stop_time, sep=&quot; &quot;)) ri.dtypes . stop_date object stop_time object driver_gender object driver_race object violation_raw object violation object search_conducted bool search_type object stop_outcome object is_arrested bool stop_duration object drugs_related_stop bool district object stop_date_time datetime64[ns] dtype: object . Setting the index . # Set &#39;stop_datetime&#39; as the index ri.set_index(&quot;stop_date_time&quot;, inplace=True) # Examine the index display(ri.index) # Examine the columns ri.columns . DatetimeIndex([&#39;2005-01-04 12:55:00&#39;, &#39;2005-01-23 23:15:00&#39;, &#39;2005-02-17 04:15:00&#39;, &#39;2005-02-20 17:15:00&#39;, &#39;2005-02-24 01:20:00&#39;, &#39;2005-03-14 10:00:00&#39;, &#39;2005-03-29 21:55:00&#39;, &#39;2005-04-04 21:25:00&#39;, &#39;2005-07-14 11:20:00&#39;, &#39;2005-07-14 19:55:00&#39;, ... &#39;2015-12-31 13:23:00&#39;, &#39;2015-12-31 18:59:00&#39;, &#39;2015-12-31 19:13:00&#39;, &#39;2015-12-31 20:20:00&#39;, &#39;2015-12-31 20:50:00&#39;, &#39;2015-12-31 21:21:00&#39;, &#39;2015-12-31 21:59:00&#39;, &#39;2015-12-31 22:04:00&#39;, &#39;2015-12-31 22:09:00&#39;, &#39;2015-12-31 22:47:00&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;stop_date_time&#39;, length=86536, freq=None) . Index([&#39;stop_date&#39;, &#39;stop_time&#39;, &#39;driver_gender&#39;, &#39;driver_race&#39;, &#39;violation_raw&#39;, &#39;violation&#39;, &#39;search_conducted&#39;, &#39;search_type&#39;, &#39;stop_outcome&#39;, &#39;is_arrested&#39;, &#39;stop_duration&#39;, &#39;drugs_related_stop&#39;, &#39;district&#39;], dtype=&#39;object&#39;) . Exploring the relationship between gender and policing . Does the gender of a driver have an impact on police behavior during a traffic stop? We will explore that question while doing filtering, grouping, method chaining, Boolean math, string methods, and more! . Do the genders commit different violations? . Examining traffic violations . Before comparing the violations being committed by each gender, we should examine the violations committed by all drivers to get a baseline understanding of the data. . We&#39;ll count the unique values in the violation column, and then separately express those counts as proportions. . # Count the unique values in &#39;violation&#39; display(ri.violation.value_counts()) # Express the counts as proportions ri.violation.value_counts(normalize=True) . Speeding 48423 Moving violation 16224 Equipment 10921 Other 4409 Registration/plates 3703 Seat belt 2856 Name: violation, dtype: int64 . Speeding 0.559571 Moving violation 0.187483 Equipment 0.126202 Other 0.050950 Registration/plates 0.042791 Seat belt 0.033004 Name: violation, dtype: float64 . More than half of all violations are for speeding, followed by other moving violations and equipment violations. . Comparing violations by gender . The question we&#39;re trying to answer is whether male and female drivers tend to commit different types of traffic violations. . We&#39;ll first create a DataFrame for each gender, and then analyze the violations in each DataFrame separately. . # Create a DataFrame of female drivers female = ri[ri.driver_gender==&quot;F&quot;] # Create a DataFrame of male drivers male = ri[ri.driver_gender==&quot;M&quot;] # Compute the violations by female drivers (as proportions) display(female.violation.value_counts(normalize=True)) # Compute the violations by male drivers (as proportions) male.violation.value_counts(normalize=True) . Speeding 0.658114 Moving violation 0.138218 Equipment 0.105199 Registration/plates 0.044418 Other 0.029738 Seat belt 0.024312 Name: violation, dtype: float64 . Speeding 0.522243 Moving violation 0.206144 Equipment 0.134158 Other 0.058985 Registration/plates 0.042175 Seat belt 0.036296 Name: violation, dtype: float64 . About two-thirds of female traffic stops are for speeding, whereas stops of males are more balanced among the six categories. This doesn&#39;t mean that females speed more often than males, however, since we didn&#39;t take into account the number of stops or drivers. . Does gender affect who gets a ticket for speeding? . Comparing speeding outcomes by gender . When a driver is pulled over for speeding, many people believe that gender has an impact on whether the driver will receive a ticket or a warning. Can we find evidence of this in the dataset? . First, we&#39;ll create two DataFrames of drivers who were stopped for speeding: one containing females and the other containing males. . Then, for each gender, we&#39;ll use the stop_outcome column to calculate what percentage of stops resulted in a &quot;Citation&quot; (meaning a ticket) versus a &quot;Warning&quot;. . # Create a DataFrame of female drivers stopped for speeding female_and_speeding = ri[(ri.driver_gender==&quot;F&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Create a DataFrame of male drivers stopped for speeding male_and_speeding = ri[(ri.driver_gender==&quot;M&quot;) &amp; (ri.violation ==&quot;Speeding&quot;)] # Compute the stop outcomes for female drivers (as proportions) display(female_and_speeding.stop_outcome.value_counts(normalize=True)) # Compute the stop outcomes for male drivers (as proportions) male_and_speeding.stop_outcome.value_counts(normalize=True) . Citation 0.952192 Warning 0.040074 Arrest Driver 0.005752 N/D 0.000959 Arrest Passenger 0.000639 No Action 0.000383 Name: stop_outcome, dtype: float64 . Citation 0.944595 Warning 0.036184 Arrest Driver 0.015895 Arrest Passenger 0.001281 No Action 0.001068 N/D 0.000976 Name: stop_outcome, dtype: float64 . The numbers are similar for males and females: about 95% of stops for speeding result in a ticket. Thus, the data fails to show that gender has an impact on who gets a ticket for speeding. . Does gender affect whose vehicle is searched? . Calculating the search rate . During a traffic stop, the police officer sometimes conducts a search of the vehicle. We&#39;ll calculate the percentage of all stops in the ri DataFrame that result in a vehicle search, also known as the search rate. . # Check the data type of &#39;search_conducted&#39; print(ri.search_conducted.dtype) # Calculate the search rate by counting the values display(ri.search_conducted.value_counts(normalize=True)) # Calculate the search rate by taking the mean ri.search_conducted.mean() . bool . False 0.961785 True 0.038215 Name: search_conducted, dtype: float64 . 0.0382153092354627 . It looks like the search rate is about 3.8%. Next, we&#39;ll examine whether the search rate varies by driver gender. . Comparing search rates by gender . We&#39;ll compare the rates at which female and male drivers are searched during a traffic stop. Remember that the vehicle search rate across all stops is about 3.8%. . First, we&#39;ll filter the DataFrame by gender and calculate the search rate for each group separately. Then, we&#39;ll perform the same calculation for both genders at once using a .groupby(). . ri[ri.driver_gender==&quot;F&quot;].search_conducted.mean() . 0.019180617481282074 . ri[ri.driver_gender==&quot;M&quot;].search_conducted.mean() . 0.04542557598546892 . ri.groupby(&quot;driver_gender&quot;).search_conducted.mean() . driver_gender F 0.019181 M 0.045426 Name: search_conducted, dtype: float64 . Male drivers are searched more than twice as often as female drivers. Why might this be? . Adding a second factor to the analysis . Even though the search rate for males is much higher than for females, it&#39;s possible that the difference is mostly due to a second factor. . For example, we might hypothesize that the search rate varies by violation type, and the difference in search rate between males and females is because they tend to commit different violations. . we can test this hypothesis by examining the search rate for each combination of gender and violation. If the hypothesis was true, out would find that males and females are searched at about the same rate for each violation. Let&#39;s find out below if that&#39;s the case! . # Calculate the search rate for each combination of gender and violation ri.groupby([&quot;driver_gender&quot;, &quot;violation&quot;]).search_conducted.mean() . driver_gender violation F Equipment 0.039984 Moving violation 0.039257 Other 0.041018 Registration/plates 0.054924 Seat belt 0.017301 Speeding 0.008309 M Equipment 0.071496 Moving violation 0.061524 Other 0.046191 Registration/plates 0.108802 Seat belt 0.035119 Speeding 0.027885 Name: search_conducted, dtype: float64 . ri.groupby([&quot;violation&quot;, &quot;driver_gender&quot;]).search_conducted.mean() . violation driver_gender Equipment F 0.039984 M 0.071496 Moving violation F 0.039257 M 0.061524 Other F 0.041018 M 0.046191 Registration/plates F 0.054924 M 0.108802 Seat belt F 0.017301 M 0.035119 Speeding F 0.008309 M 0.027885 Name: search_conducted, dtype: float64 . For all types of violations, the search rate is higher for males than for females, disproving our hypothesis. . Does gender affect who is frisked during a search? . Counting protective frisks . During a vehicle search, the police officer may pat down the driver to check if they have a weapon. This is known as a &quot;protective frisk.&quot; . We&#39;ll first check to see how many times &quot;Protective Frisk&quot; was the only search type. Then, we&#39;ll use a string method to locate all instances in which the driver was frisked. . # Count the &#39;search_type&#39; values display(ri.search_type.value_counts()) # Check if &#39;search_type&#39; contains the string &#39;Protective Frisk&#39; ri[&#39;frisk&#39;] = ri.search_type.str.contains(&#39;Protective Frisk&#39;, na=False) # Check the data type of &#39;frisk&#39; print(ri.frisk.dtype) # Take the sum of &#39;frisk&#39; print(ri.frisk.sum()) . Incident to Arrest 1290 Probable Cause 924 Inventory 219 Reasonable Suspicion 214 Protective Frisk 164 Incident to Arrest,Inventory 123 Incident to Arrest,Probable Cause 100 Probable Cause,Reasonable Suspicion 54 Probable Cause,Protective Frisk 35 Incident to Arrest,Inventory,Probable Cause 35 Incident to Arrest,Protective Frisk 33 Inventory,Probable Cause 25 Protective Frisk,Reasonable Suspicion 19 Incident to Arrest,Inventory,Protective Frisk 18 Incident to Arrest,Probable Cause,Protective Frisk 13 Inventory,Protective Frisk 12 Incident to Arrest,Reasonable Suspicion 8 Probable Cause,Protective Frisk,Reasonable Suspicion 5 Incident to Arrest,Probable Cause,Reasonable Suspicion 5 Incident to Arrest,Inventory,Reasonable Suspicion 4 Incident to Arrest,Protective Frisk,Reasonable Suspicion 2 Inventory,Reasonable Suspicion 2 Inventory,Probable Cause,Protective Frisk 1 Inventory,Probable Cause,Reasonable Suspicion 1 Inventory,Protective Frisk,Reasonable Suspicion 1 Name: search_type, dtype: int64 . bool 303 . It looks like there were 303 drivers who were frisked. Next, we&#39;ll examine whether gender affects who is frisked. . Comparing frisk rates by gender . We&#39;ll compare the rates at which female and male drivers are frisked during a search. Are males frisked more often than females, perhaps because police officers consider them to be higher risk? . Before doing any calculations, it&#39;s important to filter the DataFrame to only include the relevant subset of data, namely stops in which a search was conducted. . # Create a DataFrame of stops in which a search was conducted searched = ri[ri.search_conducted == True] # Calculate the overall frisk rate by taking the mean of &#39;frisk&#39; print(searched.frisk.mean()) # Calculate the frisk rate for each gender searched.groupby(&quot;driver_gender&quot;).frisk.mean() . 0.09162382824312065 . driver_gender F 0.074561 M 0.094353 Name: frisk, dtype: float64 . The frisk rate is higher for males than for females, though we can&#39;t conclude that this difference is caused by the driver&#39;s gender. . Visual exploratory data analysis . Are you more likely to get arrested at a certain time of day? Are drug-related stops on the rise? We will answer these and other questions by analyzing the dataset visually, since plots can help us to understand trends in a way that examining the raw data cannot. . Does time of the day affect arrest rate? . Calculating the hourly arrest rate . When a police officer stops a driver, a small percentage of those stops ends in an arrest. This is known as the arrest rate. We&#39;ll find out whether the arrest rate varies by time of day. . First, we&#39;ll calculate the arrest rate across all stops in the ri DataFrame. Then, we&#39;ll calculate the hourly arrest rate by using the hour attribute of the index. The hour ranges from 0 to 23, in which: . 0 = midnight | 12 = noon | 23 = 11 PM | . # Calculate the overall arrest rate print(ri.is_arrested.mean()) # Calculate the hourly arrest rate # Save the hourly arrest rate hourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean() hourly_arrest_rate . 0.0355690117407784 . stop_date_time 0 0.051431 1 0.064932 2 0.060798 3 0.060549 4 0.048000 5 0.042781 6 0.013813 7 0.013032 8 0.021854 9 0.025206 10 0.028213 11 0.028897 12 0.037399 13 0.030776 14 0.030605 15 0.030679 16 0.035281 17 0.040619 18 0.038204 19 0.032245 20 0.038107 21 0.064541 22 0.048666 23 0.047592 Name: is_arrested, dtype: float64 . Next we&#39;ll plot the data so that you can visually examine the arrest rate trends. . Plotting the hourly arrest rate . We&#39;ll create a line plot from the hourly_arrest_rate object. . Important: A line plot is appropriate in this case because you&#8217;re showing how a quantity changes over time. This plot should help us to spot some trends that may not have been obvious when examining the raw numbers! . # Create a line plot of &#39;hourly_arrest_rate&#39; hourly_arrest_rate.plot() # Add the xlabel, ylabel, and title plt.xlabel(&quot;Hour&quot;) plt.ylabel(&quot;Arrest Rate&quot;) plt.title(&quot;Arrest Rate by Time of Day&quot;) # Display the plot plt.show() . The arrest rate has a significant spike overnight, and then dips in the early morning hours. . Are drug-related stops on the rise? . Plotting drug-related stops . In a small portion of traffic stops, drugs are found in the vehicle during a search. In this exercise, you&#39;ll assess whether these drug-related stops are becoming more common over time. . The Boolean column drugs_related_stop indicates whether drugs were found during a given stop. We&#39;ll calculate the annual drug rate by resampling this column, and then we&#39;ll use a line plot to visualize how the rate has changed over time. . # Calculate the annual rate of drug-related stops # Save the annual rate of drug-related stops annual_drug_rate = ri.drugs_related_stop.resample(&quot;A&quot;).mean() display(annual_drug_rate) # Create a line plot of &#39;annual_drug_rate&#39; annual_drug_rate.plot() # Display the plot plt.show() . stop_date_time 2005-12-31 0.006501 2006-12-31 0.007258 2007-12-31 0.007970 2008-12-31 0.007505 2009-12-31 0.009889 2010-12-31 0.010081 2011-12-31 0.009731 2012-12-31 0.009921 2013-12-31 0.013094 2014-12-31 0.013826 2015-12-31 0.012266 Freq: A-DEC, Name: drugs_related_stop, dtype: float64 . The rate of drug-related stops nearly doubled over the course of 10 years. Why might that be the case? . Comparing drug and search rates . The rate of drug-related stops increased significantly between 2005 and 2015. We might hypothesize that the rate of vehicle searches was also increasing, which would have led to an increase in drug-related stops even if more drivers were not carrying drugs. . We can test this hypothesis by calculating the annual search rate, and then plotting it against the annual drug rate. If the hypothesis is true, then we&#39;ll see both rates increasing over time. . # Calculate and save the annual search rate annual_search_rate = ri.search_conducted.resample(&quot;A&quot;).mean() # Concatenate &#39;annual_drug_rate&#39; and &#39;annual_search_rate&#39; annual = pd.concat([annual_drug_rate, annual_search_rate], axis=&quot;columns&quot;) # Create subplots from &#39;annual&#39; annual.plot(subplots=True) # Display the subplots plt.show() . The rate of drug-related stops increased even though the search rate decreased, disproving our hypothesis. . What violations are caught in each district? . Tallying violations by district . The state of Rhode Island is broken into six police districts, also known as zones. How do the zones compare in terms of what violations are caught by police? . We&#39;ll create a frequency table to determine how many violations of each type took place in each of the six zones. Then, we&#39;ll filter the table to focus on the &quot;K&quot; zones, which we&#39;ll examine further. . # Create a frequency table of districts and violations # Save the frequency table as &#39;all_zones&#39; all_zones = pd.crosstab(ri.district, ri.violation) display(all_zones) # Select rows &#39;Zone K1&#39; through &#39;Zone K3&#39; # Save the smaller table as &#39;k_zones&#39; k_zones = all_zones.loc[&quot;Zone K1&quot;:&quot;Zone K3&quot;] k_zones . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . Zone X1 296 | 671 | 143 | 38 | 74 | 1119 | . Zone X3 2049 | 3086 | 769 | 671 | 820 | 8779 | . Zone X4 3541 | 5353 | 1560 | 1411 | 843 | 9795 | . violation Equipment Moving violation Other Registration/plates Seat belt Speeding . district . Zone K1 672 | 1254 | 290 | 120 | 0 | 5960 | . Zone K2 2061 | 2962 | 942 | 768 | 481 | 10448 | . Zone K3 2302 | 2898 | 705 | 695 | 638 | 12322 | . We&#39;ll plot the violations so that you can compare these districts. . Plotting violations by district . Now that we&#39;ve created a frequency table focused on the &quot;K&quot; zones, we&#39;ll visualize the data to help us compare what violations are being caught in each zone. . First we&#39;ll create a bar plot, which is an appropriate plot type since we&#39;re comparing categorical data. Then we&#39;ll create a stacked bar plot in order to get a slightly different look at the data. . # Create a bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;) # Display the plot plt.show() . # Create a stacked bar plot of &#39;k_zones&#39; k_zones.plot(kind=&quot;bar&quot;, stacked=True) # Display the plot plt.show() . The vast majority of traffic stops in Zone K1 are for speeding, and Zones K2 and K3 are remarkably similar to one another in terms of violations. . How long might you be stopped for a violation? . Converting stop durations to numbers . In the traffic stops dataset, the stop_duration column tells us approximately how long the driver was detained by the officer. Unfortunately, the durations are stored as strings, such as &#39;0-15 Min&#39;. How can we make this data easier to analyze? . We&#39;ll convert the stop durations to integers. Because the precise durations are not available, we&#39;ll have to estimate the numbers using reasonable values: . Convert &#39;0-15 Min&#39; to 8 | Convert &#39;16-30 Min&#39; to 23 | Convert &#39;30+ Min&#39; to 45 | . # Create a dictionary that maps strings to integers mapping = {&quot;0-15 Min&quot;:8, &#39;16-30 Min&#39;:23, &#39;30+ Min&#39;:45} # Convert the &#39;stop_duration&#39; strings to integers using the &#39;mapping&#39; ri[&#39;stop_minutes&#39;] = ri.stop_duration.map(mapping) # Print the unique values in &#39;stop_minutes&#39; ri.stop_minutes.unique() . array([ 8, 23, 45], dtype=int64) . Next we&#39;ll analyze the stop length for each type of violation. . Plotting stop length . If you were stopped for a particular violation, how long might you expect to be detained? . We&#39;ll visualize the average length of time drivers are stopped for each type of violation. Rather than using the violation column we&#39;ll use violation_raw since it contains more detailed descriptions of the violations. . # Calculate the mean &#39;stop_minutes&#39; for each value in &#39;violation_raw&#39; # Save the resulting Series as &#39;stop_length&#39; stop_length = ri.groupby(&quot;violation_raw&quot;).stop_minutes.mean() display(stop_length) # Sort &#39;stop_length&#39; by its values and create a horizontal bar plot stop_length.sort_values().plot(kind=&quot;barh&quot;) # Display the plot plt.show() . violation_raw APB 17.967033 Call for Service 22.124371 Equipment/Inspection Violation 11.445655 Motorist Assist/Courtesy 17.741463 Other Traffic Violation 13.844490 Registration Violation 13.736970 Seatbelt Violation 9.662815 Special Detail/Directed Patrol 15.123632 Speeding 10.581562 Suspicious Person 14.910714 Violation of City/Town Ordinance 13.254144 Warrant 24.055556 Name: stop_minutes, dtype: float64 . Analyzing the effect of weather on policing . We will use a second dataset to explore the impact of weather conditions on police behavior during traffic stops. We will be merging and reshaping datasets, assessing whether a data source is trustworthy, working with categorical data, and other advanced skills. . Exploring the weather dataset . Plotting the temperature . We&#39;ll examine the temperature columns from the weather dataset to assess whether the data seems trustworthy. First we&#39;ll print the summary statistics, and then you&#39;ll visualize the data using a box plot. . # Read &#39;weather.csv&#39; into a DataFrame named &#39;weather&#39; weather = pd.read_csv(&quot;../datasets/weather.csv&quot;) display(weather.head()) # Describe the temperature columns display(weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].describe().T) # Create a box plot of the temperature columns weather[[&quot;TMIN&quot;, &quot;TAVG&quot;, &quot;TMAX&quot;]].plot(kind=&#39;box&#39;) # Display the plot plt.show() . STATION DATE TAVG TMIN TMAX AWND WSF2 WT01 WT02 WT03 ... WT11 WT13 WT14 WT15 WT16 WT17 WT18 WT19 WT21 WT22 . 0 USW00014765 | 2005-01-01 | 44.0 | 35 | 53 | 8.95 | 25.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 USW00014765 | 2005-01-02 | 36.0 | 28 | 44 | 9.40 | 14.1 | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 2 USW00014765 | 2005-01-03 | 49.0 | 44 | 53 | 6.93 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 3 USW00014765 | 2005-01-04 | 42.0 | 39 | 45 | 6.93 | 16.1 | 1.0 | NaN | NaN | ... | NaN | 1.0 | 1.0 | NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . 4 USW00014765 | 2005-01-05 | 36.0 | 28 | 43 | 7.83 | 17.0 | 1.0 | NaN | NaN | ... | NaN | 1.0 | NaN | NaN | 1.0 | NaN | 1.0 | NaN | NaN | NaN | . 5 rows × 27 columns . count mean std min 25% 50% 75% max . TMIN 4017.0 | 43.484441 | 17.020298 | -5.0 | 30.0 | 44.0 | 58.0 | 77.0 | . TAVG 1217.0 | 52.493016 | 17.830714 | 6.0 | 39.0 | 54.0 | 68.0 | 86.0 | . TMAX 4017.0 | 61.268608 | 18.199517 | 15.0 | 47.0 | 62.0 | 77.0 | 102.0 | . The temperature data looks good so far: the TAVG values are in between TMIN and TMAX, and the measurements and ranges seem reasonable. . Plotting the temperature difference . We&#39;ll continue to assess whether the dataset seems trustworthy by plotting the difference between the maximum and minimum temperatures. . # Create a &#39;TDIFF&#39; column that represents temperature difference weather[&quot;TDIFF&quot;] = weather.TMAX - weather.TMIN # Describe the &#39;TDIFF&#39; column display(weather.TDIFF.describe()) # Create a histogram with 20 bins to visualize &#39;TDIFF&#39; weather.TDIFF.plot(kind=&quot;hist&quot;, bins=20) # Display the plot plt.show() . count 4017.000000 mean 17.784167 std 6.350720 min 2.000000 25% 14.000000 50% 18.000000 75% 22.000000 max 43.000000 Name: TDIFF, dtype: float64 . The TDIFF column has no negative values and its distribution is approximately normal, both of which are signs that the data is trustworthy. . Categorizing the weather . Counting bad weather conditions . The weather DataFrame contains 20 columns that start with &#39;WT&#39;, each of which represents a bad weather condition. For example: . WT05 indicates &quot;Hail&quot; | WT11 indicates &quot;High or damaging winds&quot; | WT17 indicates &quot;Freezing rain&quot; | . For every row in the dataset, each WT column contains either a 1 (meaning the condition was present that day) or NaN (meaning the condition was not present). . We&#39;ll quantify &quot;how bad&quot; the weather was each day by counting the number of 1 values in each row. . # Copy &#39;WT01&#39; through &#39;WT22&#39; to a new DataFrame WT = weather.loc[:, &quot;WT01&quot;:&quot;WT22&quot;] # Calculate the sum of each row in &#39;WT&#39; weather[&#39;bad_conditions&#39;] = WT.sum(axis=&quot;columns&quot;) # Replace missing values in &#39;bad_conditions&#39; with &#39;0&#39; weather[&#39;bad_conditions&#39;] = weather.bad_conditions.fillna(0).astype(&#39;int&#39;) # Create a histogram to visualize &#39;bad_conditions&#39; weather.bad_conditions.plot(kind=&quot;hist&quot;) # Display the plot plt.show() . It looks like many days didn&#39;t have any bad weather conditions, and only a small portion of days had more than four bad weather conditions. . Rating the weather conditions . We counted the number of bad weather conditions each day. We&#39;ll use the counts to create a rating system for the weather. . The counts range from 0 to 9, and should be converted to ratings as follows: . Convert 0 to &#39;good&#39; | Convert 1 through 4 to &#39;bad&#39; | Convert 5 through 9 to &#39;worse&#39; | . # Count the unique values in &#39;bad_conditions&#39; and sort the index display(weather.bad_conditions.value_counts().sort_index()) # Create a dictionary that maps integers to strings mapping = {0:&#39;good&#39;, 1:&#39;bad&#39;, 2:&#39;bad&#39;, 3:&#39;bad&#39;, 4:&#39;bad&#39;, 5:&#39;worse&#39;, 6:&#39;worse&#39;, 7:&#39;worse&#39;, 8:&#39;worse&#39;, 9:&#39;worse&#39;} # Convert the &#39;bad_conditions&#39; integers to strings using the &#39;mapping&#39; weather[&#39;rating&#39;] = weather.bad_conditions.map(mapping) # Count the unique values in &#39;rating&#39; weather.rating.value_counts() . 0 1749 1 613 2 367 3 380 4 476 5 282 6 101 7 41 8 4 9 4 Name: bad_conditions, dtype: int64 . bad 1836 good 1749 worse 432 Name: rating, dtype: int64 . Changing the data type to category . Since the rating column only has a few possible values, we&#39;ll change its data type to category in order to store the data more efficiently. we&#39;ll also specify a logical order for the categories, which will be useful for future work. . # Create a list of weather ratings in logical order cats = [&#39;good&#39;, &#39;bad&#39;, &#39;worse&#39;] # Change the data type of &#39;rating&#39; to category weather[&#39;rating&#39;] = weather.rating.astype(CategoricalDtype(ordered=True, categories=cats)) # Examine the head of &#39;rating&#39; weather.rating.head() . 0 bad 1 bad 2 bad 3 bad 4 bad Name: rating, dtype: category Categories (3, object): [good &lt; bad &lt; worse] . We&#39;ll use the rating column in future exercises to analyze the effects of weather on police behavior. . Merging datasets . Preparing the DataFrames . We&#39;ll prepare the traffic stop and weather rating DataFrames so that they&#39;re ready to be merged: . With the ri DataFrame, we&#39;ll move the stop_datetime index to a column since the index will be lost during the merge. | With the weather DataFrame, we&#39;ll select the DATE and rating columns and put them in a new DataFrame. | . # Reset the index of &#39;ri&#39; ri.reset_index(inplace=True) # Examine the head of &#39;ri&#39; display(ri.head()) # Create a DataFrame from the &#39;DATE&#39; and &#39;rating&#39; columns weather_rating = weather[[&quot;DATE&quot;, &quot;rating&quot;]] # Examine the head of &#39;weather_rating&#39; weather_rating.head() . stop_date_time stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes . 0 2005-01-04 12:55:00 | 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 1 2005-01-23 23:15:00 | 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | . 2 2005-02-17 04:15:00 | 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | . 3 2005-02-20 17:15:00 | 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | . 4 2005-02-24 01:20:00 | 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | . DATE rating . 0 2005-01-01 | bad | . 1 2005-01-02 | bad | . 2 2005-01-03 | bad | . 3 2005-01-04 | bad | . 4 2005-01-05 | bad | . The ri and weather_rating DataFrames are now ready to be merged. . Merging the DataFrames . We&#39;ll merge the ri and weather_rating DataFrames into a new DataFrame, ri_weather. . The DataFrames will be joined using the stop_date column from ri and the DATE column from weather_rating. Thankfully the date formatting matches exactly, which is not always the case! . Once the merge is complete, we&#39;ll set stop_datetime as the index . # Examine the shape of &#39;ri&#39; print(ri.shape) # Merge &#39;ri&#39; and &#39;weather_rating&#39; using a left join ri_weather = pd.merge(left=ri, right=weather_rating, left_on=&#39;stop_date&#39;, right_on=&#39;DATE&#39;, how=&#39;left&#39;) # Examine the shape of &#39;ri_weather&#39; print(ri_weather.shape) # Set &#39;stop_datetime&#39; as the index of &#39;ri_weather&#39; ri_weather.set_index(&#39;stop_date_time&#39;, inplace=True) ri_weather.head() . (86536, 16) (86536, 18) . stop_date stop_time driver_gender driver_race violation_raw violation search_conducted search_type stop_outcome is_arrested stop_duration drugs_related_stop district frisk stop_minutes DATE rating . stop_date_time . 2005-01-04 12:55:00 2005-01-04 | 12:55 | M | White | Equipment/Inspection Violation | Equipment | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-01-04 | bad | . 2005-01-23 23:15:00 2005-01-23 | 23:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone K3 | False | 8 | 2005-01-23 | worse | . 2005-02-17 04:15:00 2005-02-17 | 04:15 | M | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X4 | False | 8 | 2005-02-17 | good | . 2005-02-20 17:15:00 2005-02-20 | 17:15 | M | White | Call for Service | Other | False | NaN | Arrest Driver | True | 16-30 Min | False | Zone X1 | False | 23 | 2005-02-20 | bad | . 2005-02-24 01:20:00 2005-02-24 | 01:20 | F | White | Speeding | Speeding | False | NaN | Citation | False | 0-15 Min | False | Zone X3 | False | 8 | 2005-02-24 | bad | . We&#39;ll use ri_weather to analyze the relationship between weather conditions and police behavior. . Does weather affect the arrest rate? . Comparing arrest rates by weather rating . Do police officers arrest drivers more often when the weather is bad? Let&#39;s find out below! . First, we&#39;ll calculate the overall arrest rate. | Then, we&#39;ll calculate the arrest rate for each of the weather ratings we previously assigned. | Finally, we&#39;ll add violation type as a second factor in the analysis, to see if that accounts for any differences in the arrest rate. | . Since we previously defined a logical order for the weather categories, good &lt; bad &lt; worse, they will be sorted that way in the results. . # Calculate the overall arrest rate print(ri_weather.is_arrested.mean()) . 0.0355690117407784 . # Calculate the arrest rate for each &#39;rating&#39; ri_weather.groupby(&quot;rating&quot;).is_arrested.mean() . rating good 0.033715 bad 0.036261 worse 0.041667 Name: is_arrested, dtype: float64 . # Calculate the arrest rate for each &#39;violation&#39; and &#39;rating&#39; ri_weather.groupby([&quot;violation&quot;, &#39;rating&#39;]).is_arrested.mean() . violation rating Equipment good 0.059007 bad 0.066311 worse 0.097357 Moving violation good 0.056227 bad 0.058050 worse 0.065860 Other good 0.076966 bad 0.087443 worse 0.062893 Registration/plates good 0.081574 bad 0.098160 worse 0.115625 Seat belt good 0.028587 bad 0.022493 worse 0.000000 Speeding good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . The arrest rate increases as the weather gets worse, and that trend persists across many of the violation types. This doesn&#39;t prove a causal link, but it&#39;s quite an interesting result! . Selecting from a multi-indexed Series . The output of a single .groupby() operation on multiple columns is a Series with a MultiIndex. Working with this type of object is similar to working with a DataFrame: . The outer index level is like the DataFrame rows. | The inner index level is like the DataFrame columns. | . # Save the output of the groupby operation from the last exercise arrest_rate = ri_weather.groupby([&#39;violation&#39;, &#39;rating&#39;]).is_arrested.mean() # Print the arrest rate for moving violations in bad weather display(arrest_rate.loc[&quot;Moving violation&quot;, &quot;bad&quot;]) # Print the arrest rates for speeding violations in all three weather conditions arrest_rate.loc[&quot;Speeding&quot;] . 0.05804964058049641 . rating good 0.013405 bad 0.013314 worse 0.016886 Name: is_arrested, dtype: float64 . Reshaping the arrest rate data . We&#39;ll start by reshaping the arrest_rate Series into a DataFrame. This is a useful step when working with any multi-indexed Series, since it enables you to access the full range of DataFrame methods. . Then, we&#39;ll create the exact same DataFrame using a pivot table. This is a great example of how pandas often gives you more than one way to reach the same result! . # Unstack the &#39;arrest_rate&#39; Series into a DataFrame display(arrest_rate.unstack()) # Create the same DataFrame using a pivot table ri_weather.pivot_table(index=&#39;violation&#39;, columns=&#39;rating&#39;, values=&#39;is_arrested&#39;) . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | . rating good bad worse . violation . Equipment 0.059007 | 0.066311 | 0.097357 | . Moving violation 0.056227 | 0.058050 | 0.065860 | . Other 0.076966 | 0.087443 | 0.062893 | . Registration/plates 0.081574 | 0.098160 | 0.115625 | . Seat belt 0.028587 | 0.022493 | 0.000000 | . Speeding 0.013405 | 0.013314 | 0.016886 | .",
            "url": "https://victoromondi1997.github.io/blog/pandas/eda/python/data-science/data-analysis/2020/06/28/Analyzing-Police-Activity-with-pandas.html",
            "relUrl": "/pandas/eda/python/data-science/data-analysis/2020/06/28/Analyzing-Police-Activity-with-pandas.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Exploring Boston Weather Data",
            "content": "weather = readRDS(gzcon(url(&#39;https://assets.datacamp.com/production/repositories/34/datasets/b3c1036d9a60a9dfe0f99051d2474a54f76055ea/weather.rds&#39;))) . Libraries . library(readr) library(dplyr) library(lubridate) library(stringr) library(installr) library(tidyr) . Warning message: &#34;package &#39;tidyr&#39; was built under R version 3.6.3&#34; . # Verify that weather is a data.frame class(weather) # Check the dimensions dim(weather) # View the column names names(weather) . &#39;data.frame&#39; &lt;ol class=list-inline&gt; 286 | 35 | &lt;/ol&gt; &lt;ol class=list-inline&gt; &#39;X&#39; | &#39;year&#39; | &#39;month&#39; | &#39;measure&#39; | &#39;X1&#39; | &#39;X2&#39; | &#39;X3&#39; | &#39;X4&#39; | &#39;X5&#39; | &#39;X6&#39; | &#39;X7&#39; | &#39;X8&#39; | &#39;X9&#39; | &#39;X10&#39; | &#39;X11&#39; | &#39;X12&#39; | &#39;X13&#39; | &#39;X14&#39; | &#39;X15&#39; | &#39;X16&#39; | &#39;X17&#39; | &#39;X18&#39; | &#39;X19&#39; | &#39;X20&#39; | &#39;X21&#39; | &#39;X22&#39; | &#39;X23&#39; | &#39;X24&#39; | &#39;X25&#39; | &#39;X26&#39; | &#39;X27&#39; | &#39;X28&#39; | &#39;X29&#39; | &#39;X30&#39; | &#39;X31&#39; | &lt;/ol&gt; We&#39;ve confirmed that the object is a data frame with 286 rows and 35 columns. . Summarize the data . Next up is to look at some summaries of the data. This is where functions like str(), glimpse() from dplyr, and summary() come in handy. . # View the structure of the data str(weather) # Look at the structure using dplyr&#39;s glimpse() glimpse(weather) # View a summary of the data summary(weather) . &#39;data.frame&#39;: 286 obs. of 35 variables: $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... $ month : int 12 12 12 12 12 12 12 12 12 12 ... $ measure: chr &#34;Max.TemperatureF&#34; &#34;Mean.TemperatureF&#34; &#34;Min.TemperatureF&#34; &#34;Max.Dew.PointF&#34; ... $ X1 : chr &#34;64&#34; &#34;52&#34; &#34;39&#34; &#34;46&#34; ... $ X2 : chr &#34;42&#34; &#34;38&#34; &#34;33&#34; &#34;40&#34; ... $ X3 : chr &#34;51&#34; &#34;44&#34; &#34;37&#34; &#34;49&#34; ... $ X4 : chr &#34;43&#34; &#34;37&#34; &#34;30&#34; &#34;24&#34; ... $ X5 : chr &#34;42&#34; &#34;34&#34; &#34;26&#34; &#34;37&#34; ... $ X6 : chr &#34;45&#34; &#34;42&#34; &#34;38&#34; &#34;45&#34; ... $ X7 : chr &#34;38&#34; &#34;30&#34; &#34;21&#34; &#34;36&#34; ... $ X8 : chr &#34;29&#34; &#34;24&#34; &#34;18&#34; &#34;28&#34; ... $ X9 : chr &#34;49&#34; &#34;39&#34; &#34;29&#34; &#34;49&#34; ... $ X10 : chr &#34;48&#34; &#34;43&#34; &#34;38&#34; &#34;45&#34; ... $ X11 : chr &#34;39&#34; &#34;36&#34; &#34;32&#34; &#34;37&#34; ... $ X12 : chr &#34;39&#34; &#34;35&#34; &#34;31&#34; &#34;28&#34; ... $ X13 : chr &#34;42&#34; &#34;37&#34; &#34;32&#34; &#34;28&#34; ... $ X14 : chr &#34;45&#34; &#34;39&#34; &#34;33&#34; &#34;29&#34; ... $ X15 : chr &#34;42&#34; &#34;37&#34; &#34;32&#34; &#34;33&#34; ... $ X16 : chr &#34;44&#34; &#34;40&#34; &#34;35&#34; &#34;42&#34; ... $ X17 : chr &#34;49&#34; &#34;45&#34; &#34;41&#34; &#34;46&#34; ... $ X18 : chr &#34;44&#34; &#34;40&#34; &#34;36&#34; &#34;34&#34; ... $ X19 : chr &#34;37&#34; &#34;33&#34; &#34;29&#34; &#34;25&#34; ... $ X20 : chr &#34;36&#34; &#34;32&#34; &#34;27&#34; &#34;30&#34; ... $ X21 : chr &#34;36&#34; &#34;33&#34; &#34;30&#34; &#34;30&#34; ... $ X22 : chr &#34;44&#34; &#34;39&#34; &#34;33&#34; &#34;39&#34; ... $ X23 : chr &#34;47&#34; &#34;45&#34; &#34;42&#34; &#34;45&#34; ... $ X24 : chr &#34;46&#34; &#34;44&#34; &#34;41&#34; &#34;46&#34; ... $ X25 : chr &#34;59&#34; &#34;52&#34; &#34;44&#34; &#34;58&#34; ... $ X26 : chr &#34;50&#34; &#34;44&#34; &#34;37&#34; &#34;31&#34; ... $ X27 : chr &#34;52&#34; &#34;45&#34; &#34;38&#34; &#34;34&#34; ... $ X28 : chr &#34;52&#34; &#34;46&#34; &#34;40&#34; &#34;42&#34; ... $ X29 : chr &#34;41&#34; &#34;36&#34; &#34;30&#34; &#34;26&#34; ... $ X30 : chr &#34;30&#34; &#34;26&#34; &#34;22&#34; &#34;10&#34; ... $ X31 : chr &#34;30&#34; &#34;25&#34; &#34;20&#34; &#34;8&#34; ... Rows: 286 Columns: 35 $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ... $ year &lt;int&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,... $ month &lt;int&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,... $ measure &lt;chr&gt; &#34;Max.TemperatureF&#34;, &#34;Mean.TemperatureF&#34;, &#34;Min.TemperatureF&#34;... $ X1 &lt;chr&gt; &#34;64&#34;, &#34;52&#34;, &#34;39&#34;, &#34;46&#34;, &#34;40&#34;, &#34;26&#34;, &#34;74&#34;, &#34;63&#34;, &#34;52&#34;, &#34;30.4... $ X2 &lt;chr&gt; &#34;42&#34;, &#34;38&#34;, &#34;33&#34;, &#34;40&#34;, &#34;27&#34;, &#34;17&#34;, &#34;92&#34;, &#34;72&#34;, &#34;51&#34;, &#34;30.7... $ X3 &lt;chr&gt; &#34;51&#34;, &#34;44&#34;, &#34;37&#34;, &#34;49&#34;, &#34;42&#34;, &#34;24&#34;, &#34;100&#34;, &#34;79&#34;, &#34;57&#34;, &#34;30.... $ X4 &lt;chr&gt; &#34;43&#34;, &#34;37&#34;, &#34;30&#34;, &#34;24&#34;, &#34;21&#34;, &#34;13&#34;, &#34;69&#34;, &#34;54&#34;, &#34;39&#34;, &#34;30.5... $ X5 &lt;chr&gt; &#34;42&#34;, &#34;34&#34;, &#34;26&#34;, &#34;37&#34;, &#34;25&#34;, &#34;12&#34;, &#34;85&#34;, &#34;66&#34;, &#34;47&#34;, &#34;30.6... $ X6 &lt;chr&gt; &#34;45&#34;, &#34;42&#34;, &#34;38&#34;, &#34;45&#34;, &#34;40&#34;, &#34;36&#34;, &#34;100&#34;, &#34;93&#34;, &#34;85&#34;, &#34;30.... $ X7 &lt;chr&gt; &#34;38&#34;, &#34;30&#34;, &#34;21&#34;, &#34;36&#34;, &#34;20&#34;, &#34;-3&#34;, &#34;92&#34;, &#34;61&#34;, &#34;29&#34;, &#34;30.6... $ X8 &lt;chr&gt; &#34;29&#34;, &#34;24&#34;, &#34;18&#34;, &#34;28&#34;, &#34;16&#34;, &#34;3&#34;, &#34;92&#34;, &#34;70&#34;, &#34;47&#34;, &#34;30.77... $ X9 &lt;chr&gt; &#34;49&#34;, &#34;39&#34;, &#34;29&#34;, &#34;49&#34;, &#34;41&#34;, &#34;28&#34;, &#34;100&#34;, &#34;93&#34;, &#34;86&#34;, &#34;30.... $ X10 &lt;chr&gt; &#34;48&#34;, &#34;43&#34;, &#34;38&#34;, &#34;45&#34;, &#34;39&#34;, &#34;37&#34;, &#34;100&#34;, &#34;95&#34;, &#34;89&#34;, &#34;29.... $ X11 &lt;chr&gt; &#34;39&#34;, &#34;36&#34;, &#34;32&#34;, &#34;37&#34;, &#34;31&#34;, &#34;27&#34;, &#34;92&#34;, &#34;87&#34;, &#34;82&#34;, &#34;29.8... $ X12 &lt;chr&gt; &#34;39&#34;, &#34;35&#34;, &#34;31&#34;, &#34;28&#34;, &#34;27&#34;, &#34;25&#34;, &#34;85&#34;, &#34;75&#34;, &#34;64&#34;, &#34;29.8... $ X13 &lt;chr&gt; &#34;42&#34;, &#34;37&#34;, &#34;32&#34;, &#34;28&#34;, &#34;26&#34;, &#34;24&#34;, &#34;75&#34;, &#34;65&#34;, &#34;55&#34;, &#34;29.8... $ X14 &lt;chr&gt; &#34;45&#34;, &#34;39&#34;, &#34;33&#34;, &#34;29&#34;, &#34;27&#34;, &#34;25&#34;, &#34;82&#34;, &#34;68&#34;, &#34;53&#34;, &#34;29.9... $ X15 &lt;chr&gt; &#34;42&#34;, &#34;37&#34;, &#34;32&#34;, &#34;33&#34;, &#34;29&#34;, &#34;27&#34;, &#34;89&#34;, &#34;75&#34;, &#34;60&#34;, &#34;30.1... $ X16 &lt;chr&gt; &#34;44&#34;, &#34;40&#34;, &#34;35&#34;, &#34;42&#34;, &#34;36&#34;, &#34;30&#34;, &#34;96&#34;, &#34;85&#34;, &#34;73&#34;, &#34;30.1... $ X17 &lt;chr&gt; &#34;49&#34;, &#34;45&#34;, &#34;41&#34;, &#34;46&#34;, &#34;41&#34;, &#34;32&#34;, &#34;100&#34;, &#34;85&#34;, &#34;70&#34;, &#34;29.... $ X18 &lt;chr&gt; &#34;44&#34;, &#34;40&#34;, &#34;36&#34;, &#34;34&#34;, &#34;30&#34;, &#34;26&#34;, &#34;89&#34;, &#34;73&#34;, &#34;57&#34;, &#34;29.8... $ X19 &lt;chr&gt; &#34;37&#34;, &#34;33&#34;, &#34;29&#34;, &#34;25&#34;, &#34;22&#34;, &#34;20&#34;, &#34;69&#34;, &#34;63&#34;, &#34;56&#34;, &#34;30.1... $ X20 &lt;chr&gt; &#34;36&#34;, &#34;32&#34;, &#34;27&#34;, &#34;30&#34;, &#34;24&#34;, &#34;20&#34;, &#34;89&#34;, &#34;79&#34;, &#34;69&#34;, &#34;30.3... $ X21 &lt;chr&gt; &#34;36&#34;, &#34;33&#34;, &#34;30&#34;, &#34;30&#34;, &#34;27&#34;, &#34;25&#34;, &#34;85&#34;, &#34;77&#34;, &#34;69&#34;, &#34;30.3... $ X22 &lt;chr&gt; &#34;44&#34;, &#34;39&#34;, &#34;33&#34;, &#34;39&#34;, &#34;34&#34;, &#34;25&#34;, &#34;89&#34;, &#34;79&#34;, &#34;69&#34;, &#34;30.4... $ X23 &lt;chr&gt; &#34;47&#34;, &#34;45&#34;, &#34;42&#34;, &#34;45&#34;, &#34;42&#34;, &#34;37&#34;, &#34;100&#34;, &#34;91&#34;, &#34;82&#34;, &#34;30.... $ X24 &lt;chr&gt; &#34;46&#34;, &#34;44&#34;, &#34;41&#34;, &#34;46&#34;, &#34;44&#34;, &#34;41&#34;, &#34;100&#34;, &#34;98&#34;, &#34;96&#34;, &#34;30.... $ X25 &lt;chr&gt; &#34;59&#34;, &#34;52&#34;, &#34;44&#34;, &#34;58&#34;, &#34;43&#34;, &#34;29&#34;, &#34;100&#34;, &#34;75&#34;, &#34;49&#34;, &#34;29.... $ X26 &lt;chr&gt; &#34;50&#34;, &#34;44&#34;, &#34;37&#34;, &#34;31&#34;, &#34;29&#34;, &#34;28&#34;, &#34;70&#34;, &#34;60&#34;, &#34;49&#34;, &#34;30.1... $ X27 &lt;chr&gt; &#34;52&#34;, &#34;45&#34;, &#34;38&#34;, &#34;34&#34;, &#34;31&#34;, &#34;29&#34;, &#34;70&#34;, &#34;60&#34;, &#34;50&#34;, &#34;30.2... $ X28 &lt;chr&gt; &#34;52&#34;, &#34;46&#34;, &#34;40&#34;, &#34;42&#34;, &#34;35&#34;, &#34;27&#34;, &#34;76&#34;, &#34;65&#34;, &#34;53&#34;, &#34;29.9... $ X29 &lt;chr&gt; &#34;41&#34;, &#34;36&#34;, &#34;30&#34;, &#34;26&#34;, &#34;20&#34;, &#34;10&#34;, &#34;64&#34;, &#34;51&#34;, &#34;37&#34;, &#34;30.2... $ X30 &lt;chr&gt; &#34;30&#34;, &#34;26&#34;, &#34;22&#34;, &#34;10&#34;, &#34;4&#34;, &#34;-6&#34;, &#34;50&#34;, &#34;38&#34;, &#34;26&#34;, &#34;30.36... $ X31 &lt;chr&gt; &#34;30&#34;, &#34;25&#34;, &#34;20&#34;, &#34;8&#34;, &#34;5&#34;, &#34;1&#34;, &#34;57&#34;, &#34;44&#34;, &#34;31&#34;, &#34;30.32&#34;,... . X year month measure Min. : 1.00 Min. :2014 Min. : 1.000 Length:286 1st Qu.: 72.25 1st Qu.:2015 1st Qu.: 4.000 Class :character Median :143.50 Median :2015 Median : 7.000 Mode :character Mean :143.50 Mean :2015 Mean : 6.923 3rd Qu.:214.75 3rd Qu.:2015 3rd Qu.:10.000 Max. :286.00 Max. :2015 Max. :12.000 X1 X2 X3 X4 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X5 X6 X7 X8 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X9 X10 X11 X12 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X13 X14 X15 X16 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X17 X18 X19 X20 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X21 X22 X23 X24 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X25 X26 X27 X28 Length:286 Length:286 Length:286 Length:286 Class :character Class :character Class :character Class :character Mode :character Mode :character Mode :character Mode :character X29 X30 X31 Length:286 Length:286 Length:286 Class :character Class :character Class :character Mode :character Mode :character Mode :character . Now that we have a pretty good feel for how the table is structured, we&#39;ll take a look at some real observations! . Take a closer look . After understanding the structure of the data and looking at some brief summaries, it often helps to preview the actual data. The functions head() and tail() allow us to view the top and bottom rows of the data, respectively. . # View first 6 rows head(weather) # View first 15 rows head(weather, n=15) # View the last 6 rows tail(weather) # View the last 10 rows tail(weather, n=10) . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 1 | 2014 | 12 | Max.TemperatureF | 64 | 42 | 51 | 43 | 42 | 45 | ... | 44 | 47 | 46 | 59 | 50 | 52 | 52 | 41 | 30 | 30 | . 2 | 2014 | 12 | Mean.TemperatureF | 52 | 38 | 44 | 37 | 34 | 42 | ... | 39 | 45 | 44 | 52 | 44 | 45 | 46 | 36 | 26 | 25 | . 3 | 2014 | 12 | Min.TemperatureF | 39 | 33 | 37 | 30 | 26 | 38 | ... | 33 | 42 | 41 | 44 | 37 | 38 | 40 | 30 | 22 | 20 | . 4 | 2014 | 12 | Max.Dew.PointF | 46 | 40 | 49 | 24 | 37 | 45 | ... | 39 | 45 | 46 | 58 | 31 | 34 | 42 | 26 | 10 | 8 | . 5 | 2014 | 12 | MeanDew.PointF | 40 | 27 | 42 | 21 | 25 | 40 | ... | 34 | 42 | 44 | 43 | 29 | 31 | 35 | 20 | 4 | 5 | . 6 | 2014 | 12 | Min.DewpointF | 26 | 17 | 24 | 13 | 12 | 36 | ... | 25 | 37 | 41 | 29 | 28 | 29 | 27 | 10 | -6 | 1 | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 1 | 2014 | 12 | Max.TemperatureF | 64 | 42 | 51 | 43 | 42 | 45 | ... | 44 | 47 | 46 | 59 | 50 | 52 | 52 | 41 | 30 | 30 | . 2 | 2014 | 12 | Mean.TemperatureF | 52 | 38 | 44 | 37 | 34 | 42 | ... | 39 | 45 | 44 | 52 | 44 | 45 | 46 | 36 | 26 | 25 | . 3 | 2014 | 12 | Min.TemperatureF | 39 | 33 | 37 | 30 | 26 | 38 | ... | 33 | 42 | 41 | 44 | 37 | 38 | 40 | 30 | 22 | 20 | . 4 | 2014 | 12 | Max.Dew.PointF | 46 | 40 | 49 | 24 | 37 | 45 | ... | 39 | 45 | 46 | 58 | 31 | 34 | 42 | 26 | 10 | 8 | . 5 | 2014 | 12 | MeanDew.PointF | 40 | 27 | 42 | 21 | 25 | 40 | ... | 34 | 42 | 44 | 43 | 29 | 31 | 35 | 20 | 4 | 5 | . 6 | 2014 | 12 | Min.DewpointF | 26 | 17 | 24 | 13 | 12 | 36 | ... | 25 | 37 | 41 | 29 | 28 | 29 | 27 | 10 | -6 | 1 | . 7 | 2014 | 12 | Max.Humidity | 74 | 92 | 100 | 69 | 85 | 100 | ... | 89 | 100 | 100 | 100 | 70 | 70 | 76 | 64 | 50 | 57 | . 8 | 2014 | 12 | Mean.Humidity | 63 | 72 | 79 | 54 | 66 | 93 | ... | 79 | 91 | 98 | 75 | 60 | 60 | 65 | 51 | 38 | 44 | . 9 | 2014 | 12 | Min.Humidity | 52 | 51 | 57 | 39 | 47 | 85 | ... | 69 | 82 | 96 | 49 | 49 | 50 | 53 | 37 | 26 | 31 | . 10 | 2014 | 12 | Max.Sea.Level.PressureIn | 30.45 | 30.71 | 30.4 | 30.56 | 30.68 | 30.42 | ... | 30.4 | 30.31 | 30.13 | 29.96 | 30.16 | 30.22 | 29.99 | 30.22 | 30.36 | 30.32 | . 11 | 2014 | 12 | Mean.Sea.Level.PressureIn | 30.13 | 30.59 | 30.07 | 30.33 | 30.59 | 30.24 | ... | 30.35 | 30.23 | 29.9 | 29.63 | 30.11 | 30.14 | 29.87 | 30.12 | 30.32 | 30.25 | . 12 | 2014 | 12 | Min.Sea.Level.PressureIn | 30.01 | 30.4 | 29.87 | 30.09 | 30.45 | 30.16 | ... | 30.3 | 30.16 | 29.55 | 29.47 | 29.99 | 30.03 | 29.77 | 30 | 30.23 | 30.13 | . 13 | 2014 | 12 | Max.VisibilityMiles | 10 | 10 | 10 | 10 | 10 | 10 | ... | 10 | 10 | 2 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | . 14 | 2014 | 12 | Mean.VisibilityMiles | 10 | 8 | 5 | 10 | 10 | 4 | ... | 10 | 5 | 1 | 8 | 10 | 10 | 10 | 10 | 10 | 10 | . 15 | 2014 | 12 | Min.VisibilityMiles | 10 | 2 | 1 | 10 | 5 | 0 | ... | 4 | 1 | 0 | 1 | 10 | 10 | 10 | 10 | 10 | 10 | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 281281 | 2015 | 12 | Mean.Wind.SpeedMPH | 6 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 282282 | 2015 | 12 | Max.Gust.SpeedMPH | 17 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 283283 | 2015 | 12 | PrecipitationIn | 0.14 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 284284 | 2015 | 12 | CloudCover | 7 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 285285 | 2015 | 12 | Events | Rain | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 286286 | 2015 | 12 | WindDirDegrees | 109 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . XyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31 . 277277 | 2015 | 12 | Max.VisibilityMiles | 10 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 278278 | 2015 | 12 | Mean.VisibilityMiles | 8 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 279279 | 2015 | 12 | Min.VisibilityMiles | 1 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 280280 | 2015 | 12 | Max.Wind.SpeedMPH | 15 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 281281 | 2015 | 12 | Mean.Wind.SpeedMPH | 6 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 282282 | 2015 | 12 | Max.Gust.SpeedMPH | 17 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 283283 | 2015 | 12 | PrecipitationIn | 0.14 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 284284 | 2015 | 12 | CloudCover | 7 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 285285 | 2015 | 12 | Events | Rain | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 286286 | 2015 | 12 | WindDirDegrees | 109 | NA | NA | NA | NA | NA | ... | NA | NA | NA | NA | NA | NA | NA | NA | NA | NA | . Let&#39;s tidy the data . Column names are values . The weather dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. . The tidyr package provides the gather() function for exactly this scenario. . gather(df, time, val, t1:t3) . gather() allows us to select multiple columns to be gathered by using the : operator. . # Gather the columns weather2 &lt;- gather(weather, day, value, X1:X31, na.rm = TRUE) # View the head head(weather2) . Xyearmonthmeasuredayvalue . 1 | 2014 | 12 | Max.TemperatureF | X1 | 64 | . 2 | 2014 | 12 | Mean.TemperatureF | X1 | 52 | . 3 | 2014 | 12 | Min.TemperatureF | X1 | 39 | . 4 | 2014 | 12 | Max.Dew.PointF | X1 | 46 | . 5 | 2014 | 12 | MeanDew.PointF | X1 | 40 | . 6 | 2014 | 12 | Min.DewpointF | X1 | 26 | . Values are variable names . Our data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. . The spread() function from tidyr is designed to help with this. . spread(df2, time, val) . # First remove column of row names without_x &lt;- weather2[, -1] # Spread the data weather3 &lt;- spread(without_x, measure, value) # View the head head(weather3) . yearmonthdayCloudCoverEventsMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureF...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014 | 12 | X1 | 6 | Rain | 46 | 29 | 74 | 30.45 | 64 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014 | 12 | X10 | 8 | Rain | 45 | 29 | 100 | 29.58 | 48 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014 | 12 | X11 | 8 | Rain-Snow | 37 | 28 | 92 | 29.81 | 39 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014 | 12 | X12 | 7 | Snow | 28 | 21 | 85 | 29.88 | 39 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014 | 12 | X13 | 5 | | 28 | 23 | 75 | 29.86 | 42 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014 | 12 | X14 | 4 | | 29 | 20 | 82 | 29.91 | 45 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . This dataset is looking much better already! . Prepare the data for analysis . Clean up dates . Now that the weather dataset adheres to tidy data principles, the next step is to prepare it for analysis. We&#39;ll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task. . # Remove X&#39;s from day column weather3$day &lt;- str_replace(weather3$day, &#39;X&#39;, &#39;&#39;) # Unite the year, month, and day columns weather4 &lt;- unite(weather3, date, year, month, day, sep = &quot;-&quot;) # Convert date column to proper date format using lubridates&#39;s ymd() weather4$date &lt;- ymd(weather4$date) # Rearrange columns using dplyr&#39;s select() weather5 &lt;- select(weather4, date, Events, CloudCover:WindDirDegrees) # View the head of weather5 head(weather5) . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014-12-13 | | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014-12-14 | | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . A closer look at column types . It&#39;s important for analysis that variables are coded appropriately. This is not yet the case with our weather data. . # View the structure of weather5 str(weather5) # Examine the first 20 rows of weather5. Are most of the characters numeric? head(weather5, 20) # See what happens if we try to convert PrecipitationIn to numeric as.numeric(weather5$PrecipitationIn) . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ Events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ CloudCover : chr &#34;6&#34; &#34;8&#34; &#34;8&#34; &#34;7&#34; ... $ Max.Dew.PointF : chr &#34;46&#34; &#34;45&#34; &#34;37&#34; &#34;28&#34; ... $ Max.Gust.SpeedMPH : chr &#34;29&#34; &#34;29&#34; &#34;28&#34; &#34;21&#34; ... $ Max.Humidity : chr &#34;74&#34; &#34;100&#34; &#34;92&#34; &#34;85&#34; ... $ Max.Sea.Level.PressureIn : chr &#34;30.45&#34; &#34;29.58&#34; &#34;29.81&#34; &#34;29.88&#34; ... $ Max.TemperatureF : chr &#34;64&#34; &#34;48&#34; &#34;39&#34; &#34;39&#34; ... $ Max.VisibilityMiles : chr &#34;10&#34; &#34;10&#34; &#34;10&#34; &#34;10&#34; ... $ Max.Wind.SpeedMPH : chr &#34;22&#34; &#34;23&#34; &#34;21&#34; &#34;16&#34; ... $ Mean.Humidity : chr &#34;63&#34; &#34;95&#34; &#34;87&#34; &#34;75&#34; ... $ Mean.Sea.Level.PressureIn: chr &#34;30.13&#34; &#34;29.5&#34; &#34;29.61&#34; &#34;29.85&#34; ... $ Mean.TemperatureF : chr &#34;52&#34; &#34;43&#34; &#34;36&#34; &#34;35&#34; ... $ Mean.VisibilityMiles : chr &#34;10&#34; &#34;3&#34; &#34;7&#34; &#34;10&#34; ... $ Mean.Wind.SpeedMPH : chr &#34;13&#34; &#34;13&#34; &#34;13&#34; &#34;11&#34; ... $ MeanDew.PointF : chr &#34;40&#34; &#34;39&#34; &#34;31&#34; &#34;27&#34; ... $ Min.DewpointF : chr &#34;26&#34; &#34;37&#34; &#34;27&#34; &#34;25&#34; ... $ Min.Humidity : chr &#34;52&#34; &#34;89&#34; &#34;82&#34; &#34;64&#34; ... $ Min.Sea.Level.PressureIn : chr &#34;30.01&#34; &#34;29.43&#34; &#34;29.44&#34; &#34;29.81&#34; ... $ Min.TemperatureF : chr &#34;39&#34; &#34;38&#34; &#34;32&#34; &#34;31&#34; ... $ Min.VisibilityMiles : chr &#34;10&#34; &#34;1&#34; &#34;1&#34; &#34;7&#34; ... $ PrecipitationIn : chr &#34;0.01&#34; &#34;0.28&#34; &#34;0.02&#34; &#34;T&#34; ... $ WindDirDegrees : chr &#34;268&#34; &#34;357&#34; &#34;230&#34; &#34;286&#34; ... . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | T | 286 | . 2014-12-13 | | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | T | 298 | . 2014-12-14 | | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . 2014-12-15 | | 2 | 33 | 21 | 89 | 30.15 | 42 | 10 | 15 | ... | 10 | 6 | 29 | 27 | 60 | 29.91 | 32 | 10 | 0.00 | 324 | . 2014-12-16 | Rain | 8 | 42 | 10 | 96 | 30.17 | 44 | 10 | 8 | ... | 9 | 4 | 36 | 30 | 73 | 29.92 | 35 | 5 | T | 79 | . 2014-12-17 | Rain | 8 | 46 | 26 | 100 | 29.91 | 49 | 10 | 20 | ... | 6 | 11 | 41 | 32 | 70 | 29.69 | 41 | 1 | 0.43 | 311 | . 2014-12-18 | Rain | 7 | 34 | 30 | 89 | 29.87 | 44 | 10 | 23 | ... | 10 | 14 | 30 | 26 | 57 | 29.71 | 36 | 10 | 0.01 | 281 | . 2014-12-19 | | 4 | 25 | 23 | 69 | 30.15 | 37 | 10 | 17 | ... | 10 | 11 | 22 | 20 | 56 | 29.86 | 29 | 10 | 0.00 | 305 | . 2014-12-02 | Rain-Snow | 7 | 40 | 29 | 92 | 30.71 | 42 | 10 | 24 | ... | 8 | 15 | 27 | 17 | 51 | 30.4 | 33 | 2 | 0.10 | 62 | . 2014-12-20 | Snow | 6 | 30 | 26 | 89 | 30.31 | 36 | 10 | 21 | ... | 10 | 10 | 24 | 20 | 69 | 30.17 | 27 | 7 | T | 350 | . 2014-12-21 | Snow | 8 | 30 | 20 | 85 | 30.37 | 36 | 10 | 16 | ... | 9 | 9 | 27 | 25 | 69 | 30.28 | 30 | 6 | T | 2 | . 2014-12-22 | Rain | 7 | 39 | 22 | 89 | 30.4 | 44 | 10 | 18 | ... | 10 | 8 | 34 | 25 | 69 | 30.3 | 33 | 4 | 0.05 | 24 | . 2014-12-23 | Rain | 8 | 45 | 25 | 100 | 30.31 | 47 | 10 | 20 | ... | 5 | 13 | 42 | 37 | 82 | 30.16 | 42 | 1 | 0.25 | 63 | . 2014-12-24 | Fog-Rain | 8 | 46 | 15 | 100 | 30.13 | 46 | 2 | 13 | ... | 1 | 6 | 44 | 41 | 96 | 29.55 | 41 | 0 | 0.56 | 12 | . 2014-12-25 | Rain | 6 | 58 | 40 | 100 | 29.96 | 59 | 10 | 28 | ... | 8 | 14 | 43 | 29 | 49 | 29.47 | 44 | 1 | 0.14 | 250 | . 2014-12-26 | | 1 | 31 | 25 | 70 | 30.16 | 50 | 10 | 18 | ... | 10 | 11 | 29 | 28 | 49 | 29.99 | 37 | 10 | 0.00 | 255 | . 2014-12-27 | | 3 | 34 | 21 | 70 | 30.22 | 52 | 10 | 17 | ... | 10 | 9 | 31 | 29 | 50 | 30.03 | 38 | 10 | 0.00 | 251 | . Warning message in eval(expr, envir, enclos): &#34;NAs introduced by coercion&#34; . &lt;ol class=list-inline&gt; 0.01 | 0.28 | 0.02 | &lt;NA&gt; | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0.43 | 0.01 | 0 | 0.1 | &lt;NA&gt; | &lt;NA&gt; | 0.05 | 0.25 | 0.56 | 0.14 | 0 | 0 | 0.01 | 0 | 0.44 | 0 | 0 | 0 | 0.11 | 1.09 | 0.13 | 0.03 | 2.9 | 0 | 0 | 0 | 0.2 | 0 | &lt;NA&gt; | 0.12 | 0 | 0 | 0.15 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.71 | 0 | 0.1 | 0.95 | 0.01 | &lt;NA&gt; | 0.62 | 0.06 | 0.05 | 0.57 | 0 | 0.02 | &lt;NA&gt; | 0 | 0.01 | 0 | 0.05 | 0.01 | 0.03 | 0 | 0.23 | 0.39 | 0 | 0.02 | 0.01 | 0.06 | 0.78 | 0 | 0.17 | 0.11 | 0 | &lt;NA&gt; | 0.07 | 0.02 | 0 | 0 | 0 | 0 | 0.09 | &lt;NA&gt; | 0.07 | 0.37 | 0.88 | 0.17 | 0.06 | 0.01 | 0 | 0 | 0.8 | 0.27 | 0 | 0.14 | 0 | 0 | 0.01 | 0.05 | 0.09 | 0 | 0 | 0 | 0.04 | 0.8 | 0.21 | 0.12 | 0 | 0.26 | &lt;NA&gt; | 0 | 0.02 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0.09 | 0 | 0 | 0 | 0.01 | 0 | 0 | 0.06 | 0 | 0 | 0 | 0.61 | 0.54 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0 | 0 | 0.1 | 0.07 | 0 | 0.03 | 0 | 0.39 | 0 | 0 | 0.03 | 0.26 | 0.09 | 0 | 0 | 0 | 0.02 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0.27 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0.91 | 0 | 0.02 | 0 | 0 | 0 | 0 | 0.38 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.4 | &lt;NA&gt; | 0 | 0 | 0 | 0.74 | 0.04 | 1.72 | 0 | 0.01 | 0 | 0 | &lt;NA&gt; | 0.2 | 1.43 | &lt;NA&gt; | 0 | 0 | 0 | &lt;NA&gt; | 0.09 | 0 | &lt;NA&gt; | &lt;NA&gt; | 0.5 | 1.12 | 0 | 0 | 0 | 0.03 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0.14 | &lt;NA&gt; | 0 | &lt;NA&gt; | &lt;NA&gt; | 0 | 0 | 0.01 | 0 | &lt;NA&gt; | 0.06 | 0 | 0 | 0 | 0.02 | 0 | &lt;NA&gt; | 0 | 0 | 0.02 | &lt;NA&gt; | 0.15 | &lt;NA&gt; | 0 | 0.83 | 0 | 0 | 0 | 0.08 | 0 | 0 | 0.14 | 0 | 0 | 0 | 0.63 | &lt;NA&gt; | 0.02 | &lt;NA&gt; | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0 | 0 | 0 | 0.49 | 0 | 0 | 0 | 0 | 0 | 0 | 0.17 | 0.66 | 0.01 | 0.38 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.04 | 0.01 | 2.46 | &lt;NA&gt; | 0 | 0 | 0 | 0.2 | 0 | &lt;NA&gt; | 0 | 0 | 0 | 0.12 | 0 | 0 | &lt;NA&gt; | &lt;NA&gt; | &lt;NA&gt; | 0 | 0.08 | &lt;NA&gt; | 0.07 | &lt;NA&gt; | 0 | 0 | 0.03 | 0 | 0 | 0.36 | 0.73 | 0.01 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.34 | &lt;NA&gt; | 0.07 | 0.54 | 0.04 | 0.01 | 0 | 0 | 0 | 0 | 0 | &lt;NA&gt; | 0 | 0.86 | 0 | 0.3 | 0.04 | 0 | 0 | 0 | 0 | 0.21 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.14 | &lt;/ol&gt; Column type conversions . &quot;T&quot; was used to denote a trace amount (i.e. too small to be accurately measured) of precipitation in the PrecipitationIn column. In order to coerce this column to numeric, wwe&#39;ll need to deal with this somehow. To keep things simple, we will just replace &quot;T&quot; with zero, as a string (&quot;0&quot;). . # Replace &quot;T&quot; with &quot;0&quot; (T = trace) weather5$PrecipitationIn &lt;- str_replace(weather5$PrecipitationIn, &quot;T&quot;, &quot;0&quot;) # Convert characters to numerics weather6 &lt;- mutate_at(weather5, vars(CloudCover:WindDirDegrees), funs(as.numeric)) # Look at result str(weather6) . Warning message: &#34;`funs()` is deprecated as of dplyr 0.8.0. Please use a list of either functions or lambdas: # Simple named list: list(mean = mean, median = median) # Auto named with `tibble::lst()`: tibble::lst(mean, median) # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) This warning is displayed once every 8 hours. Call `lifecycle::last_warnings()` to see where this warning was generated.&#34; . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ Events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ CloudCover : num 6 8 8 7 5 4 2 8 8 7 ... $ Max.Dew.PointF : num 46 45 37 28 28 29 33 42 46 34 ... $ Max.Gust.SpeedMPH : num 29 29 28 21 23 20 21 10 26 30 ... $ Max.Humidity : num 74 100 92 85 75 82 89 96 100 89 ... $ Max.Sea.Level.PressureIn : num 30.4 29.6 29.8 29.9 29.9 ... $ Max.TemperatureF : num 64 48 39 39 42 45 42 44 49 44 ... $ Max.VisibilityMiles : num 10 10 10 10 10 10 10 10 10 10 ... $ Max.Wind.SpeedMPH : num 22 23 21 16 17 15 15 8 20 23 ... $ Mean.Humidity : num 63 95 87 75 65 68 75 85 85 73 ... $ Mean.Sea.Level.PressureIn: num 30.1 29.5 29.6 29.9 29.8 ... $ Mean.TemperatureF : num 52 43 36 35 37 39 37 40 45 40 ... $ Mean.VisibilityMiles : num 10 3 7 10 10 10 10 9 6 10 ... $ Mean.Wind.SpeedMPH : num 13 13 13 11 12 10 6 4 11 14 ... $ MeanDew.PointF : num 40 39 31 27 26 27 29 36 41 30 ... $ Min.DewpointF : num 26 37 27 25 24 25 27 30 32 26 ... $ Min.Humidity : num 52 89 82 64 55 53 60 73 70 57 ... $ Min.Sea.Level.PressureIn : num 30 29.4 29.4 29.8 29.8 ... $ Min.TemperatureF : num 39 38 32 31 32 33 32 35 41 36 ... $ Min.VisibilityMiles : num 10 1 1 7 10 10 10 5 1 10 ... $ PrecipitationIn : num 0.01 0.28 0.02 0 0 0 0 0 0.43 0.01 ... $ WindDirDegrees : num 268 357 230 286 298 306 324 79 311 281 ... . It looks like our data are finally in the correct formats and organized in a logical manner! Now that our data are in the right form, we can begin the analysis. . Missing, extreme, and unexpected values . Find missing values . Before dealing with missing values in the data, it&#39;s important to find them and figure out why they exist in the first place. . If the dataset is too big to look at all at once, like it is here, we will use sum() and is.na() to quickly size up the situation by counting the number of NA values. . The summary() function also come in handy for identifying which variables contain the missing values. Finally, the which() function is useful for locating the missing values within a particular column. . # Count missing values sum(is.na(weather6)) # Find missing values summary(weather6) # Find indices of NAs in Max.Gust.SpeedMPH ind &lt;- which(is.na(weather6$Max.Gust.SpeedMPH)) # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind, ] . 6 date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 85.69 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :1000.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. :-1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.861 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1612015-05-18 | Fog | 6 | 52 | NA | 100 | 30.30 | 58 | 10 | 16 | ... | 8 | 10 | 48 | 43 | 57 | 30.12 | 49 | 0 | 0 | 72 | . 2052015-06-03 | | 7 | 48 | NA | 93 | 30.31 | 56 | 10 | 14 | ... | 10 | 7 | 45 | 43 | 71 | 30.19 | 47 | 10 | 0 | 90 | . 2732015-08-08 | | 4 | 61 | NA | 87 | 30.02 | 76 | 10 | 14 | ... | 10 | 6 | 57 | 54 | 49 | 29.95 | 61 | 10 | 0 | 45 | . 2752015-09-01 | | 1 | 63 | NA | 78 | 30.06 | 79 | 10 | 15 | ... | 10 | 9 | 62 | 59 | 52 | 29.96 | 69 | 10 | 0 | 54 | . 3082015-10-12 | | 0 | 56 | NA | 89 | 29.86 | 76 | 10 | 15 | ... | 10 | 8 | 51 | 48 | 41 | 29.74 | 51 | 10 | 0 | 199 | . 3582015-11-03 | | 1 | 44 | NA | 82 | 30.25 | 73 | 10 | 16 | ... | 10 | 8 | 42 | 40 | 31 | 30.06 | 47 | 10 | 0 | 281 | . In this situation it&#39;s unclear why these values are missing and there doesn&#39;t appear to be any obvious pattern to their missingness, so we&#39;ll leave them alone for now. . An obvious error . Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with summary(). . Once implausible values are identified, they must be dealt with in an intelligent and informed way. . Sometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data. . # Review distributions for all variables summary(weather6) # Find row with Max.Humidity of 1000 ind &lt;- which(weather6$Max.Humidity==1000) # Look at the data for that day weather6[ind, ] # Change 1000 to 100 weather6$Max.Humidity[ind] &lt;- 100 . date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 85.69 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :1000.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. :-1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.861 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1352015-04-21 | Fog-Rain-Thunderstorm | 6 | 57 | 94 | 1000 | 29.75 | 65 | 10 | 20 | ... | 5 | 10 | 49 | 36 | 42 | 29.53 | 46 | 0 | 0.54 | 184 | . Once you find obvious errors, it&#39;s not too hard to fix them if you know which values they should take. . Another obvious error . We&#39;ve discovered and repaired one obvious error in the data, but it appears that there&#39;s another. Sometimes we get lucky and can infer the correct or intended value from the other data. For example, if you know the minimum and maximum values of a particular metric on a given day... . # Look at summary of Mean.VisibilityMiles summary(weather6$Mean.VisibilityMiles) # Get index of row with -1 value ind &lt;- which(weather6$Mean.VisibilityMiles == -1) # Look at full row weather6[ind,] # Set Mean.VisibilityMiles to the appropriate value weather6$Mean.VisibilityMiles[ind] &lt;- 10 . Min. 1st Qu. Median Mean 3rd Qu. Max. -1.000 8.000 10.000 8.861 10.000 10.000 . dateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees . 1922015-06-18 | | 5 | 54 | 23 | 72 | 30.14 | 76 | 10 | 17 | ... | -1 | 10 | 49 | 45 | 46 | 29.93 | 57 | 10 | 0 | 189 | . Our data are looking tidy. Just a quick sanity check left! . Check other extreme values . In addition to dealing with obvious errors in the data, we want to see if there are other extreme values. In addition to the trusty summary() function, hist() is useful for quickly getting a feel for how different variables are distributed. . # Review summary of full data once more summary(weather6) # Look at histogram for MeanDew.PointF hist(weather6$MeanDew.PointF) # Look at histogram for Min.TemperatureF hist(weather6$Min.TemperatureF) # Compare to histogram for Mean.TemperatureF hist(weather6$Mean.TemperatureF) . date Events CloudCover Max.Dew.PointF Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 Max.Gust.SpeedMPH Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Min. : 0.00 Min. : 39.00 Min. :29.58 Min. :18.00 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 1st Qu.:42.00 Median :25.50 Median : 86.00 Median :30.14 Median :60.00 Mean :26.99 Mean : 83.23 Mean :30.16 Mean :58.93 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 3rd Qu.:76.00 Max. :94.00 Max. :100.00 Max. :30.88 Max. :96.00 NA&#39;s :6 Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :10.000 Median :20.00 Median :66.00 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :10.000 Max. :38.00 Max. :98.00 Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles Min. :29.49 Min. : 8.00 Min. : 1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.891 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF Min.Humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn Min. :29.16 Min. :-3.00 Min. : 0.000 Min. :0.0000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:0.0000 Median :29.94 Median :46.00 Median :10.000 Median :0.0000 Mean :29.93 Mean :43.33 Mean : 6.716 Mean :0.1016 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 3rd Qu.:0.0400 Max. :30.64 Max. :74.00 Max. :10.000 Max. :2.9000 WindDirDegrees Min. : 1.0 1st Qu.:113.0 Median :222.0 Mean :200.1 3rd Qu.:275.0 Max. :360.0 . It looks like you have sufficiently tidied your data! . Finishing touches . Before officially calling our weather data clean, we want to put a couple of finishing touches on the data. These are a bit more subjective and may not be necessary for analysis, but they will make the data easier for others to interpret, which is generally a good thing. . There are a number of stylistic conventions in the R language. Depending on who you ask, these conventions may vary. Because the period (.) has special meaning in certain situations, we will be using underscores (_) to separate words in variable names. We also prefer all lowercase letters so that no one has to remember which letters are uppercase or lowercase. . Finally, the events column (renamed to be all lowercase in the first instruction) contains an empty string (&quot;&quot;) for any day on which there was no significant weather event such as rain, fog, a thunderstorm, etc. However, if it&#39;s the first time you&#39;re seeing these data, it may not be obvious that this is the case, so it&#39;s best for us to be explicit and replace the empty strings with something more meaningful. . new_colnames = c(&quot;date&quot;, &quot;events&quot;, &quot;cloud_cover&quot;, &quot;max_dew_point_f&quot;, &quot;max_gust_speed_mph&quot;, &quot;max_humidity&quot;, &quot;max_sea_level_pressure_in&quot;, &quot;max_temperature_f&quot;, &quot;max_visibility_miles&quot;, &quot;max_wind_speed_mph&quot;, &quot;mean_humidity&quot;, &quot;mean_sea_level_pressure_in&quot;, &quot;mean_temperature_f&quot;, &quot;mean_visibility_miles&quot;, &quot;mean_wind_speed_mph&quot;, &quot;mean_dew_point_f&quot;, &quot;min_dew_point_f&quot;, &quot;min_humidity&quot;, &quot;min_sea_level_pressure_in&quot;, &quot;min_temperature_f&quot;, &quot;min_visibility_miles&quot;, &quot;precipitation_in&quot;,&quot;wind_dir_degrees&quot;) . # Clean up column names names(weather6) &lt;- new_colnames # Replace empty cells in events column weather6$events[weather6$events == &quot;&quot;] &lt;- &quot;None&quot; # Print the first 6 rows of weather6 head(weather6) . dateeventscloud_covermax_dew_point_fmax_gust_speed_mphmax_humiditymax_sea_level_pressure_inmax_temperature_fmax_visibility_milesmax_wind_speed_mph...mean_visibility_milesmean_wind_speed_mphmean_dew_point_fmin_dew_point_fmin_humiditymin_sea_level_pressure_inmin_temperature_fmin_visibility_milesprecipitation_inwind_dir_degrees . 2014-12-01 | Rain | 6 | 46 | 29 | 74 | 30.45 | 64 | 10 | 22 | ... | 10 | 13 | 40 | 26 | 52 | 30.01 | 39 | 10 | 0.01 | 268 | . 2014-12-10 | Rain | 8 | 45 | 29 | 100 | 29.58 | 48 | 10 | 23 | ... | 3 | 13 | 39 | 37 | 89 | 29.43 | 38 | 1 | 0.28 | 357 | . 2014-12-11 | Rain-Snow | 8 | 37 | 28 | 92 | 29.81 | 39 | 10 | 21 | ... | 7 | 13 | 31 | 27 | 82 | 29.44 | 32 | 1 | 0.02 | 230 | . 2014-12-12 | Snow | 7 | 28 | 21 | 85 | 29.88 | 39 | 10 | 16 | ... | 10 | 11 | 27 | 25 | 64 | 29.81 | 31 | 7 | 0.00 | 286 | . 2014-12-13 | None | 5 | 28 | 23 | 75 | 29.86 | 42 | 10 | 17 | ... | 10 | 12 | 26 | 24 | 55 | 29.78 | 32 | 10 | 0.00 | 298 | . 2014-12-14 | None | 4 | 29 | 20 | 82 | 29.91 | 45 | 10 | 15 | ... | 10 | 10 | 27 | 25 | 53 | 29.78 | 33 | 10 | 0.00 | 306 | . tail(weather6) . dateeventscloud_covermax_dew_point_fmax_gust_speed_mphmax_humiditymax_sea_level_pressure_inmax_temperature_fmax_visibility_milesmax_wind_speed_mph...mean_visibility_milesmean_wind_speed_mphmean_dew_point_fmin_dew_point_fmin_humiditymin_sea_level_pressure_inmin_temperature_fmin_visibility_milesprecipitation_inwind_dir_degrees . 3612015-11-05 | None | 4 | 61 | 31 | 100 | 30.30 | 76 | 10 | 22 | ... | 9 | 12 | 55 | 48 | 53 | 30.09 | 50 | 5 | 0.00 | 224 | . 3622015-11-06 | None | 4 | 62 | 32 | 93 | 30.07 | 73 | 10 | 26 | ... | 10 | 15 | 61 | 54 | 64 | 29.71 | 62 | 10 | 0.00 | 222 | . 3632015-11-07 | None | 6 | 45 | 33 | 57 | 30.02 | 69 | 10 | 25 | ... | 10 | 13 | 38 | 33 | 39 | 29.83 | 50 | 10 | 0.00 | 280 | . 3642015-11-08 | None | 0 | 34 | 25 | 65 | 30.38 | 56 | 10 | 18 | ... | 10 | 12 | 30 | 24 | 30 | 30.04 | 44 | 10 | 0.00 | 283 | . 3652015-11-09 | None | 2 | 36 | 20 | 70 | 30.43 | 60 | 10 | 16 | ... | 10 | 9 | 32 | 30 | 33 | 30.32 | 41 | 10 | 0.00 | 237 | . 3662015-12-01 | Rain | 7 | 43 | 17 | 96 | 30.40 | 45 | 10 | 15 | ... | 8 | 6 | 35 | 25 | 69 | 30.01 | 32 | 1 | 0.14 | 109 | . str(weather6) . &#39;data.frame&#39;: 366 obs. of 23 variables: $ date : Date, format: &#34;2014-12-01&#34; &#34;2014-12-10&#34; ... $ events : chr &#34;Rain&#34; &#34;Rain&#34; &#34;Rain-Snow&#34; &#34;Snow&#34; ... $ cloud_cover : num 6 8 8 7 5 4 2 8 8 7 ... $ max_dew_point_f : num 46 45 37 28 28 29 33 42 46 34 ... $ max_gust_speed_mph : num 29 29 28 21 23 20 21 10 26 30 ... $ max_humidity : num 74 100 92 85 75 82 89 96 100 89 ... $ max_sea_level_pressure_in : num 30.4 29.6 29.8 29.9 29.9 ... $ max_temperature_f : num 64 48 39 39 42 45 42 44 49 44 ... $ max_visibility_miles : num 10 10 10 10 10 10 10 10 10 10 ... $ max_wind_speed_mph : num 22 23 21 16 17 15 15 8 20 23 ... $ mean_humidity : num 63 95 87 75 65 68 75 85 85 73 ... $ mean_sea_level_pressure_in: num 30.1 29.5 29.6 29.9 29.8 ... $ mean_temperature_f : num 52 43 36 35 37 39 37 40 45 40 ... $ mean_visibility_miles : num 10 3 7 10 10 10 10 9 6 10 ... $ mean_wind_speed_mph : num 13 13 13 11 12 10 6 4 11 14 ... $ mean_dew_point_f : num 40 39 31 27 26 27 29 36 41 30 ... $ min_dew_point_f : num 26 37 27 25 24 25 27 30 32 26 ... $ min_humidity : num 52 89 82 64 55 53 60 73 70 57 ... $ min_sea_level_pressure_in : num 30 29.4 29.4 29.8 29.8 ... $ min_temperature_f : num 39 38 32 31 32 33 32 35 41 36 ... $ min_visibility_miles : num 10 1 1 7 10 10 10 5 1 10 ... $ precipitation_in : num 0.01 0.28 0.02 0 0 0 0 0 0.43 0.01 ... $ wind_dir_degrees : num 268 357 230 286 298 306 324 79 311 281 ... . glimpse(weather6) . Rows: 366 Columns: 23 $ date &lt;date&gt; 2014-12-01, 2014-12-10, 2014-12-11, 201... $ events &lt;chr&gt; &#34;Rain&#34;, &#34;Rain&#34;, &#34;Rain-Snow&#34;, &#34;Snow&#34;, &#34;No... $ cloud_cover &lt;dbl&gt; 6, 8, 8, 7, 5, 4, 2, 8, 8, 7, 4, 7, 6, 8... $ max_dew_point_f &lt;dbl&gt; 46, 45, 37, 28, 28, 29, 33, 42, 46, 34, ... $ max_gust_speed_mph &lt;dbl&gt; 29, 29, 28, 21, 23, 20, 21, 10, 26, 30, ... $ max_humidity &lt;dbl&gt; 74, 100, 92, 85, 75, 82, 89, 96, 100, 89... $ max_sea_level_pressure_in &lt;dbl&gt; 30.45, 29.58, 29.81, 29.88, 29.86, 29.91... $ max_temperature_f &lt;dbl&gt; 64, 48, 39, 39, 42, 45, 42, 44, 49, 44, ... $ max_visibility_miles &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, ... $ max_wind_speed_mph &lt;dbl&gt; 22, 23, 21, 16, 17, 15, 15, 8, 20, 23, 1... $ mean_humidity &lt;dbl&gt; 63, 95, 87, 75, 65, 68, 75, 85, 85, 73, ... $ mean_sea_level_pressure_in &lt;dbl&gt; 30.13, 29.50, 29.61, 29.85, 29.82, 29.83... $ mean_temperature_f &lt;dbl&gt; 52, 43, 36, 35, 37, 39, 37, 40, 45, 40, ... $ mean_visibility_miles &lt;dbl&gt; 10, 3, 7, 10, 10, 10, 10, 9, 6, 10, 10, ... $ mean_wind_speed_mph &lt;dbl&gt; 13, 13, 13, 11, 12, 10, 6, 4, 11, 14, 11... $ mean_dew_point_f &lt;dbl&gt; 40, 39, 31, 27, 26, 27, 29, 36, 41, 30, ... $ min_dew_point_f &lt;dbl&gt; 26, 37, 27, 25, 24, 25, 27, 30, 32, 26, ... $ min_humidity &lt;dbl&gt; 52, 89, 82, 64, 55, 53, 60, 73, 70, 57, ... $ min_sea_level_pressure_in &lt;dbl&gt; 30.01, 29.43, 29.44, 29.81, 29.78, 29.78... $ min_temperature_f &lt;dbl&gt; 39, 38, 32, 31, 32, 33, 32, 35, 41, 36, ... $ min_visibility_miles &lt;dbl&gt; 10, 1, 1, 7, 10, 10, 10, 5, 1, 10, 10, 2... $ precipitation_in &lt;dbl&gt; 0.01, 0.28, 0.02, 0.00, 0.00, 0.00, 0.00... $ wind_dir_degrees &lt;dbl&gt; 268, 357, 230, 286, 298, 306, 324, 79, 3... . summary(weather6) . date events cloud_cover max_dew_point_f Min. :2014-12-01 Length:366 Min. :0.000 Min. :-6.00 1st Qu.:2015-03-02 Class :character 1st Qu.:3.000 1st Qu.:32.00 Median :2015-06-01 Mode :character Median :5.000 Median :47.50 Mean :2015-06-01 Mean :4.708 Mean :45.48 3rd Qu.:2015-08-31 3rd Qu.:7.000 3rd Qu.:61.00 Max. :2015-12-01 Max. :8.000 Max. :75.00 max_gust_speed_mph max_humidity max_sea_level_pressure_in Min. : 0.00 Min. : 39.00 Min. :29.58 1st Qu.:21.00 1st Qu.: 73.25 1st Qu.:30.00 Median :25.50 Median : 86.00 Median :30.14 Mean :26.99 Mean : 83.23 Mean :30.16 3rd Qu.:31.25 3rd Qu.: 93.00 3rd Qu.:30.31 Max. :94.00 Max. :100.00 Max. :30.88 NA&#39;s :6 max_temperature_f max_visibility_miles max_wind_speed_mph mean_humidity Min. :18.00 Min. : 2.000 Min. : 8.00 Min. :28.00 1st Qu.:42.00 1st Qu.:10.000 1st Qu.:16.00 1st Qu.:56.00 Median :60.00 Median :10.000 Median :20.00 Median :66.00 Mean :58.93 Mean : 9.907 Mean :20.62 Mean :66.02 3rd Qu.:76.00 3rd Qu.:10.000 3rd Qu.:24.00 3rd Qu.:76.75 Max. :96.00 Max. :10.000 Max. :38.00 Max. :98.00 mean_sea_level_pressure_in mean_temperature_f mean_visibility_miles Min. :29.49 Min. : 8.00 Min. : 1.000 1st Qu.:29.87 1st Qu.:36.25 1st Qu.: 8.000 Median :30.03 Median :53.50 Median :10.000 Mean :30.04 Mean :51.40 Mean : 8.891 3rd Qu.:30.19 3rd Qu.:68.00 3rd Qu.:10.000 Max. :30.77 Max. :84.00 Max. :10.000 mean_wind_speed_mph mean_dew_point_f min_dew_point_f min_humidity Min. : 4.00 Min. :-11.00 Min. :-18.00 Min. :16.00 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 1st Qu.:35.00 Median :10.00 Median : 41.00 Median : 35.00 Median :46.00 Mean :10.68 Mean : 38.96 Mean : 32.25 Mean :48.31 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 3rd Qu.:60.00 Max. :22.00 Max. : 71.00 Max. : 68.00 Max. :96.00 min_sea_level_pressure_in min_temperature_f min_visibility_miles Min. :29.16 Min. :-3.00 Min. : 0.000 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 Median :29.94 Median :46.00 Median :10.000 Mean :29.93 Mean :43.33 Mean : 6.716 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 Max. :30.64 Max. :74.00 Max. :10.000 precipitation_in wind_dir_degrees Min. :0.0000 Min. : 1.0 1st Qu.:0.0000 1st Qu.:113.0 Median :0.0000 Median :222.0 Mean :0.1016 Mean :200.1 3rd Qu.:0.0400 3rd Qu.:275.0 Max. :2.9000 Max. :360.0 .",
            "url": "https://victoromondi1997.github.io/blog/2020/06/15/Exploring-Boston-Weather-Data.html",
            "relUrl": "/2020/06/15/Exploring-Boston-Weather-Data.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://victoromondi1997.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://victoromondi1997.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! Thanks for visiting my blog, I’m Victor Omondi, Data Scientist with industry experience analyzing, visualizing and modeling data. I specialize in Python &amp; R and have professional experience working with C++ and JavaScript | , currently am a student at Multimedia University of Kenya pursuing Bachelor of Science (Computer Technology) | . Twitter . There was a &#39;Not Found&#39; error fetching URL: &#39;https://twitter.com/VictorOmondi1997&#39; . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://victoromondi1997.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://victoromondi1997.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}