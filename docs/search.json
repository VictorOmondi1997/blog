[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nHyperparameter Tuning with Python\n\n\n\n\n\n\n\nmachine-learning\n\n\nhyperparameter-tuning\n\n\n\n\nperform hyperparameter tuning techniques to your most accurate model in an effort to achieve optimal predictions.\n\n\n\n\n\n\nSep 21, 2021\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nData Wrangling in Python\n\n\n\n\n\n\n\ndata-wrangling\n\n\npython\n\n\ncit-club\n\n\n\n\nCIT Club event blog & Notebook. The aim of this blog/notebook was to introduce CIT Club members in data wrangling.\n\n\n\n\n\n\nJun 11, 2021\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nA Quick Tour of Variables and Data Types in Python\n\n\n\n\n\n\n\npython\n\n\ndata-types\n\n\n\n\nThis tutorial is the second in a series on introduction to programming using the Python language. These tutorials take a practical coding-based approach, and the best way to learn the material is to execute the code and experiment with the examples.\n\n\n\n\n\n\nApr 30, 2021\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nWriting Reusable Code using Functions in Python\n\n\n\n\n\n\n\npython\n\n\n\n\nLearn how to create and use functions in python\n\n\n\n\n\n\nApr 30, 2021\n\n\nVictor Omondi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAppliances Energy Prediction data\n\n\n\n\n\n\n\nenergy\n\n\nmachine-learning\n\n\n\n\n\nauthor: Victor Omondi\n\n\n\n\n\n\n\nOct 19, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the Gender Gap in College Degrees\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction and Exproling Raw data\n\n\n\n\n\nNo one likes missing data, but it is dangerous to assume that it can simply be removed or replaced. Sometimes missing data tells us something important about whatever it is that we’re measuring (i.e. the value of the variable that is missing may be related to - the reason it is missing). Such data is called Missing not at Random, or MNAR.\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeight\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvocado\n\n\n\n\n\nBar plots are great for revealing relationships between categorical (size) and numeric (number sold) variables, but manipulation on the data first is needed in order to get the numbers needed for plotting.\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nCluster Analysis in R\n\n\n\n\n\n\n\nmachine-learning\n\n\ncluster-analysis\n\n\nr\n\n\n\n\nbuild a strong intuition for how they work and how to interpret hierarchical clustering and k-means clustering results\n\n\n\n\n\n\nOct 19, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nCluster Analysis in Python\n\n\n\n\n\n\n\ncluster-analysis\n\n\nunsupervised-learning\n\n\n\n\nexploring unsupervised learning through clustering using the SciPy library in Python\n\n\n\n\n\n\nOct 19, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Police Activity with Pandas\n\n\n\n\n\nExploring the Stanford Open Policing Project dataset and analyzing the impact of gender on police behavior.\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing Hacker News Dataset\n\n\n\n\n\n\n\ndata-analysis\n\n\nhacker-news\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nWeather Visualization\n\n\n\n\n\n\n\ndata-visualization\n\n\nweather\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing Women in USA Degrees\n\n\n\n\n\n\n\ndata-visualization\n\n\nwomen\n\n\n\n\nExplore how we can communicate the nuanced narrative of gender gap using effective data visualization. Author:\n\n\n\n\n\n\nAug 25, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Missing Data\n\n\n\n\n\n\n\nmissing-data\n\n\ndata-analysis\n\n\n\n\nwe’ll handle missing data without having to drop rows and columns using data on motor vehicle collisions released by New York City and published on the NYC OpenData website. There is data on over 1.5 million collisions dating back to 2012, with additional data continuously added. We’ll work with an extract of the full data: Crashes from the year 2018.\n\n\n\n\n\n\nAug 25, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nWorking With Dates and Times in Python\n\n\n\n\n\n\n\ndates\n\n\ntimes\n\n\npython\n\n\n\n\nHow to work with Dates and Times in python\n\n\n\n\n\n\nJul 26, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Dates and Times in R\n\n\n\n\n\n\n\ndates\n\n\ntimes\n\n\nr\n\n\n\n\nthe essentials of parsing, manipulating, and computing with dates and times in R. By VICTOR OMONDI\n\n\n\n\n\n\nJul 26, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nWriting Functions in Python\n\n\n\n\n\n\n\nfunctions\n\n\npython\n\n\n\n\nuseful tricks, like how to write context managers and decorators\n\n\n\n\n\n\nJul 26, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nUnsupervised Learning in Python\n\n\n\n\n\n\n\nmachine-learning\n\n\nunsupervised-learning\n\n\n\n\nUnsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension reduction to matrix factorization. In this blog, we’ll explore the fundamentals of unsupervised learning and implement the essential algorithms using scikit-learn and scipy.\n\n\n\n\n\n\nJul 14, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nSupervised Learning with scikit-learn\n\n\n\n\n\n\n\nmachine-learning\n\n\nsupervised-learning\n\n\nscikit-learn\n\n\n\n\nSupervised learning, an essential component of machine learning. We’ll build predictive models, tune their parameters, and determine how well they will perform with unseen data—all while using real world datasets. We’ll be learning how to use scikit-learn, one of the most popular and user-friendly machine learning libraries for Python.\n\n\n\n\n\n\nJul 10, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Thinking in Python (Part 2)\n\n\n\n\n\n\n\nstatistical-thinking\n\n\nhypothesis-testing\n\n\ndata-science\n\n\n\n\nExpanding and honing hacker stats toolbox to perform the two key tasks in statistical inference, parameter estimation and hypothesis testing.\n\n\n\n\n\n\nJul 8, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Thinking in Python (Part 1)\n\n\n\n\n\n\n\nstatistical-thinking\n\n\neda\n\n\ndata-science\n\n\n\n\nBuilding the foundation you need to think statistically, speak the language of your data, and understand what your data is telling you.\n\n\n\n\n\n\nJul 3, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n  \n\n\n\n\nDr. Semmelweis and the Discovery of Handwashing\n\n\n\n\n\n\n\nignaz\n\n\ndata-analysis\n\n\nhandwashing\n\n\n\n\nReanalyse the data behind one of the most important discoveries of modern medicine: handwashing.\n\n\n\n\n\n\nJul 1, 2020\n\n\nVictor Omondi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Police Activity with Pandas\n\n\n\n\n\nExploring the Stanford Open Policing Project dataset and analyzing the impact of gender on police behavior.\n\n\n\n\n\n\nJun 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Boston Weather Data\n\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\nFastpages Notebook Blog Post\n\n\n\n\n\n\n\njupyter\n\n\n\n\nA tutorial of fastpages for Jupyter notebooks.\n\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "",
    "text": "Before diving into sophisticated statistical inference techniques, we should first explore our data by plotting them and computing simple summary statistics. This process, called exploratory data analysis, is a crucial first step in statistical analysis of data.\n\n\nExploratory Data Analysis is the process of organizing, plo!ing, and summarizing a data set\n\n“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone.” > ~ John Tukey\n\n\n\n\nExploratory data analysis is detective work.\nThere is no excuse for failing to plot and look.\nThe greatest value of a picture is that it forces us to notice what we never expected to see.\nIt is important to understand what you can do before you learn how to measure how well you seem to have done it.\n\n\nIf you don’t have time to do EDA, you really don’t have time to do hypothesis tests. And you should always do EDA first.\n\n\n\n\n\nIt often involves converting tabular data into graphical form.\nIf done well, graphical representations can allow for more rapid interpretation of data.\nThere is no excuse for neglecting to do graphical EDA.\n\n\nWhile a good, informative plot can sometimes be the end point of an analysis, it is more like a beginning: it helps guide you in the quantitative statistical analyses that come next.\n\n\n\n\n\n\n\nWe will use a classic data set collected by botanist Edward Anderson and made famous by Ronald Fisher, one of the most prolific statisticians in history. Anderson carefully measured the anatomical properties of samples of three different species of iris, Iris setosa, Iris versicolor, and Iris virginica. The full data set is available as part of scikit-learn. Here, you will work with his measurements of petal length.\nWe will plot a histogram of the petal lengths of his 50 samples of Iris versicolor using matplotlib/seaborn’s default settings.\nThe subset of the data set containing the Iris versicolor petal lengths in units of centimeters (cm) is stored in the NumPy array versicolor_petal_length."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#plot-all-data-bee-swarm-plots",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#plot-all-data-bee-swarm-plots",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Plot all data: Bee swarm plots",
    "text": "Plot all data: Bee swarm plots\n\nBee swarm plot\nWe will make a bee swarm plot of the iris petal lengths. The x-axis will contain each of the three species, and the y-axis the petal lengths.\n\niris_petal_lengths = pd.read_csv(\"../datasets/iris_petal_lengths.csv\")\niris_petal_lengths.head()\n\n\n\n\n\n  \n    \n      \n      sepal length (cm)\n      sepal width (cm)\n      petal length (cm)\n      petal width (cm)\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\niris_petal_lengths.shape\n\n(150, 5)\n\n\n\niris_petal_lengths.tail()\n\n\n\n\n\n  \n    \n      \n      sepal length (cm)\n      sepal width (cm)\n      petal length (cm)\n      petal width (cm)\n      species\n    \n  \n  \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n\n\n\n\n# Create bee swarm plot with Seaborn's default settings\n_ = sns.swarmplot(data=iris_petal_lengths, x=\"species\", y=\"petal length (cm)\")\n\n# Label the axes\n_ = plt.xlabel(\"species\")\n_ = plt.ylabel(\"petal length (cm)\")\n# Show the plot\n\nplt.show()\n\n\n\n\n\n\nInterpreting a bee swarm plot\n\nI. virginica petals tend to be the longest, and I. setosa petals tend to be the shortest of the three species.\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that we said “tend to be.” Some individual I. virginica flowers may be shorter than individual I. versicolor flowers. It is also possible that an individual I. setosa flower may have longer petals than in individual I. versicolor flower, though this is highly unlikely, and was not observed by Anderson."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#plot-all-data-ecdfs",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#plot-all-data-ecdfs",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Plot all data: ECDFs",
    "text": "Plot all data: ECDFs\n\n\n\n\n\n\nNote\n\n\n\nEmpirical cumulative distribution function (ECDF)\n\n\n\nComputing the ECDF\nWe will write a function that takes as input a 1D array of data and then returns the x and y values of the ECDF.\n\n\n\n\n\n\nImportant\n\n\n\nECDFs are among the most important plots in statistical analysis.\n\n\n\ndef ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) / n\n\n    return x, y\n\n\n\nPlotting the ECDF\nWe will now use ecdf() function to compute the ECDF for the petal lengths of Anderson’s Iris versicolor flowers. We will then plot the ECDF.\n\n\n\n\n\n\nWarning\n\n\n\necdf() function returns two arrays so we will need to unpack them. An example of such unpacking is x, y = foo(data), for some function foo().\n\n\n\n# Compute ECDF for versicolor data: x_vers, y_vers\nx_vers, y_vers = ecdf(versicolor_petal_length)\n\n# Generate plot\n_ = plt.plot(x_vers, y_vers, marker=\".\", linestyle=\"none\")\n\n# Label the axes\n_ = plt.xlabel(\"versicolor petal length, (cm)\")\n_ = plt.ylabel(\"ECDF\")\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\nComparison of ECDFs\nECDFs also allow us to compare two or more distributions (though plots get cluttered if you have too many). Here, we will plot ECDFs for the petal lengths of all three iris species.\n\n\n\n\n\n\nImportant\n\n\n\nwe already wrote a function to generate ECDFs so we can put it to good use!\n\n\n\nsetosa_petal_length = iris_petal_lengths[\"petal length (cm)\"][iris_petal_lengths.species == \"setosa\"]\nversicolor_petal_length = iris_petal_lengths[\"petal length (cm)\"][iris_petal_lengths.species == \"versicolor\"]\nvirginica_petal_length = iris_petal_lengths[\"petal length (cm)\"][iris_petal_lengths.species == \"virginica\"]\nsetosa_petal_length.head()\n\n0    1.4\n1    1.4\n2    1.3\n3    1.5\n4    1.4\nName: petal length (cm), dtype: float64\n\n\n\n# Compute ECDFs\nx_set, y_set = ecdf(setosa_petal_length)\nx_vers, y_vers = ecdf(versicolor_petal_length)\nx_virg, y_virg = ecdf(virginica_petal_length)\n\n\n# Plot all ECDFs on the same plot\n_ = plt.plot(x_set, y_set, marker=\".\", linestyle=\"none\")\n_ = plt.plot(x_vers, y_vers, marker=\".\", linestyle=\"none\")\n_ = plt.plot(x_virg, y_virg, marker=\".\", linestyle=\"none\")\n\n# Annotate the plot\nplt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')\n_ = plt.xlabel('petal length (cm)')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe ECDFs expose clear differences among the species. Setosa is much shorter, also with less absolute variability in petal length than versicolor and virginica."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#onward-toward-the-whole-story",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#onward-toward-the-whole-story",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Onward toward the whole story!",
    "text": "Onward toward the whole story!\n\n\n\n\n\n\nImportant\n\n\n\n“Exploratory data analysis can never be the\n\n\nwhole story, but nothing else can serve as the foundation stone.” —John Tukey"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#introduction-to-summary-statistics-the-sample-mean-and-median",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#introduction-to-summary-statistics-the-sample-mean-and-median",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Introduction to summary statistics: The sample mean and median",
    "text": "Introduction to summary statistics: The sample mean and median\n\\[\nmean = \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n\nOutliers\n● Data points whose value is far greater or less than most of the rest of the data\n\n\nThe median\n● The middle value of a data set\n\n\n\n\n\n\n\nNote\n\n\n\nAn outlier can significantly affect the value of the mean, but not the median\n\n\n\nComputing means\nThe mean of all measurements gives an indication of the typical magnitude of a measurement. It is computed using np.mean().\n\n# Compute the mean: mean_length_vers\nmean_length_vers = np.mean(versicolor_petal_length)\n\n# Print the result with some nice formatting\nprint('I. versicolor:', mean_length_vers, 'cm')\n\nI. versicolor: 4.26 cm"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#percentiles-outliers-and-box-plots",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#percentiles-outliers-and-box-plots",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Percentiles, outliers, and box plots",
    "text": "Percentiles, outliers, and box plots\n\nComputing percentiles\nWe will compute the percentiles of petal length of Iris versicolor.\n\n# Specify array of percentiles: percentiles\npercentiles = np.array([2.5, 25, 50, 75, 97.5])\n\n# Compute percentiles: ptiles_vers\nptiles_vers = np.percentile(versicolor_petal_length, percentiles)\n\n# Print the result\nptiles_vers\n\narray([3.3   , 4.    , 4.35  , 4.6   , 4.9775])\n\n\n\n\nComparing percentiles to ECDF\nTo see how the percentiles relate to the ECDF, we will plot the percentiles of Iris versicolor petal lengths on the ECDF plot.\n\n# Plot the ECDF\n_ = plt.plot(x_vers, y_vers, '.')\n_ = plt.xlabel('petal length (cm)')\n_ = plt.ylabel('ECDF')\n\n# Overlay percentiles as red diamonds.\n_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',\n         linestyle=\"none\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nBox-and-whisker plot\n\n\n\n\n\n\nWarning\n\n\n\nMaking a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine.\n\n\nWe will Make a box plot of the iris petal lengths.\n\n# Create box plot with Seaborn's default settings\n_ = sns.boxplot(data=iris_petal_lengths, x=\"species\", y=\"petal length (cm)\")\n\n# Label the axes\n_ = plt.xlabel(\"species\")\n_ = plt.ylabel(\"petal length (cm)\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#variance-and-standard-deviation",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#variance-and-standard-deviation",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\n\nVariance\n● The mean squared distance of the data from their mean\n\n\n\n\n\n\n\nTip\n\n\n\nVariance; nformally, a measure of the spread of data\n\n\n\n\\[\nvariance = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\n\nstandard Deviation\n\\[\nstd = \\sqrt {\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\n\n\nComputing the variance\nwe will explicitly compute the variance of the petal length of Iris veriscolor, we will then use np.var() to compute it.\n\n# Array of differences to mean: differences\ndifferences = versicolor_petal_length-np.mean(versicolor_petal_length)\n\n# Square the differences: diff_sq\ndiff_sq = differences**2\n\n# Compute the mean square difference: variance_explicit\nvariance_explicit = np.mean(diff_sq)\n\n# Compute the variance using NumPy: variance_np\nvariance_np = np.var(versicolor_petal_length)\n\n# Print the results\nprint(variance_explicit, variance_np)\n\n0.21640000000000004 0.21640000000000004\n\n\n\n\nThe standard deviation and the variance\nthe standard deviation is the square root of the variance.\n\n# Compute the variance: variance\nvariance = np.var(versicolor_petal_length)\n\n# Print the square root of the variance\nprint(np.sqrt(variance))\n\n# Print the standard deviation\nprint(np.std(versicolor_petal_length))\n\n0.4651881339845203\n0.4651881339845203"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#covariance-and-the-pearson-correlation-coefficient",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#covariance-and-the-pearson-correlation-coefficient",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Covariance and the Pearson correlation coefficient",
    "text": "Covariance and the Pearson correlation coefficient\n\nCovariance\n● A measure of how two quantities vary together \\[\ncovariance = \\frac{1}{n} \\sum_{i=1}^{n} (x_i\\ \\bar{x})\\ (y_i \\ - \\bar{y})\n\\]\n\n\nPearson correlation coefficient\n\\[\n\\rho = Pearson\\ correlation = \\frac{covariance}{(std\\ of\\ x)\\ (std\\ of\\ y)} = \\frac{variability\\ due\\ to\\ codependence}{independent variability}\n\\]\n\n\nScatter plots\nWhen we made bee swarm plots, box plots, and ECDF plots in previous exercises, we compared the petal lengths of different species of iris. But what if we want to compare two properties of a single species? This is exactly what we will do, we will make a scatter plot of the petal length and width measurements of Anderson’s Iris versicolor flowers.\n\n\n\n\n\n\nImportant\n\n\n\nIf the flower scales (that is, it preserves its proportion as it grows), we would expect the length and width to be correlated.\n\n\n\nversicolor_petal_width = np.array([1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1. , 1.3, 1.4, 1. , 1.5, 1. ,\n       1.4, 1.3, 1.4, 1.5, 1. , 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4,\n       1.4, 1.7, 1.5, 1. , 1.1, 1. , 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3,\n       1.3, 1.2, 1.4, 1.2, 1. , 1.3, 1.2, 1.3, 1.3, 1.1, 1.3])\n\n\n# Make a scatter plot\n_ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker=\".\", linestyle=\"none\")\n\n\n# Label the axes\n_ = plt.xlabel(\"petal length, (cm)\")\n_ = plt.ylabel(\"petal length, (cm)\")\n\n# Show the result\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nwe see some correlation. Longer petals also tend to be wider.\n\n\n\n\nComputing the covariance\nThe covariance may be computed using the Numpy function np.cov(). For example, we have two sets of data \\(x\\) and \\(y\\), np.cov(x, y) returns a 2D array where entries [0,1] and [1,0] are the covariances. Entry [0,0] is the variance of the data in x, and entry [1,1] is the variance of the data in y. This 2D output array is called the covariance matrix, since it organizes the self- and covariance.\n\n# Compute the covariance matrix: covariance_matrix\ncovariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)\n\n# Print covariance matrix\nprint(covariance_matrix)\n\n# Extract covariance of length and width of petals: petal_cov\npetal_cov = covariance_matrix[0,1]\n\n# Print the length/width covariance\n\nprint(petal_cov)\n\n[[0.22081633 0.07310204]\n [0.07310204 0.03910612]]\n0.07310204081632653\n\n\n\n\nComputing the Pearson correlation coefficient\nthe Pearson correlation coefficient, also called the Pearson r, is often easier to interpret than the covariance. It is computed using the np.corrcoef() function. Like np.cov(), it takes two arrays as arguments and returns a 2D array. Entries [0,0] and [1,1] are necessarily equal to 1, and the value we are after is entry [0,1].\nWe will write a function, pearson_r(x, y) that takes in two arrays and returns the Pearson correlation coefficient. We will then use this function to compute it for the petal lengths and widths of \\(I.\\ versicolor\\).\n\ndef pearson_r(x, y):\n    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n    # Compute correlation matrix: corr_mat\n    corr_mat = np.corrcoef(x,y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]\n\n# Compute Pearson correlation coefficient for I. versicolor: r\nr = pearson_r(versicolor_petal_length, versicolor_petal_width)\n\n# Print the result\nprint(r)\n\n0.7866680885228169"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#probabilistic-logic-and-statistical-inference",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#probabilistic-logic-and-statistical-inference",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Probabilistic logic and statistical inference",
    "text": "Probabilistic logic and statistical inference\n\nthe goal of statistical inference\n\nTo draw probabilistic conclusions about what we might expect if we collected the same data again.\nTo draw actionable conclusions from data.\nTo draw more general conclusions from relatively few data or observations.\n\n\n\n\n\n\n\nNote\n\n\n\nStatistical inference involves taking your data to probabilistic conclusions about what you would expect if you took even more data, and you can make decisions based on these conclusions.\n\n\n\n\nWhy we use the probabilistic language in statistical inference\n\nProbability provides a measure of uncertainty and this is crucial because we can quantify what we might expect if the data were acquired again.\nData are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary. We need probability to say how data might vary if acquired again.\n\n\n\n\n\n\n\nNote\n\n\n\nProbabilistic language is in fact very precise. It precisely describes uncertainty."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#random-number-generators-and-hacker-statistics",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#random-number-generators-and-hacker-statistics",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Random number generators and hacker statistics",
    "text": "Random number generators and hacker statistics\n\nHacker statistics\n\nUses simulated repeated measurements to compute probabilities.\n\n\n\nThe np.random module\n\nSuite of functions based on random number generation\nnp.random.random(): draw a number between \\(0\\) and \\(1\\)\n\n\n\nBernoulli trial\n● An experiment that has two options, “success” (True) and “failure” (False).\n\n\nRandom number seed\n\nInteger fed into random number generating algorithm\nManually seed random number generator if you need reproducibility\nSpecified using np.random.seed()\n\n\n\nHacker stats probabilities\n\nDetermine how to simulate data\nSimulate many many times\nProbability is approximately fraction of trials with the outcome of interest\n\n\n\nGenerating random numbers using the np.random module\nwe’ll generate lots of random numbers between zero and one, and then plot a histogram of the results. If the numbers are truly random, all bars in the histogram should be of (close to) equal height.\n\n# Seed the random number generator\nnp.random.seed(42)\n\n# Initialize random numbers: random_numbers\nrandom_numbers = np.empty(100000)\n\n# Generate random numbers by looping over range(100000)\nfor i in range(100000):\n    random_numbers[i] = np.random.random()\n\n# Plot a histogram\n_ = plt.hist(random_numbers, bins=316, histtype=\"step\", density=True)\n_ = plt.xlabel(\"random numbers\")\n_ = plt.ylabel(\"counts\")\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe histogram is almost exactly flat across the top, indicating that there is equal chance that a randomly-generated number is in any of the bins of the histogram.\n\n\n\n\nThe np.random module and Bernoulli trials\n\n\n\n\n\n\nTip\n\n\n\nYou can think of a Bernoulli trial as a flip of a possibly biased coin. Each coin flip has a probability \\(p\\) of landing heads (success) and probability \\(1−p\\) of landing tails (failure).\n\n\nWe will write a function to perform n Bernoulli trials, perform_bernoulli_trials(n, p), which returns the number of successes out of n Bernoulli trials, each of which has probability \\(p\\) of success. To perform each Bernoulli trial, we will use the np.random.random() function, which returns a random number between zero and one.\n\ndef perform_bernoulli_trials(n, p):\n    \"\"\"Perform n Bernoulli trials with success probability p\n    and return number of successes.\"\"\"\n    # Initialize number of successes: n_success\n    n_success = False\n\n    # Perform trials\n    for i in range(n):\n        # Choose random number between zero and one: random_number\n        random_number = np.random.random()\n\n        # If less than p, it's a success so add one to n_success\n        if random_number < p:\n            n_success += 1\n\n    return n_success\n\n\n\nHow many defaults might we expect?\nLet’s say a bank made 100 mortgage loans. It is possible that anywhere between \\(0\\) and \\(100\\) of the loans will be defaulted upon. We would like to know the probability of getting a given number of defaults, given that the probability of a default is \\(p = 0.05\\). To investigate this, we will do a simulation. We will perform 100 Bernoulli trials using the perform_bernoulli_trials() function and record how many defaults we get. Here, a success is a default.\n\n\n\n\n\n\nImportant\n\n\n\nRemember that the word “success” just means that the Bernoulli trial evaluates to True, i.e., did the loan recipient default?\n\n\nYou will do this for another \\(100\\) Bernoulli trials. And again and again until we have tried it \\(1000\\) times. Then, we will plot a histogram describing the probability of the number of defaults.\n\n# Seed random number generator\nnp.random.seed(42)\n\n# Initialize the number of defaults: n_defaults\nn_defaults = np.empty(1000)\n\n# Compute the number of defaults\nfor i in range(1000):\n    n_defaults[i] = perform_bernoulli_trials(100, 0.05)\n\n\n# Plot the histogram with default number of bins; label your axes\n_ = plt.hist(n_defaults, density=True)\n_ = plt.xlabel('number of defaults out of 100 loans')\n_ = plt.ylabel('probability')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is actually not an optimal way to plot a histogram when the results are known to be integers. We will revisit this\n\n\n\n\nWill the bank fail?\nIf interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money?\n\n# Compute ECDF: x, y\nx,y = ecdf(n_defaults)\n\n# Plot the ECDF with labeled axes\n_ = plt.plot(x,y, marker=\".\", linestyle=\"none\")\n_ = plt.xlabel(\"number of defaults\")\n_ = plt.ylabel(\"ECDF\")\n\n# Show the plot\nplt.show()\n\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\nn_lose_money = np.sum(n_defaults >= 10)\n\n# Compute and print probability of losing money\nprint('Probability of losing money =', n_lose_money / len(n_defaults))\n\n\n\n\nProbability of losing money = 0.022\n\n\n\n\n\n\n\n\nNote\n\n\n\nwe most likely get 5/100 defaults. But we still have about a 2% chance of getting 10 or more defaults out of 100 loans."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#probability-distributions-and-stories-the-binomial-distribution",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#probability-distributions-and-stories-the-binomial-distribution",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Probability distributions and stories: The Binomial distribution",
    "text": "Probability distributions and stories: The Binomial distribution\n\nProbability mass function (PMF)\n\nThe set of probabilities of discrete outcomes\n\n\n\nProbability distribution\n\nA mathematical description of outcomes\n\n\n\nDiscrete Uniform distribution: the story\n\nThe outcome of rolling a single fair die is Discrete Uniformly distributed.\n\n\n\nBinomial distribution: the story\n\nThe number \\(r\\) of successes in \\(n\\) Bernoulli trials with probability \\(p\\) of success, is Binomially distributed\nThe number \\(r\\) of heads in \\(4\\) coin flips with probability \\(0.5\\) of heads, is Binomially distributed\n\n\n\nSampling out of the Binomial distribution\nWe will compute the probability mass function for the number of defaults we would expect for \\(100\\) loans as in the last section, but instead of simulating all of the Bernoulli trials, we will perform the sampling using np.random.binomial(){% fn 1 %}.\n\n\n\n\n\n\nNote\n\n\n\nThis is identical to the calculation we did in the last set of exercises using our custom-written perform_bernoulli_trials() function, but far more computationally efficient.\n\n\nGiven this extra efficiency, we will take \\(10,000\\) samples instead of \\(1000\\). After taking the samples, we will plot the CDF. This CDF that we are plotting is that of the Binomial distribution.\n\n# Take 10,000 samples out of the binomial distribution: n_defaults\n\nn_defaults = np.random.binomial(100, 0.05, size=10000)\n# Compute CDF: x, y\nx,y = ecdf(n_defaults)\n\n# Plot the CDF with axis labels\n_ = plt.plot(x,y, marker=\".\", linestyle=\"-\")\n_ = plt.xlabel(\"number of defaults out of 100 loans\")\n_ = plt.ylabel(\"CDF\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you know the story, using built-in algorithms to directly sample out of the distribution is much faster.\n\n\n\n\nPlotting the Binomial PMF\n\n\n\n\n\n\nWarning\n\n\n\nplotting a nice looking PMF requires a bit of matplotlib trickery that we will not go into here.\n\n\nwe will plot the PMF of the Binomial distribution as a histogram. The trick is setting up the edges of the bins to pass to plt.hist() via the bins keyword argument. We want the bins centered on the integers. So, the edges of the bins should be \\(-0.5, 0.5, 1.5, 2.5, ...\\) up to max(n_defaults) + 1.5. We can generate an array like this using np.arange()and then subtracting 0.5 from the array.\n\n# Compute bin edges: bins\nbins = np.arange(0, max(n_defaults) + 1.5) - 0.5\n\n# Generate histogram\n_ = plt.hist(n_defaults, density=True, bins=bins)\n\n# Label axes\n_ = plt.xlabel(\"number of defaults out of 100 loans\")\n_ = plt.ylabel(\"probability\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#poisson-processes-and-the-poisson-distribution",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#poisson-processes-and-the-poisson-distribution",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Poisson processes and the Poisson distribution",
    "text": "Poisson processes and the Poisson distribution\n\nPoisson process\n\nThe timing of the next event is completely independent of when the previous event happened\n\n\n\nExamples of Poisson processes\n\nNatural births in a given hospital\nHit on a website during a given hour\nMeteor strikes\nMolecular collisions in a gas\nAviation incidents\nBuses in Poissonville\n\n\n\nPoisson distribution\n\nThe number \\(r\\) of arrivals of a Poisson process in a given time interval with average rate of \\(λ\\) arrivals per interval is Poisson distributed.\nThe number r of hits on a website in one hour with an average hit rate of 6 hits per hour is Poisson distributed.\n\n\n\nPoisson Distribution\n\nLimit of the Binomial distribution for low probability of success and large number of trials.\nThat is, for rare events.\n\n\n\nRelationship between Binomial and Poisson distributions\n\n\n\n\n\n\nImportant\n\n\n\nPoisson distribution is a limit of the Binomial distribution for rare events.\n\n\n\n\n\n\n\n\nTip\n\n\n\nPoisson distribution with arrival rate equal to \\(np\\) approximates a Binomial distribution for \\(n\\) Bernoulli trials with probability \\(p\\) of success (with \\(n\\) large and \\(p\\) small). Importantly, the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution.\n\n\nLet’s explore these two distributions computationally. We will compute the mean and standard deviation of samples from a Poisson distribution with an arrival rate of \\(10\\). Then, we will compute the mean and standard deviation of samples from a Binomial distribution with parameters \\(n\\) and \\(p\\) such that \\(np = 10\\).\n\n# Draw 10,000 samples out of Poisson distribution: samples_poisson\nsamples_poisson = np.random.poisson(10, size=10000)\n\n# Print the mean and standard deviation\nprint('Poisson:     ', np.mean(samples_poisson),\n                       np.std(samples_poisson))\n\n# Specify values of n and p to consider for Binomial: n, p\nn = [20, 100, 1000]\np = [.5, .1, .01]\n\n# Draw 10,000 samples for each n,p pair: samples_binomial\nfor i in range(3):\n    samples_binomial = np.random.binomial(n[i],p[i], size=10000)\n\n    # Print results\n    print('n =', n[i], 'Binom:', np.mean(samples_binomial),\n                                 np.std(samples_binomial))\n\nPoisson:      10.0145 3.1713545607516043\nn = 20 Binom: 10.0592 2.23523944131272\nn = 100 Binom: 10.0441 2.9942536949964675\nn = 1000 Binom: 10.0129 3.139639085946026\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe means are all about the same, which can be shown to be true by doing some pen-and-paper work. The standard deviation of the Binomial distribution gets closer and closer to that of the Poisson distribution as the probability \\(p\\) gets lower and lower.\n\n\n\n\nWas 2015 anomalous?\nIn baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. The ECDF of the number of no-hitters in a season is shown to the right. The probability distribution that would be appropriate to describe the number of no-hitters we would expect in a given season? is Both Binomial and Poisson, though Poisson is easier to model and compute.\n\n\n\n\n\n\nImportant\n\n\n\nWhen we have rare events (low \\(p\\), high \\(n\\)), the Binomial distribution is Poisson. This has a single parameter, the mean number of successes per time interval, in our case the mean number of no-hitters per season.\n\n\n1990 and 2015 featured the most no-hitters of any season of baseball (there were seven). Given that there are on average \\(\\frac{251}{115}\\) no-hitters per season, what is the probability of having seven or more in a season? Let’s find out\n\n# Draw 10,000 samples out of Poisson distribution: n_nohitters\nn_nohitters = np.random.poisson(251/115, size=10000)\n\n# Compute number of samples that are seven or greater: n_large\nn_large = np.sum(n_nohitters >= 7)\n\n# Compute probability of getting seven or more: p_large\np_large = n_large/10000\n\n# Print the result\nprint('Probability of seven or more no-hitters:', p_large)\n\nProbability of seven or more no-hitters: 0.0072\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe result is about \\(0.007\\). This means that it is not that improbable to see a 7-or-more no-hitter season in a century. We have seen two in a century and a half, so it is not unreasonable."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#probability-density-functions",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#probability-density-functions",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Probability density functions",
    "text": "Probability density functions\n\nContinuous variables\n\nQuantities that can take any value, not just discrete values\n\n\n\nProbability density function (PDF)\n\nContinuous analog to the PMF\nMathematical description of the relative likelihood of observing a value of a continuous variable"
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#introduction-to-the-normal-distribution",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#introduction-to-the-normal-distribution",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "Introduction to the Normal distribution",
    "text": "Introduction to the Normal distribution\n\nNormal distribution\n\nDescribes a continuous variable whose PDF has a single symmetric peak.\n\n\n\n\n\n\n\n\n\n\n>\nParameter\n\n\n\n\n\nmean of a Normal distribution\n≠\nmean computed from data\n\n\nst. dev. of a Normal distribution\n≠\nstandard deviation computed from data\n\n\n\n\nThe Normal PDF\n\n# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10\nsamples_std1 = np.random.normal(20,1,size=100000)\nsamples_std3 = np.random.normal(20, 3, size=100000)\nsamples_std10 = np.random.normal(20, 10, size=100000)\n\n# Make histograms\n_ = plt.hist(samples_std1, density=True, histtype=\"step\", bins=100)\n_ = plt.hist(samples_std3, density=True, histtype=\"step\", bins=100)\n_ = plt.hist(samples_std10, density=True, histtype=\"step\", bins=100)\n\n# Make a legend, set limits and show plot\n_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))\nplt.ylim(-0.01, 0.42)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can see how the different standard deviations result in PDFs of different widths. The peaks are all centered at the mean of 20.\n\n\n\n\nThe Normal CDF\n\n# Generate CDFs\nx_std1, y_std1 = ecdf(samples_std1)\nx_std3, y_std3 = ecdf(samples_std3)\nx_std10, y_std10 = ecdf(samples_std10)\n\n# Plot CDFs\n_ = plt.plot(x_std1, y_std1, marker=\".\", linestyle=\"none\")\n_ = plt.plot(x_std3, y_std3, marker=\".\", linestyle=\"none\")\n_ = plt.plot(x_std10, y_std10, marker=\".\", linestyle=\"none\")\n# Make a legend and show the plot\n_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe CDFs all pass through the mean at the 50th percentile; the mean and median of a Normal distribution are equal. The width of the CDF varies with the standard deviation."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#the-normal-distribution-properties-and-warnings",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#the-normal-distribution-properties-and-warnings",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "The Normal distribution: Properties and warnings",
    "text": "The Normal distribution: Properties and warnings\n\nAre the Belmont Stakes results Normally distributed?\nSince 1926, the Belmont Stakes is a \\(1.5\\) mile-long race of 3-year old thoroughbred horses. Secretariat ran the fastest Belmont Stakes in history in \\(1973\\). While that was the fastest year, 1970 was the slowest because of unusually wet and sloppy conditions. With these two outliers removed from the data set, we will compute the mean and standard deviation of the Belmont winners’ times. We will sample out of a Normal distribution with this mean and standard deviation using the np.random.normal() function and plot a CDF. Overlay the ECDF from the winning Belmont times {% fn 2 %}.\n\nbelmont_no_outliers = np.array([148.51, 146.65, 148.52, 150.7 , 150.42, 150.88, 151.57, 147.54,\n       149.65, 148.74, 147.86, 148.75, 147.5 , 148.26, 149.71, 146.56,\n       151.19, 147.88, 149.16, 148.82, 148.96, 152.02, 146.82, 149.97,\n       146.13, 148.1 , 147.2 , 146.  , 146.4 , 148.2 , 149.8 , 147.  ,\n       147.2 , 147.8 , 148.2 , 149.  , 149.8 , 148.6 , 146.8 , 149.6 ,\n       149.  , 148.2 , 149.2 , 148.  , 150.4 , 148.8 , 147.2 , 148.8 ,\n       149.6 , 148.4 , 148.4 , 150.2 , 148.8 , 149.2 , 149.2 , 148.4 ,\n       150.2 , 146.6 , 149.8 , 149.  , 150.8 , 148.6 , 150.2 , 149.  ,\n       148.6 , 150.2 , 148.2 , 149.4 , 150.8 , 150.2 , 152.2 , 148.2 ,\n       149.2 , 151.  , 149.6 , 149.6 , 149.4 , 148.6 , 150.  , 150.6 ,\n       149.2 , 152.6 , 152.8 , 149.6 , 151.6 , 152.8 , 153.2 , 152.4 ,\n       152.2 ])\n\n\n# Compute mean and standard deviation: mu, sigma\nmu = np.mean(belmont_no_outliers)\nsigma = np.std(belmont_no_outliers)\n\n# Sample out of a normal distribution with this mu and sigma: samples\nsamples = np.random.normal(mu, sigma, size=10000)\n\n# Get the CDF of the samples and of the data\nx_theor, y_theor = ecdf(samples)\nx,y = ecdf(belmont_no_outliers)\n\n# Plot the CDFs and show the plot\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x, y, marker='.', linestyle='none')\n_ = plt.xlabel('Belmont winning time (sec.)')\n_ = plt.ylabel('CDF')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe theoretical CDF and the ECDF of the data suggest that the winning Belmont times are, indeed, Normally distributed. This also suggests that in the last 100 years or so, there have not been major technological or training advances that have significantly affected the speed at which horses can run this race.\n\n\n\n\nWhat are the chances of a horse matching or beating Secretariat’s record?\nThe probability that the winner of a given Belmont Stakes will run it as fast or faster than Secretariat assuming that the Belmont winners’ times are Normally distributed (with the 1970 and 1973 years removed)\n\n# Take a million samples out of the Normal distribution: samples\nsamples = np.random.normal(mu, sigma, size=1000000)\n\n# Compute the fraction that are faster than 144 seconds: prob\nprob = np.sum(samples<=144)/len(samples)\n\n# Print the result\nprint('Probability of besting Secretariat:', prob)\n\nProbability of besting Secretariat: 0.000614\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe had to take a million samples because the probability of a fast time is very low and we had to be sure to sample enough. We get that there is only a 0.06% chance of a horse running the Belmont as fast as Secretariat."
  },
  {
    "objectID": "posts/2020-07-03-statistical thinking in python (part 1).html#the-exponential-distribution",
    "href": "posts/2020-07-03-statistical thinking in python (part 1).html#the-exponential-distribution",
    "title": "Statistical Thinking in Python (Part 1)",
    "section": "The Exponential distribution",
    "text": "The Exponential distribution\nThe waiting time between arrivals of a Poisson process is Exponentially distributed\n\nPossible Poisson process\n\nNuclear incidents:\nTiming of one is independent of all others\n\n\n\\(f(x; \\frac{1}{\\beta}) = \\frac{1}{\\beta} \\exp(-\\frac{x}{\\beta})\\)\n\nIf you have a story, you can simulate it!\nSometimes, the story describing our probability distribution does not have a named distribution to go along with it. In these cases, fear not! You can always simulate it.\nwe looked at the rare event of no-hitters in Major League Baseball. Hitting the cycle is another rare baseball event. When a batter hits the cycle, he gets all four kinds of hits, a single, double, triple, and home run, in a single game. Like no-hitters, this can be modeled as a Poisson process, so the time between hits of the cycle are also Exponentially distributed.\nHow long must we wait to see both a no-hitter and then a batter hit the cycle? The idea is that we have to wait some time for the no-hitter, and then after the no-hitter, we have to wait for hitting the cycle. Stated another way, what is the total waiting time for the arrival of two different Poisson processes? The total waiting time is the time waited for the no-hitter, plus the time waited for the hitting the cycle.\n\n\n\n\n\n\nImportant\n\n\n\nWe will write a function to sample out of the distribution described by this story.\n\n\n\ndef successive_poisson(tau1, tau2, size=1):\n    \"\"\"Compute time for arrival of 2 successive Poisson processes.\"\"\"\n    # Draw samples out of first exponential distribution: t1\n    t1 = np.random.exponential(tau1, size=size)\n\n    # Draw samples out of second exponential distribution: t2\n    t2 = np.random.exponential(tau2, size=size)\n\n    return t1 + t2\n\n\n\nDistribution of no-hitters and cycles\nWe’ll use the sampling function to compute the waiting time to observe a no-hitter and hitting of the cycle. The mean waiting time for a no-hitter is \\(764\\) games, and the mean waiting time for hitting the cycle is \\(715\\) games.\n\n# Draw samples of waiting times: waiting_times\nwaiting_times = successive_poisson(764, 715, size=100000)\n\n# Make the histogram\n_ = plt.hist(waiting_times, bins=100, density=True, histtype=\"step\")\n# Label axes\n_ = plt.xlabel(\"Waiting times\")\n_ = plt.ylabel(\"probability\")\n\n# Show the plot\nplt.show()\n\n\n\n\nNotice that the PDF is peaked, unlike the waiting time for a single Poisson process. For fun (and enlightenment), Let’s also plot the CDF.\n\nx,y = ecdf(waiting_times)\n_ = plt.plot(x,y)\n_ = plt.plot(x,y, marker=\".\", linestyle=\"none\")\n_ = plt.xlabel(\"Waiting times\")\n_ = plt.ylabel(\"CDF\")\nplt.show()\n\n\n\n\n{{‘For this exercise and all going forward, the random number generator is pre-seeded for you (with np.random.seed(42)) to save you typing that each time.’ | fndetail: 1 }} {{‘we scraped the data concerning the Belmont Stakes from the Belmont Wikipedia page.’ | fndetail: 2 }}"
  },
  {
    "objectID": "posts/2020-10-19-visualizing_countries.html",
    "href": "posts/2020-10-19-visualizing_countries.html",
    "title": "Dataset",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nplt.style.use('ggplot')\nthis plot does not show a linear relationship between GDP and percent literate, countries with a lower GDP do seem more likely to have a lower percent of the population that can read and write."
  },
  {
    "objectID": "posts/2020-10-19-visualizing_countries.html#how-many-countries-are-in-each-region-of-the-world",
    "href": "posts/2020-10-19-visualizing_countries.html#how-many-countries-are-in-each-region-of-the-world",
    "title": "Dataset",
    "section": "how many countries are in each region of the world?",
    "text": "how many countries are in each region of the world?\n\nsns.countplot(y=countries['Region'])\nplt.show()\n\n\n\n\nSub-Saharan Africa contains the most countries in this list."
  },
  {
    "objectID": "posts/2020-10-19-appliances energy prediction data.html",
    "href": "posts/2020-10-19-appliances energy prediction data.html",
    "title": "Appliances Energy Prediction data",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max.columns\", None)\npd.set_option(\"display.max_colwidth\", None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use(\"ggplot\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import (LinearRegression, \n                                  Ridge, \n                                  Lasso)\nfrom sklearn.metrics import (r2_score, \n                             mean_absolute_error, \n                             mean_squared_error)"
  },
  {
    "objectID": "posts/2020-10-19-appliances energy prediction data.html#dataset-description",
    "href": "posts/2020-10-19-appliances energy prediction data.html#dataset-description",
    "title": "Appliances Energy Prediction data",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe attribute information can be seen below.\n\nAttribute Information:\n\n\n\nAttribute\nDescription\nUnits\n\n\n\n\nDate\ntime\nyear-month-day hour:minute:second\n\n\nAppliances\nenergy use\nin Wh\n\n\nlights\nenergy use of light fixtures in the house\nin Wh\n\n\nT1\nTemperature in kitchen area\nin Celsius\n\n\nRH_1\nHumidity in kitchen area\nin %\n\n\nT2\nTemperature in living room area\nin Celsius\n\n\nRH_2\nHumidity in living room area\nin %\n\n\nT3\nTemperature in laundry room area\n\n\n\nRH_3\nHumidity in laundry room area\nin %\n\n\nT4\nTemperature in office room\nin Celsius\n\n\nRH_4\nHumidity in office room\nin %\n\n\nT5\nTemperature in bathroom\nin Celsius\n\n\nRH_5\nHumidity in bathroom\nin %\n\n\nT6\nTemperature outside the building (north side)\nin Celsius\n\n\nRH_6\nHumidity outside the building (north side)\nin %\n\n\nT7\nTemperature in ironing room\nin Celsius\n\n\nRH_7\nHumidity in ironing room\nin %\n\n\nT8\nTemperature in teenager room 2\nin Celsius\n\n\nRH_8\nHumidity in teenager room 2\nin %\n\n\nT9\nTemperature in parents room\nin Celsius\n\n\nRH_9\nHumidity in parents room\nin %\n\n\nTo\nTemperature outside (from Chievres weather station)\nin Celsius\n\n\nPressure\n(from Chievres weather station)\nin mm Hg\n\n\nRH_out\nHumidity outside (from Chievres weather station)\nin %\n\n\nWind speed\n(from Chievres weather station)\nin m/s\n\n\nVisibility\n(from Chievres weather station)\nin km\n\n\nTdewpoint\n(from Chievres weather station)\nÂ °C\n\n\nrv1\nRandom variable 1\nnondimensional\n\n\nrv2\nRandom variable 2\nnondimensional\n\n\n\n\nenergy.describe()\n\n\n\n\n\n  \n    \n      \n      Appliances\n      lights\n      T1\n      RH_1\n      T2\n      RH_2\n      T3\n      RH_3\n      T4\n      RH_4\n      T5\n      RH_5\n      T6\n      RH_6\n      T7\n      RH_7\n      T8\n      RH_8\n      T9\n      RH_9\n      T_out\n      Press_mm_hg\n      RH_out\n      Windspeed\n      Visibility\n      Tdewpoint\n      rv1\n      rv2\n    \n  \n  \n    \n      count\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n      19735.000000\n    \n    \n      mean\n      97.694958\n      3.801875\n      21.686571\n      40.259739\n      20.341219\n      40.420420\n      22.267611\n      39.242500\n      20.855335\n      39.026904\n      19.592106\n      50.949283\n      7.910939\n      54.609083\n      20.267106\n      35.388200\n      22.029107\n      42.936165\n      19.485828\n      41.552401\n      7.411665\n      755.522602\n      79.750418\n      4.039752\n      38.330834\n      3.760707\n      24.988033\n      24.988033\n    \n    \n      std\n      102.524891\n      7.935988\n      1.606066\n      3.979299\n      2.192974\n      4.069813\n      2.006111\n      3.254576\n      2.042884\n      4.341321\n      1.844623\n      9.022034\n      6.090347\n      31.149806\n      2.109993\n      5.114208\n      1.956162\n      5.224361\n      2.014712\n      4.151497\n      5.317409\n      7.399441\n      14.901088\n      2.451221\n      11.794719\n      4.194648\n      14.496634\n      14.496634\n    \n    \n      min\n      10.000000\n      0.000000\n      16.790000\n      27.023333\n      16.100000\n      20.463333\n      17.200000\n      28.766667\n      15.100000\n      27.660000\n      15.330000\n      29.815000\n      -6.065000\n      1.000000\n      15.390000\n      23.200000\n      16.306667\n      29.600000\n      14.890000\n      29.166667\n      -5.000000\n      729.300000\n      24.000000\n      0.000000\n      1.000000\n      -6.600000\n      0.005322\n      0.005322\n    \n    \n      25%\n      50.000000\n      0.000000\n      20.760000\n      37.333333\n      18.790000\n      37.900000\n      20.790000\n      36.900000\n      19.530000\n      35.530000\n      18.277500\n      45.400000\n      3.626667\n      30.025000\n      18.700000\n      31.500000\n      20.790000\n      39.066667\n      18.000000\n      38.500000\n      3.666667\n      750.933333\n      70.333333\n      2.000000\n      29.000000\n      0.900000\n      12.497889\n      12.497889\n    \n    \n      50%\n      60.000000\n      0.000000\n      21.600000\n      39.656667\n      20.000000\n      40.500000\n      22.100000\n      38.530000\n      20.666667\n      38.400000\n      19.390000\n      49.090000\n      7.300000\n      55.290000\n      20.033333\n      34.863333\n      22.100000\n      42.375000\n      19.390000\n      40.900000\n      6.916667\n      756.100000\n      83.666667\n      3.666667\n      40.000000\n      3.433333\n      24.897653\n      24.897653\n    \n    \n      75%\n      100.000000\n      0.000000\n      22.600000\n      43.066667\n      21.500000\n      43.260000\n      23.290000\n      41.760000\n      22.100000\n      42.156667\n      20.619643\n      53.663333\n      11.256000\n      83.226667\n      21.600000\n      39.000000\n      23.390000\n      46.536000\n      20.600000\n      44.338095\n      10.408333\n      760.933333\n      91.666667\n      5.500000\n      40.000000\n      6.566667\n      37.583769\n      37.583769\n    \n    \n      max\n      1080.000000\n      70.000000\n      26.260000\n      63.360000\n      29.856667\n      56.026667\n      29.236000\n      50.163333\n      26.200000\n      51.090000\n      25.795000\n      96.321667\n      28.290000\n      99.900000\n      26.000000\n      51.400000\n      27.230000\n      58.780000\n      24.500000\n      53.326667\n      26.100000\n      772.300000\n      100.000000\n      14.000000\n      66.000000\n      15.500000\n      49.996530\n      49.996530\n    \n  \n\n\n\n\n\nenergy.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 19735 entries, 0 to 19734\nData columns (total 29 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   date         19735 non-null  object \n 1   Appliances   19735 non-null  int64  \n 2   lights       19735 non-null  int64  \n 3   T1           19735 non-null  float64\n 4   RH_1         19735 non-null  float64\n 5   T2           19735 non-null  float64\n 6   RH_2         19735 non-null  float64\n 7   T3           19735 non-null  float64\n 8   RH_3         19735 non-null  float64\n 9   T4           19735 non-null  float64\n 10  RH_4         19735 non-null  float64\n 11  T5           19735 non-null  float64\n 12  RH_5         19735 non-null  float64\n 13  T6           19735 non-null  float64\n 14  RH_6         19735 non-null  float64\n 15  T7           19735 non-null  float64\n 16  RH_7         19735 non-null  float64\n 17  T8           19735 non-null  float64\n 18  RH_8         19735 non-null  float64\n 19  T9           19735 non-null  float64\n 20  RH_9         19735 non-null  float64\n 21  T_out        19735 non-null  float64\n 22  Press_mm_hg  19735 non-null  float64\n 23  RH_out       19735 non-null  float64\n 24  Windspeed    19735 non-null  float64\n 25  Visibility   19735 non-null  float64\n 26  Tdewpoint    19735 non-null  float64\n 27  rv1          19735 non-null  float64\n 28  rv2          19735 non-null  float64\ndtypes: float64(26), int64(2), object(1)\nmemory usage: 4.4+ MB\n\n\nThere are no missing values in the dataset.\n\nscaler = MinMaxScaler()\nnormalised_df = pd.DataFrame(scaler.fit_transform(energy.drop(columns=['date', 'lights'])), \n                             columns=energy.drop(columns=['date', 'lights']).columns)\nfeatures_df = normalised_df.drop(columns=['Appliances'])\nenergy_target = normalised_df.Appliances\nX_train, X_test, y_train, y_test = train_test_split(features_df, energy_target, test_size=.3, random_state=42)\n\nFrom the dataset, fit a linear model on the relationship between the temperature in the living room in Celsius (x = T2) and the temperature outside the building (y = T6).\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train[['T2']], X_train.T6)\nT6_pred = lin_reg.predict(X_test[['T2']])\nprint(f'r^2 score: {round(r2_score(X_test.T6, T6_pred), 2)}')\n\nr^2 score: 0.64\n\n\n\nprint(f'MAE: {round(mean_absolute_error(X_test.T6, T6_pred), 2)}')\n\nMAE: 0.08\n\n\n\nprint(f'Residual Sum of Squares: {round(np.sum(np.square(X_test.T6 - T6_pred)), 2)}')\n\nResidual Sum of Squares: 274.9\n\n\n\nprint(f'Root Mean Squared Error: {round(np.sqrt(mean_squared_error(X_test.T6, T6_pred)), 3)}')\n\nRoot Mean Squared Error: 0.215\n\n\n\n\n\nT7             1.0\nT1             1.0\nT2             1.0\nT9             1.0\nRH_8           1.0\nRH_out         1.0\nTdewpoint      1.0\nVisibility     1.0\nWindspeed      1.0\nPress_mm_hg    1.0\nT_out          1.0\nRH_9           1.0\nT8             1.0\nRH_7           1.0\nAppliances     1.0\nRH_6           1.0\nT6             1.0\nRH_5           1.0\nRH_4           1.0\nRH_3           1.0\nT3             1.0\nRH_2           1.0\nRH_1           1.0\nrv1            1.0\nrv2            1.0\nT5             1.0\nT4             1.0\ndtype: float64\n\n\n\nenergy.drop(columns=['date', 'lights']).max().sort_values()\n\nWindspeed        14.000000\nTdewpoint        15.500000\nT9               24.500000\nT5               25.795000\nT7               26.000000\nT_out            26.100000\nT4               26.200000\nT1               26.260000\nT8               27.230000\nT6               28.290000\nT3               29.236000\nT2               29.856667\nrv1              49.996530\nrv2              49.996530\nRH_3             50.163333\nRH_4             51.090000\nRH_7             51.400000\nRH_9             53.326667\nRH_2             56.026667\nRH_8             58.780000\nRH_1             63.360000\nVisibility       66.000000\nRH_5             96.321667\nRH_6             99.900000\nRH_out          100.000000\nPress_mm_hg     772.300000\nAppliances     1080.000000\ndtype: float64\n\n\n\nenergy.drop(columns=['date', 'lights']).min().sort_values()\n\nTdewpoint       -6.600000\nT6              -6.065000\nT_out           -5.000000\nWindspeed        0.000000\nrv2              0.005322\nrv1              0.005322\nVisibility       1.000000\nRH_6             1.000000\nAppliances      10.000000\nT9              14.890000\nT4              15.100000\nT5              15.330000\nT7              15.390000\nT2              16.100000\nT8              16.306667\nT1              16.790000\nT3              17.200000\nRH_2            20.463333\nRH_7            23.200000\nRH_out          24.000000\nRH_1            27.023333\nRH_4            27.660000\nRH_3            28.766667\nRH_9            29.166667\nRH_8            29.600000\nRH_5            29.815000\nPress_mm_hg    729.300000\ndtype: float64\n\n\n\ndef get_weights_df(model, feat, col_name):\n    #this function returns the weight of every feature\n    weights = pd.Series(model.coef_, feat.columns).sort_values()\n    weights_df = pd.DataFrame(weights).reset_index()\n    weights_df.columns = ['Features', col_name]\n    weights_df[col_name].round(3)\n    return weights_df\n\n\nridge_reg = Ridge(alpha=0.4)\nridge_reg.fit(X_train, y_train)\n\nRidge(alpha=0.4)\n\n\n\nlasso_reg = Lasso(alpha=0.001)\nlasso_reg.fit(X_train, y_train)\n\nLasso(alpha=0.001)\n\n\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nLinearRegression()\n\n\n\nlinear_model_weights = get_weights_df(model, X_train, 'Linear_Model_Weight')\nridge_weights_df = get_weights_df(ridge_reg, X_train, 'Ridge_Weight')\nlasso_weights_df = get_weights_df(lasso_reg, X_train, 'Lasso_weight')\n\nfinal_weights = pd.merge(linear_model_weights, ridge_weights_df, on='Features')\nfinal_weights = pd.merge(final_weights, lasso_weights_df, on='Features')\n\n\nfinal_weights.sort_values(\"Linear_Model_Weight\", ascending=False)\n\n\n\n\n\n  \n    \n      \n      Features\n      Linear_Model_Weight\n      Ridge_Weight\n      Lasso_weight\n    \n  \n  \n    \n      25\n      RH_1\n      0.553547\n      0.519525\n      0.017880\n    \n    \n      24\n      T3\n      0.290627\n      0.288087\n      0.000000\n    \n    \n      23\n      T6\n      0.236425\n      0.217292\n      0.000000\n    \n    \n      22\n      Tdewpoint\n      0.117758\n      0.083128\n      0.000000\n    \n    \n      21\n      T8\n      0.101995\n      0.101028\n      0.000000\n    \n    \n      20\n      RH_3\n      0.096048\n      0.095135\n      0.000000\n    \n    \n      19\n      RH_6\n      0.038049\n      0.035519\n      -0.000000\n    \n    \n      18\n      Windspeed\n      0.029183\n      0.030268\n      0.002912\n    \n    \n      17\n      T4\n      0.028981\n      0.027384\n      -0.000000\n    \n    \n      16\n      RH_4\n      0.026386\n      0.024579\n      0.000000\n    \n    \n      15\n      RH_5\n      0.016006\n      0.016152\n      0.000000\n    \n    \n      14\n      Visibility\n      0.012307\n      0.012076\n      0.000000\n    \n    \n      13\n      T7\n      0.010319\n      0.010098\n      -0.000000\n    \n    \n      12\n      Press_mm_hg\n      0.006839\n      0.006584\n      -0.000000\n    \n    \n      11\n      rv2\n      0.000770\n      0.000748\n      -0.000000\n    \n    \n      10\n      rv1\n      0.000770\n      0.000748\n      -0.000000\n    \n    \n      9\n      T1\n      -0.003281\n      -0.018406\n      0.000000\n    \n    \n      8\n      T5\n      -0.015657\n      -0.019853\n      -0.000000\n    \n    \n      7\n      RH_9\n      -0.039800\n      -0.041367\n      -0.000000\n    \n    \n      6\n      RH_7\n      -0.044614\n      -0.045977\n      -0.000000\n    \n    \n      5\n      RH_out\n      -0.077671\n      -0.054724\n      -0.049557\n    \n    \n      4\n      RH_8\n      -0.157595\n      -0.156830\n      -0.000110\n    \n    \n      3\n      T9\n      -0.189941\n      -0.188916\n      -0.000000\n    \n    \n      2\n      T2\n      -0.236178\n      -0.201397\n      0.000000\n    \n    \n      1\n      T_out\n      -0.321860\n      -0.262172\n      0.000000\n    \n    \n      0\n      RH_2\n      -0.456698\n      -0.411071\n      -0.000000\n    \n  \n\n\n\n\n\ny_pred_lg = model.predict(X_test)\ny_pred_r = ridge_reg.predict(X_test)\ny_pred_l = lasso_reg.predict(X_test)\n\n\nprint(f'Root Mean Squared Error: {round(np.sqrt(mean_squared_error(y_test, y_pred_r)), 3)}')\n\nRoot Mean Squared Error: 0.088\n\n\n\nprint(f'Root Mean Squared Error: {round(np.sqrt(mean_squared_error(y_test, y_pred_l)), 3)}')\n\nRoot Mean Squared Error: 0.094"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html",
    "href": "posts/2021-06-11-data_wrangling_cit.html",
    "title": "Data Wrangling in Python",
    "section": "",
    "text": "Wikipedia Definition"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#data-sets",
    "href": "posts/2021-06-11-data_wrangling_cit.html#data-sets",
    "title": "Data Wrangling in Python",
    "section": "Data Sets",
    "text": "Data Sets\n\nclient status: Shows the client status and loan status at the end of the month their status remained the same.\nkenya_subcounties: shows the county name and sub counties"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#first-few-rows",
    "href": "posts/2021-06-11-data_wrangling_cit.html#first-few-rows",
    "title": "Data Wrangling in Python",
    "section": "3.1 First few rows",
    "text": "3.1 First few rows\nBy Default, without passing any argument in head(), it will return the first 5 rows.\n\n\nCode\nstatus.head()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      0\n      South Rift AgriBiz\n      Ndunyu Njeru\n      85119.0\n      New\n      P+I Current\n      NaN\n      2021-02-28\n    \n    \n      1\n      South Rift Region\n      Nakuru\n      2005.0\n      Dormant\n      Penalties\n      44.83\n      2018-09-30\n    \n    \n      2\n      Eastern Region\n      Wote\n      69865.0\n      Dormant\n      Penalties\n      50.65\n      2020-11-30\n    \n    \n      3\n      Western Region\n      Bungoma\n      55339.0\n      Dormant\n      Penalties\n      60.71\n      2020-07-31\n    \n    \n      4\n      South Rift Region\n      Kericho\n      38049.0\n      Dormant\n      Penalties\n      0.15\n      2019-06-30\n    \n  \n\n\n\n\nPassing the number of rows you want\n\n\nCode\nstatus.head(10)\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      0\n      South Rift AgriBiz\n      Ndunyu Njeru\n      85119.0\n      New\n      P+I Current\n      NaN\n      2021-02-28\n    \n    \n      1\n      South Rift Region\n      Nakuru\n      2005.0\n      Dormant\n      Penalties\n      44.83\n      2018-09-30\n    \n    \n      2\n      Eastern Region\n      Wote\n      69865.0\n      Dormant\n      Penalties\n      50.65\n      2020-11-30\n    \n    \n      3\n      Western Region\n      Bungoma\n      55339.0\n      Dormant\n      Penalties\n      60.71\n      2020-07-31\n    \n    \n      4\n      South Rift Region\n      Kericho\n      38049.0\n      Dormant\n      Penalties\n      0.15\n      2019-06-30\n    \n    \n      5\n      South Rift Region\n      Narok\n      33391.0\n      Dormant\n      Penalties\n      0.94\n      2019-04-30\n    \n    \n      6\n      North Rift Region\n      Iten\n      53675.0\n      Dormant\n      Penalties\n      35.24\n      2020-05-31\n    \n    \n      7\n      North Rift Region\n      Eldoret3\n      16893.0\n      Active\n      Penalties\n      26.21\n      2020-12-31\n    \n    \n      8\n      Nairobi Region\n      Kiambu\n      35681.0\n      Dormant\n      Penalties\n      46.78\n      2019-05-31\n    \n    \n      9\n      North Rift Region\n      Kitale\n      24807.0\n      Dormant\n      Penalties\n      36.94\n      2019-02-28\n    \n  \n\n\n\n\n\nEXERCISE\nFor the dataframe we created (counties) return the first 15 rows."
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#last-few-rows",
    "href": "posts/2021-06-11-data_wrangling_cit.html#last-few-rows",
    "title": "Data Wrangling in Python",
    "section": "3.2 Last few rows",
    "text": "3.2 Last few rows\nThe tail method returns the last rows of a dataframe, by default it returns the 5 last rows, but you can specify the number of last rows that you need to return.\n\n\nCode\nstatus.tail()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      40019\n      Eastern Region\n      Matuu\n      66991.0\n      Dormant\n      Penalties\n      33.38\n      2020-09-30\n    \n    \n      40020\n      Report Title: Client Status (Monthly)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40021\n      Generated By: Victor Omondi\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40022\n      Generated Time: 2021-03-26 02:44:32\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40023\n      Report Parameters:Office: Head Office month: 2 year: 2021\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs you can see the last 4 rows have metadata on when the data was generated. We will hand this situation in the Reshaping data part.\n\n\n\n\nCode\nstatus.tail(10)\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      40014\n      South Rift AgriBiz\n      Maili Nne\n      48513.0\n      Active\n      Penalties\n      56.17\n      2021-01-31\n    \n    \n      40015\n      North Rift Region\n      Iten\n      42221.0\n      Dormant\n      Penalties\n      44.50\n      2019-08-31\n    \n    \n      40016\n      Western Region\n      Mbale\n      54391.0\n      Active\n      P+I In Default\n      56.98\n      2021-02-28\n    \n    \n      40017\n      Coast Region\n      Mariakani\n      67944.0\n      Dormant\n      P+I In Default\n      0.15\n      2020-10-31\n    \n    \n      40018\n      North Rift Region\n      Kitale\n      6888.0\n      Dormant\n      Penalties\n      32.42\n      2020-11-30\n    \n    \n      40019\n      Eastern Region\n      Matuu\n      66991.0\n      Dormant\n      Penalties\n      33.38\n      2020-09-30\n    \n    \n      40020\n      Report Title: Client Status (Monthly)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40021\n      Generated By: Victor Omondi\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40022\n      Generated Time: 2021-03-26 02:44:32\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40023\n      Report Parameters:Office: Head Office month: 2 year: 2021\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nEXERCISE\nFor the dataframe (counties) we created return the last 15 rows."
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#sample",
    "href": "posts/2021-06-11-data_wrangling_cit.html#sample",
    "title": "Data Wrangling in Python",
    "section": "3.3 sample",
    "text": "3.3 sample\ndf.sample mainly is used during sampling techniques, it a random sample of items from an axis of object.\n\n\nCode\nstatus.sample(5)\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      24131\n      North Rift Region\n      Nandi Hills\n      39393.0\n      Dormant\n      Penalties\n      43.14\n      2020-01-31\n    \n    \n      2638\n      Western Region\n      Bungoma\n      71754.0\n      Dormant\n      Penalties\n      55.37\n      2021-02-28\n    \n    \n      3398\n      North Rift Region\n      Eldoret1\n      32182.0\n      Dormant\n      Penalties\n      4.35\n      2019-04-30\n    \n    \n      24100\n      Coast Region\n      Voi\n      73956.0\n      Dormant\n      Penalties\n      40.49\n      2021-02-28\n    \n    \n      31920\n      North Rift Region\n      Kapsabet\n      6664.0\n      Dormant\n      Penalties\n      34.83\n      2019-03-31\n    \n  \n\n\n\n\n\nEXERCISE\nFor the dataframe (counties) we created return a sample of 10 rows"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#number-of-rows-and-columns",
    "href": "posts/2021-06-11-data_wrangling_cit.html#number-of-rows-and-columns",
    "title": "Data Wrangling in Python",
    "section": "3.4. Number of Rows and columns",
    "text": "3.4. Number of Rows and columns\nIn pandas, there are various ways to know how many columns (variables) and rows (observations) a dataframe has. 1. len(df) - returns number of rows 2. df.shape - returns number of rows and columns as a tuple 3. df.info() - returns data frame info, non null values, columns and data columns and the data type of columns.\n\n\nCode\nstatus.shape\n\n\n(40024, 7)\n\n\n\nEXERCISE\nFor the dataframe (counties) we created how many rows and columns does it have?"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#info",
    "href": "posts/2021-06-11-data_wrangling_cit.html#info",
    "title": "Data Wrangling in Python",
    "section": "3.5 Info",
    "text": "3.5 Info\ndf.info prints a summary of the data frame, ie, number of rows, columns, data column and their non-null values and the dtype\n\n\nCode\nstatus.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 40024 entries, 0 to 40023\nData columns (total 7 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Region         39944 non-null  object \n 1   Office         40020 non-null  object \n 2   Client ID      40020 non-null  float64\n 3   Client Status  40020 non-null  object \n 4   Loan Status    40020 non-null  object \n 5   Client Score   36720 non-null  float64\n 6   Status Date    40020 non-null  object \ndtypes: float64(2), object(5)\nmemory usage: 2.1+ MB\n\n\n\nEXERCISE\nFor the dataframe (counties) we created how many missing values are there in the name columns?"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#describe",
    "href": "posts/2021-06-11-data_wrangling_cit.html#describe",
    "title": "Data Wrangling in Python",
    "section": "3.6. Describe",
    "text": "3.6. Describe\nWe use df.describe() to get summary statistics, by default it returns the summary statistics of the numerical columns.\n\n\nCode\nstatus.describe()\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Client Score\n    \n  \n  \n    \n      count\n      40020.000000\n      36720.000000\n    \n    \n      mean\n      43747.996652\n      35.593555\n    \n    \n      std\n      24860.118889\n      20.538354\n    \n    \n      min\n      2.000000\n      0.150000\n    \n    \n      25%\n      22291.750000\n      21.887500\n    \n    \n      50%\n      44105.000000\n      36.700000\n    \n    \n      75%\n      65170.500000\n      49.822500\n    \n    \n      max\n      86654.000000\n      92.540000\n    \n  \n\n\n\n\nTo show all summary statistics including those for objects (strings) you can pass include=\"all\" to the df.describe.\n\n\nCode\nstatus.describe(include='all')\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      count\n      39944\n      40020\n      40020.000000\n      40020\n      40020\n      36720.000000\n      40020\n    \n    \n      unique\n      12\n      57\n      NaN\n      4\n      4\n      NaN\n      67\n    \n    \n      top\n      Nairobi Region\n      Embakasi\n      NaN\n      Dormant\n      Penalties\n      NaN\n      2021-02-28\n    \n    \n      freq\n      9860\n      1561\n      NaN\n      30535\n      28071\n      NaN\n      8430\n    \n    \n      mean\n      NaN\n      NaN\n      43747.996652\n      NaN\n      NaN\n      35.593555\n      NaN\n    \n    \n      std\n      NaN\n      NaN\n      24860.118889\n      NaN\n      NaN\n      20.538354\n      NaN\n    \n    \n      min\n      NaN\n      NaN\n      2.000000\n      NaN\n      NaN\n      0.150000\n      NaN\n    \n    \n      25%\n      NaN\n      NaN\n      22291.750000\n      NaN\n      NaN\n      21.887500\n      NaN\n    \n    \n      50%\n      NaN\n      NaN\n      44105.000000\n      NaN\n      NaN\n      36.700000\n      NaN\n    \n    \n      75%\n      NaN\n      NaN\n      65170.500000\n      NaN\n      NaN\n      49.822500\n      NaN\n    \n    \n      max\n      NaN\n      NaN\n      86654.000000\n      NaN\n      NaN\n      92.540000\n      NaN\n    \n  \n\n\n\n\n\nEXERCISE\nFor the dataframe (counties) we created how many unique sub counties are there in Kenya?"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#droping-irrelevant-data.",
    "href": "posts/2021-06-11-data_wrangling_cit.html#droping-irrelevant-data.",
    "title": "Data Wrangling in Python",
    "section": "4.1. Droping Irrelevant Data.",
    "text": "4.1. Droping Irrelevant Data.\nLet’s check some of the irrelevant data.\n\n\nCode\nstatus.tail()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      40019\n      Eastern Region\n      Matuu\n      66991.0\n      Dormant\n      Penalties\n      33.38\n      2020-09-30\n    \n    \n      40020\n      Report Title: Client Status (Monthly)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40021\n      Generated By: Victor Omondi\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40022\n      Generated Time: 2021-03-26 02:44:32\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      40023\n      Report Parameters:Office: Head Office month: 2 year: 2021\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nwe can drop these 4 last columns by using df.drop and assigning the index argument to a list of indeces we want to drop.\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to drop columns, use df.drop(columns=cols) were cols is a list of column names you want to drop.\n\n\n\n\nCode\nstatus_2 = status.drop(index=[40020, 40021, 40022, 40023])\nstatus_2.tail()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      40015\n      North Rift Region\n      Iten\n      42221.0\n      Dormant\n      Penalties\n      44.50\n      2019-08-31\n    \n    \n      40016\n      Western Region\n      Mbale\n      54391.0\n      Active\n      P+I In Default\n      56.98\n      2021-02-28\n    \n    \n      40017\n      Coast Region\n      Mariakani\n      67944.0\n      Dormant\n      P+I In Default\n      0.15\n      2020-10-31\n    \n    \n      40018\n      North Rift Region\n      Kitale\n      6888.0\n      Dormant\n      Penalties\n      32.42\n      2020-11-30\n    \n    \n      40019\n      Eastern Region\n      Matuu\n      66991.0\n      Dormant\n      Penalties\n      33.38\n      2020-09-30\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can also drop inplace, no need of assigning it to a variable. This can be done by specifying the inplace argument to True\n\n\n\n\nCode\nstatus.drop(index=[40020, 40021, 40022, 40023], inplace=True)\nstatus.tail()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client ID\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      40015\n      North Rift Region\n      Iten\n      42221.0\n      Dormant\n      Penalties\n      44.50\n      2019-08-31\n    \n    \n      40016\n      Western Region\n      Mbale\n      54391.0\n      Active\n      P+I In Default\n      56.98\n      2021-02-28\n    \n    \n      40017\n      Coast Region\n      Mariakani\n      67944.0\n      Dormant\n      P+I In Default\n      0.15\n      2020-10-31\n    \n    \n      40018\n      North Rift Region\n      Kitale\n      6888.0\n      Dormant\n      Penalties\n      32.42\n      2020-11-30\n    \n    \n      40019\n      Eastern Region\n      Matuu\n      66991.0\n      Dormant\n      Penalties\n      33.38\n      2020-09-30"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#set-index",
    "href": "posts/2021-06-11-data_wrangling_cit.html#set-index",
    "title": "Data Wrangling in Python",
    "section": "4.2. Set Index",
    "text": "4.2. Set Index\nFor our data frame, Client ID column uniquely identifies rows, for our analysis, we won’t be doing analysis on this column, we can set Client ID to be the index of our dataframe.\n\n\nCode\nstatus.set_index('Client ID', inplace=True)\nstatus.head()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n    \n      Client ID\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      85119.0\n      South Rift AgriBiz\n      Ndunyu Njeru\n      New\n      P+I Current\n      NaN\n      2021-02-28\n    \n    \n      2005.0\n      South Rift Region\n      Nakuru\n      Dormant\n      Penalties\n      44.83\n      2018-09-30\n    \n    \n      69865.0\n      Eastern Region\n      Wote\n      Dormant\n      Penalties\n      50.65\n      2020-11-30\n    \n    \n      55339.0\n      Western Region\n      Bungoma\n      Dormant\n      Penalties\n      60.71\n      2020-07-31\n    \n    \n      38049.0\n      South Rift Region\n      Kericho\n      Dormant\n      Penalties\n      0.15\n      2019-06-30"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#sorting",
    "href": "posts/2021-06-11-data_wrangling_cit.html#sorting",
    "title": "Data Wrangling in Python",
    "section": "4.3. Sorting",
    "text": "4.3. Sorting\nSorting is arranging values/index in data either in ascending or descending order. To sort index we use df.sort_index, to sort values in columns we use df.sort_values(cols) where cols is column name or list of column names we want to sort their values.\n\n\nCode\nstatus.sort_index(inplace=True)\nstatus.head()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n    \n      Client ID\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2.0\n      Nairobi Region\n      Ngong Road\n      Dormant\n      Penalties\n      37.98\n      2020-04-30\n    \n    \n      7.0\n      NaN\n      Head Office\n      Inactive\n      Pending Renewal\n      NaN\n      2015-02-28\n    \n    \n      10.0\n      South Rift Region\n      Nakuru\n      Active\n      Penalties\n      42.19\n      2020-12-31\n    \n    \n      12.0\n      South Rift Region\n      Nakuru\n      Active\n      P+I Current\n      65.34\n      2021-02-28\n    \n    \n      13.0\n      South Rift Region\n      Nakuru\n      Dormant\n      Pending Renewal\n      57.59\n      2016-02-29\n    \n  \n\n\n\n\n\n\nCode\nstatus_date_sorted = status.sort_values('Status Date')\nstatus_date_sorted.head()\n\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n    \n      Client ID\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      7.0\n      NaN\n      Head Office\n      Inactive\n      Pending Renewal\n      NaN\n      2015-02-28\n    \n    \n      2598.0\n      Nairobi Region\n      Rongai\n      Inactive\n      Pending Renewal\n      NaN\n      2015-02-28\n    \n    \n      1201.0\n      Nairobi Region\n      Ngong Road\n      Inactive\n      Pending Renewal\n      NaN\n      2015-02-28\n    \n    \n      2859.0\n      South Rift Region\n      Naivasha\n      Inactive\n      Pending Renewal\n      NaN\n      2015-06-30\n    \n    \n      1058.0\n      Nairobi Region\n      Kahawa\n      Dormant\n      Pending Renewal\n      NaN\n      2015-10-31\n    \n  \n\n\n\n\nBy defauld the sorting are in ascending order, we can also sort in descending order. To do this, we use ascending=False argument.\n\nstatus.sort_values('Client Score', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n    \n      Client ID\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      28468.0\n      North Rift Region\n      Kapenguria\n      Active\n      P+I Current\n      92.54\n      2021-02-28\n    \n    \n      74947.0\n      South Rift AgriBiz\n      Ndunyu Njeru\n      Active\n      P+I Current\n      92.17\n      2021-02-28\n    \n    \n      64738.0\n      Eastern Region\n      Mwingi\n      Active\n      P+I Current\n      92.02\n      2021-02-28\n    \n    \n      31851.0\n      Nairobi Region\n      Rongai\n      Active\n      P+I Current\n      91.34\n      2021-02-28\n    \n    \n      49324.0\n      South Rift AgriBiz\n      Maili Nne\n      Active\n      P+I Current\n      91.21\n      2021-02-28\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      86301.0\n      Nairobi Region\n      Ngong Road\n      New\n      P+I Current\n      NaN\n      2021-02-28\n    \n    \n      86303.0\n      Nairobi Region\n      Kahawa\n      New\n      Pending Renewal\n      NaN\n      2021-02-28\n    \n    \n      86308.0\n      Nairobi Region\n      Kahawa\n      New\n      P+I Current\n      NaN\n      2021-02-28\n    \n    \n      86313.0\n      North Rift Region\n      Eldoret2\n      New\n      Pending Renewal\n      NaN\n      2021-02-28\n    \n    \n      86654.0\n      Western Region\n      Bondo\n      New\n      Pending Renewal\n      NaN\n      2021-02-28\n    \n  \n\n40020 rows × 6 columns\n\n\n\n\nEXERCISE\nFor the dataframe (counties) we created sort the data frame as follows 1. name: descending 2. subCounty: ascending\n\n\n\n\n\n\nTip\n\n\n\nyou can use ascending=[False, True]."
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#duplicates",
    "href": "posts/2021-06-11-data_wrangling_cit.html#duplicates",
    "title": "Data Wrangling in Python",
    "section": "4.3. Duplicates",
    "text": "4.3. Duplicates\nDuplicates are rows that are identical to each other, ie, they have the same values. to check for duplicates we use df.duplicated this will return a boolean series we can use the boolean series to filter out the repeting values and we can use df.drop_duplicates to remove any duplicates in the data frame.\n\n\n\n\n\n\nImportant\n\n\n\nthe first occurence won’t be removed by default.\n\n\nto check for duplicates, we will first reset the index, this will make Client ID to be a column as it was before.\n\n\nCode\nstatus.reset_index(inplace=True)\n\n\n\n\nCode\nstatus.duplicated()\n\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n40015    False\n40016    False\n40017    False\n40018    False\n40019    False\nLength: 40020, dtype: bool\n\n\n\n\nCode\nstatus[status.duplicated()]\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      5051\n      11331.0\n      Nairobi Region\n      Ngong Road\n      Dormant\n      Penalties\n      23.84\n      2018-02-28\n    \n    \n      5052\n      11331.0\n      Nairobi Region\n      Ngong Road\n      Dormant\n      Penalties\n      23.84\n      2018-02-28\n    \n    \n      5053\n      11331.0\n      Nairobi Region\n      Ngong Road\n      Dormant\n      Penalties\n      23.84\n      2018-02-28\n    \n    \n      5054\n      11331.0\n      Nairobi Region\n      Ngong Road\n      Dormant\n      Penalties\n      23.84\n      2018-02-28\n    \n    \n      9624\n      21440.0\n      North Rift Region\n      Eldoret1\n      Dormant\n      Penalties\n      16.01\n      2018-08-31\n    \n    \n      9625\n      21440.0\n      North Rift Region\n      Eldoret1\n      Dormant\n      Penalties\n      16.01\n      2018-08-31\n    \n    \n      9626\n      21440.0\n      North Rift Region\n      Eldoret1\n      Dormant\n      Penalties\n      16.01\n      2018-08-31\n    \n    \n      9627\n      21440.0\n      North Rift Region\n      Eldoret1\n      Dormant\n      Penalties\n      16.01\n      2018-08-31\n    \n    \n      11194\n      24807.0\n      North Rift Region\n      Kitale\n      Dormant\n      Penalties\n      36.94\n      2019-02-28\n    \n    \n      11195\n      24807.0\n      North Rift Region\n      Kitale\n      Dormant\n      Penalties\n      36.94\n      2019-02-28\n    \n    \n      11196\n      24807.0\n      North Rift Region\n      Kitale\n      Dormant\n      Penalties\n      36.94\n      2019-02-28\n    \n    \n      11197\n      24807.0\n      North Rift Region\n      Kitale\n      Dormant\n      Penalties\n      36.94\n      2019-02-28\n    \n    \n      22397\n      49227.0\n      Coast Region\n      Voi\n      Dormant\n      Penalties\n      0.24\n      2020-02-29\n    \n    \n      22398\n      49227.0\n      Coast Region\n      Voi\n      Dormant\n      Penalties\n      0.24\n      2020-02-29\n    \n    \n      22399\n      49227.0\n      Coast Region\n      Voi\n      Dormant\n      Penalties\n      0.24\n      2020-02-29\n    \n    \n      22400\n      49227.0\n      Coast Region\n      Voi\n      Dormant\n      Penalties\n      0.24\n      2020-02-29\n    \n    \n      35401\n      76644.0\n      Western Region\n      Kisumu\n      Active\n      Penalties\n      48.92\n      2020-09-30\n    \n    \n      35402\n      76644.0\n      Western Region\n      Kisumu\n      Active\n      Penalties\n      48.92\n      2020-09-30\n    \n    \n      35403\n      76644.0\n      Western Region\n      Kisumu\n      Active\n      Penalties\n      48.92\n      2020-09-30\n    \n    \n      35404\n      76644.0\n      Western Region\n      Kisumu\n      Active\n      Penalties\n      48.92\n      2020-09-30\n    \n  \n\n\n\n\n\nEXERCISE\nFor the dataframe (counties), are there any duplicates in that dataframe"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#drop-duplicates",
    "href": "posts/2021-06-11-data_wrangling_cit.html#drop-duplicates",
    "title": "Data Wrangling in Python",
    "section": "4.4. Drop Duplicates",
    "text": "4.4. Drop Duplicates\ndf.drop_duplicates removes duplicates keeping first occurence by default.\n\n\nCode\nstatus.drop_duplicates(inplace=True)\n\n\n\nstatus.shape\n\n(40000, 7)"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#missing-values",
    "href": "posts/2021-06-11-data_wrangling_cit.html#missing-values",
    "title": "Data Wrangling in Python",
    "section": "4.5. Missing Values",
    "text": "4.5. Missing Values\nMajorly involed during data cleaning. Missing values are values that are missing, alaaa!. In this session we won’t go deeper on how to handle missing values. In python, to check for missing values we use df.isnull() or df.isna, to remove missing values we use df.dropna\n\nstatus.isna()\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      1\n      False\n      True\n      False\n      False\n      False\n      True\n      False\n    \n    \n      2\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      3\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      40015\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      40016\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      40017\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      40018\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      40019\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n  \n\n40000 rows × 7 columns\n\n\n\nThat returns a dataframe with boolean showing if a value is null\n\nstatus.isnull().sum()\n\nClient ID           0\nRegion             80\nOffice              0\nClient Status       0\nLoan Status         0\nClient Score     3300\nStatus Date         0\ndtype: int64\n\n\ndf.isnull().sum() is the most widely used way of cheking the number of missing values per column."
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#value-counts",
    "href": "posts/2021-06-11-data_wrangling_cit.html#value-counts",
    "title": "Data Wrangling in Python",
    "section": "5.1. Value counts",
    "text": "5.1. Value counts\nAs a data scientist it is good to check the counts of unique values, these is mainly used to check for imbalance at later states. To check for the number of unique values, we use df[col].value_counts() where col is the column name.\n\n\n\n\n\n\nNote\n\n\n\ndf[col] is selecting a column returning a series, where col is the column name\n\n\n\n\nCode\nstatus['Region'].value_counts()\n\n\nNairobi Region        9856\nNorth Rift Region     8506\nSouth Rift Region     6781\nCoast Region          4661\nSouth Rift AgriBiz    3848\nWestern Region        3360\nEastern Region        2252\nCentral AgriBiz        656\nName: Region, dtype: int64\n\n\nBy default .value_counts drops values which are missing values, we can include the counts of missing values by setting dropna=False\n\n\nCode\nstatus['Region'].value_counts(dropna=False)\n\n\nNairobi Region        9856\nNorth Rift Region     8506\nSouth Rift Region     6781\nCoast Region          4661\nSouth Rift AgriBiz    3848\nWestern Region        3360\nEastern Region        2252\nCentral AgriBiz        656\nNaN                     80\nName: Region, dtype: int64\n\n\n\nEXERCISE\nFor the dataframe (counties) Show the value counts of the name variable"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#data-filteration",
    "href": "posts/2021-06-11-data_wrangling_cit.html#data-filteration",
    "title": "Data Wrangling in Python",
    "section": "5.2. Data Filteration",
    "text": "5.2. Data Filteration\nIn pandas - we can use comparison operators to filter data meeting a certain condition, - to filter columns we can use df[[col1....coln], - to filter rows based on their index we can use - iloc[i] or loc[strn] for integer based or label based indexing respectively.\n\n\nCode\nstatus[status['Region'].isnull()]\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      1\n      7.0\n      NaN\n      Head Office\n      Inactive\n      Pending Renewal\n      NaN\n      2015-02-28\n    \n    \n      1993\n      4700.0\n      NaN\n      Head Office\n      Inactive\n      Pending Renewal\n      15.15\n      2020-09-30\n    \n    \n      2156\n      5064.0\n      NaN\n      Fraud\n      Dormant\n      Penalties\n      23.38\n      2020-08-31\n    \n    \n      2279\n      5322.0\n      NaN\n      Fraud\n      Dormant\n      Penalties\n      23.96\n      2020-08-31\n    \n    \n      2407\n      5621.0\n      NaN\n      Fraud\n      Dormant\n      Penalties\n      13.77\n      2020-08-31\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      38004\n      82121.0\n      NaN\n      Fraud\n      Active\n      P+I In Default\n      NaN\n      2020-12-31\n    \n    \n      38100\n      82284.0\n      NaN\n      Fraud\n      Active\n      P+I In Default\n      NaN\n      2020-12-31\n    \n    \n      38227\n      82560.0\n      NaN\n      Fraud\n      Active\n      P+I In Default\n      NaN\n      2020-12-31\n    \n    \n      38296\n      82709.0\n      NaN\n      Fraud\n      Active\n      P+I In Default\n      NaN\n      2020-12-31\n    \n    \n      38503\n      83150.0\n      NaN\n      Fraud\n      Active\n      P+I In Default\n      NaN\n      2020-12-31\n    \n  \n\n80 rows × 7 columns\n\n\n\nWe can use comparison operators for filtering values that meet a certain condition. - > - Greater than - < - Less than - >= - Greater or equal to - <= - Less than or equal to - == equal to - != not equal to\n\n\nCode\nstatus['Client Score']>90\n\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n40015    False\n40016    False\n40017    False\n40018    False\n40019    False\nName: Client Score, Length: 40000, dtype: bool\n\n\ndf[col]>value returns a boolean series, we can use the boolean series to filter out values in dataframe not meeting that condition.\n\n\nCode\nstatus[status['Client Score']>90]\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      2641\n      6150.0\n      South Rift Region\n      Kericho\n      Active\n      Pending Renewal\n      90.59\n      2021-02-28\n    \n    \n      8535\n      19100.0\n      South Rift Region\n      Litein\n      Active\n      P+I Current\n      90.85\n      2021-02-28\n    \n    \n      12657\n      27981.0\n      South Rift AgriBiz\n      Ndunyu Njeru\n      Active\n      Pending Renewal\n      90.58\n      2021-02-28\n    \n    \n      12876\n      28468.0\n      North Rift Region\n      Kapenguria\n      Active\n      P+I Current\n      92.54\n      2021-02-28\n    \n    \n      14405\n      31851.0\n      Nairobi Region\n      Rongai\n      Active\n      P+I Current\n      91.34\n      2021-02-28\n    \n    \n      22443\n      49324.0\n      South Rift AgriBiz\n      Maili Nne\n      Active\n      P+I Current\n      91.21\n      2021-02-28\n    \n    \n      26438\n      57810.0\n      South Rift AgriBiz\n      Maili Nne\n      Active\n      P+I In Default\n      90.92\n      2021-02-28\n    \n    \n      29804\n      64738.0\n      Eastern Region\n      Mwingi\n      Active\n      P+I Current\n      92.02\n      2021-02-28\n    \n    \n      30564\n      66344.0\n      Central AgriBiz\n      Githunguri\n      Active\n      P+I Current\n      90.33\n      2021-02-28\n    \n    \n      32576\n      70685.0\n      South Rift AgriBiz\n      Londiani\n      Active\n      P+I Current\n      90.77\n      2021-02-28\n    \n    \n      34593\n      74947.0\n      South Rift AgriBiz\n      Ndunyu Njeru\n      Active\n      P+I Current\n      92.17\n      2021-02-28\n    \n    \n      35239\n      76301.0\n      Nairobi Region\n      Kiambu\n      Active\n      P+I Current\n      90.04\n      2021-02-28\n    \n  \n\n\n\n\n\n5.2.1. Multiple conditions filterring\nWe can use logical conditions to support two or more expressions. In pandas we we can use the following operators:\n\n&: for logical and\n|: for logical or\n~: for logical not\n\n\n\n\n\n\n\nImportant\n\n\n\nParanthesis are very important. enclose expressions in a paranthesis (.....)\n\n\n\n\nCode\nstatus[(status['Client Score']>90) & (status['Loan Status']=='P+I In Default')]\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n    \n  \n  \n    \n      26438\n      57810.0\n      South Rift AgriBiz\n      Maili Nne\n      Active\n      P+I In Default\n      90.92\n      2021-02-28\n    \n  \n\n\n\n\n\n\nEXERCISE\nFor the dataframe (counties) Filter the data to select dataset where name is equal to 'Nakuru' & subCounty is not equal to 'Naivasha'"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#grouping-data",
    "href": "posts/2021-06-11-data_wrangling_cit.html#grouping-data",
    "title": "Data Wrangling in Python",
    "section": "5.3. Grouping Data",
    "text": "5.3. Grouping Data\nGrouping data is a critical step in data wrangling. in pandas we can group DataFrame using a mapper or by a Series of columns.\nA groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.\n\n\nCode\nstatus.groupby('Region')\n\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f2757be5a00>\n\n\nThe groupby returns an object, we can use the object to perform actions in the groups.\n\n\nCode\nstatus.groupby('Region').agg('sum')\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Client Score\n    \n    \n      Region\n      \n      \n    \n  \n  \n    \n      Central AgriBiz\n      36163955.0\n      20355.84\n    \n    \n      Coast Region\n      223714866.0\n      157343.10\n    \n    \n      Eastern Region\n      151738957.0\n      61644.78\n    \n    \n      Nairobi Region\n      306914995.0\n      309115.63\n    \n    \n      North Rift Region\n      326377934.0\n      270228.72\n    \n    \n      South Rift AgriBiz\n      201143618.0\n      168818.01\n    \n    \n      South Rift Region\n      270779301.0\n      211088.41\n    \n    \n      Western Region\n      229155555.0\n      107269.71\n    \n  \n\n\n\n\n\n#Collapse-hide\nstatus.groupby('Region').agg({'Client Score':np.sum})\n\n\n\n\n\n  \n    \n      \n      Client Score\n    \n    \n      Region\n      \n    \n  \n  \n    \n      Central AgriBiz\n      20355.84\n    \n    \n      Coast Region\n      157343.10\n    \n    \n      Eastern Region\n      61644.78\n    \n    \n      Nairobi Region\n      309115.63\n    \n    \n      North Rift Region\n      270228.72\n    \n    \n      South Rift AgriBiz\n      168818.01\n    \n    \n      South Rift Region\n      211088.41\n    \n    \n      Western Region\n      107269.71\n    \n  \n\n\n\n\n\n\nCode\nstatus.groupby('Region').agg('mean')\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Client Score\n    \n    \n      Region\n      \n      \n    \n  \n  \n    \n      Central AgriBiz\n      55127.980183\n      49.527591\n    \n    \n      Coast Region\n      47997.182150\n      35.792334\n    \n    \n      Eastern Region\n      67379.643428\n      31.244187\n    \n    \n      Nairobi Region\n      31139.914265\n      32.839226\n    \n    \n      North Rift Region\n      38370.319069\n      34.059582\n    \n    \n      South Rift AgriBiz\n      52272.250000\n      53.729475\n    \n    \n      South Rift Region\n      39932.060316\n      33.389499\n    \n    \n      Western Region\n      68201.058036\n      35.205025\n    \n  \n\n\n\n\nTo check for the size of each group, we can use size() method of a groupby object.\n\n\nCode\nstatus.groupby('Region').size()\n\n\nRegion\nCentral AgriBiz        656\nCoast Region          4661\nEastern Region        2252\nNairobi Region        9856\nNorth Rift Region     8506\nSouth Rift AgriBiz    3848\nSouth Rift Region     6781\nWestern Region        3360\ndtype: int64\n\n\n\n\nCode\nstatus.groupby('Region')['Office'].nunique()\n\n\nRegion\nCentral AgriBiz       2\nCoast Region          8\nEastern Region        7\nNairobi Region        8\nNorth Rift Region     9\nSouth Rift AgriBiz    6\nSouth Rift Region     8\nWestern Region        7\nName: Office, dtype: int64\n\n\nPivot table (derived from excel) is an advanced pandas grouping method. Here you can decide on the index, columns, values and the aggregate functions. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.\nthe aggfunc by default is mean. You can pass string aggregates eg 'mean', 'sum' ect or aggregate functions such as np.mean, np.sum etc\n\n\nCode\nstatus_pivot=pd.pivot_table(status, index='Office', columns='Region', values='Client Score', aggfunc='mean')\nstatus_pivot.head()\n\n\n\n\n\n\n  \n    \n      Region\n      Central AgriBiz\n      Coast Region\n      Eastern Region\n      Nairobi Region\n      North Rift Region\n      South Rift AgriBiz\n      South Rift Region\n      Western Region\n    \n    \n      Office\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Bomet\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      33.836232\n      NaN\n    \n    \n      Bondo\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      54.472500\n    \n    \n      Bungoma\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      32.724249\n    \n    \n      Busia\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      36.784428\n    \n    \n      Changamwe\n      NaN\n      36.532172\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nsetting margins=True will result to row aggregate and column column aggregate.\n\n\nCode\npd.pivot_table(status, index='Status Date', columns='Region', values='Client Score', aggfunc=np.mean, margins=True)\n\n\n\n\n\n\n  \n    \n      Region\n      Central AgriBiz\n      Coast Region\n      Eastern Region\n      Nairobi Region\n      North Rift Region\n      South Rift AgriBiz\n      South Rift Region\n      Western Region\n      All\n    \n    \n      Status Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2016-01-31\n      NaN\n      NaN\n      NaN\n      44.733333\n      NaN\n      NaN\n      NaN\n      NaN\n      44.733333\n    \n    \n      2016-02-29\n      NaN\n      NaN\n      NaN\n      35.430541\n      NaN\n      NaN\n      37.207143\n      NaN\n      35.713182\n    \n    \n      2016-03-31\n      NaN\n      NaN\n      NaN\n      34.614533\n      NaN\n      NaN\n      33.599565\n      NaN\n      34.376327\n    \n    \n      2016-04-30\n      NaN\n      NaN\n      NaN\n      30.816381\n      NaN\n      NaN\n      33.833704\n      NaN\n      31.433561\n    \n    \n      2016-05-31\n      NaN\n      NaN\n      NaN\n      37.960885\n      NaN\n      NaN\n      41.225000\n      NaN\n      38.681241\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2020-11-30\n      47.642000\n      34.752153\n      31.791702\n      34.263287\n      38.249737\n      50.445581\n      33.603857\n      32.730902\n      34.959187\n    \n    \n      2020-12-31\n      43.470714\n      37.190167\n      30.627154\n      35.245165\n      39.696990\n      49.850078\n      34.649405\n      38.350320\n      37.861864\n    \n    \n      2021-01-31\n      50.570377\n      36.696154\n      34.157852\n      38.617857\n      41.534072\n      54.294533\n      36.930952\n      35.335667\n      41.915679\n    \n    \n      2021-02-28\n      55.504430\n      48.855651\n      41.270897\n      47.413198\n      49.778757\n      56.811797\n      47.444950\n      45.280237\n      49.599350\n    \n    \n      All\n      49.527591\n      35.792334\n      31.244187\n      32.839226\n      34.059582\n      53.729475\n      33.389499\n      35.205025\n      35.642344\n    \n  \n\n63 rows × 9 columns"
  },
  {
    "objectID": "posts/2021-06-11-data_wrangling_cit.html#combining-data",
    "href": "posts/2021-06-11-data_wrangling_cit.html#combining-data",
    "title": "Data Wrangling in Python",
    "section": "5.4. Combining Data",
    "text": "5.4. Combining Data\nthe are varies ways of combining data sets - append - add rows to the end of the caller. - concatenate - merge - inner join - left join - right join\nType of merge to be performed. - left: use only keys from left frame, similar to a SQL left outer join; preserve key order. - right: use only keys from right frame, similar to a SQL right outer join; preserve key order. - outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. - inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. - cross: creates the cartesian product from both frames, preserves the order of the left keys.\nWe use df.merge/pd.merge to import columns from other datasets.\n\n\nCode\ncounties = pd.read_csv('data/kenya_subcounty.csv')\ncounties.head()\n\n\n\n\n\n\n  \n    \n      \n      name\n      subCounty\n    \n  \n  \n    \n      0\n      Mombasa\n      Changamwe\n    \n    \n      1\n      Mombasa\n      Jomvu\n    \n    \n      2\n      Mombasa\n      Kisauni\n    \n    \n      3\n      Mombasa\n      Likoni\n    \n    \n      4\n      Mombasa\n      Mvita\n    \n  \n\n\n\n\nWe can merge status and counties using Office and subCounty (this is because that is the key present in both columns. Since we have diffent column names in both data frames, we have to use left_on and right_on to specify the columns.\nif the columns are the same on both columns, we can use on argument.\n\n\nCode\ncombined_ = status.merge(counties, left_on='Office', right_on='subCounty')\ncombined_.head()\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n      name\n      subCounty\n    \n  \n  \n    \n      0\n      33.0\n      South Rift Region\n      Naivasha\n      Dormant\n      Penalties\n      40.00\n      2020-06-30\n      Nakuru\n      Naivasha\n    \n    \n      1\n      34.0\n      South Rift Region\n      Naivasha\n      Dormant\n      Pending Renewal\n      40.23\n      2016-03-31\n      Nakuru\n      Naivasha\n    \n    \n      2\n      206.0\n      South Rift Region\n      Naivasha\n      Active\n      P+I Current\n      76.87\n      2021-02-28\n      Nakuru\n      Naivasha\n    \n    \n      3\n      615.0\n      South Rift Region\n      Naivasha\n      Active\n      P+I Current\n      58.43\n      2021-02-28\n      Nakuru\n      Naivasha\n    \n    \n      4\n      702.0\n      South Rift Region\n      Naivasha\n      Dormant\n      Pending Renewal\n      43.73\n      2016-08-31\n      Nakuru\n      Naivasha\n    \n  \n\n\n\n\nAnother method is using the pd.merge\n\n\nCode\ncombined_2 = pd.merge(status, counties, left_on='Office', right_on='subCounty')\ncombined_2.head()\n\n\n\n\n\n\n  \n    \n      \n      Client ID\n      Region\n      Office\n      Client Status\n      Loan Status\n      Client Score\n      Status Date\n      name\n      subCounty\n    \n  \n  \n    \n      0\n      33.0\n      South Rift Region\n      Naivasha\n      Dormant\n      Penalties\n      40.00\n      2020-06-30\n      Nakuru\n      Naivasha\n    \n    \n      1\n      34.0\n      South Rift Region\n      Naivasha\n      Dormant\n      Pending Renewal\n      40.23\n      2016-03-31\n      Nakuru\n      Naivasha\n    \n    \n      2\n      206.0\n      South Rift Region\n      Naivasha\n      Active\n      P+I Current\n      76.87\n      2021-02-28\n      Nakuru\n      Naivasha\n    \n    \n      3\n      615.0\n      South Rift Region\n      Naivasha\n      Active\n      P+I Current\n      58.43\n      2021-02-28\n      Nakuru\n      Naivasha\n    \n    \n      4\n      702.0\n      South Rift Region\n      Naivasha\n      Dormant\n      Pending Renewal\n      43.73\n      2016-08-31\n      Nakuru\n      Naivasha\n    \n  \n\n\n\n\n\nEXERCISE\nFor the dataframe (counties) do a full outer join with status dataframe, how many rows does it have?"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html",
    "href": "posts/2020-07-26-working with dates and times in python.html",
    "title": "Working With Dates and Times in Python",
    "section": "",
    "text": "Hurricanes (also known as cyclones or typhoons) hit the U.S. state of Florida several times per year. To start off we will work with date objects in Python, starting with the dates of every hurricane to hit Florida since 1950.\n\n\n\n\nHurricane Andrew, which hit Florida on August 24, 1992, was one of the costliest and deadliest hurricanes in US history. Which day of the week did it make landfall?\n\nimport datetime\n\n\nhurricane_andrew = datetime.date(1992, 8, 24)\nhurricane_andrew.weekday()\n\n0\n\n\n\n\n\n\nflorida_hurricane_dates = [datetime.date(1950, 8, 31), datetime.date(1950, 9, 5), datetime.date(1950, 10, 18),\n                           datetime.date(1950, 10, 21), datetime.date(1951, 5, 18), datetime.date(1951, 10, 2),\n                           datetime.date(1952, 2, 3), datetime.date(1952, 8, 30), datetime.date(1953, 6, 6),\n                           datetime.date(1953, 8, 29), datetime.date(1953, 9, 20), datetime.date(1953, 9, 26),\n                           datetime.date(1953, 10, 9), datetime.date(1955, 8, 21), datetime.date(1956, 7, 6),\n                           datetime.date(1956, 9, 24), datetime.date(1956, 10, 15), datetime.date(1957, 6, 8),\n                           datetime.date(1957, 9, 8), datetime.date(1958, 9, 4), datetime.date(1959, 6, 18),\n                           datetime.date(1959, 10, 8), datetime.date(1959, 10, 18), datetime.date(1960, 7, 29),\n                           datetime.date(1960, 9, 10), datetime.date(1960, 9, 15), datetime.date(1960, 9, 23), \n                           datetime.date(1961, 9, 11), datetime.date(1961, 10, 29), datetime.date(1962, 8, 26),\n                           datetime.date(1963, 10, 21), datetime.date(1964, 6, 6), datetime.date(1964, 8, 27),\n                           datetime.date(1964, 9, 10), datetime.date(1964, 9, 20), datetime.date(1964, 10, 5),\n                           datetime.date(1964, 10, 14),\n datetime.date(1965, 6, 15),\n datetime.date(1965, 9, 8),\n datetime.date(1965, 9, 30),\n datetime.date(1966, 6, 9),\n datetime.date(1966, 6, 30),\n datetime.date(1966, 7, 24),\n datetime.date(1966, 10, 4),\n datetime.date(1968, 6, 4),\n datetime.date(1968, 6, 18),\n datetime.date(1968, 7, 5),\n datetime.date(1968, 8, 10),\n datetime.date(1968, 8, 28),\n datetime.date(1968, 9, 26),\n datetime.date(1968, 10, 19),\n datetime.date(1969, 6, 9),\n datetime.date(1969, 8, 18),\n datetime.date(1969, 8, 29),\n datetime.date(1969, 9, 7),\n datetime.date(1969, 9, 21),\n datetime.date(1969, 10, 1),\n datetime.date(1969, 10, 2),\n datetime.date(1969, 10, 21),\n datetime.date(1970, 5, 25),\n datetime.date(1970, 7, 22),\n datetime.date(1970, 8, 6),\n datetime.date(1970, 9, 13),\n datetime.date(1970, 9, 27),\n datetime.date(1971, 8, 10),\n datetime.date(1971, 8, 13),\n datetime.date(1971, 8, 29),\n datetime.date(1971, 9, 1),\n datetime.date(1971, 9, 16),\n datetime.date(1971, 10, 13),\n datetime.date(1972, 5, 28),\n datetime.date(1972, 6, 19),\n datetime.date(1972, 9, 5),\n datetime.date(1973, 6, 7),\n datetime.date(1973, 6, 23),\n datetime.date(1973, 9, 3),\n datetime.date(1973, 9, 25),\n datetime.date(1974, 6, 25),\n datetime.date(1974, 9, 8),\n datetime.date(1974, 9, 27),\n datetime.date(1974, 10, 7),\n datetime.date(1975, 6, 27),\n datetime.date(1975, 7, 29),\n datetime.date(1975, 9, 23),\n datetime.date(1975, 10, 1),\n datetime.date(1975, 10, 16),\n datetime.date(1976, 5, 23),\n datetime.date(1976, 6, 11),\n datetime.date(1976, 8, 19),\n datetime.date(1976, 9, 13),\n datetime.date(1977, 8, 27),\n datetime.date(1977, 9, 5),\n datetime.date(1978, 6, 22),\n datetime.date(1979, 7, 11),\n datetime.date(1979, 9, 3),\n datetime.date(1979, 9, 12),\n datetime.date(1979, 9, 24),\n datetime.date(1980, 8, 7),\n datetime.date(1980, 11, 18),\n datetime.date(1981, 8, 17),\n datetime.date(1982, 6, 18),\n datetime.date(1982, 9, 11),\n datetime.date(1983, 8, 28),\n datetime.date(1984, 9, 9),\n datetime.date(1984, 9, 27),\n datetime.date(1984, 10, 26),\n datetime.date(1985, 7, 23),\n datetime.date(1985, 8, 15),\n datetime.date(1985, 10, 10),\n datetime.date(1985, 11, 21),\n datetime.date(1986, 6, 26),\n datetime.date(1986, 8, 13),\n datetime.date(1987, 8, 14),\n datetime.date(1987, 9, 7),\n datetime.date(1987, 10, 12),\n datetime.date(1987, 11, 4),\n datetime.date(1988, 5, 30),\n datetime.date(1988, 8, 4),\n datetime.date(1988, 8, 13),\n datetime.date(1988, 8, 23),\n datetime.date(1988, 9, 4),\n datetime.date(1988, 9, 10),\n datetime.date(1988, 9, 13),\n datetime.date(1988, 11, 23),\n datetime.date(1989, 9, 22),\n datetime.date(1990, 5, 25),\n datetime.date(1990, 10, 9),\n datetime.date(1990, 10, 12),\n datetime.date(1991, 6, 30),\n datetime.date(1991, 10, 16),\n datetime.date(1992, 6, 25),\n datetime.date(1992, 8, 24),\n datetime.date(1992, 9, 29),\n datetime.date(1993, 6, 1),\n datetime.date(1994, 7, 3),\n datetime.date(1994, 8, 15),\n datetime.date(1994, 10, 2),\n datetime.date(1994, 11, 16),\n datetime.date(1995, 6, 5),\n datetime.date(1995, 7, 27),\n datetime.date(1995, 8, 2),\n datetime.date(1995, 8, 23),\n datetime.date(1995, 10, 4),\n datetime.date(1996, 7, 11),\n datetime.date(1996, 9, 2),\n datetime.date(1996, 10, 8),\n datetime.date(1996, 10, 18),\n datetime.date(1997, 7, 19),\n datetime.date(1998, 9, 3),\n datetime.date(1998, 9, 20),\n datetime.date(1998, 9, 25),\n datetime.date(1998, 11, 5),\n datetime.date(1999, 8, 29),\n datetime.date(1999, 9, 15),\n datetime.date(1999, 9, 21),\n datetime.date(1999, 10, 15),\n datetime.date(2000, 8, 23),\n datetime.date(2000, 9, 9),\n datetime.date(2000, 9, 18),\n datetime.date(2000, 9, 22),\n datetime.date(2000, 10, 3),\n datetime.date(2001, 6, 12),\n datetime.date(2001, 8, 6),\n datetime.date(2001, 9, 14),\n datetime.date(2001, 11, 5),\n datetime.date(2002, 7, 13),\n datetime.date(2002, 8, 4),\n datetime.date(2002, 9, 4),\n datetime.date(2002, 9, 14),\n datetime.date(2002, 9, 26),\n datetime.date(2002, 10, 3),\n datetime.date(2002, 10, 11),\n datetime.date(2003, 4, 20),\n datetime.date(2003, 6, 30),\n datetime.date(2003, 7, 25),\n datetime.date(2003, 8, 14),\n datetime.date(2003, 8, 30),\n datetime.date(2003, 9, 6),\n datetime.date(2003, 9, 13),\n datetime.date(2004, 8, 12),\n datetime.date(2004, 8, 13),\n datetime.date(2004, 9, 5),\n datetime.date(2004, 9, 13),\n datetime.date(2004, 9, 16),\n datetime.date(2004, 10, 10),\n datetime.date(2005, 6, 11),\n datetime.date(2005, 7, 6),\n datetime.date(2005, 7, 10),\n datetime.date(2005, 8, 25),\n datetime.date(2005, 9, 12),\n datetime.date(2005, 9, 20),\n datetime.date(2005, 10, 5),\n datetime.date(2005, 10, 24),\n datetime.date(2006, 6, 13),\n datetime.date(2006, 8, 30),\n datetime.date(2007, 5, 9),\n datetime.date(2007, 6, 2),\n datetime.date(2007, 8, 23),\n datetime.date(2007, 9, 8),\n datetime.date(2007, 9, 13),\n datetime.date(2007, 9, 22),\n datetime.date(2007, 10, 31),\n datetime.date(2007, 12, 13),\n datetime.date(2008, 7, 16),\n datetime.date(2008, 7, 22),\n datetime.date(2008, 8, 18),\n datetime.date(2008, 8, 31),\n datetime.date(2008, 9, 2),\n datetime.date(2009, 8, 16),\n datetime.date(2009, 8, 21),\n datetime.date(2009, 11, 9),\n datetime.date(2010, 6, 30),\n datetime.date(2010, 7, 23),\n datetime.date(2010, 8, 10),\n datetime.date(2010, 8, 31),\n datetime.date(2010, 9, 29),\n datetime.date(2011, 7, 18),\n datetime.date(2011, 8, 25),\n datetime.date(2011, 9, 3),\n datetime.date(2011, 10, 28),\n datetime.date(2011, 11, 9),\n datetime.date(2012, 5, 28),\n datetime.date(2012, 6, 23),\n datetime.date(2012, 8, 25),\n datetime.date(2012, 10, 25),\n datetime.date(2015, 8, 30),\n datetime.date(2015, 10, 1),\n datetime.date(2016, 6, 6),\n datetime.date(2016, 9, 1),\n datetime.date(2016, 9, 14),\n datetime.date(2016, 10, 7),\n datetime.date(2017, 6, 21),\n datetime.date(2017, 7, 31),\n datetime.date(2017, 9, 10),\n datetime.date(2017, 10, 29)]\n\nAtlantic hurricane season officially begins on June 1. How many hurricanes since 1950 have made landfall in Florida before the official start of hurricane season?\n\nearly_hurricanes = 0\n\n# We loop over dates\nfor hurricane in florida_hurricane_dates:\n    # Check if the month if before june\n    if hurricane.month < 6:\n        early_hurricanes+=1\nearly_hurricanes\n\n10\n\n\n\n\n\n\n\n\n\n# Create a date object for May 9th, 2007\nstart = datetime.date(2007, 5, 9)\n\n# Create a date object for December 13th, 2007\nend = datetime.date(2007, 12, 13)\n\n# Subtract the two dates and print the number of days\n(end - start).days\n\n218\n\n\n\n\n\nHurricanes can make landfall in Florida throughout the year. As we’ve already discussed, some months are more hurricane-prone than others. let’s see how hurricanes in Florida were distributed across months throughout the year. We’ve created a dictionary called hurricanes_each_month to hold counts and set the initial counts to zero. We will loop over the list of hurricanes, incrementing the correct month in hurricanes_each_month as we go, and then print the result.\n\n# A dictionary to count hurricanes per calendar month\nhurricanes_each_month = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6:0,7: 0, 8:0, 9:0, 10:0, 11:0, 12:0}\n\n# Loop over all hurricanes\nfor hurricane in florida_hurricane_dates:\n    # Pull out the month\n    month = hurricane.month\n    # Increment the count in your dictionary by one\n    hurricanes_each_month[month] += 1\n    \nprint(hurricanes_each_month)\n\n{1: 0, 2: 1, 3: 0, 4: 1, 5: 8, 6: 32, 7: 21, 8: 49, 9: 70, 10: 43, 11: 9, 12: 1}\n\n\n\n\n\n\n\n\nLet’s see what event was recorded first in the Florida hurricane data set. We will format the earliest date in the florida_hurriance_dates list in two ways so we can decide which one you want to use: either the ISO standard or the typical US style.\n\n# Assign the earliest date to first_date\nfirst_date = min(florida_hurricane_dates)\n\n# Convert to ISO and US formats\niso = \"Our earliest hurricane date: \" + first_date.isoformat()\nus = \"Our earliest hurricane date: \" + first_date.strftime(\"%m/%d/%Y\")\n\nprint(\"ISO: \" + iso)\nprint(\"US: \" + us)\n\nISO: Our earliest hurricane date: 1950-08-31\nUS: Our earliest hurricane date: 08/31/1950\n\n\nprinting out the same date, August 26, 1992 (the day that Hurricane Andrew made landfall in Florida), in a number of different ways, to practice using the .strftime() method.\n\n# Create a date object\nandrew = datetime.date(1992, 8, 26)\n\n# Print the date in the format 'YYYY-DDD'\nprint(andrew.strftime('%Y-%j'))\n\n1992-239"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#dates-and-times",
    "href": "posts/2020-07-26-working with dates and times in python.html#dates-and-times",
    "title": "Working With Dates and Times in Python",
    "section": "Dates and times",
    "text": "Dates and times\n\nCreating Datetimes by hand\n\n# Create a datetime object\ndt = datetime.datetime(year=2017, month=10, day=1, hour=15, minute=26, second=26)\n\n# Print the results in ISO 8601 format\nprint(dt.isoformat())\n\n2017-10-01T15:26:26\n\n\n\n\n# Create a datetime object\ndt = datetime.datetime(2017, 12, 31, 15, 19, 13)\n\n# Replace the year with 1917\ndt_old = dt.replace(year=1917)\n\n# Print the results in ISO 8601 format\nprint(dt_old)\n\n1917-12-31 15:19:13\n\n\n\n# Pull out the start of the first trip\nfirst_start = onebike_datetimes[0]['start']\n\n# Format to feed to strftime()\nfmt = \"%Y-%m-%dT%H:%M:%S\"\n\n# Print out date with .isoformat(), then with .strftime() to compare\nprint(first_start.isoformat())\nprint(first_start.strftime(fmt))\n\nNameError: name 'onebike_datetimes' is not defined\n\n\n\n\nUnix timestamps\nDatetimes are sometimes stored as Unix timestamps: the number of seconds since January 1, 1970. This is especially common with computer infrastructure, like the log files that websites keep when they get visitors.\nThe largest number that some older computers can hold in one variable is 2147483648, which as a Unix timestamp is in January 2038. On that day, many computers which haven’t been upgraded will fail. Hopefully, none of them are running anything critical!\n\n# Starting timestamps\ntimestamps = [1514665153, 1514664543]\n\n# Datetime objects\ndts = []\n\n# Loop\nfor ts in timestamps:\n  dts.append(datetime.datetime.fromtimestamp(ts))\n  \n# Print results\nprint(dts)\n\n[datetime.datetime(2017, 12, 30, 23, 19, 13), datetime.datetime(2017, 12, 30, 23, 9, 3)]"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#working-with-durations",
    "href": "posts/2020-07-26-working with dates and times in python.html#working-with-durations",
    "title": "Working With Dates and Times in Python",
    "section": "Working with durations",
    "text": "Working with durations\n\nTurning pairs of datetimes into durations\n\nRemember that timedelta objects are represented in Python as a number of days and seconds of elapsed time. Be careful not to use .seconds on a timedelta object, since you’ll just get the number of seconds without the days!\n\n\nonebike_datetimes = [{'end': datetime.datetime(2017, 10, 1, 15, 26, 26),\n  'start': datetime.datetime(2017, 10, 1, 15, 23, 25)},\n {'end': datetime.datetime(2017, 10, 1, 17, 49, 59),\n  'start': datetime.datetime(2017, 10, 1, 15, 42, 57)},\n {'end': datetime.datetime(2017, 10, 2, 6, 42, 53),\n  'start': datetime.datetime(2017, 10, 2, 6, 37, 10)},\n {'end': datetime.datetime(2017, 10, 2, 9, 18, 3),\n  'start': datetime.datetime(2017, 10, 2, 8, 56, 45)},\n {'end': datetime.datetime(2017, 10, 2, 18, 45, 5),\n  'start': datetime.datetime(2017, 10, 2, 18, 23, 48)},\n {'end': datetime.datetime(2017, 10, 2, 19, 10, 54),\n  'start': datetime.datetime(2017, 10, 2, 18, 48, 8)},\n {'end': datetime.datetime(2017, 10, 2, 19, 31, 45),\n  'start': datetime.datetime(2017, 10, 2, 19, 18, 10)},\n {'end': datetime.datetime(2017, 10, 2, 19, 46, 37),\n  'start': datetime.datetime(2017, 10, 2, 19, 37, 32)},\n {'end': datetime.datetime(2017, 10, 3, 8, 32, 27),\n  'start': datetime.datetime(2017, 10, 3, 8, 24, 16)},\n {'end': datetime.datetime(2017, 10, 3, 18, 27, 46),\n  'start': datetime.datetime(2017, 10, 3, 18, 17, 7)},\n {'end': datetime.datetime(2017, 10, 3, 19, 52, 8),\n  'start': datetime.datetime(2017, 10, 3, 19, 24, 10)},\n {'end': datetime.datetime(2017, 10, 3, 20, 23, 52),\n  'start': datetime.datetime(2017, 10, 3, 20, 17, 6)},\n {'end': datetime.datetime(2017, 10, 3, 20, 57, 10),\n  'start': datetime.datetime(2017, 10, 3, 20, 45, 21)},\n {'end': datetime.datetime(2017, 10, 4, 7, 13, 31),\n  'start': datetime.datetime(2017, 10, 4, 7, 4, 57)},\n {'end': datetime.datetime(2017, 10, 4, 7, 21, 54),\n  'start': datetime.datetime(2017, 10, 4, 7, 13, 42)},\n {'end': datetime.datetime(2017, 10, 4, 14, 50),\n  'start': datetime.datetime(2017, 10, 4, 14, 22, 12)},\n {'end': datetime.datetime(2017, 10, 4, 15, 44, 49),\n  'start': datetime.datetime(2017, 10, 4, 15, 7, 27)},\n {'end': datetime.datetime(2017, 10, 4, 16, 32, 33),\n  'start': datetime.datetime(2017, 10, 4, 15, 46, 41)},\n {'end': datetime.datetime(2017, 10, 4, 16, 46, 59),\n  'start': datetime.datetime(2017, 10, 4, 16, 34, 44)},\n {'end': datetime.datetime(2017, 10, 4, 17, 31, 36),\n  'start': datetime.datetime(2017, 10, 4, 17, 26, 6)},\n {'end': datetime.datetime(2017, 10, 4, 17, 50, 41),\n  'start': datetime.datetime(2017, 10, 4, 17, 42, 3)},\n {'end': datetime.datetime(2017, 10, 5, 8, 12, 55),\n  'start': datetime.datetime(2017, 10, 5, 7, 49, 2)},\n {'end': datetime.datetime(2017, 10, 5, 8, 29, 45),\n  'start': datetime.datetime(2017, 10, 5, 8, 26, 21)},\n {'end': datetime.datetime(2017, 10, 5, 8, 38, 31),\n  'start': datetime.datetime(2017, 10, 5, 8, 33, 27)},\n {'end': datetime.datetime(2017, 10, 5, 16, 51, 52),\n  'start': datetime.datetime(2017, 10, 5, 16, 35, 35)},\n {'end': datetime.datetime(2017, 10, 5, 18, 16, 50),\n  'start': datetime.datetime(2017, 10, 5, 17, 53, 31)},\n {'end': datetime.datetime(2017, 10, 6, 8, 38, 1),\n  'start': datetime.datetime(2017, 10, 6, 8, 17, 17)},\n {'end': datetime.datetime(2017, 10, 6, 11, 50, 38),\n  'start': datetime.datetime(2017, 10, 6, 11, 39, 40)},\n {'end': datetime.datetime(2017, 10, 6, 13, 13, 14),\n  'start': datetime.datetime(2017, 10, 6, 12, 59, 54)},\n {'end': datetime.datetime(2017, 10, 6, 14, 14, 56),\n  'start': datetime.datetime(2017, 10, 6, 13, 43, 5)},\n {'end': datetime.datetime(2017, 10, 6, 15, 9, 26),\n  'start': datetime.datetime(2017, 10, 6, 14, 28, 15)},\n {'end': datetime.datetime(2017, 10, 6, 16, 12, 34),\n  'start': datetime.datetime(2017, 10, 6, 15, 50, 10)},\n {'end': datetime.datetime(2017, 10, 6, 16, 39, 31),\n  'start': datetime.datetime(2017, 10, 6, 16, 32, 16)},\n {'end': datetime.datetime(2017, 10, 6, 16, 48, 39),\n  'start': datetime.datetime(2017, 10, 6, 16, 44, 8)},\n {'end': datetime.datetime(2017, 10, 6, 17, 9, 3),\n  'start': datetime.datetime(2017, 10, 6, 16, 53, 43)},\n {'end': datetime.datetime(2017, 10, 7, 11, 53, 6),\n  'start': datetime.datetime(2017, 10, 7, 11, 38, 55)},\n {'end': datetime.datetime(2017, 10, 7, 14, 7, 5),\n  'start': datetime.datetime(2017, 10, 7, 14, 3, 36)},\n {'end': datetime.datetime(2017, 10, 7, 14, 27, 36),\n  'start': datetime.datetime(2017, 10, 7, 14, 20, 3)},\n {'end': datetime.datetime(2017, 10, 7, 14, 44, 51),\n  'start': datetime.datetime(2017, 10, 7, 14, 30, 50)},\n {'end': datetime.datetime(2017, 10, 8, 0, 30, 48),\n  'start': datetime.datetime(2017, 10, 8, 0, 28, 26)},\n {'end': datetime.datetime(2017, 10, 8, 11, 33, 24),\n  'start': datetime.datetime(2017, 10, 8, 11, 16, 21)},\n {'end': datetime.datetime(2017, 10, 8, 13, 1, 29),\n  'start': datetime.datetime(2017, 10, 8, 12, 37, 3)},\n {'end': datetime.datetime(2017, 10, 8, 13, 57, 53),\n  'start': datetime.datetime(2017, 10, 8, 13, 30, 37)},\n {'end': datetime.datetime(2017, 10, 8, 15, 7, 19),\n  'start': datetime.datetime(2017, 10, 8, 14, 16, 40)},\n {'end': datetime.datetime(2017, 10, 8, 15, 50, 1),\n  'start': datetime.datetime(2017, 10, 8, 15, 23, 50)},\n {'end': datetime.datetime(2017, 10, 8, 16, 17, 42),\n  'start': datetime.datetime(2017, 10, 8, 15, 54, 12)},\n {'end': datetime.datetime(2017, 10, 8, 16, 35, 18),\n  'start': datetime.datetime(2017, 10, 8, 16, 28, 52)},\n {'end': datetime.datetime(2017, 10, 8, 23, 33, 41),\n  'start': datetime.datetime(2017, 10, 8, 23, 8, 14)},\n {'end': datetime.datetime(2017, 10, 8, 23, 45, 11),\n  'start': datetime.datetime(2017, 10, 8, 23, 34, 49)},\n {'end': datetime.datetime(2017, 10, 9, 0, 10, 57),\n  'start': datetime.datetime(2017, 10, 8, 23, 46, 47)},\n {'end': datetime.datetime(2017, 10, 9, 0, 36, 40),\n  'start': datetime.datetime(2017, 10, 9, 0, 12, 58)},\n {'end': datetime.datetime(2017, 10, 9, 0, 53, 33),\n  'start': datetime.datetime(2017, 10, 9, 0, 37, 2)},\n {'end': datetime.datetime(2017, 10, 9, 1, 48, 13),\n  'start': datetime.datetime(2017, 10, 9, 1, 23, 29)},\n {'end': datetime.datetime(2017, 10, 9, 2, 13, 35),\n  'start': datetime.datetime(2017, 10, 9, 1, 49, 25)},\n {'end': datetime.datetime(2017, 10, 9, 2, 29, 40),\n  'start': datetime.datetime(2017, 10, 9, 2, 14, 11)},\n {'end': datetime.datetime(2017, 10, 9, 13, 13, 25),\n  'start': datetime.datetime(2017, 10, 9, 13, 4, 32)},\n {'end': datetime.datetime(2017, 10, 9, 14, 38, 55),\n  'start': datetime.datetime(2017, 10, 9, 14, 30, 10)},\n {'end': datetime.datetime(2017, 10, 9, 15, 11, 30),\n  'start': datetime.datetime(2017, 10, 9, 15, 6, 47)},\n {'end': datetime.datetime(2017, 10, 9, 16, 45, 38),\n  'start': datetime.datetime(2017, 10, 9, 16, 43, 25)},\n {'end': datetime.datetime(2017, 10, 10, 15, 51, 24),\n  'start': datetime.datetime(2017, 10, 10, 15, 32, 58)},\n {'end': datetime.datetime(2017, 10, 10, 17, 3, 47),\n  'start': datetime.datetime(2017, 10, 10, 16, 47, 55)},\n {'end': datetime.datetime(2017, 10, 10, 18, 0, 18),\n  'start': datetime.datetime(2017, 10, 10, 17, 51, 5)},\n {'end': datetime.datetime(2017, 10, 10, 18, 19, 11),\n  'start': datetime.datetime(2017, 10, 10, 18, 8, 12)},\n {'end': datetime.datetime(2017, 10, 10, 19, 14, 32),\n  'start': datetime.datetime(2017, 10, 10, 19, 9, 35)},\n {'end': datetime.datetime(2017, 10, 10, 19, 23, 8),\n  'start': datetime.datetime(2017, 10, 10, 19, 17, 11)},\n {'end': datetime.datetime(2017, 10, 10, 19, 44, 40),\n  'start': datetime.datetime(2017, 10, 10, 19, 28, 11)},\n {'end': datetime.datetime(2017, 10, 10, 20, 11, 54),\n  'start': datetime.datetime(2017, 10, 10, 19, 55, 35)},\n {'end': datetime.datetime(2017, 10, 10, 22, 33, 23),\n  'start': datetime.datetime(2017, 10, 10, 22, 20, 43)},\n {'end': datetime.datetime(2017, 10, 11, 4, 59, 22),\n  'start': datetime.datetime(2017, 10, 11, 4, 40, 52)},\n {'end': datetime.datetime(2017, 10, 11, 6, 40, 13),\n  'start': datetime.datetime(2017, 10, 11, 6, 28, 58)},\n {'end': datetime.datetime(2017, 10, 11, 17, 1, 14),\n  'start': datetime.datetime(2017, 10, 11, 16, 41, 7)},\n {'end': datetime.datetime(2017, 10, 12, 8, 35, 3),\n  'start': datetime.datetime(2017, 10, 12, 8, 8, 30)},\n {'end': datetime.datetime(2017, 10, 12, 8, 59, 50),\n  'start': datetime.datetime(2017, 10, 12, 8, 47, 2)},\n {'end': datetime.datetime(2017, 10, 12, 13, 37, 45),\n  'start': datetime.datetime(2017, 10, 12, 13, 13, 39)},\n {'end': datetime.datetime(2017, 10, 12, 13, 48, 17),\n  'start': datetime.datetime(2017, 10, 12, 13, 40, 12)},\n {'end': datetime.datetime(2017, 10, 12, 13, 53, 16),\n  'start': datetime.datetime(2017, 10, 12, 13, 49, 56)},\n {'end': datetime.datetime(2017, 10, 12, 14, 39, 57),\n  'start': datetime.datetime(2017, 10, 12, 14, 33, 18)},\n {'end': datetime.datetime(2017, 10, 13, 15, 59, 41),\n  'start': datetime.datetime(2017, 10, 13, 15, 55, 39)},\n {'end': datetime.datetime(2017, 10, 17, 18, 1, 38),\n  'start': datetime.datetime(2017, 10, 17, 17, 58, 48)},\n {'end': datetime.datetime(2017, 10, 19, 20, 29, 15),\n  'start': datetime.datetime(2017, 10, 19, 20, 21, 45)},\n {'end': datetime.datetime(2017, 10, 19, 21, 29, 37),\n  'start': datetime.datetime(2017, 10, 19, 21, 11, 39)},\n {'end': datetime.datetime(2017, 10, 19, 21, 47, 23),\n  'start': datetime.datetime(2017, 10, 19, 21, 30, 1)},\n {'end': datetime.datetime(2017, 10, 19, 21, 57, 7),\n  'start': datetime.datetime(2017, 10, 19, 21, 47, 34)},\n {'end': datetime.datetime(2017, 10, 19, 22, 9, 52),\n  'start': datetime.datetime(2017, 10, 19, 21, 57, 24)},\n {'end': datetime.datetime(2017, 10, 21, 12, 36, 24),\n  'start': datetime.datetime(2017, 10, 21, 12, 24, 9)},\n {'end': datetime.datetime(2017, 10, 21, 12, 42, 13),\n  'start': datetime.datetime(2017, 10, 21, 12, 36, 37)},\n {'end': datetime.datetime(2017, 10, 22, 11, 9, 36),\n  'start': datetime.datetime(2017, 10, 21, 13, 47, 43)},\n {'end': datetime.datetime(2017, 10, 22, 13, 31, 44),\n  'start': datetime.datetime(2017, 10, 22, 13, 28, 53)},\n {'end': datetime.datetime(2017, 10, 22, 13, 56, 33),\n  'start': datetime.datetime(2017, 10, 22, 13, 47, 5)},\n {'end': datetime.datetime(2017, 10, 22, 14, 32, 39),\n  'start': datetime.datetime(2017, 10, 22, 14, 26, 41)},\n {'end': datetime.datetime(2017, 10, 22, 15, 9, 58),\n  'start': datetime.datetime(2017, 10, 22, 14, 54, 41)},\n {'end': datetime.datetime(2017, 10, 22, 16, 51, 40),\n  'start': datetime.datetime(2017, 10, 22, 16, 40, 29)},\n {'end': datetime.datetime(2017, 10, 22, 18, 28, 37),\n  'start': datetime.datetime(2017, 10, 22, 17, 58, 46)},\n {'end': datetime.datetime(2017, 10, 22, 18, 50, 34),\n  'start': datetime.datetime(2017, 10, 22, 18, 45, 16)},\n {'end': datetime.datetime(2017, 10, 22, 19, 11, 10),\n  'start': datetime.datetime(2017, 10, 22, 18, 56, 22)},\n {'end': datetime.datetime(2017, 10, 23, 10, 35, 32),\n  'start': datetime.datetime(2017, 10, 23, 10, 14, 8)},\n {'end': datetime.datetime(2017, 10, 23, 14, 38, 34),\n  'start': datetime.datetime(2017, 10, 23, 11, 29, 36)},\n {'end': datetime.datetime(2017, 10, 23, 15, 32, 58),\n  'start': datetime.datetime(2017, 10, 23, 15, 4, 52)},\n {'end': datetime.datetime(2017, 10, 23, 17, 6, 47),\n  'start': datetime.datetime(2017, 10, 23, 15, 33, 48)},\n {'end': datetime.datetime(2017, 10, 23, 19, 31, 26),\n  'start': datetime.datetime(2017, 10, 23, 17, 13, 16)},\n {'end': datetime.datetime(2017, 10, 23, 20, 25, 53),\n  'start': datetime.datetime(2017, 10, 23, 19, 55, 3)},\n {'end': datetime.datetime(2017, 10, 23, 22, 18, 4),\n  'start': datetime.datetime(2017, 10, 23, 21, 47, 54)},\n {'end': datetime.datetime(2017, 10, 23, 22, 48, 42),\n  'start': datetime.datetime(2017, 10, 23, 22, 34, 12)},\n {'end': datetime.datetime(2017, 10, 24, 7, 2, 17),\n  'start': datetime.datetime(2017, 10, 24, 6, 55, 1)},\n {'end': datetime.datetime(2017, 10, 24, 15, 3, 16),\n  'start': datetime.datetime(2017, 10, 24, 14, 56, 7)},\n {'end': datetime.datetime(2017, 10, 24, 15, 59, 50),\n  'start': datetime.datetime(2017, 10, 24, 15, 51, 36)},\n {'end': datetime.datetime(2017, 10, 24, 16, 55, 9),\n  'start': datetime.datetime(2017, 10, 24, 16, 31, 10)},\n {'end': datetime.datetime(2017, 10, 28, 14, 32, 34),\n  'start': datetime.datetime(2017, 10, 28, 14, 26, 14)},\n {'end': datetime.datetime(2017, 11, 1, 9, 52, 23),\n  'start': datetime.datetime(2017, 11, 1, 9, 41, 54)},\n {'end': datetime.datetime(2017, 11, 1, 20, 32, 13),\n  'start': datetime.datetime(2017, 11, 1, 20, 16, 11)},\n {'end': datetime.datetime(2017, 11, 2, 19, 50, 56),\n  'start': datetime.datetime(2017, 11, 2, 19, 44, 29)},\n {'end': datetime.datetime(2017, 11, 2, 20, 30, 29),\n  'start': datetime.datetime(2017, 11, 2, 20, 14, 37)},\n {'end': datetime.datetime(2017, 11, 2, 21, 38, 57),\n  'start': datetime.datetime(2017, 11, 2, 21, 35, 47)},\n {'end': datetime.datetime(2017, 11, 3, 10, 11, 46),\n  'start': datetime.datetime(2017, 11, 3, 9, 59, 27)},\n {'end': datetime.datetime(2017, 11, 3, 10, 32, 2),\n  'start': datetime.datetime(2017, 11, 3, 10, 13, 22)},\n {'end': datetime.datetime(2017, 11, 3, 10, 50, 34),\n  'start': datetime.datetime(2017, 11, 3, 10, 44, 25)},\n {'end': datetime.datetime(2017, 11, 3, 16, 44, 38),\n  'start': datetime.datetime(2017, 11, 3, 16, 6, 43)},\n {'end': datetime.datetime(2017, 11, 3, 17, 0, 27),\n  'start': datetime.datetime(2017, 11, 3, 16, 45, 54)},\n {'end': datetime.datetime(2017, 11, 3, 17, 35, 5),\n  'start': datetime.datetime(2017, 11, 3, 17, 7, 15)},\n {'end': datetime.datetime(2017, 11, 3, 17, 46, 48),\n  'start': datetime.datetime(2017, 11, 3, 17, 36, 5)},\n {'end': datetime.datetime(2017, 11, 3, 18, 0, 3),\n  'start': datetime.datetime(2017, 11, 3, 17, 50, 31)},\n {'end': datetime.datetime(2017, 11, 3, 19, 45, 51),\n  'start': datetime.datetime(2017, 11, 3, 19, 22, 56)},\n {'end': datetime.datetime(2017, 11, 4, 13, 26, 15),\n  'start': datetime.datetime(2017, 11, 4, 13, 14, 10)},\n {'end': datetime.datetime(2017, 11, 4, 14, 30, 5),\n  'start': datetime.datetime(2017, 11, 4, 14, 18, 37)},\n {'end': datetime.datetime(2017, 11, 4, 15, 3, 20),\n  'start': datetime.datetime(2017, 11, 4, 14, 45, 59)},\n {'end': datetime.datetime(2017, 11, 4, 15, 44, 30),\n  'start': datetime.datetime(2017, 11, 4, 15, 16, 3)},\n {'end': datetime.datetime(2017, 11, 4, 16, 58, 22),\n  'start': datetime.datetime(2017, 11, 4, 16, 37, 46)},\n {'end': datetime.datetime(2017, 11, 4, 17, 34, 50),\n  'start': datetime.datetime(2017, 11, 4, 17, 13, 19)},\n {'end': datetime.datetime(2017, 11, 4, 18, 58, 44),\n  'start': datetime.datetime(2017, 11, 4, 18, 10, 34)},\n {'end': datetime.datetime(2017, 11, 5, 1, 1, 4),\n  'start': datetime.datetime(2017, 11, 5, 1, 56, 50)},\n {'end': datetime.datetime(2017, 11, 5, 8, 53, 46),\n  'start': datetime.datetime(2017, 11, 5, 8, 33, 33)},\n {'end': datetime.datetime(2017, 11, 5, 9, 3, 39),\n  'start': datetime.datetime(2017, 11, 5, 8, 58, 8)},\n {'end': datetime.datetime(2017, 11, 5, 11, 30, 5),\n  'start': datetime.datetime(2017, 11, 5, 11, 5, 8)},\n {'end': datetime.datetime(2017, 11, 6, 8, 59, 5),\n  'start': datetime.datetime(2017, 11, 6, 8, 50, 18)},\n {'end': datetime.datetime(2017, 11, 6, 9, 13, 47),\n  'start': datetime.datetime(2017, 11, 6, 9, 4, 3)},\n {'end': datetime.datetime(2017, 11, 6, 17, 2, 55),\n  'start': datetime.datetime(2017, 11, 6, 16, 19, 36)},\n {'end': datetime.datetime(2017, 11, 6, 17, 34, 6),\n  'start': datetime.datetime(2017, 11, 6, 17, 21, 27)},\n {'end': datetime.datetime(2017, 11, 6, 17, 57, 32),\n  'start': datetime.datetime(2017, 11, 6, 17, 36, 1)},\n {'end': datetime.datetime(2017, 11, 6, 18, 15, 8),\n  'start': datetime.datetime(2017, 11, 6, 17, 59, 52)},\n {'end': datetime.datetime(2017, 11, 6, 18, 21, 17),\n  'start': datetime.datetime(2017, 11, 6, 18, 18, 36)},\n {'end': datetime.datetime(2017, 11, 6, 19, 37, 57),\n  'start': datetime.datetime(2017, 11, 6, 19, 24, 31)},\n {'end': datetime.datetime(2017, 11, 6, 20, 3, 14),\n  'start': datetime.datetime(2017, 11, 6, 19, 49, 16)},\n {'end': datetime.datetime(2017, 11, 7, 8, 1, 32),\n  'start': datetime.datetime(2017, 11, 7, 7, 50, 48)},\n {'end': datetime.datetime(2017, 11, 8, 13, 18, 5),\n  'start': datetime.datetime(2017, 11, 8, 13, 11, 51)},\n {'end': datetime.datetime(2017, 11, 8, 21, 46, 5),\n  'start': datetime.datetime(2017, 11, 8, 21, 34, 47)},\n {'end': datetime.datetime(2017, 11, 8, 22, 4, 47),\n  'start': datetime.datetime(2017, 11, 8, 22, 2, 30)},\n {'end': datetime.datetime(2017, 11, 9, 7, 12, 10),\n  'start': datetime.datetime(2017, 11, 9, 7, 1, 11)},\n {'end': datetime.datetime(2017, 11, 9, 8, 8, 28),\n  'start': datetime.datetime(2017, 11, 9, 8, 2, 2)},\n {'end': datetime.datetime(2017, 11, 9, 8, 32, 24),\n  'start': datetime.datetime(2017, 11, 9, 8, 19, 59)},\n {'end': datetime.datetime(2017, 11, 9, 8, 48, 59),\n  'start': datetime.datetime(2017, 11, 9, 8, 41, 31)},\n {'end': datetime.datetime(2017, 11, 9, 9, 9, 24),\n  'start': datetime.datetime(2017, 11, 9, 9, 0, 6)},\n {'end': datetime.datetime(2017, 11, 9, 9, 24, 25),\n  'start': datetime.datetime(2017, 11, 9, 9, 9, 37)},\n {'end': datetime.datetime(2017, 11, 9, 13, 25, 39),\n  'start': datetime.datetime(2017, 11, 9, 13, 14, 37)},\n {'end': datetime.datetime(2017, 11, 9, 15, 31, 10),\n  'start': datetime.datetime(2017, 11, 9, 15, 20, 7)},\n {'end': datetime.datetime(2017, 11, 9, 18, 53, 10),\n  'start': datetime.datetime(2017, 11, 9, 18, 47, 8)},\n {'end': datetime.datetime(2017, 11, 9, 23, 43, 35),\n  'start': datetime.datetime(2017, 11, 9, 23, 35, 2)},\n {'end': datetime.datetime(2017, 11, 10, 8, 2, 28),\n  'start': datetime.datetime(2017, 11, 10, 7, 51, 33)},\n {'end': datetime.datetime(2017, 11, 10, 8, 42, 9),\n  'start': datetime.datetime(2017, 11, 10, 8, 38, 28)},\n {'end': datetime.datetime(2017, 11, 11, 18, 13, 14),\n  'start': datetime.datetime(2017, 11, 11, 18, 5, 25)},\n {'end': datetime.datetime(2017, 11, 11, 19, 46, 22),\n  'start': datetime.datetime(2017, 11, 11, 19, 39, 12)},\n {'end': datetime.datetime(2017, 11, 11, 21, 16, 31),\n  'start': datetime.datetime(2017, 11, 11, 21, 13, 19)},\n {'end': datetime.datetime(2017, 11, 12, 9, 51, 43),\n  'start': datetime.datetime(2017, 11, 12, 9, 46, 19)},\n {'end': datetime.datetime(2017, 11, 13, 13, 54, 15),\n  'start': datetime.datetime(2017, 11, 13, 13, 33, 42)},\n {'end': datetime.datetime(2017, 11, 14, 8, 55, 52),\n  'start': datetime.datetime(2017, 11, 14, 8, 40, 29)},\n {'end': datetime.datetime(2017, 11, 15, 6, 30, 6),\n  'start': datetime.datetime(2017, 11, 15, 6, 14, 5)},\n {'end': datetime.datetime(2017, 11, 15, 8, 23, 44),\n  'start': datetime.datetime(2017, 11, 15, 8, 14, 59)},\n {'end': datetime.datetime(2017, 11, 15, 10, 33, 41),\n  'start': datetime.datetime(2017, 11, 15, 10, 16, 44)},\n {'end': datetime.datetime(2017, 11, 15, 10, 54, 14),\n  'start': datetime.datetime(2017, 11, 15, 10, 33, 58)},\n {'end': datetime.datetime(2017, 11, 15, 11, 14, 42),\n  'start': datetime.datetime(2017, 11, 15, 11, 2, 15)},\n {'end': datetime.datetime(2017, 11, 16, 9, 38, 49),\n  'start': datetime.datetime(2017, 11, 16, 9, 27, 41)},\n {'end': datetime.datetime(2017, 11, 16, 10, 18),\n  'start': datetime.datetime(2017, 11, 16, 9, 57, 41)},\n {'end': datetime.datetime(2017, 11, 16, 17, 44, 47),\n  'start': datetime.datetime(2017, 11, 16, 17, 25, 5)},\n {'end': datetime.datetime(2017, 11, 17, 16, 36, 56),\n  'start': datetime.datetime(2017, 11, 17, 13, 45, 54)},\n {'end': datetime.datetime(2017, 11, 17, 19, 31, 15),\n  'start': datetime.datetime(2017, 11, 17, 19, 12, 49)},\n {'end': datetime.datetime(2017, 11, 18, 10, 55, 45),\n  'start': datetime.datetime(2017, 11, 18, 10, 49, 6)},\n {'end': datetime.datetime(2017, 11, 18, 11, 44, 16),\n  'start': datetime.datetime(2017, 11, 18, 11, 32, 12)},\n {'end': datetime.datetime(2017, 11, 18, 18, 14, 31),\n  'start': datetime.datetime(2017, 11, 18, 18, 9, 1)},\n {'end': datetime.datetime(2017, 11, 18, 19, 1, 29),\n  'start': datetime.datetime(2017, 11, 18, 18, 53, 10)},\n {'end': datetime.datetime(2017, 11, 19, 14, 31, 49),\n  'start': datetime.datetime(2017, 11, 19, 14, 15, 41)},\n {'end': datetime.datetime(2017, 11, 20, 21, 41, 9),\n  'start': datetime.datetime(2017, 11, 20, 21, 19, 19)},\n {'end': datetime.datetime(2017, 11, 20, 23, 23, 37),\n  'start': datetime.datetime(2017, 11, 20, 22, 39, 48)},\n {'end': datetime.datetime(2017, 11, 21, 17, 51, 32),\n  'start': datetime.datetime(2017, 11, 21, 17, 44, 25)},\n {'end': datetime.datetime(2017, 11, 21, 18, 34, 51),\n  'start': datetime.datetime(2017, 11, 21, 18, 20, 52)},\n {'end': datetime.datetime(2017, 11, 21, 18, 51, 50),\n  'start': datetime.datetime(2017, 11, 21, 18, 47, 32)},\n {'end': datetime.datetime(2017, 11, 21, 19, 14, 33),\n  'start': datetime.datetime(2017, 11, 21, 19, 7, 57)},\n {'end': datetime.datetime(2017, 11, 21, 20, 8, 54),\n  'start': datetime.datetime(2017, 11, 21, 20, 4, 56)},\n {'end': datetime.datetime(2017, 11, 21, 22, 8, 12),\n  'start': datetime.datetime(2017, 11, 21, 21, 55, 47)},\n {'end': datetime.datetime(2017, 11, 23, 23, 57, 56),\n  'start': datetime.datetime(2017, 11, 23, 23, 47, 43)},\n {'end': datetime.datetime(2017, 11, 24, 6, 53, 15),\n  'start': datetime.datetime(2017, 11, 24, 6, 41, 25)},\n {'end': datetime.datetime(2017, 11, 24, 7, 33, 24),\n  'start': datetime.datetime(2017, 11, 24, 6, 58, 56)},\n {'end': datetime.datetime(2017, 11, 26, 12, 41, 36),\n  'start': datetime.datetime(2017, 11, 26, 12, 25, 49)},\n {'end': datetime.datetime(2017, 11, 27, 5, 54, 13),\n  'start': datetime.datetime(2017, 11, 27, 5, 29, 4)},\n {'end': datetime.datetime(2017, 11, 27, 6, 11, 1),\n  'start': datetime.datetime(2017, 11, 27, 6, 6, 47)},\n {'end': datetime.datetime(2017, 11, 27, 6, 55, 39),\n  'start': datetime.datetime(2017, 11, 27, 6, 45, 14)},\n {'end': datetime.datetime(2017, 11, 27, 9, 47, 43),\n  'start': datetime.datetime(2017, 11, 27, 9, 39, 44)},\n {'end': datetime.datetime(2017, 11, 27, 11, 20, 46),\n  'start': datetime.datetime(2017, 11, 27, 11, 9, 18)},\n {'end': datetime.datetime(2017, 11, 27, 11, 35, 44),\n  'start': datetime.datetime(2017, 11, 27, 11, 31, 46)},\n {'end': datetime.datetime(2017, 11, 27, 12, 12, 36),\n  'start': datetime.datetime(2017, 11, 27, 12, 7, 14)},\n {'end': datetime.datetime(2017, 11, 27, 12, 26, 44),\n  'start': datetime.datetime(2017, 11, 27, 12, 21, 40)},\n {'end': datetime.datetime(2017, 11, 27, 17, 36, 7),\n  'start': datetime.datetime(2017, 11, 27, 17, 26, 31)},\n {'end': datetime.datetime(2017, 11, 27, 18, 29, 4),\n  'start': datetime.datetime(2017, 11, 27, 18, 11, 49)},\n {'end': datetime.datetime(2017, 11, 27, 19, 47, 17),\n  'start': datetime.datetime(2017, 11, 27, 19, 36, 16)},\n {'end': datetime.datetime(2017, 11, 27, 20, 17, 33),\n  'start': datetime.datetime(2017, 11, 27, 20, 12, 57)},\n {'end': datetime.datetime(2017, 11, 28, 8, 41, 53),\n  'start': datetime.datetime(2017, 11, 28, 8, 18, 6)},\n {'end': datetime.datetime(2017, 11, 28, 19, 34, 1),\n  'start': datetime.datetime(2017, 11, 28, 19, 17, 23)},\n {'end': datetime.datetime(2017, 11, 28, 19, 46, 24),\n  'start': datetime.datetime(2017, 11, 28, 19, 34, 15)},\n {'end': datetime.datetime(2017, 11, 28, 21, 39, 32),\n  'start': datetime.datetime(2017, 11, 28, 21, 27, 29)},\n {'end': datetime.datetime(2017, 11, 29, 7, 51, 18),\n  'start': datetime.datetime(2017, 11, 29, 7, 47, 38)},\n {'end': datetime.datetime(2017, 11, 29, 9, 53, 44),\n  'start': datetime.datetime(2017, 11, 29, 9, 50, 12)},\n {'end': datetime.datetime(2017, 11, 29, 17, 16, 21),\n  'start': datetime.datetime(2017, 11, 29, 17, 3, 42)},\n {'end': datetime.datetime(2017, 11, 29, 18, 23, 43),\n  'start': datetime.datetime(2017, 11, 29, 18, 19, 15)},\n {'end': datetime.datetime(2017, 12, 1, 17, 10, 12),\n  'start': datetime.datetime(2017, 12, 1, 17, 3, 58)},\n {'end': datetime.datetime(2017, 12, 2, 8, 1, 1),\n  'start': datetime.datetime(2017, 12, 2, 7, 55, 56)},\n {'end': datetime.datetime(2017, 12, 2, 9, 21, 18),\n  'start': datetime.datetime(2017, 12, 2, 9, 16, 14)},\n {'end': datetime.datetime(2017, 12, 2, 19, 53, 18),\n  'start': datetime.datetime(2017, 12, 2, 19, 48, 29)},\n {'end': datetime.datetime(2017, 12, 3, 15, 20, 9),\n  'start': datetime.datetime(2017, 12, 3, 14, 36, 29)},\n {'end': datetime.datetime(2017, 12, 3, 16, 25, 30),\n  'start': datetime.datetime(2017, 12, 3, 16, 4, 2)},\n {'end': datetime.datetime(2017, 12, 3, 16, 43, 58),\n  'start': datetime.datetime(2017, 12, 3, 16, 40, 26)},\n {'end': datetime.datetime(2017, 12, 3, 18, 4, 33),\n  'start': datetime.datetime(2017, 12, 3, 17, 20, 17)},\n {'end': datetime.datetime(2017, 12, 4, 8, 51),\n  'start': datetime.datetime(2017, 12, 4, 8, 34, 24)},\n {'end': datetime.datetime(2017, 12, 4, 17, 53, 57),\n  'start': datetime.datetime(2017, 12, 4, 17, 49, 26)},\n {'end': datetime.datetime(2017, 12, 4, 18, 50, 33),\n  'start': datetime.datetime(2017, 12, 4, 18, 38, 52)},\n {'end': datetime.datetime(2017, 12, 4, 21, 46, 58),\n  'start': datetime.datetime(2017, 12, 4, 21, 39, 20)},\n {'end': datetime.datetime(2017, 12, 4, 21, 56, 17),\n  'start': datetime.datetime(2017, 12, 4, 21, 54, 21)},\n {'end': datetime.datetime(2017, 12, 5, 8, 52, 54),\n  'start': datetime.datetime(2017, 12, 5, 8, 50, 50)},\n {'end': datetime.datetime(2017, 12, 6, 8, 24, 14),\n  'start': datetime.datetime(2017, 12, 6, 8, 19, 38)},\n {'end': datetime.datetime(2017, 12, 6, 18, 28, 11),\n  'start': datetime.datetime(2017, 12, 6, 18, 19, 19)},\n {'end': datetime.datetime(2017, 12, 6, 18, 33, 12),\n  'start': datetime.datetime(2017, 12, 6, 18, 28, 55)},\n {'end': datetime.datetime(2017, 12, 6, 20, 21, 38),\n  'start': datetime.datetime(2017, 12, 6, 20, 3, 29)},\n {'end': datetime.datetime(2017, 12, 6, 20, 39, 57),\n  'start': datetime.datetime(2017, 12, 6, 20, 36, 42)},\n {'end': datetime.datetime(2017, 12, 7, 6, 1, 15),\n  'start': datetime.datetime(2017, 12, 7, 5, 54, 51)},\n {'end': datetime.datetime(2017, 12, 8, 16, 55, 49),\n  'start': datetime.datetime(2017, 12, 8, 16, 47, 18)},\n {'end': datetime.datetime(2017, 12, 8, 19, 29, 12),\n  'start': datetime.datetime(2017, 12, 8, 19, 15, 2)},\n {'end': datetime.datetime(2017, 12, 9, 22, 47, 19),\n  'start': datetime.datetime(2017, 12, 9, 22, 39, 37)},\n {'end': datetime.datetime(2017, 12, 9, 23, 5, 32),\n  'start': datetime.datetime(2017, 12, 9, 23, 0, 10)},\n {'end': datetime.datetime(2017, 12, 10, 0, 56, 2),\n  'start': datetime.datetime(2017, 12, 10, 0, 39, 24)},\n {'end': datetime.datetime(2017, 12, 10, 1, 8, 9),\n  'start': datetime.datetime(2017, 12, 10, 1, 2, 42)},\n {'end': datetime.datetime(2017, 12, 10, 1, 11, 30),\n  'start': datetime.datetime(2017, 12, 10, 1, 8, 57)},\n {'end': datetime.datetime(2017, 12, 10, 13, 51, 41),\n  'start': datetime.datetime(2017, 12, 10, 13, 49, 9)},\n {'end': datetime.datetime(2017, 12, 10, 15, 18, 19),\n  'start': datetime.datetime(2017, 12, 10, 15, 14, 29)},\n {'end': datetime.datetime(2017, 12, 10, 15, 36, 28),\n  'start': datetime.datetime(2017, 12, 10, 15, 31, 7)},\n {'end': datetime.datetime(2017, 12, 10, 16, 30, 31),\n  'start': datetime.datetime(2017, 12, 10, 16, 20, 6)},\n {'end': datetime.datetime(2017, 12, 10, 17, 14, 25),\n  'start': datetime.datetime(2017, 12, 10, 17, 7, 54)},\n {'end': datetime.datetime(2017, 12, 10, 17, 45, 25),\n  'start': datetime.datetime(2017, 12, 10, 17, 23, 47)},\n {'end': datetime.datetime(2017, 12, 11, 6, 34, 4),\n  'start': datetime.datetime(2017, 12, 11, 6, 17, 6)},\n {'end': datetime.datetime(2017, 12, 11, 9, 12, 21),\n  'start': datetime.datetime(2017, 12, 11, 9, 8, 41)},\n {'end': datetime.datetime(2017, 12, 11, 9, 20, 18),\n  'start': datetime.datetime(2017, 12, 11, 9, 15, 41)},\n {'end': datetime.datetime(2017, 12, 12, 8, 59, 34),\n  'start': datetime.datetime(2017, 12, 12, 8, 55, 53)},\n {'end': datetime.datetime(2017, 12, 13, 17, 18, 32),\n  'start': datetime.datetime(2017, 12, 13, 17, 14, 56)},\n {'end': datetime.datetime(2017, 12, 13, 19, 0, 45),\n  'start': datetime.datetime(2017, 12, 13, 18, 52, 16)},\n {'end': datetime.datetime(2017, 12, 14, 9, 11, 6),\n  'start': datetime.datetime(2017, 12, 14, 9, 1, 10)},\n {'end': datetime.datetime(2017, 12, 14, 9, 19, 6),\n  'start': datetime.datetime(2017, 12, 14, 9, 12, 59)},\n {'end': datetime.datetime(2017, 12, 14, 12, 2),\n  'start': datetime.datetime(2017, 12, 14, 11, 54, 33)},\n {'end': datetime.datetime(2017, 12, 14, 14, 44, 40),\n  'start': datetime.datetime(2017, 12, 14, 14, 40, 23)},\n {'end': datetime.datetime(2017, 12, 14, 15, 26, 24),\n  'start': datetime.datetime(2017, 12, 14, 15, 8, 55)},\n {'end': datetime.datetime(2017, 12, 14, 18, 9, 4),\n  'start': datetime.datetime(2017, 12, 14, 17, 46, 17)},\n {'end': datetime.datetime(2017, 12, 15, 9, 23, 45),\n  'start': datetime.datetime(2017, 12, 15, 9, 8, 12)},\n {'end': datetime.datetime(2017, 12, 16, 9, 36, 17),\n  'start': datetime.datetime(2017, 12, 16, 9, 33, 46)},\n {'end': datetime.datetime(2017, 12, 16, 11, 5, 4),\n  'start': datetime.datetime(2017, 12, 16, 11, 2, 31)},\n {'end': datetime.datetime(2017, 12, 17, 10, 32, 3),\n  'start': datetime.datetime(2017, 12, 17, 10, 9, 47)},\n {'end': datetime.datetime(2017, 12, 18, 8, 7, 34),\n  'start': datetime.datetime(2017, 12, 18, 8, 2, 36)},\n {'end': datetime.datetime(2017, 12, 18, 16, 9, 20),\n  'start': datetime.datetime(2017, 12, 18, 16, 3)},\n {'end': datetime.datetime(2017, 12, 18, 16, 53, 12),\n  'start': datetime.datetime(2017, 12, 18, 16, 30, 7)},\n {'end': datetime.datetime(2017, 12, 18, 19, 22, 8),\n  'start': datetime.datetime(2017, 12, 18, 19, 18, 23)},\n {'end': datetime.datetime(2017, 12, 18, 20, 17, 47),\n  'start': datetime.datetime(2017, 12, 18, 20, 14, 46)},\n {'end': datetime.datetime(2017, 12, 19, 19, 23, 49),\n  'start': datetime.datetime(2017, 12, 19, 19, 14, 8)},\n {'end': datetime.datetime(2017, 12, 19, 19, 43, 46),\n  'start': datetime.datetime(2017, 12, 19, 19, 39, 36)},\n {'end': datetime.datetime(2017, 12, 20, 8, 10, 46),\n  'start': datetime.datetime(2017, 12, 20, 8, 5, 14)},\n {'end': datetime.datetime(2017, 12, 20, 8, 29, 50),\n  'start': datetime.datetime(2017, 12, 20, 8, 15, 45)},\n {'end': datetime.datetime(2017, 12, 20, 8, 38, 9),\n  'start': datetime.datetime(2017, 12, 20, 8, 33, 32)},\n {'end': datetime.datetime(2017, 12, 20, 13, 54, 39),\n  'start': datetime.datetime(2017, 12, 20, 13, 43, 36)},\n {'end': datetime.datetime(2017, 12, 20, 19, 6, 54),\n  'start': datetime.datetime(2017, 12, 20, 18, 57, 53)},\n {'end': datetime.datetime(2017, 12, 21, 7, 32, 3),\n  'start': datetime.datetime(2017, 12, 21, 7, 21, 11)},\n {'end': datetime.datetime(2017, 12, 21, 8, 6, 15),\n  'start': datetime.datetime(2017, 12, 21, 8, 1, 58)},\n {'end': datetime.datetime(2017, 12, 21, 13, 33, 49),\n  'start': datetime.datetime(2017, 12, 21, 13, 20, 54)},\n {'end': datetime.datetime(2017, 12, 21, 15, 34, 27),\n  'start': datetime.datetime(2017, 12, 21, 15, 26, 8)},\n {'end': datetime.datetime(2017, 12, 21, 18, 38, 50),\n  'start': datetime.datetime(2017, 12, 21, 18, 9, 46)},\n {'end': datetime.datetime(2017, 12, 22, 16, 21, 46),\n  'start': datetime.datetime(2017, 12, 22, 16, 14, 21)},\n {'end': datetime.datetime(2017, 12, 22, 16, 34, 14),\n  'start': datetime.datetime(2017, 12, 22, 16, 29, 17)},\n {'end': datetime.datetime(2017, 12, 25, 13, 18, 27),\n  'start': datetime.datetime(2017, 12, 25, 12, 49, 51)},\n {'end': datetime.datetime(2017, 12, 25, 14, 20, 50),\n  'start': datetime.datetime(2017, 12, 25, 13, 46, 44)},\n {'end': datetime.datetime(2017, 12, 26, 10, 53, 45),\n  'start': datetime.datetime(2017, 12, 26, 10, 40, 16)},\n {'end': datetime.datetime(2017, 12, 27, 17, 17, 39),\n  'start': datetime.datetime(2017, 12, 27, 16, 56, 12)},\n {'end': datetime.datetime(2017, 12, 29, 6, 12, 30),\n  'start': datetime.datetime(2017, 12, 29, 6, 2, 34)},\n {'end': datetime.datetime(2017, 12, 29, 12, 46, 16),\n  'start': datetime.datetime(2017, 12, 29, 12, 21, 3)},\n {'end': datetime.datetime(2017, 12, 29, 14, 43, 46),\n  'start': datetime.datetime(2017, 12, 29, 14, 32, 55)},\n {'end': datetime.datetime(2017, 12, 29, 15, 18, 51),\n  'start': datetime.datetime(2017, 12, 29, 15, 8, 26)},\n {'end': datetime.datetime(2017, 12, 29, 20, 38, 13),\n  'start': datetime.datetime(2017, 12, 29, 20, 33, 34)},\n {'end': datetime.datetime(2017, 12, 30, 13, 54, 33),\n  'start': datetime.datetime(2017, 12, 30, 13, 51, 3)},\n {'end': datetime.datetime(2017, 12, 30, 15, 19, 13),\n  'start': datetime.datetime(2017, 12, 30, 15, 9, 3)}]\n\n\n# Initialize a list for all the trip durations\nonebike_durations = []\n\nfor trip in onebike_datetimes:\n  # Create a timedelta object corresponding to the length of the trip\n  trip_duration = trip['end'] - trip['start']\n  \n  # Get the total elapsed seconds in trip_duration\n  trip_length_seconds = trip_duration.total_seconds()\n  \n  # Append the results to our list\n  onebike_durations.append(trip_length_seconds)\nonebike_durations\n\n[181.0,\n 7622.0,\n 343.0,\n 1278.0,\n 1277.0,\n 1366.0,\n 815.0,\n 545.0,\n 491.0,\n 639.0,\n 1678.0,\n 406.0,\n 709.0,\n 514.0,\n 492.0,\n 1668.0,\n 2242.0,\n 2752.0,\n 735.0,\n 330.0,\n 518.0,\n 1433.0,\n 204.0,\n 304.0,\n 977.0,\n 1399.0,\n 1244.0,\n 658.0,\n 800.0,\n 1911.0,\n 2471.0,\n 1344.0,\n 435.0,\n 271.0,\n 920.0,\n 851.0,\n 209.0,\n 453.0,\n 841.0,\n 142.0,\n 1023.0,\n 1466.0,\n 1636.0,\n 3039.0,\n 1571.0,\n 1410.0,\n 386.0,\n 1527.0,\n 622.0,\n 1450.0,\n 1422.0,\n 991.0,\n 1484.0,\n 1450.0,\n 929.0,\n 533.0,\n 525.0,\n 283.0,\n 133.0,\n 1106.0,\n 952.0,\n 553.0,\n 659.0,\n 297.0,\n 357.0,\n 989.0,\n 979.0,\n 760.0,\n 1110.0,\n 675.0,\n 1207.0,\n 1593.0,\n 768.0,\n 1446.0,\n 485.0,\n 200.0,\n 399.0,\n 242.0,\n 170.0,\n 450.0,\n 1078.0,\n 1042.0,\n 573.0,\n 748.0,\n 735.0,\n 336.0,\n 76913.0,\n 171.0,\n 568.0,\n 358.0,\n 917.0,\n 671.0,\n 1791.0,\n 318.0,\n 888.0,\n 1284.0,\n 11338.0,\n 1686.0,\n 5579.0,\n 8290.0,\n 1850.0,\n 1810.0,\n 870.0,\n 436.0,\n 429.0,\n 494.0,\n 1439.0,\n 380.0,\n 629.0,\n 962.0,\n 387.0,\n 952.0,\n 190.0,\n 739.0,\n 1120.0,\n 369.0,\n 2275.0,\n 873.0,\n 1670.0,\n 643.0,\n 572.0,\n 1375.0,\n 725.0,\n 688.0,\n 1041.0,\n 1707.0,\n 1236.0,\n 1291.0,\n 2890.0,\n -3346.0,\n 1213.0,\n 331.0,\n 1497.0,\n 527.0,\n 584.0,\n 2599.0,\n 759.0,\n 1291.0,\n 916.0,\n 161.0,\n 806.0,\n 838.0,\n 644.0,\n 374.0,\n 678.0,\n 137.0,\n 659.0,\n 386.0,\n 745.0,\n 448.0,\n 558.0,\n 888.0,\n 662.0,\n 663.0,\n 362.0,\n 513.0,\n 655.0,\n 221.0,\n 469.0,\n 430.0,\n 192.0,\n 324.0,\n 1233.0,\n 923.0,\n 961.0,\n 525.0,\n 1017.0,\n 1216.0,\n 747.0,\n 668.0,\n 1219.0,\n 1182.0,\n 10262.0,\n 1106.0,\n 399.0,\n 724.0,\n 330.0,\n 499.0,\n 968.0,\n 1310.0,\n 2629.0,\n 427.0,\n 839.0,\n 258.0,\n 396.0,\n 238.0,\n 745.0,\n 613.0,\n 710.0,\n 2068.0,\n 947.0,\n 1509.0,\n 254.0,\n 625.0,\n 479.0,\n 688.0,\n 238.0,\n 322.0,\n 304.0,\n 576.0,\n 1035.0,\n 661.0,\n 276.0,\n 1427.0,\n 998.0,\n 729.0,\n 723.0,\n 220.0,\n 212.0,\n 759.0,\n 268.0,\n 374.0,\n 305.0,\n 304.0,\n 289.0,\n 2620.0,\n 1288.0,\n 212.0,\n 2656.0,\n 996.0,\n 271.0,\n 701.0,\n 458.0,\n 116.0,\n 124.0,\n 276.0,\n 532.0,\n 257.0,\n 1089.0,\n 195.0,\n 384.0,\n 511.0,\n 850.0,\n 462.0,\n 322.0,\n 998.0,\n 327.0,\n 153.0,\n 152.0,\n 230.0,\n 321.0,\n 625.0,\n 391.0,\n 1298.0,\n 1018.0,\n 220.0,\n 277.0,\n 221.0,\n 216.0,\n 509.0,\n 596.0,\n 367.0,\n 447.0,\n 257.0,\n 1049.0,\n 1367.0,\n 933.0,\n 151.0,\n 153.0,\n 1336.0,\n 298.0,\n 380.0,\n 1385.0,\n 225.0,\n 181.0,\n 581.0,\n 250.0,\n 332.0,\n 845.0,\n 277.0,\n 663.0,\n 541.0,\n 652.0,\n 257.0,\n 775.0,\n 499.0,\n 1744.0,\n 445.0,\n 297.0,\n 1716.0,\n 2046.0,\n 809.0,\n 1287.0,\n 596.0,\n 1513.0,\n 651.0,\n 625.0,\n 279.0,\n 210.0,\n 610.0]\n\n\n\n\nAverage trip time\n\n# What was the total duration of all trips?\ntotal_elapsed_time = sum(onebike_durations)\n\n# What was the total number of trips?\nnumber_of_trips = len(onebike_durations)\n  \n# Divide the total duration by the number of trips\nprint(total_elapsed_time / number_of_trips)\n\n1178.9310344827586\n\n\nFor the average to be a helpful summary of the data, we need for all of our durations to be reasonable numbers, and not a few that are way too big, way too small, or even malformed. For example, if there is anything fishy happening in the data, and our trip ended before it started, we’d have a negative trip length.\n\n\nThe long and the short of why time is hard\nOut of 291 trips taken by W20529, how long was the longest? How short was the shortest? Does anything look fishy?\n\n# Calculate shortest and longest trips\nshortest_trip = min(onebike_durations)\nlongest_trip = max(onebike_durations)\n\n# Print out the results\nprint(\"The shortest trip was \" + str(shortest_trip) + \" seconds\")\nprint(\"The longest trip was \" + str(longest_trip) + \" seconds\")\n\nThe shortest trip was -3346.0 seconds\nThe longest trip was 76913.0 seconds\n\n\nFor at least one trip, the bike returned before it left. Why could that be? Here’s a hint: it happened in early November, around 2AM local time. What happens to clocks around that time each year?\n\n\nCounting events before and after noon\nWe will be working with a list of all bike trips for one Capital Bikeshare bike, W20529, from October 1, 2017 to December 31, 2017. This list has been loaded as onebike_datetimes. Each element of the list is a dictionary with two entries: start is a datetime object corresponding to the start of a trip (when a bike is removed from the dock) and end is a datetime object corresponding to the end of a trip (when a bike is put back into a dock). We can use this data set to understand better how this bike was used.\n\nDid more trips start before noon or after noon?\n\n# Create dictionary to hold results\ntrip_counts = {'AM': 0, 'PM': 0}\n  \n# Loop over all trips\nfor trip in onebike_datetimes:\n  # Check to see if the trip starts before noon\n  if trip['start'].hour < 12:\n    # Increment the counter for before noon\n    trip_counts['AM'] += 1\n  else:\n    # Increment the counter for after noon\n    trip_counts['PM'] += 1\n  \nprint(trip_counts)\n\n{'AM': 94, 'PM': 196}\n\n\nIt looks like this bike is used about twice as much after noon than it is before noon. One obvious follow up would be to see which hours the bike is most likely to be taken out for a ride."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#printing-and-parsing-datetimes",
    "href": "posts/2020-07-26-working with dates and times in python.html#printing-and-parsing-datetimes",
    "title": "Working With Dates and Times in Python",
    "section": "Printing and Parsing Datetimes",
    "text": "Printing and Parsing Datetimes\n\nTurning strings into datetimes\n\n# Starting string, in YYYY-MM-DD HH:MM:SS format\ns = '2017-02-03 00:00:01'\n\n# Write a format string to parse s\nfmt = '%Y-%m-%d %H:%M:%S'\n\n# Create a datetime object d\nd = datetime.datetime.strptime(s, fmt)\n\n# Print d\nprint(d)\n\n2017-02-03 00:00:01\n\n\n\n# Starting string, in YYYY-MM-DD format\ns = '2030-10-15'\n\n# Write a format string to parse s\nfmt = '%Y-%m-%d'\n\n# Create a datetime object d\nd = datetime.datetime.strptime(s, fmt)\n\n# Print d\nprint(d)\n\n2030-10-15 00:00:00\n\n\n\n# Starting string, in MM/DD/YYYY HH:MM:SS format\ns = '12/15/1986 08:00:00'\n\n# Write a format string to parse s\nfmt = '%m/%d/%Y %H:%M:%S'\n\n# Create a datetime object d\nd = datetime.datetime.strptime(s, fmt)\n\n# Print d\nprint(d)\n\n1986-12-15 08:00:00\n\n\n\n\nParsing pairs of strings as datetimes\n\nonebike_datetime_strings = [('2017-10-01 15:23:25', '2017-10-01 15:26:26'),\n ('2017-10-01 15:42:57', '2017-10-01 17:49:59'),\n ('2017-10-02 06:37:10', '2017-10-02 06:42:53'),\n ('2017-10-02 08:56:45', '2017-10-02 09:18:03'),\n ('2017-10-02 18:23:48', '2017-10-02 18:45:05'),\n ('2017-10-02 18:48:08', '2017-10-02 19:10:54'),\n ('2017-10-02 19:18:10', '2017-10-02 19:31:45'),\n ('2017-10-02 19:37:32', '2017-10-02 19:46:37'),\n ('2017-10-03 08:24:16', '2017-10-03 08:32:27'),\n ('2017-10-03 18:17:07', '2017-10-03 18:27:46'),\n ('2017-10-03 19:24:10', '2017-10-03 19:52:08'),\n ('2017-10-03 20:17:06', '2017-10-03 20:23:52'),\n ('2017-10-03 20:45:21', '2017-10-03 20:57:10'),\n ('2017-10-04 07:04:57', '2017-10-04 07:13:31'),\n ('2017-10-04 07:13:42', '2017-10-04 07:21:54'),\n ('2017-10-04 14:22:12', '2017-10-04 14:50:00'),\n ('2017-10-04 15:07:27', '2017-10-04 15:44:49'),\n ('2017-10-04 15:46:41', '2017-10-04 16:32:33'),\n ('2017-10-04 16:34:44', '2017-10-04 16:46:59'),\n ('2017-10-04 17:26:06', '2017-10-04 17:31:36'),\n ('2017-10-04 17:42:03', '2017-10-04 17:50:41'),\n ('2017-10-05 07:49:02', '2017-10-05 08:12:55'),\n ('2017-10-05 08:26:21', '2017-10-05 08:29:45'),\n ('2017-10-05 08:33:27', '2017-10-05 08:38:31'),\n ('2017-10-05 16:35:35', '2017-10-05 16:51:52'),\n ('2017-10-05 17:53:31', '2017-10-05 18:16:50'),\n ('2017-10-06 08:17:17', '2017-10-06 08:38:01'),\n ('2017-10-06 11:39:40', '2017-10-06 11:50:38'),\n ('2017-10-06 12:59:54', '2017-10-06 13:13:14'),\n ('2017-10-06 13:43:05', '2017-10-06 14:14:56'),\n ('2017-10-06 14:28:15', '2017-10-06 15:09:26'),\n ('2017-10-06 15:50:10', '2017-10-06 16:12:34'),\n ('2017-10-06 16:32:16', '2017-10-06 16:39:31'),\n ('2017-10-06 16:44:08', '2017-10-06 16:48:39'),\n ('2017-10-06 16:53:43', '2017-10-06 17:09:03'),\n ('2017-10-07 11:38:55', '2017-10-07 11:53:06'),\n ('2017-10-07 14:03:36', '2017-10-07 14:07:05'),\n ('2017-10-07 14:20:03', '2017-10-07 14:27:36'),\n ('2017-10-07 14:30:50', '2017-10-07 14:44:51'),\n ('2017-10-08 00:28:26', '2017-10-08 00:30:48'),\n ('2017-10-08 11:16:21', '2017-10-08 11:33:24'),\n ('2017-10-08 12:37:03', '2017-10-08 13:01:29'),\n ('2017-10-08 13:30:37', '2017-10-08 13:57:53'),\n ('2017-10-08 14:16:40', '2017-10-08 15:07:19'),\n ('2017-10-08 15:23:50', '2017-10-08 15:50:01'),\n ('2017-10-08 15:54:12', '2017-10-08 16:17:42'),\n ('2017-10-08 16:28:52', '2017-10-08 16:35:18'),\n ('2017-10-08 23:08:14', '2017-10-08 23:33:41'),\n ('2017-10-08 23:34:49', '2017-10-08 23:45:11'),\n ('2017-10-08 23:46:47', '2017-10-09 00:10:57'),\n ('2017-10-09 00:12:58', '2017-10-09 00:36:40'),\n ('2017-10-09 00:37:02', '2017-10-09 00:53:33'),\n ('2017-10-09 01:23:29', '2017-10-09 01:48:13'),\n ('2017-10-09 01:49:25', '2017-10-09 02:13:35'),\n ('2017-10-09 02:14:11', '2017-10-09 02:29:40'),\n ('2017-10-09 13:04:32', '2017-10-09 13:13:25'),\n ('2017-10-09 14:30:10', '2017-10-09 14:38:55'),\n ('2017-10-09 15:06:47', '2017-10-09 15:11:30'),\n ('2017-10-09 16:43:25', '2017-10-09 16:45:38'),\n ('2017-10-10 15:32:58', '2017-10-10 15:51:24'),\n ('2017-10-10 16:47:55', '2017-10-10 17:03:47'),\n ('2017-10-10 17:51:05', '2017-10-10 18:00:18'),\n ('2017-10-10 18:08:12', '2017-10-10 18:19:11'),\n ('2017-10-10 19:09:35', '2017-10-10 19:14:32'),\n ('2017-10-10 19:17:11', '2017-10-10 19:23:08'),\n ('2017-10-10 19:28:11', '2017-10-10 19:44:40'),\n ('2017-10-10 19:55:35', '2017-10-10 20:11:54'),\n ('2017-10-10 22:20:43', '2017-10-10 22:33:23'),\n ('2017-10-11 04:40:52', '2017-10-11 04:59:22'),\n ('2017-10-11 06:28:58', '2017-10-11 06:40:13'),\n ('2017-10-11 16:41:07', '2017-10-11 17:01:14'),\n ('2017-10-12 08:08:30', '2017-10-12 08:35:03'),\n ('2017-10-12 08:47:02', '2017-10-12 08:59:50'),\n ('2017-10-12 13:13:39', '2017-10-12 13:37:45'),\n ('2017-10-12 13:40:12', '2017-10-12 13:48:17'),\n ('2017-10-12 13:49:56', '2017-10-12 13:53:16'),\n ('2017-10-12 14:33:18', '2017-10-12 14:39:57'),\n ('2017-10-13 15:55:39', '2017-10-13 15:59:41'),\n ('2017-10-17 17:58:48', '2017-10-17 18:01:38'),\n ('2017-10-19 20:21:45', '2017-10-19 20:29:15'),\n ('2017-10-19 21:11:39', '2017-10-19 21:29:37'),\n ('2017-10-19 21:30:01', '2017-10-19 21:47:23'),\n ('2017-10-19 21:47:34', '2017-10-19 21:57:07'),\n ('2017-10-19 21:57:24', '2017-10-19 22:09:52'),\n ('2017-10-21 12:24:09', '2017-10-21 12:36:24'),\n ('2017-10-21 12:36:37', '2017-10-21 12:42:13'),\n ('2017-10-21 13:47:43', '2017-10-22 11:09:36'),\n ('2017-10-22 13:28:53', '2017-10-22 13:31:44'),\n ('2017-10-22 13:47:05', '2017-10-22 13:56:33'),\n ('2017-10-22 14:26:41', '2017-10-22 14:32:39'),\n ('2017-10-22 14:54:41', '2017-10-22 15:09:58'),\n ('2017-10-22 16:40:29', '2017-10-22 16:51:40'),\n ('2017-10-22 17:58:46', '2017-10-22 18:28:37'),\n ('2017-10-22 18:45:16', '2017-10-22 18:50:34'),\n ('2017-10-22 18:56:22', '2017-10-22 19:11:10'),\n ('2017-10-23 10:14:08', '2017-10-23 10:35:32'),\n ('2017-10-23 11:29:36', '2017-10-23 14:38:34'),\n ('2017-10-23 15:04:52', '2017-10-23 15:32:58'),\n ('2017-10-23 15:33:48', '2017-10-23 17:06:47'),\n ('2017-10-23 17:13:16', '2017-10-23 19:31:26'),\n ('2017-10-23 19:55:03', '2017-10-23 20:25:53'),\n ('2017-10-23 21:47:54', '2017-10-23 22:18:04'),\n ('2017-10-23 22:34:12', '2017-10-23 22:48:42'),\n ('2017-10-24 06:55:01', '2017-10-24 07:02:17'),\n ('2017-10-24 14:56:07', '2017-10-24 15:03:16'),\n ('2017-10-24 15:51:36', '2017-10-24 15:59:50'),\n ('2017-10-24 16:31:10', '2017-10-24 16:55:09'),\n ('2017-10-28 14:26:14', '2017-10-28 14:32:34'),\n ('2017-11-01 09:41:54', '2017-11-01 09:52:23'),\n ('2017-11-01 20:16:11', '2017-11-01 20:32:13'),\n ('2017-11-02 19:44:29', '2017-11-02 19:50:56'),\n ('2017-11-02 20:14:37', '2017-11-02 20:30:29'),\n ('2017-11-02 21:35:47', '2017-11-02 21:38:57'),\n ('2017-11-03 09:59:27', '2017-11-03 10:11:46'),\n ('2017-11-03 10:13:22', '2017-11-03 10:32:02'),\n ('2017-11-03 10:44:25', '2017-11-03 10:50:34'),\n ('2017-11-03 16:06:43', '2017-11-03 16:44:38'),\n ('2017-11-03 16:45:54', '2017-11-03 17:00:27'),\n ('2017-11-03 17:07:15', '2017-11-03 17:35:05'),\n ('2017-11-03 17:36:05', '2017-11-03 17:46:48'),\n ('2017-11-03 17:50:31', '2017-11-03 18:00:03'),\n ('2017-11-03 19:22:56', '2017-11-03 19:45:51'),\n ('2017-11-04 13:14:10', '2017-11-04 13:26:15'),\n ('2017-11-04 14:18:37', '2017-11-04 14:30:05'),\n ('2017-11-04 14:45:59', '2017-11-04 15:03:20'),\n ('2017-11-04 15:16:03', '2017-11-04 15:44:30'),\n ('2017-11-04 16:37:46', '2017-11-04 16:58:22'),\n ('2017-11-04 17:13:19', '2017-11-04 17:34:50'),\n ('2017-11-04 18:10:34', '2017-11-04 18:58:44'),\n ('2017-11-05 01:56:50', '2017-11-05 01:01:04'),\n ('2017-11-05 08:33:33', '2017-11-05 08:53:46'),\n ('2017-11-05 08:58:08', '2017-11-05 09:03:39'),\n ('2017-11-05 11:05:08', '2017-11-05 11:30:05'),\n ('2017-11-06 08:50:18', '2017-11-06 08:59:05'),\n ('2017-11-06 09:04:03', '2017-11-06 09:13:47'),\n ('2017-11-06 16:19:36', '2017-11-06 17:02:55'),\n ('2017-11-06 17:21:27', '2017-11-06 17:34:06'),\n ('2017-11-06 17:36:01', '2017-11-06 17:57:32'),\n ('2017-11-06 17:59:52', '2017-11-06 18:15:08'),\n ('2017-11-06 18:18:36', '2017-11-06 18:21:17'),\n ('2017-11-06 19:24:31', '2017-11-06 19:37:57'),\n ('2017-11-06 19:49:16', '2017-11-06 20:03:14'),\n ('2017-11-07 07:50:48', '2017-11-07 08:01:32'),\n ('2017-11-08 13:11:51', '2017-11-08 13:18:05'),\n ('2017-11-08 21:34:47', '2017-11-08 21:46:05'),\n ('2017-11-08 22:02:30', '2017-11-08 22:04:47'),\n ('2017-11-09 07:01:11', '2017-11-09 07:12:10'),\n ('2017-11-09 08:02:02', '2017-11-09 08:08:28'),\n ('2017-11-09 08:19:59', '2017-11-09 08:32:24'),\n ('2017-11-09 08:41:31', '2017-11-09 08:48:59'),\n ('2017-11-09 09:00:06', '2017-11-09 09:09:24'),\n ('2017-11-09 09:09:37', '2017-11-09 09:24:25'),\n ('2017-11-09 13:14:37', '2017-11-09 13:25:39'),\n ('2017-11-09 15:20:07', '2017-11-09 15:31:10'),\n ('2017-11-09 18:47:08', '2017-11-09 18:53:10'),\n ('2017-11-09 23:35:02', '2017-11-09 23:43:35'),\n ('2017-11-10 07:51:33', '2017-11-10 08:02:28'),\n ('2017-11-10 08:38:28', '2017-11-10 08:42:09'),\n ('2017-11-11 18:05:25', '2017-11-11 18:13:14'),\n ('2017-11-11 19:39:12', '2017-11-11 19:46:22'),\n ('2017-11-11 21:13:19', '2017-11-11 21:16:31'),\n ('2017-11-12 09:46:19', '2017-11-12 09:51:43'),\n ('2017-11-13 13:33:42', '2017-11-13 13:54:15'),\n ('2017-11-14 08:40:29', '2017-11-14 08:55:52'),\n ('2017-11-15 06:14:05', '2017-11-15 06:30:06'),\n ('2017-11-15 08:14:59', '2017-11-15 08:23:44'),\n ('2017-11-15 10:16:44', '2017-11-15 10:33:41'),\n ('2017-11-15 10:33:58', '2017-11-15 10:54:14'),\n ('2017-11-15 11:02:15', '2017-11-15 11:14:42'),\n ('2017-11-16 09:27:41', '2017-11-16 09:38:49'),\n ('2017-11-16 09:57:41', '2017-11-16 10:18:00'),\n ('2017-11-16 17:25:05', '2017-11-16 17:44:47'),\n ('2017-11-17 13:45:54', '2017-11-17 16:36:56'),\n ('2017-11-17 19:12:49', '2017-11-17 19:31:15'),\n ('2017-11-18 10:49:06', '2017-11-18 10:55:45'),\n ('2017-11-18 11:32:12', '2017-11-18 11:44:16'),\n ('2017-11-18 18:09:01', '2017-11-18 18:14:31'),\n ('2017-11-18 18:53:10', '2017-11-18 19:01:29'),\n ('2017-11-19 14:15:41', '2017-11-19 14:31:49'),\n ('2017-11-20 21:19:19', '2017-11-20 21:41:09'),\n ('2017-11-20 22:39:48', '2017-11-20 23:23:37'),\n ('2017-11-21 17:44:25', '2017-11-21 17:51:32'),\n ('2017-11-21 18:20:52', '2017-11-21 18:34:51'),\n ('2017-11-21 18:47:32', '2017-11-21 18:51:50'),\n ('2017-11-21 19:07:57', '2017-11-21 19:14:33'),\n ('2017-11-21 20:04:56', '2017-11-21 20:08:54'),\n ('2017-11-21 21:55:47', '2017-11-21 22:08:12'),\n ('2017-11-23 23:47:43', '2017-11-23 23:57:56'),\n ('2017-11-24 06:41:25', '2017-11-24 06:53:15'),\n ('2017-11-24 06:58:56', '2017-11-24 07:33:24'),\n ('2017-11-26 12:25:49', '2017-11-26 12:41:36'),\n ('2017-11-27 05:29:04', '2017-11-27 05:54:13'),\n ('2017-11-27 06:06:47', '2017-11-27 06:11:01'),\n ('2017-11-27 06:45:14', '2017-11-27 06:55:39'),\n ('2017-11-27 09:39:44', '2017-11-27 09:47:43'),\n ('2017-11-27 11:09:18', '2017-11-27 11:20:46'),\n ('2017-11-27 11:31:46', '2017-11-27 11:35:44'),\n ('2017-11-27 12:07:14', '2017-11-27 12:12:36'),\n ('2017-11-27 12:21:40', '2017-11-27 12:26:44'),\n ('2017-11-27 17:26:31', '2017-11-27 17:36:07'),\n ('2017-11-27 18:11:49', '2017-11-27 18:29:04'),\n ('2017-11-27 19:36:16', '2017-11-27 19:47:17'),\n ('2017-11-27 20:12:57', '2017-11-27 20:17:33'),\n ('2017-11-28 08:18:06', '2017-11-28 08:41:53'),\n ('2017-11-28 19:17:23', '2017-11-28 19:34:01'),\n ('2017-11-28 19:34:15', '2017-11-28 19:46:24'),\n ('2017-11-28 21:27:29', '2017-11-28 21:39:32'),\n ('2017-11-29 07:47:38', '2017-11-29 07:51:18'),\n ('2017-11-29 09:50:12', '2017-11-29 09:53:44'),\n ('2017-11-29 17:03:42', '2017-11-29 17:16:21'),\n ('2017-11-29 18:19:15', '2017-11-29 18:23:43'),\n ('2017-12-01 17:03:58', '2017-12-01 17:10:12'),\n ('2017-12-02 07:55:56', '2017-12-02 08:01:01'),\n ('2017-12-02 09:16:14', '2017-12-02 09:21:18'),\n ('2017-12-02 19:48:29', '2017-12-02 19:53:18'),\n ('2017-12-03 14:36:29', '2017-12-03 15:20:09'),\n ('2017-12-03 16:04:02', '2017-12-03 16:25:30'),\n ('2017-12-03 16:40:26', '2017-12-03 16:43:58'),\n ('2017-12-03 17:20:17', '2017-12-03 18:04:33'),\n ('2017-12-04 08:34:24', '2017-12-04 08:51:00'),\n ('2017-12-04 17:49:26', '2017-12-04 17:53:57'),\n ('2017-12-04 18:38:52', '2017-12-04 18:50:33'),\n ('2017-12-04 21:39:20', '2017-12-04 21:46:58'),\n ('2017-12-04 21:54:21', '2017-12-04 21:56:17'),\n ('2017-12-05 08:50:50', '2017-12-05 08:52:54'),\n ('2017-12-06 08:19:38', '2017-12-06 08:24:14'),\n ('2017-12-06 18:19:19', '2017-12-06 18:28:11'),\n ('2017-12-06 18:28:55', '2017-12-06 18:33:12'),\n ('2017-12-06 20:03:29', '2017-12-06 20:21:38'),\n ('2017-12-06 20:36:42', '2017-12-06 20:39:57'),\n ('2017-12-07 05:54:51', '2017-12-07 06:01:15'),\n ('2017-12-08 16:47:18', '2017-12-08 16:55:49'),\n ('2017-12-08 19:15:02', '2017-12-08 19:29:12'),\n ('2017-12-09 22:39:37', '2017-12-09 22:47:19'),\n ('2017-12-09 23:00:10', '2017-12-09 23:05:32'),\n ('2017-12-10 00:39:24', '2017-12-10 00:56:02'),\n ('2017-12-10 01:02:42', '2017-12-10 01:08:09'),\n ('2017-12-10 01:08:57', '2017-12-10 01:11:30'),\n ('2017-12-10 13:49:09', '2017-12-10 13:51:41'),\n ('2017-12-10 15:14:29', '2017-12-10 15:18:19'),\n ('2017-12-10 15:31:07', '2017-12-10 15:36:28'),\n ('2017-12-10 16:20:06', '2017-12-10 16:30:31'),\n ('2017-12-10 17:07:54', '2017-12-10 17:14:25'),\n ('2017-12-10 17:23:47', '2017-12-10 17:45:25'),\n ('2017-12-11 06:17:06', '2017-12-11 06:34:04'),\n ('2017-12-11 09:08:41', '2017-12-11 09:12:21'),\n ('2017-12-11 09:15:41', '2017-12-11 09:20:18'),\n ('2017-12-12 08:55:53', '2017-12-12 08:59:34'),\n ('2017-12-13 17:14:56', '2017-12-13 17:18:32'),\n ('2017-12-13 18:52:16', '2017-12-13 19:00:45'),\n ('2017-12-14 09:01:10', '2017-12-14 09:11:06'),\n ('2017-12-14 09:12:59', '2017-12-14 09:19:06'),\n ('2017-12-14 11:54:33', '2017-12-14 12:02:00'),\n ('2017-12-14 14:40:23', '2017-12-14 14:44:40'),\n ('2017-12-14 15:08:55', '2017-12-14 15:26:24'),\n ('2017-12-14 17:46:17', '2017-12-14 18:09:04'),\n ('2017-12-15 09:08:12', '2017-12-15 09:23:45'),\n ('2017-12-16 09:33:46', '2017-12-16 09:36:17'),\n ('2017-12-16 11:02:31', '2017-12-16 11:05:04'),\n ('2017-12-17 10:09:47', '2017-12-17 10:32:03'),\n ('2017-12-18 08:02:36', '2017-12-18 08:07:34'),\n ('2017-12-18 16:03:00', '2017-12-18 16:09:20'),\n ('2017-12-18 16:30:07', '2017-12-18 16:53:12'),\n ('2017-12-18 19:18:23', '2017-12-18 19:22:08'),\n ('2017-12-18 20:14:46', '2017-12-18 20:17:47'),\n ('2017-12-19 19:14:08', '2017-12-19 19:23:49'),\n ('2017-12-19 19:39:36', '2017-12-19 19:43:46'),\n ('2017-12-20 08:05:14', '2017-12-20 08:10:46'),\n ('2017-12-20 08:15:45', '2017-12-20 08:29:50'),\n ('2017-12-20 08:33:32', '2017-12-20 08:38:09'),\n ('2017-12-20 13:43:36', '2017-12-20 13:54:39'),\n ('2017-12-20 18:57:53', '2017-12-20 19:06:54'),\n ('2017-12-21 07:21:11', '2017-12-21 07:32:03'),\n ('2017-12-21 08:01:58', '2017-12-21 08:06:15'),\n ('2017-12-21 13:20:54', '2017-12-21 13:33:49'),\n ('2017-12-21 15:26:08', '2017-12-21 15:34:27'),\n ('2017-12-21 18:09:46', '2017-12-21 18:38:50'),\n ('2017-12-22 16:14:21', '2017-12-22 16:21:46'),\n ('2017-12-22 16:29:17', '2017-12-22 16:34:14'),\n ('2017-12-25 12:49:51', '2017-12-25 13:18:27'),\n ('2017-12-25 13:46:44', '2017-12-25 14:20:50'),\n ('2017-12-26 10:40:16', '2017-12-26 10:53:45'),\n ('2017-12-27 16:56:12', '2017-12-27 17:17:39'),\n ('2017-12-29 06:02:34', '2017-12-29 06:12:30'),\n ('2017-12-29 12:21:03', '2017-12-29 12:46:16'),\n ('2017-12-29 14:32:55', '2017-12-29 14:43:46'),\n ('2017-12-29 15:08:26', '2017-12-29 15:18:51'),\n ('2017-12-29 20:33:34', '2017-12-29 20:38:13'),\n ('2017-12-30 13:51:03', '2017-12-30 13:54:33'),\n ('2017-12-30 15:09:03', '2017-12-30 15:19:13')]\n\n\n# Write down the format string\nfmt = \"%Y-%m-%d %H:%M:%S\"\n\n# Initialize a list for holding the pairs of datetime objects\nonebike_datetimes = []\n\n# Loop over all trips\nfor (start, end) in onebike_datetime_strings:\n  trip = {'start': datetime.datetime.strptime(start, fmt),\n          'end': datetime.datetime.strptime(end, fmt)}\n  \n  # Append the trip\n  onebike_datetimes.append(trip)\nonebike_datetimes\n\n[{'start': datetime.datetime(2017, 10, 1, 15, 23, 25),\n  'end': datetime.datetime(2017, 10, 1, 15, 26, 26)},\n {'start': datetime.datetime(2017, 10, 1, 15, 42, 57),\n  'end': datetime.datetime(2017, 10, 1, 17, 49, 59)},\n {'start': datetime.datetime(2017, 10, 2, 6, 37, 10),\n  'end': datetime.datetime(2017, 10, 2, 6, 42, 53)},\n {'start': datetime.datetime(2017, 10, 2, 8, 56, 45),\n  'end': datetime.datetime(2017, 10, 2, 9, 18, 3)},\n {'start': datetime.datetime(2017, 10, 2, 18, 23, 48),\n  'end': datetime.datetime(2017, 10, 2, 18, 45, 5)},\n {'start': datetime.datetime(2017, 10, 2, 18, 48, 8),\n  'end': datetime.datetime(2017, 10, 2, 19, 10, 54)},\n {'start': datetime.datetime(2017, 10, 2, 19, 18, 10),\n  'end': datetime.datetime(2017, 10, 2, 19, 31, 45)},\n {'start': datetime.datetime(2017, 10, 2, 19, 37, 32),\n  'end': datetime.datetime(2017, 10, 2, 19, 46, 37)},\n {'start': datetime.datetime(2017, 10, 3, 8, 24, 16),\n  'end': datetime.datetime(2017, 10, 3, 8, 32, 27)},\n {'start': datetime.datetime(2017, 10, 3, 18, 17, 7),\n  'end': datetime.datetime(2017, 10, 3, 18, 27, 46)},\n {'start': datetime.datetime(2017, 10, 3, 19, 24, 10),\n  'end': datetime.datetime(2017, 10, 3, 19, 52, 8)},\n {'start': datetime.datetime(2017, 10, 3, 20, 17, 6),\n  'end': datetime.datetime(2017, 10, 3, 20, 23, 52)},\n {'start': datetime.datetime(2017, 10, 3, 20, 45, 21),\n  'end': datetime.datetime(2017, 10, 3, 20, 57, 10)},\n {'start': datetime.datetime(2017, 10, 4, 7, 4, 57),\n  'end': datetime.datetime(2017, 10, 4, 7, 13, 31)},\n {'start': datetime.datetime(2017, 10, 4, 7, 13, 42),\n  'end': datetime.datetime(2017, 10, 4, 7, 21, 54)},\n {'start': datetime.datetime(2017, 10, 4, 14, 22, 12),\n  'end': datetime.datetime(2017, 10, 4, 14, 50)},\n {'start': datetime.datetime(2017, 10, 4, 15, 7, 27),\n  'end': datetime.datetime(2017, 10, 4, 15, 44, 49)},\n {'start': datetime.datetime(2017, 10, 4, 15, 46, 41),\n  'end': datetime.datetime(2017, 10, 4, 16, 32, 33)},\n {'start': datetime.datetime(2017, 10, 4, 16, 34, 44),\n  'end': datetime.datetime(2017, 10, 4, 16, 46, 59)},\n {'start': datetime.datetime(2017, 10, 4, 17, 26, 6),\n  'end': datetime.datetime(2017, 10, 4, 17, 31, 36)},\n {'start': datetime.datetime(2017, 10, 4, 17, 42, 3),\n  'end': datetime.datetime(2017, 10, 4, 17, 50, 41)},\n {'start': datetime.datetime(2017, 10, 5, 7, 49, 2),\n  'end': datetime.datetime(2017, 10, 5, 8, 12, 55)},\n {'start': datetime.datetime(2017, 10, 5, 8, 26, 21),\n  'end': datetime.datetime(2017, 10, 5, 8, 29, 45)},\n {'start': datetime.datetime(2017, 10, 5, 8, 33, 27),\n  'end': datetime.datetime(2017, 10, 5, 8, 38, 31)},\n {'start': datetime.datetime(2017, 10, 5, 16, 35, 35),\n  'end': datetime.datetime(2017, 10, 5, 16, 51, 52)},\n {'start': datetime.datetime(2017, 10, 5, 17, 53, 31),\n  'end': datetime.datetime(2017, 10, 5, 18, 16, 50)},\n {'start': datetime.datetime(2017, 10, 6, 8, 17, 17),\n  'end': datetime.datetime(2017, 10, 6, 8, 38, 1)},\n {'start': datetime.datetime(2017, 10, 6, 11, 39, 40),\n  'end': datetime.datetime(2017, 10, 6, 11, 50, 38)},\n {'start': datetime.datetime(2017, 10, 6, 12, 59, 54),\n  'end': datetime.datetime(2017, 10, 6, 13, 13, 14)},\n {'start': datetime.datetime(2017, 10, 6, 13, 43, 5),\n  'end': datetime.datetime(2017, 10, 6, 14, 14, 56)},\n {'start': datetime.datetime(2017, 10, 6, 14, 28, 15),\n  'end': datetime.datetime(2017, 10, 6, 15, 9, 26)},\n {'start': datetime.datetime(2017, 10, 6, 15, 50, 10),\n  'end': datetime.datetime(2017, 10, 6, 16, 12, 34)},\n {'start': datetime.datetime(2017, 10, 6, 16, 32, 16),\n  'end': datetime.datetime(2017, 10, 6, 16, 39, 31)},\n {'start': datetime.datetime(2017, 10, 6, 16, 44, 8),\n  'end': datetime.datetime(2017, 10, 6, 16, 48, 39)},\n {'start': datetime.datetime(2017, 10, 6, 16, 53, 43),\n  'end': datetime.datetime(2017, 10, 6, 17, 9, 3)},\n {'start': datetime.datetime(2017, 10, 7, 11, 38, 55),\n  'end': datetime.datetime(2017, 10, 7, 11, 53, 6)},\n {'start': datetime.datetime(2017, 10, 7, 14, 3, 36),\n  'end': datetime.datetime(2017, 10, 7, 14, 7, 5)},\n {'start': datetime.datetime(2017, 10, 7, 14, 20, 3),\n  'end': datetime.datetime(2017, 10, 7, 14, 27, 36)},\n {'start': datetime.datetime(2017, 10, 7, 14, 30, 50),\n  'end': datetime.datetime(2017, 10, 7, 14, 44, 51)},\n {'start': datetime.datetime(2017, 10, 8, 0, 28, 26),\n  'end': datetime.datetime(2017, 10, 8, 0, 30, 48)},\n {'start': datetime.datetime(2017, 10, 8, 11, 16, 21),\n  'end': datetime.datetime(2017, 10, 8, 11, 33, 24)},\n {'start': datetime.datetime(2017, 10, 8, 12, 37, 3),\n  'end': datetime.datetime(2017, 10, 8, 13, 1, 29)},\n {'start': datetime.datetime(2017, 10, 8, 13, 30, 37),\n  'end': datetime.datetime(2017, 10, 8, 13, 57, 53)},\n {'start': datetime.datetime(2017, 10, 8, 14, 16, 40),\n  'end': datetime.datetime(2017, 10, 8, 15, 7, 19)},\n {'start': datetime.datetime(2017, 10, 8, 15, 23, 50),\n  'end': datetime.datetime(2017, 10, 8, 15, 50, 1)},\n {'start': datetime.datetime(2017, 10, 8, 15, 54, 12),\n  'end': datetime.datetime(2017, 10, 8, 16, 17, 42)},\n {'start': datetime.datetime(2017, 10, 8, 16, 28, 52),\n  'end': datetime.datetime(2017, 10, 8, 16, 35, 18)},\n {'start': datetime.datetime(2017, 10, 8, 23, 8, 14),\n  'end': datetime.datetime(2017, 10, 8, 23, 33, 41)},\n {'start': datetime.datetime(2017, 10, 8, 23, 34, 49),\n  'end': datetime.datetime(2017, 10, 8, 23, 45, 11)},\n {'start': datetime.datetime(2017, 10, 8, 23, 46, 47),\n  'end': datetime.datetime(2017, 10, 9, 0, 10, 57)},\n {'start': datetime.datetime(2017, 10, 9, 0, 12, 58),\n  'end': datetime.datetime(2017, 10, 9, 0, 36, 40)},\n {'start': datetime.datetime(2017, 10, 9, 0, 37, 2),\n  'end': datetime.datetime(2017, 10, 9, 0, 53, 33)},\n {'start': datetime.datetime(2017, 10, 9, 1, 23, 29),\n  'end': datetime.datetime(2017, 10, 9, 1, 48, 13)},\n {'start': datetime.datetime(2017, 10, 9, 1, 49, 25),\n  'end': datetime.datetime(2017, 10, 9, 2, 13, 35)},\n {'start': datetime.datetime(2017, 10, 9, 2, 14, 11),\n  'end': datetime.datetime(2017, 10, 9, 2, 29, 40)},\n {'start': datetime.datetime(2017, 10, 9, 13, 4, 32),\n  'end': datetime.datetime(2017, 10, 9, 13, 13, 25)},\n {'start': datetime.datetime(2017, 10, 9, 14, 30, 10),\n  'end': datetime.datetime(2017, 10, 9, 14, 38, 55)},\n {'start': datetime.datetime(2017, 10, 9, 15, 6, 47),\n  'end': datetime.datetime(2017, 10, 9, 15, 11, 30)},\n {'start': datetime.datetime(2017, 10, 9, 16, 43, 25),\n  'end': datetime.datetime(2017, 10, 9, 16, 45, 38)},\n {'start': datetime.datetime(2017, 10, 10, 15, 32, 58),\n  'end': datetime.datetime(2017, 10, 10, 15, 51, 24)},\n {'start': datetime.datetime(2017, 10, 10, 16, 47, 55),\n  'end': datetime.datetime(2017, 10, 10, 17, 3, 47)},\n {'start': datetime.datetime(2017, 10, 10, 17, 51, 5),\n  'end': datetime.datetime(2017, 10, 10, 18, 0, 18)},\n {'start': datetime.datetime(2017, 10, 10, 18, 8, 12),\n  'end': datetime.datetime(2017, 10, 10, 18, 19, 11)},\n {'start': datetime.datetime(2017, 10, 10, 19, 9, 35),\n  'end': datetime.datetime(2017, 10, 10, 19, 14, 32)},\n {'start': datetime.datetime(2017, 10, 10, 19, 17, 11),\n  'end': datetime.datetime(2017, 10, 10, 19, 23, 8)},\n {'start': datetime.datetime(2017, 10, 10, 19, 28, 11),\n  'end': datetime.datetime(2017, 10, 10, 19, 44, 40)},\n {'start': datetime.datetime(2017, 10, 10, 19, 55, 35),\n  'end': datetime.datetime(2017, 10, 10, 20, 11, 54)},\n {'start': datetime.datetime(2017, 10, 10, 22, 20, 43),\n  'end': datetime.datetime(2017, 10, 10, 22, 33, 23)},\n {'start': datetime.datetime(2017, 10, 11, 4, 40, 52),\n  'end': datetime.datetime(2017, 10, 11, 4, 59, 22)},\n {'start': datetime.datetime(2017, 10, 11, 6, 28, 58),\n  'end': datetime.datetime(2017, 10, 11, 6, 40, 13)},\n {'start': datetime.datetime(2017, 10, 11, 16, 41, 7),\n  'end': datetime.datetime(2017, 10, 11, 17, 1, 14)},\n {'start': datetime.datetime(2017, 10, 12, 8, 8, 30),\n  'end': datetime.datetime(2017, 10, 12, 8, 35, 3)},\n {'start': datetime.datetime(2017, 10, 12, 8, 47, 2),\n  'end': datetime.datetime(2017, 10, 12, 8, 59, 50)},\n {'start': datetime.datetime(2017, 10, 12, 13, 13, 39),\n  'end': datetime.datetime(2017, 10, 12, 13, 37, 45)},\n {'start': datetime.datetime(2017, 10, 12, 13, 40, 12),\n  'end': datetime.datetime(2017, 10, 12, 13, 48, 17)},\n {'start': datetime.datetime(2017, 10, 12, 13, 49, 56),\n  'end': datetime.datetime(2017, 10, 12, 13, 53, 16)},\n {'start': datetime.datetime(2017, 10, 12, 14, 33, 18),\n  'end': datetime.datetime(2017, 10, 12, 14, 39, 57)},\n {'start': datetime.datetime(2017, 10, 13, 15, 55, 39),\n  'end': datetime.datetime(2017, 10, 13, 15, 59, 41)},\n {'start': datetime.datetime(2017, 10, 17, 17, 58, 48),\n  'end': datetime.datetime(2017, 10, 17, 18, 1, 38)},\n {'start': datetime.datetime(2017, 10, 19, 20, 21, 45),\n  'end': datetime.datetime(2017, 10, 19, 20, 29, 15)},\n {'start': datetime.datetime(2017, 10, 19, 21, 11, 39),\n  'end': datetime.datetime(2017, 10, 19, 21, 29, 37)},\n {'start': datetime.datetime(2017, 10, 19, 21, 30, 1),\n  'end': datetime.datetime(2017, 10, 19, 21, 47, 23)},\n {'start': datetime.datetime(2017, 10, 19, 21, 47, 34),\n  'end': datetime.datetime(2017, 10, 19, 21, 57, 7)},\n {'start': datetime.datetime(2017, 10, 19, 21, 57, 24),\n  'end': datetime.datetime(2017, 10, 19, 22, 9, 52)},\n {'start': datetime.datetime(2017, 10, 21, 12, 24, 9),\n  'end': datetime.datetime(2017, 10, 21, 12, 36, 24)},\n {'start': datetime.datetime(2017, 10, 21, 12, 36, 37),\n  'end': datetime.datetime(2017, 10, 21, 12, 42, 13)},\n {'start': datetime.datetime(2017, 10, 21, 13, 47, 43),\n  'end': datetime.datetime(2017, 10, 22, 11, 9, 36)},\n {'start': datetime.datetime(2017, 10, 22, 13, 28, 53),\n  'end': datetime.datetime(2017, 10, 22, 13, 31, 44)},\n {'start': datetime.datetime(2017, 10, 22, 13, 47, 5),\n  'end': datetime.datetime(2017, 10, 22, 13, 56, 33)},\n {'start': datetime.datetime(2017, 10, 22, 14, 26, 41),\n  'end': datetime.datetime(2017, 10, 22, 14, 32, 39)},\n {'start': datetime.datetime(2017, 10, 22, 14, 54, 41),\n  'end': datetime.datetime(2017, 10, 22, 15, 9, 58)},\n {'start': datetime.datetime(2017, 10, 22, 16, 40, 29),\n  'end': datetime.datetime(2017, 10, 22, 16, 51, 40)},\n {'start': datetime.datetime(2017, 10, 22, 17, 58, 46),\n  'end': datetime.datetime(2017, 10, 22, 18, 28, 37)},\n {'start': datetime.datetime(2017, 10, 22, 18, 45, 16),\n  'end': datetime.datetime(2017, 10, 22, 18, 50, 34)},\n {'start': datetime.datetime(2017, 10, 22, 18, 56, 22),\n  'end': datetime.datetime(2017, 10, 22, 19, 11, 10)},\n {'start': datetime.datetime(2017, 10, 23, 10, 14, 8),\n  'end': datetime.datetime(2017, 10, 23, 10, 35, 32)},\n {'start': datetime.datetime(2017, 10, 23, 11, 29, 36),\n  'end': datetime.datetime(2017, 10, 23, 14, 38, 34)},\n {'start': datetime.datetime(2017, 10, 23, 15, 4, 52),\n  'end': datetime.datetime(2017, 10, 23, 15, 32, 58)},\n {'start': datetime.datetime(2017, 10, 23, 15, 33, 48),\n  'end': datetime.datetime(2017, 10, 23, 17, 6, 47)},\n {'start': datetime.datetime(2017, 10, 23, 17, 13, 16),\n  'end': datetime.datetime(2017, 10, 23, 19, 31, 26)},\n {'start': datetime.datetime(2017, 10, 23, 19, 55, 3),\n  'end': datetime.datetime(2017, 10, 23, 20, 25, 53)},\n {'start': datetime.datetime(2017, 10, 23, 21, 47, 54),\n  'end': datetime.datetime(2017, 10, 23, 22, 18, 4)},\n {'start': datetime.datetime(2017, 10, 23, 22, 34, 12),\n  'end': datetime.datetime(2017, 10, 23, 22, 48, 42)},\n {'start': datetime.datetime(2017, 10, 24, 6, 55, 1),\n  'end': datetime.datetime(2017, 10, 24, 7, 2, 17)},\n {'start': datetime.datetime(2017, 10, 24, 14, 56, 7),\n  'end': datetime.datetime(2017, 10, 24, 15, 3, 16)},\n {'start': datetime.datetime(2017, 10, 24, 15, 51, 36),\n  'end': datetime.datetime(2017, 10, 24, 15, 59, 50)},\n {'start': datetime.datetime(2017, 10, 24, 16, 31, 10),\n  'end': datetime.datetime(2017, 10, 24, 16, 55, 9)},\n {'start': datetime.datetime(2017, 10, 28, 14, 26, 14),\n  'end': datetime.datetime(2017, 10, 28, 14, 32, 34)},\n {'start': datetime.datetime(2017, 11, 1, 9, 41, 54),\n  'end': datetime.datetime(2017, 11, 1, 9, 52, 23)},\n {'start': datetime.datetime(2017, 11, 1, 20, 16, 11),\n  'end': datetime.datetime(2017, 11, 1, 20, 32, 13)},\n {'start': datetime.datetime(2017, 11, 2, 19, 44, 29),\n  'end': datetime.datetime(2017, 11, 2, 19, 50, 56)},\n {'start': datetime.datetime(2017, 11, 2, 20, 14, 37),\n  'end': datetime.datetime(2017, 11, 2, 20, 30, 29)},\n {'start': datetime.datetime(2017, 11, 2, 21, 35, 47),\n  'end': datetime.datetime(2017, 11, 2, 21, 38, 57)},\n {'start': datetime.datetime(2017, 11, 3, 9, 59, 27),\n  'end': datetime.datetime(2017, 11, 3, 10, 11, 46)},\n {'start': datetime.datetime(2017, 11, 3, 10, 13, 22),\n  'end': datetime.datetime(2017, 11, 3, 10, 32, 2)},\n {'start': datetime.datetime(2017, 11, 3, 10, 44, 25),\n  'end': datetime.datetime(2017, 11, 3, 10, 50, 34)},\n {'start': datetime.datetime(2017, 11, 3, 16, 6, 43),\n  'end': datetime.datetime(2017, 11, 3, 16, 44, 38)},\n {'start': datetime.datetime(2017, 11, 3, 16, 45, 54),\n  'end': datetime.datetime(2017, 11, 3, 17, 0, 27)},\n {'start': datetime.datetime(2017, 11, 3, 17, 7, 15),\n  'end': datetime.datetime(2017, 11, 3, 17, 35, 5)},\n {'start': datetime.datetime(2017, 11, 3, 17, 36, 5),\n  'end': datetime.datetime(2017, 11, 3, 17, 46, 48)},\n {'start': datetime.datetime(2017, 11, 3, 17, 50, 31),\n  'end': datetime.datetime(2017, 11, 3, 18, 0, 3)},\n {'start': datetime.datetime(2017, 11, 3, 19, 22, 56),\n  'end': datetime.datetime(2017, 11, 3, 19, 45, 51)},\n {'start': datetime.datetime(2017, 11, 4, 13, 14, 10),\n  'end': datetime.datetime(2017, 11, 4, 13, 26, 15)},\n {'start': datetime.datetime(2017, 11, 4, 14, 18, 37),\n  'end': datetime.datetime(2017, 11, 4, 14, 30, 5)},\n {'start': datetime.datetime(2017, 11, 4, 14, 45, 59),\n  'end': datetime.datetime(2017, 11, 4, 15, 3, 20)},\n {'start': datetime.datetime(2017, 11, 4, 15, 16, 3),\n  'end': datetime.datetime(2017, 11, 4, 15, 44, 30)},\n {'start': datetime.datetime(2017, 11, 4, 16, 37, 46),\n  'end': datetime.datetime(2017, 11, 4, 16, 58, 22)},\n {'start': datetime.datetime(2017, 11, 4, 17, 13, 19),\n  'end': datetime.datetime(2017, 11, 4, 17, 34, 50)},\n {'start': datetime.datetime(2017, 11, 4, 18, 10, 34),\n  'end': datetime.datetime(2017, 11, 4, 18, 58, 44)},\n {'start': datetime.datetime(2017, 11, 5, 1, 56, 50),\n  'end': datetime.datetime(2017, 11, 5, 1, 1, 4)},\n {'start': datetime.datetime(2017, 11, 5, 8, 33, 33),\n  'end': datetime.datetime(2017, 11, 5, 8, 53, 46)},\n {'start': datetime.datetime(2017, 11, 5, 8, 58, 8),\n  'end': datetime.datetime(2017, 11, 5, 9, 3, 39)},\n {'start': datetime.datetime(2017, 11, 5, 11, 5, 8),\n  'end': datetime.datetime(2017, 11, 5, 11, 30, 5)},\n {'start': datetime.datetime(2017, 11, 6, 8, 50, 18),\n  'end': datetime.datetime(2017, 11, 6, 8, 59, 5)},\n {'start': datetime.datetime(2017, 11, 6, 9, 4, 3),\n  'end': datetime.datetime(2017, 11, 6, 9, 13, 47)},\n {'start': datetime.datetime(2017, 11, 6, 16, 19, 36),\n  'end': datetime.datetime(2017, 11, 6, 17, 2, 55)},\n {'start': datetime.datetime(2017, 11, 6, 17, 21, 27),\n  'end': datetime.datetime(2017, 11, 6, 17, 34, 6)},\n {'start': datetime.datetime(2017, 11, 6, 17, 36, 1),\n  'end': datetime.datetime(2017, 11, 6, 17, 57, 32)},\n {'start': datetime.datetime(2017, 11, 6, 17, 59, 52),\n  'end': datetime.datetime(2017, 11, 6, 18, 15, 8)},\n {'start': datetime.datetime(2017, 11, 6, 18, 18, 36),\n  'end': datetime.datetime(2017, 11, 6, 18, 21, 17)},\n {'start': datetime.datetime(2017, 11, 6, 19, 24, 31),\n  'end': datetime.datetime(2017, 11, 6, 19, 37, 57)},\n {'start': datetime.datetime(2017, 11, 6, 19, 49, 16),\n  'end': datetime.datetime(2017, 11, 6, 20, 3, 14)},\n {'start': datetime.datetime(2017, 11, 7, 7, 50, 48),\n  'end': datetime.datetime(2017, 11, 7, 8, 1, 32)},\n {'start': datetime.datetime(2017, 11, 8, 13, 11, 51),\n  'end': datetime.datetime(2017, 11, 8, 13, 18, 5)},\n {'start': datetime.datetime(2017, 11, 8, 21, 34, 47),\n  'end': datetime.datetime(2017, 11, 8, 21, 46, 5)},\n {'start': datetime.datetime(2017, 11, 8, 22, 2, 30),\n  'end': datetime.datetime(2017, 11, 8, 22, 4, 47)},\n {'start': datetime.datetime(2017, 11, 9, 7, 1, 11),\n  'end': datetime.datetime(2017, 11, 9, 7, 12, 10)},\n {'start': datetime.datetime(2017, 11, 9, 8, 2, 2),\n  'end': datetime.datetime(2017, 11, 9, 8, 8, 28)},\n {'start': datetime.datetime(2017, 11, 9, 8, 19, 59),\n  'end': datetime.datetime(2017, 11, 9, 8, 32, 24)},\n {'start': datetime.datetime(2017, 11, 9, 8, 41, 31),\n  'end': datetime.datetime(2017, 11, 9, 8, 48, 59)},\n {'start': datetime.datetime(2017, 11, 9, 9, 0, 6),\n  'end': datetime.datetime(2017, 11, 9, 9, 9, 24)},\n {'start': datetime.datetime(2017, 11, 9, 9, 9, 37),\n  'end': datetime.datetime(2017, 11, 9, 9, 24, 25)},\n {'start': datetime.datetime(2017, 11, 9, 13, 14, 37),\n  'end': datetime.datetime(2017, 11, 9, 13, 25, 39)},\n {'start': datetime.datetime(2017, 11, 9, 15, 20, 7),\n  'end': datetime.datetime(2017, 11, 9, 15, 31, 10)},\n {'start': datetime.datetime(2017, 11, 9, 18, 47, 8),\n  'end': datetime.datetime(2017, 11, 9, 18, 53, 10)},\n {'start': datetime.datetime(2017, 11, 9, 23, 35, 2),\n  'end': datetime.datetime(2017, 11, 9, 23, 43, 35)},\n {'start': datetime.datetime(2017, 11, 10, 7, 51, 33),\n  'end': datetime.datetime(2017, 11, 10, 8, 2, 28)},\n {'start': datetime.datetime(2017, 11, 10, 8, 38, 28),\n  'end': datetime.datetime(2017, 11, 10, 8, 42, 9)},\n {'start': datetime.datetime(2017, 11, 11, 18, 5, 25),\n  'end': datetime.datetime(2017, 11, 11, 18, 13, 14)},\n {'start': datetime.datetime(2017, 11, 11, 19, 39, 12),\n  'end': datetime.datetime(2017, 11, 11, 19, 46, 22)},\n {'start': datetime.datetime(2017, 11, 11, 21, 13, 19),\n  'end': datetime.datetime(2017, 11, 11, 21, 16, 31)},\n {'start': datetime.datetime(2017, 11, 12, 9, 46, 19),\n  'end': datetime.datetime(2017, 11, 12, 9, 51, 43)},\n {'start': datetime.datetime(2017, 11, 13, 13, 33, 42),\n  'end': datetime.datetime(2017, 11, 13, 13, 54, 15)},\n {'start': datetime.datetime(2017, 11, 14, 8, 40, 29),\n  'end': datetime.datetime(2017, 11, 14, 8, 55, 52)},\n {'start': datetime.datetime(2017, 11, 15, 6, 14, 5),\n  'end': datetime.datetime(2017, 11, 15, 6, 30, 6)},\n {'start': datetime.datetime(2017, 11, 15, 8, 14, 59),\n  'end': datetime.datetime(2017, 11, 15, 8, 23, 44)},\n {'start': datetime.datetime(2017, 11, 15, 10, 16, 44),\n  'end': datetime.datetime(2017, 11, 15, 10, 33, 41)},\n {'start': datetime.datetime(2017, 11, 15, 10, 33, 58),\n  'end': datetime.datetime(2017, 11, 15, 10, 54, 14)},\n {'start': datetime.datetime(2017, 11, 15, 11, 2, 15),\n  'end': datetime.datetime(2017, 11, 15, 11, 14, 42)},\n {'start': datetime.datetime(2017, 11, 16, 9, 27, 41),\n  'end': datetime.datetime(2017, 11, 16, 9, 38, 49)},\n {'start': datetime.datetime(2017, 11, 16, 9, 57, 41),\n  'end': datetime.datetime(2017, 11, 16, 10, 18)},\n {'start': datetime.datetime(2017, 11, 16, 17, 25, 5),\n  'end': datetime.datetime(2017, 11, 16, 17, 44, 47)},\n {'start': datetime.datetime(2017, 11, 17, 13, 45, 54),\n  'end': datetime.datetime(2017, 11, 17, 16, 36, 56)},\n {'start': datetime.datetime(2017, 11, 17, 19, 12, 49),\n  'end': datetime.datetime(2017, 11, 17, 19, 31, 15)},\n {'start': datetime.datetime(2017, 11, 18, 10, 49, 6),\n  'end': datetime.datetime(2017, 11, 18, 10, 55, 45)},\n {'start': datetime.datetime(2017, 11, 18, 11, 32, 12),\n  'end': datetime.datetime(2017, 11, 18, 11, 44, 16)},\n {'start': datetime.datetime(2017, 11, 18, 18, 9, 1),\n  'end': datetime.datetime(2017, 11, 18, 18, 14, 31)},\n {'start': datetime.datetime(2017, 11, 18, 18, 53, 10),\n  'end': datetime.datetime(2017, 11, 18, 19, 1, 29)},\n {'start': datetime.datetime(2017, 11, 19, 14, 15, 41),\n  'end': datetime.datetime(2017, 11, 19, 14, 31, 49)},\n {'start': datetime.datetime(2017, 11, 20, 21, 19, 19),\n  'end': datetime.datetime(2017, 11, 20, 21, 41, 9)},\n {'start': datetime.datetime(2017, 11, 20, 22, 39, 48),\n  'end': datetime.datetime(2017, 11, 20, 23, 23, 37)},\n {'start': datetime.datetime(2017, 11, 21, 17, 44, 25),\n  'end': datetime.datetime(2017, 11, 21, 17, 51, 32)},\n {'start': datetime.datetime(2017, 11, 21, 18, 20, 52),\n  'end': datetime.datetime(2017, 11, 21, 18, 34, 51)},\n {'start': datetime.datetime(2017, 11, 21, 18, 47, 32),\n  'end': datetime.datetime(2017, 11, 21, 18, 51, 50)},\n {'start': datetime.datetime(2017, 11, 21, 19, 7, 57),\n  'end': datetime.datetime(2017, 11, 21, 19, 14, 33)},\n {'start': datetime.datetime(2017, 11, 21, 20, 4, 56),\n  'end': datetime.datetime(2017, 11, 21, 20, 8, 54)},\n {'start': datetime.datetime(2017, 11, 21, 21, 55, 47),\n  'end': datetime.datetime(2017, 11, 21, 22, 8, 12)},\n {'start': datetime.datetime(2017, 11, 23, 23, 47, 43),\n  'end': datetime.datetime(2017, 11, 23, 23, 57, 56)},\n {'start': datetime.datetime(2017, 11, 24, 6, 41, 25),\n  'end': datetime.datetime(2017, 11, 24, 6, 53, 15)},\n {'start': datetime.datetime(2017, 11, 24, 6, 58, 56),\n  'end': datetime.datetime(2017, 11, 24, 7, 33, 24)},\n {'start': datetime.datetime(2017, 11, 26, 12, 25, 49),\n  'end': datetime.datetime(2017, 11, 26, 12, 41, 36)},\n {'start': datetime.datetime(2017, 11, 27, 5, 29, 4),\n  'end': datetime.datetime(2017, 11, 27, 5, 54, 13)},\n {'start': datetime.datetime(2017, 11, 27, 6, 6, 47),\n  'end': datetime.datetime(2017, 11, 27, 6, 11, 1)},\n {'start': datetime.datetime(2017, 11, 27, 6, 45, 14),\n  'end': datetime.datetime(2017, 11, 27, 6, 55, 39)},\n {'start': datetime.datetime(2017, 11, 27, 9, 39, 44),\n  'end': datetime.datetime(2017, 11, 27, 9, 47, 43)},\n {'start': datetime.datetime(2017, 11, 27, 11, 9, 18),\n  'end': datetime.datetime(2017, 11, 27, 11, 20, 46)},\n {'start': datetime.datetime(2017, 11, 27, 11, 31, 46),\n  'end': datetime.datetime(2017, 11, 27, 11, 35, 44)},\n {'start': datetime.datetime(2017, 11, 27, 12, 7, 14),\n  'end': datetime.datetime(2017, 11, 27, 12, 12, 36)},\n {'start': datetime.datetime(2017, 11, 27, 12, 21, 40),\n  'end': datetime.datetime(2017, 11, 27, 12, 26, 44)},\n {'start': datetime.datetime(2017, 11, 27, 17, 26, 31),\n  'end': datetime.datetime(2017, 11, 27, 17, 36, 7)},\n {'start': datetime.datetime(2017, 11, 27, 18, 11, 49),\n  'end': datetime.datetime(2017, 11, 27, 18, 29, 4)},\n {'start': datetime.datetime(2017, 11, 27, 19, 36, 16),\n  'end': datetime.datetime(2017, 11, 27, 19, 47, 17)},\n {'start': datetime.datetime(2017, 11, 27, 20, 12, 57),\n  'end': datetime.datetime(2017, 11, 27, 20, 17, 33)},\n {'start': datetime.datetime(2017, 11, 28, 8, 18, 6),\n  'end': datetime.datetime(2017, 11, 28, 8, 41, 53)},\n {'start': datetime.datetime(2017, 11, 28, 19, 17, 23),\n  'end': datetime.datetime(2017, 11, 28, 19, 34, 1)},\n {'start': datetime.datetime(2017, 11, 28, 19, 34, 15),\n  'end': datetime.datetime(2017, 11, 28, 19, 46, 24)},\n {'start': datetime.datetime(2017, 11, 28, 21, 27, 29),\n  'end': datetime.datetime(2017, 11, 28, 21, 39, 32)},\n {'start': datetime.datetime(2017, 11, 29, 7, 47, 38),\n  'end': datetime.datetime(2017, 11, 29, 7, 51, 18)},\n {'start': datetime.datetime(2017, 11, 29, 9, 50, 12),\n  'end': datetime.datetime(2017, 11, 29, 9, 53, 44)},\n {'start': datetime.datetime(2017, 11, 29, 17, 3, 42),\n  'end': datetime.datetime(2017, 11, 29, 17, 16, 21)},\n {'start': datetime.datetime(2017, 11, 29, 18, 19, 15),\n  'end': datetime.datetime(2017, 11, 29, 18, 23, 43)},\n {'start': datetime.datetime(2017, 12, 1, 17, 3, 58),\n  'end': datetime.datetime(2017, 12, 1, 17, 10, 12)},\n {'start': datetime.datetime(2017, 12, 2, 7, 55, 56),\n  'end': datetime.datetime(2017, 12, 2, 8, 1, 1)},\n {'start': datetime.datetime(2017, 12, 2, 9, 16, 14),\n  'end': datetime.datetime(2017, 12, 2, 9, 21, 18)},\n {'start': datetime.datetime(2017, 12, 2, 19, 48, 29),\n  'end': datetime.datetime(2017, 12, 2, 19, 53, 18)},\n {'start': datetime.datetime(2017, 12, 3, 14, 36, 29),\n  'end': datetime.datetime(2017, 12, 3, 15, 20, 9)},\n {'start': datetime.datetime(2017, 12, 3, 16, 4, 2),\n  'end': datetime.datetime(2017, 12, 3, 16, 25, 30)},\n {'start': datetime.datetime(2017, 12, 3, 16, 40, 26),\n  'end': datetime.datetime(2017, 12, 3, 16, 43, 58)},\n {'start': datetime.datetime(2017, 12, 3, 17, 20, 17),\n  'end': datetime.datetime(2017, 12, 3, 18, 4, 33)},\n {'start': datetime.datetime(2017, 12, 4, 8, 34, 24),\n  'end': datetime.datetime(2017, 12, 4, 8, 51)},\n {'start': datetime.datetime(2017, 12, 4, 17, 49, 26),\n  'end': datetime.datetime(2017, 12, 4, 17, 53, 57)},\n {'start': datetime.datetime(2017, 12, 4, 18, 38, 52),\n  'end': datetime.datetime(2017, 12, 4, 18, 50, 33)},\n {'start': datetime.datetime(2017, 12, 4, 21, 39, 20),\n  'end': datetime.datetime(2017, 12, 4, 21, 46, 58)},\n {'start': datetime.datetime(2017, 12, 4, 21, 54, 21),\n  'end': datetime.datetime(2017, 12, 4, 21, 56, 17)},\n {'start': datetime.datetime(2017, 12, 5, 8, 50, 50),\n  'end': datetime.datetime(2017, 12, 5, 8, 52, 54)},\n {'start': datetime.datetime(2017, 12, 6, 8, 19, 38),\n  'end': datetime.datetime(2017, 12, 6, 8, 24, 14)},\n {'start': datetime.datetime(2017, 12, 6, 18, 19, 19),\n  'end': datetime.datetime(2017, 12, 6, 18, 28, 11)},\n {'start': datetime.datetime(2017, 12, 6, 18, 28, 55),\n  'end': datetime.datetime(2017, 12, 6, 18, 33, 12)},\n {'start': datetime.datetime(2017, 12, 6, 20, 3, 29),\n  'end': datetime.datetime(2017, 12, 6, 20, 21, 38)},\n {'start': datetime.datetime(2017, 12, 6, 20, 36, 42),\n  'end': datetime.datetime(2017, 12, 6, 20, 39, 57)},\n {'start': datetime.datetime(2017, 12, 7, 5, 54, 51),\n  'end': datetime.datetime(2017, 12, 7, 6, 1, 15)},\n {'start': datetime.datetime(2017, 12, 8, 16, 47, 18),\n  'end': datetime.datetime(2017, 12, 8, 16, 55, 49)},\n {'start': datetime.datetime(2017, 12, 8, 19, 15, 2),\n  'end': datetime.datetime(2017, 12, 8, 19, 29, 12)},\n {'start': datetime.datetime(2017, 12, 9, 22, 39, 37),\n  'end': datetime.datetime(2017, 12, 9, 22, 47, 19)},\n {'start': datetime.datetime(2017, 12, 9, 23, 0, 10),\n  'end': datetime.datetime(2017, 12, 9, 23, 5, 32)},\n {'start': datetime.datetime(2017, 12, 10, 0, 39, 24),\n  'end': datetime.datetime(2017, 12, 10, 0, 56, 2)},\n {'start': datetime.datetime(2017, 12, 10, 1, 2, 42),\n  'end': datetime.datetime(2017, 12, 10, 1, 8, 9)},\n {'start': datetime.datetime(2017, 12, 10, 1, 8, 57),\n  'end': datetime.datetime(2017, 12, 10, 1, 11, 30)},\n {'start': datetime.datetime(2017, 12, 10, 13, 49, 9),\n  'end': datetime.datetime(2017, 12, 10, 13, 51, 41)},\n {'start': datetime.datetime(2017, 12, 10, 15, 14, 29),\n  'end': datetime.datetime(2017, 12, 10, 15, 18, 19)},\n {'start': datetime.datetime(2017, 12, 10, 15, 31, 7),\n  'end': datetime.datetime(2017, 12, 10, 15, 36, 28)},\n {'start': datetime.datetime(2017, 12, 10, 16, 20, 6),\n  'end': datetime.datetime(2017, 12, 10, 16, 30, 31)},\n {'start': datetime.datetime(2017, 12, 10, 17, 7, 54),\n  'end': datetime.datetime(2017, 12, 10, 17, 14, 25)},\n {'start': datetime.datetime(2017, 12, 10, 17, 23, 47),\n  'end': datetime.datetime(2017, 12, 10, 17, 45, 25)},\n {'start': datetime.datetime(2017, 12, 11, 6, 17, 6),\n  'end': datetime.datetime(2017, 12, 11, 6, 34, 4)},\n {'start': datetime.datetime(2017, 12, 11, 9, 8, 41),\n  'end': datetime.datetime(2017, 12, 11, 9, 12, 21)},\n {'start': datetime.datetime(2017, 12, 11, 9, 15, 41),\n  'end': datetime.datetime(2017, 12, 11, 9, 20, 18)},\n {'start': datetime.datetime(2017, 12, 12, 8, 55, 53),\n  'end': datetime.datetime(2017, 12, 12, 8, 59, 34)},\n {'start': datetime.datetime(2017, 12, 13, 17, 14, 56),\n  'end': datetime.datetime(2017, 12, 13, 17, 18, 32)},\n {'start': datetime.datetime(2017, 12, 13, 18, 52, 16),\n  'end': datetime.datetime(2017, 12, 13, 19, 0, 45)},\n {'start': datetime.datetime(2017, 12, 14, 9, 1, 10),\n  'end': datetime.datetime(2017, 12, 14, 9, 11, 6)},\n {'start': datetime.datetime(2017, 12, 14, 9, 12, 59),\n  'end': datetime.datetime(2017, 12, 14, 9, 19, 6)},\n {'start': datetime.datetime(2017, 12, 14, 11, 54, 33),\n  'end': datetime.datetime(2017, 12, 14, 12, 2)},\n {'start': datetime.datetime(2017, 12, 14, 14, 40, 23),\n  'end': datetime.datetime(2017, 12, 14, 14, 44, 40)},\n {'start': datetime.datetime(2017, 12, 14, 15, 8, 55),\n  'end': datetime.datetime(2017, 12, 14, 15, 26, 24)},\n {'start': datetime.datetime(2017, 12, 14, 17, 46, 17),\n  'end': datetime.datetime(2017, 12, 14, 18, 9, 4)},\n {'start': datetime.datetime(2017, 12, 15, 9, 8, 12),\n  'end': datetime.datetime(2017, 12, 15, 9, 23, 45)},\n {'start': datetime.datetime(2017, 12, 16, 9, 33, 46),\n  'end': datetime.datetime(2017, 12, 16, 9, 36, 17)},\n {'start': datetime.datetime(2017, 12, 16, 11, 2, 31),\n  'end': datetime.datetime(2017, 12, 16, 11, 5, 4)},\n {'start': datetime.datetime(2017, 12, 17, 10, 9, 47),\n  'end': datetime.datetime(2017, 12, 17, 10, 32, 3)},\n {'start': datetime.datetime(2017, 12, 18, 8, 2, 36),\n  'end': datetime.datetime(2017, 12, 18, 8, 7, 34)},\n {'start': datetime.datetime(2017, 12, 18, 16, 3),\n  'end': datetime.datetime(2017, 12, 18, 16, 9, 20)},\n {'start': datetime.datetime(2017, 12, 18, 16, 30, 7),\n  'end': datetime.datetime(2017, 12, 18, 16, 53, 12)},\n {'start': datetime.datetime(2017, 12, 18, 19, 18, 23),\n  'end': datetime.datetime(2017, 12, 18, 19, 22, 8)},\n {'start': datetime.datetime(2017, 12, 18, 20, 14, 46),\n  'end': datetime.datetime(2017, 12, 18, 20, 17, 47)},\n {'start': datetime.datetime(2017, 12, 19, 19, 14, 8),\n  'end': datetime.datetime(2017, 12, 19, 19, 23, 49)},\n {'start': datetime.datetime(2017, 12, 19, 19, 39, 36),\n  'end': datetime.datetime(2017, 12, 19, 19, 43, 46)},\n {'start': datetime.datetime(2017, 12, 20, 8, 5, 14),\n  'end': datetime.datetime(2017, 12, 20, 8, 10, 46)},\n {'start': datetime.datetime(2017, 12, 20, 8, 15, 45),\n  'end': datetime.datetime(2017, 12, 20, 8, 29, 50)},\n {'start': datetime.datetime(2017, 12, 20, 8, 33, 32),\n  'end': datetime.datetime(2017, 12, 20, 8, 38, 9)},\n {'start': datetime.datetime(2017, 12, 20, 13, 43, 36),\n  'end': datetime.datetime(2017, 12, 20, 13, 54, 39)},\n {'start': datetime.datetime(2017, 12, 20, 18, 57, 53),\n  'end': datetime.datetime(2017, 12, 20, 19, 6, 54)},\n {'start': datetime.datetime(2017, 12, 21, 7, 21, 11),\n  'end': datetime.datetime(2017, 12, 21, 7, 32, 3)},\n {'start': datetime.datetime(2017, 12, 21, 8, 1, 58),\n  'end': datetime.datetime(2017, 12, 21, 8, 6, 15)},\n {'start': datetime.datetime(2017, 12, 21, 13, 20, 54),\n  'end': datetime.datetime(2017, 12, 21, 13, 33, 49)},\n {'start': datetime.datetime(2017, 12, 21, 15, 26, 8),\n  'end': datetime.datetime(2017, 12, 21, 15, 34, 27)},\n {'start': datetime.datetime(2017, 12, 21, 18, 9, 46),\n  'end': datetime.datetime(2017, 12, 21, 18, 38, 50)},\n {'start': datetime.datetime(2017, 12, 22, 16, 14, 21),\n  'end': datetime.datetime(2017, 12, 22, 16, 21, 46)},\n {'start': datetime.datetime(2017, 12, 22, 16, 29, 17),\n  'end': datetime.datetime(2017, 12, 22, 16, 34, 14)},\n {'start': datetime.datetime(2017, 12, 25, 12, 49, 51),\n  'end': datetime.datetime(2017, 12, 25, 13, 18, 27)},\n {'start': datetime.datetime(2017, 12, 25, 13, 46, 44),\n  'end': datetime.datetime(2017, 12, 25, 14, 20, 50)},\n {'start': datetime.datetime(2017, 12, 26, 10, 40, 16),\n  'end': datetime.datetime(2017, 12, 26, 10, 53, 45)},\n {'start': datetime.datetime(2017, 12, 27, 16, 56, 12),\n  'end': datetime.datetime(2017, 12, 27, 17, 17, 39)},\n {'start': datetime.datetime(2017, 12, 29, 6, 2, 34),\n  'end': datetime.datetime(2017, 12, 29, 6, 12, 30)},\n {'start': datetime.datetime(2017, 12, 29, 12, 21, 3),\n  'end': datetime.datetime(2017, 12, 29, 12, 46, 16)},\n {'start': datetime.datetime(2017, 12, 29, 14, 32, 55),\n  'end': datetime.datetime(2017, 12, 29, 14, 43, 46)},\n {'start': datetime.datetime(2017, 12, 29, 15, 8, 26),\n  'end': datetime.datetime(2017, 12, 29, 15, 18, 51)},\n {'start': datetime.datetime(2017, 12, 29, 20, 33, 34),\n  'end': datetime.datetime(2017, 12, 29, 20, 38, 13)},\n {'start': datetime.datetime(2017, 12, 30, 13, 51, 3),\n  'end': datetime.datetime(2017, 12, 30, 13, 54, 33)},\n {'start': datetime.datetime(2017, 12, 30, 15, 9, 3),\n  'end': datetime.datetime(2017, 12, 30, 15, 19, 13)}]\n\n\n\n\nRecreating ISO format with strftime()"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#utc-offsets",
    "href": "posts/2020-07-26-working with dates and times in python.html#utc-offsets",
    "title": "Working With Dates and Times in Python",
    "section": "UTC offsets",
    "text": "UTC offsets\n\nCreating timezone aware datetimes\n\n# Import datetime, timezone\nfrom datetime import datetime, timezone, timedelta\n\n# October 1, 2017 at 15:26:26, UTC\ndt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=timezone.utc)\n\n# Print results\nprint(dt.isoformat())\n\n2017-10-01T15:26:26+00:00\n\n\n\n# Import datetime, timedelta, timezone\nfrom datetime import datetime, timedelta, timezone\n\n# Create a timezone for Pacific Standard Time, or UTC-8\npst = timezone(timedelta(hours=-8))\n\n# October 1, 2017 at 15:26:26, UTC-8\ndt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=pst)\n\n# Print results\nprint(dt.isoformat())\n\n2017-10-01T15:26:26-08:00\n\n\n\n# Import datetime, timedelta, timezone\nfrom datetime import datetime, timedelta, timezone\n\n# Create a timezone for Australian Eastern Daylight Time, or UTC+11\naedt = timezone(timedelta(hours=11))\n\n# October 1, 2017 at 15:26:26, UTC+11\ndt = datetime(2017, 10, 1, 15, 26, 26, tzinfo=aedt)\n\n# Print results\nprint(dt.isoformat())\n\n2017-10-01T15:26:26+11:00\n\n\n\nDid you know that Russia and France are tied for the most number of time zones, with 12 each? The French mainland only has one timezone, but because France has so many overseas dependencies they really add up!\n\n\n\nSetting timezones\n\n# Create a timezone object corresponding to UTC-4\nedt = timezone(timedelta(hours=-4))\n\n# Loop over trips, updating the start and end datetimes to be in UTC-4\nfor trip in onebike_datetimes[:10]:\n  # Update trip['start'] and trip['end']\n  trip['start'] = trip['start'].replace(tzinfo=edt)\n  trip['end'] = trip['end'].replace(tzinfo=edt)\n\n\nDid you know that despite being over 2,500 miles (4,200 km) wide (about as wide as the continential United States or the European Union) China has only one official timezone? There’s a second, unofficial timezone, too. It is used by much of the Uyghurs population in the Xinjiang province in the far west of China.\n\n\n\nWhat time did the bike leave in UTC?\nHaving set the timezone for the first ten rides that W20529 took, let’s see what time the bike left in UTC.\n\n# Loop over the trips\nfor trip in onebike_datetimes[:10]:\n  # Pull out the start and set it to UTC\n  dt = trip['start'].astimezone(timezone.utc)\n  \n  # Print the start time in UTC\n  print('Original:', trip['start'], '| UTC:', dt.isoformat())\n\nOriginal: 2017-10-01 15:23:25-04:00 | UTC: 2017-10-01T19:23:25+00:00\nOriginal: 2017-10-01 15:42:57-04:00 | UTC: 2017-10-01T19:42:57+00:00\nOriginal: 2017-10-02 06:37:10-04:00 | UTC: 2017-10-02T10:37:10+00:00\nOriginal: 2017-10-02 08:56:45-04:00 | UTC: 2017-10-02T12:56:45+00:00\nOriginal: 2017-10-02 18:23:48-04:00 | UTC: 2017-10-02T22:23:48+00:00\nOriginal: 2017-10-02 18:48:08-04:00 | UTC: 2017-10-02T22:48:08+00:00\nOriginal: 2017-10-02 19:18:10-04:00 | UTC: 2017-10-02T23:18:10+00:00\nOriginal: 2017-10-02 19:37:32-04:00 | UTC: 2017-10-02T23:37:32+00:00\nOriginal: 2017-10-03 08:24:16-04:00 | UTC: 2017-10-03T12:24:16+00:00\nOriginal: 2017-10-03 18:17:07-04:00 | UTC: 2017-10-03T22:17:07+00:00\n\n\n\nDid you know that there is no official time zone at the North or South pole? Since all the lines of longitude meet each other, it’s up to each traveler (or research station) to decide what time they want to use."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#timezones-database",
    "href": "posts/2020-07-26-working with dates and times in python.html#timezones-database",
    "title": "Working With Dates and Times in Python",
    "section": "Timezones database",
    "text": "Timezones database\n\nPutting the bike trips into the right time zone\nInstead of setting the timezones for W20529 by hand, let’s assign them to their IANA timezone: ‘America/New_York’. Since we know their political jurisdiction, we don’t need to look up their UTC offset. Python will do that for us.\n\n# Import tz\nfrom dateutil import tz\n\n# Create a timezone object for Eastern Time\net = tz.gettz('America/New_York')\n\n# Loop over trips, updating the datetimes to be in Eastern Time\nfor trip in onebike_datetimes[:10]:\n  # Update trip['start'] and trip['end']\n  trip['start'] = trip['start'].replace(tzinfo=et)\n  trip['end'] = trip['end'].replace(tzinfo=et)\n\n\nTime zone rules actually change quite frequently. IANA time zone data gets updated every 3-4 months, as different jurisdictions make changes to their laws about time or as more historical information about timezones are uncovered. tz is smart enough to use the date in your datetime to determine which rules to use historically.\n\n\n\nWhat time did the bike leave? (Global edition)\n\n# Create the timezone object\nuk = tz.gettz('Europe/London')\n\n# Pull out the start of the first trip\nlocal = onebike_datetimes[0]['start']\n\n# What time was it in the UK?\nnotlocal = local.astimezone(uk)\n\n# Print them out and see the difference\nprint(local.isoformat())\nprint(notlocal.isoformat())\n\n2017-10-01T15:23:25-04:00\n2017-10-01T20:23:25+01:00\n\n\n\n# Create the timezone object\nist = tz.gettz('Asia/Kolkata')\n\n# Pull out the start of the first trip\nlocal = onebike_datetimes[0]['start']\n\n# What time was it in India?\nnotlocal = local.astimezone(ist)\n\n# Print them out and see the difference\nprint(local.isoformat())\nprint(notlocal.isoformat())\n\n2017-10-01T15:23:25-04:00\n2017-10-02T00:53:25+05:30\n\n\n\n# Create the timezone object\nsm = tz.gettz('Pacific/Apia')\n\n# Pull out the start of the first trip\nlocal = onebike_datetimes[0]['start']\n\n# What time was it in Samoa?\nnotlocal = local.astimezone(sm)\n\n# Print them out and see the difference\nprint(local.isoformat())\nprint(notlocal.isoformat())\n\n2017-10-01T15:23:25-04:00\n2017-10-02T09:23:25+14:00\n\n\n\nDid you notice the time offset for this one? It’s at UTC+14! Samoa used to be UTC-10, but in 2011 it changed to the other side of the International Date Line to better match New Zealand, its closest trading partner. However, they wanted to keep the clocks the same, so the UTC offset shifted from -10 to +14, since 24-10 is 14. Timezones… not simple!"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#starting-daylight-saving-time",
    "href": "posts/2020-07-26-working with dates and times in python.html#starting-daylight-saving-time",
    "title": "Working With Dates and Times in Python",
    "section": "Starting Daylight saving time",
    "text": "Starting Daylight saving time\n\nHow many hours elapsed around daylight saving?\nSince our bike data takes place in the fall, you’ll have to do something else to learn about the start of daylight savings time.\nLet’s look at March 12, 2017, in the Eastern United States, when Daylight Saving kicked in at 2 AM.\n\n# Import datetime, timedelta, tz, timezone\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import tz\n\n# Start on March 12, 2017, midnight, then add 6 hours\nstart = datetime(2017, 3, 12, tzinfo = tz.gettz('America/New_York'))\nend = start + timedelta(hours=6)\nprint(start.isoformat() + \" to \" + end.isoformat())\n\n2017-03-12T00:00:00-05:00 to 2017-03-12T06:00:00-04:00\n\n\n\n# Import datetime, timedelta, tz, timezone\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import tz\n\n# Start on March 12, 2017, midnight, then add 6 hours\nstart = datetime(2017, 3, 12, tzinfo = tz.gettz('America/New_York'))\nend = start + timedelta(hours=6)\nprint(start.isoformat() + \" to \" + end.isoformat())\n\n# How many hours have elapsed?\nprint((end - start).seconds/(60*60))\n\n2017-03-12T00:00:00-05:00 to 2017-03-12T06:00:00-04:00\n6.0\n\n\n\n# Import datetime, timedelta, tz, timezone\nfrom datetime import datetime, timedelta, timezone\nfrom dateutil import tz\n\n# Start on March 12, 2017, midnight, then add 6 hours\nstart = datetime(2017, 3, 12, tzinfo = tz.gettz('America/New_York'))\nend = start + timedelta(hours=6)\nprint(start.isoformat() + \" to \" + end.isoformat())\n\n# How many hours have elapsed?\nprint((end - start).total_seconds()/(60*60))\n\n# What if we move to UTC?\nprint((end.astimezone(timezone.utc) - start.astimezone(timezone.utc))\\\n      .total_seconds()/(60*60))\n\n2017-03-12T00:00:00-05:00 to 2017-03-12T06:00:00-04:00\n6.0\n5.0\n\n\n\nWhen we compare times in local time zones, everything gets converted into clock time. Remember if you want to get absolute time differences, always move to UTC!\n\n\n\nMarch 29, throughout a decade\nDaylight Saving rules are complicated: they’re different in different places, they change over time, and they usually start on a Sunday (and so they move around the calendar).\nFor example, in the United Kingdom, as of the time this code was written, Daylight Saving begins on the last Sunday in March. Let’s look at the UTC offset for March 29, at midnight, for the years 2000 to 2010.\n\n# Import datetime and tz\nfrom datetime import datetime\nfrom dateutil import tz\n\n# Create starting date\ndt = datetime(2000, 3, 29, tzinfo = tz.gettz('Europe/London'))\n\n# Loop over the dates, replacing the year, and print the ISO timestamp\nfor y in range(2000, 2011):\n  print(dt.replace(year=y).isoformat())\n\n2000-03-29T00:00:00+01:00\n2001-03-29T00:00:00+01:00\n2002-03-29T00:00:00+00:00\n2003-03-29T00:00:00+00:00\n2004-03-29T00:00:00+01:00\n2005-03-29T00:00:00+01:00\n2006-03-29T00:00:00+01:00\n2007-03-29T00:00:00+01:00\n2008-03-29T00:00:00+00:00\n2009-03-29T00:00:00+00:00\n2010-03-29T00:00:00+01:00\n\n\n\nAs you can see, the rules for Daylight Saving are not trivial. When in doubt, always use tz instead of hand-rolling timezones, so it will catch the Daylight Saving rules (and rule changes!) for you."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#ending-daylight-saving-time",
    "href": "posts/2020-07-26-working with dates and times in python.html#ending-daylight-saving-time",
    "title": "Working With Dates and Times in Python",
    "section": "Ending daylight saving time",
    "text": "Ending daylight saving time\n\nFinding ambiguous datetimes\nwe saw something anomalous in our bike trip duration data. Let’s see if we can identify what the problem might be.\n\nNote that tz.datetime_ambiguous() only catches ambiguous datetimes from Daylight Saving changes. Other weird edge cases, like jurisdictions which change their Daylight Saving rules, hopefully should be caught by tz. And if they’re not, at least those kinds of things are pretty rare in most data sets!\n\n\n\nCleaning daylight saving data with fold\nAs we’ve just discovered, there is a ride in our data set which is being messed up by a Daylight Savings shift. Let’s clean up the data set so we actually have a correct minimum ride length. We can use the fact that we know the end of the ride happened after the beginning to fix up the duration messed up by the shift out of Daylight Savings.\nSince Python does not handle tz.enfold() when doing arithmetic, we must put our datetime objects into UTC, where ambiguities have been resolved."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#reading-date-and-time-data-in-pandas",
    "href": "posts/2020-07-26-working with dates and times in python.html#reading-date-and-time-data-in-pandas",
    "title": "Working With Dates and Times in Python",
    "section": "Reading date and time data in Pandas",
    "text": "Reading date and time data in Pandas\n\nimport pandas as pd\n\n\n# Load CSV into the rides variable\nrides = pd.read_csv('../datasets/capital-onebike.csv', \n                    parse_dates = ['Start date', 'End date'])\n\n# Print the initial (0th) row\nprint(rides.iloc[0])\n\nStart date                        2017-10-01 15:23:25\nEnd date                          2017-10-01 15:26:26\nStart station number                            31038\nStart station                    Glebe Rd & 11th St N\nEnd station number                              31036\nEnd station             George Mason Dr & Wilson Blvd\nBike number                                    W20529\nMember type                                    Member\nName: 0, dtype: object\n\n\n\nMaking timedelta columns\n\nrides['Duration'] = (rides['End date']-rides['Start date']).dt.total_seconds()\nrides.Duration.head()\n\n0     181.0\n1    7622.0\n2     343.0\n3    1278.0\n4    1277.0\nName: Duration, dtype: float64"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in python.html#summarizing-datetime-data-in-pandas",
    "href": "posts/2020-07-26-working with dates and times in python.html#summarizing-datetime-data-in-pandas",
    "title": "Working With Dates and Times in Python",
    "section": "Summarizing datetime data in Pandas",
    "text": "Summarizing datetime data in Pandas\n\nHow many joyrides?\nSuppose we have a theory that some people take long bike rides before putting their bike back in the same dock. Let’s call these rides “joyrides”. Are there many joyrides? How long were they in our data set? Lets use the median instead of the mean, because we know there are some very long trips in our data set that might skew the answer, and the median is less sensitive to outliers.\n\n# Create joyrides\njoyrides = (rides['Start station'] == rides['End station'])\n\n# Total number of joyrides\nprint(\"{} rides were joyrides\".format(joyrides.sum()))\n\n# Median of all rides\nprint(\"The median duration overall was {:.2f} seconds\"\\\n      .format(rides['Duration'].median()))\n\n# Median of joyrides\nprint(\"The median duration for joyrides was {:.2f} seconds\"\\\n      .format(rides[joyrides]['Duration'].median()))\n\n6 rides were joyrides\nThe median duration overall was 660.00 seconds\nThe median duration for joyrides was 2642.50 seconds\n\n\n\n\nIt’s getting cold outside, W20529\nWashington, D.C. has mild weather overall, but the average high temperature in October (68ºF / 20ºC) is certainly higher than the average high temperature in December (47ºF / 8ºC). People also travel more in December, and they work fewer days so they commute less.\nHow might the weather or the season have affected the length of bike trips?\n\n# Import matplotlib\nimport matplotlib.pyplot as plt\n\n# Resample rides to daily, take the size, plot the results\nrides.resample('D', on = 'Start date')\\\n  .size()\\\n  .plot(ylim = [0, 15])\n\n# Show the results\nplt.show()\n\n\n\n\nSince the daily time series is so noisy for this one bike, change the resampling to be monthly.\n\n# Import matplotlib\nimport matplotlib.pyplot as plt\n\n# Resample rides to monthly, take the size, plot the results\nrides.resample('M', on = 'Start date')\\\n  .size()\\\n  .plot(ylim = [0, 150])\n\n# Show the results\nplt.show()\n\n\n\n\nAs you can see, the pattern is clearer at the monthly level: there were fewer rides in November, and then fewer still in December, possibly because the temperature got colder.\n\n\nMembers vs casual riders over time\nRiders can either be “Members”, meaning they pay yearly for the ability to take a bike at any time, or “Casual”, meaning they pay at the kiosk attached to the bike dock.\nDo members and casual riders drop off at the same rate over October to December, or does one drop off faster than the other?\n\n# Resample rides to be monthly on the basis of Start date\nmonthly_rides = rides.resample('M', on='Start date')['Member type']\n\n# Take the ratio of the .value_counts() over the total number of rides\nprint(monthly_rides.value_counts() / monthly_rides.size())\n\nStart date  Member type\n2017-10-31  Member         0.768519\n            Casual         0.231481\n2017-11-30  Member         0.825243\n            Casual         0.174757\n2017-12-31  Member         0.860759\n            Casual         0.139241\nName: Member type, dtype: float64\n\n\n.resample() labels Monthly resampling with the last day in the month and not the first. It certainly looks like the fraction of Casual riders went down as the number of rides dropped. With a little more digging, you could figure out if keeping Member rides only would be enough to stabilize the usage numbers throughout the fall.\n\n\nCombining groupby() and resample()\n\n# Group rides by member type, and resample to the month\ngrouped = rides.groupby('Member type')\\\n  .resample('M', on='Start date')\n\n# Print the median duration for each group\nprint(grouped['Duration'].median())\n\nMember type  Start date\nCasual       2017-10-31    1636.0\n             2017-11-30    1159.5\n             2017-12-31     850.0\nMember       2017-10-31     671.0\n             2017-11-30     655.0\n             2017-12-31     387.5\nName: Duration, dtype: float64\n\n\nIt looks like casual riders consistently took longer rides, but that both groups took shorter rides as the months went by. Note that, by combining grouping and resampling, you can answer a lot of questions about nearly any data set that includes time as a feature. Keep in mind that you can also group by more than one column at once.\n### Timezones in Pandas\n\n# Localize the Start date column to America/New_York\nrides['Start date'] = rides['Start date'].dt.tz_localize(\"America/New_York\", ambiguous='NaT')\n\n# Print first value\nprint(rides['Start date'].iloc[0])\n\n2017-10-01 15:23:25-04:00\n\n\n\n# Convert the Start date column to Europe/London\nrides['Start date'] = rides['Start date'].dt.tz_convert(\"Europe/London\")\n\n# Print the new value\nprint(rides['Start date'].iloc[0])\n\n2017-10-01 20:23:25+01:00\n\n\n\ndt.tzconvert() converts to a new timezone, whereas dt.tzlocalize() sets a timezone in the first place.\n\n\n\nHow long per weekday?\n\n# Add a column for the weekday of the start of the ride\nrides['Ride start weekday'] = rides['Start date'].dt.weekday\n\n# Print the median trip time per weekday\nprint(rides.groupby('Ride start weekday')['Duration'].median())\n\nRide start weekday\n0.0    922.5\n1.0    644.0\n2.0    629.0\n3.0    659.0\n4.0    684.0\n5.0    610.0\n6.0    625.0\nName: Duration, dtype: float64"
  },
  {
    "objectID": "posts/2020-10-19-visualizing the gender gap in college degrees.html",
    "href": "posts/2020-10-19-visualizing the gender gap in college degrees.html",
    "title": "Visualizing the Gender Gap in College Degrees",
    "section": "",
    "text": "%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nwomen_degrees = pd.read_csv('percent-bachelors-degrees-women-usa.csv')\ncb_dark_blue = (0/255,107/255,164/255)\ncb_orange = (255/255, 128/255, 14/255)\nstem_cats = ['Engineering', 'Computer Science', 'Psychology', 'Biology', 'Physical Sciences', 'Math and Statistics']\n\nfig = plt.figure(figsize=(18, 3))\n\nfor sp in range(0,6):\n    ax = fig.add_subplot(1,6,sp+1)\n    ax.plot(women_degrees['Year'], women_degrees[stem_cats[sp]], c=cb_dark_blue, label='Women', linewidth=3)\n    ax.plot(women_degrees['Year'], 100-women_degrees[stem_cats[sp]], c=cb_orange, label='Men', linewidth=3)\n    ax.spines[\"right\"].set_visible(False)    \n    ax.spines[\"left\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)    \n    ax.spines[\"bottom\"].set_visible(False)\n    ax.set_xlim(1968, 2011)\n    ax.set_ylim(0,100)\n    ax.set_title(stem_cats[sp])\n    ax.tick_params(bottom=\"off\", top=\"off\", left=\"off\", right=\"off\")\n    \n    if sp == 0:\n        ax.text(2005, 87, 'Men')\n        ax.text(2002, 8, 'Women')\n    elif sp == 5:\n        ax.text(2005, 62, 'Men')\n        ax.text(2001, 35, 'Women')\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-19-visualizing the gender gap in college degrees.html#comparing-across-all-degrees",
    "href": "posts/2020-10-19-visualizing the gender gap in college degrees.html#comparing-across-all-degrees",
    "title": "Visualizing the Gender Gap in College Degrees",
    "section": "Comparing across all degrees",
    "text": "Comparing across all degrees\nThere are seventeen degrees, to generate line chart, we’ll use a subplot grid layout of 6 rows by 3 columns.\n\nstem_cats = ['Psychology', 'Biology', 'Math and Statistics', 'Physical Sciences', 'Computer Science', 'Engineering']\nlib_arts_cats = ['Foreign Languages', 'English', 'Communications and Journalism', 'Art and Performance', 'Social Sciences and History']\nother_cats = ['Health Professions', 'Public Administration', 'Education', 'Agriculture','Business', 'Architecture']\n\n\nfig = plt.figure(figsize=(16,20))\n\nfor sp in range(0,18,3):\n    cat_index = int(sp/3)\n    ax = fig.add_subplot(6,3,sp+1)\n    ax.plot(women_degrees['Year'], women_degrees[stem_cats[cat_index]], c=cb_dark_blue, label='Women', linewidth=3)\n    ax.plot(women_degrees['Year'], 100-women_degrees[stem_cats[cat_index]], c=cb_orange, label='Men', linewidth=3)\n    for key,spine in ax.spines.items():\n        spine.set_visible(False)\n    ax.set_xlim(1968, 2011)\n    ax.set_ylim(0,100)\n    ax.set_title(stem_cats[cat_index])\n    ax.tick_params(bottom=\"off\", top=\"off\", left=\"off\", right=\"off\")\n    ax.set_yticks([0,100])\n    ax.axhline(50, c=(171/255, 171/255, 171/255), alpha=0.3)\n    \n    if cat_index == 0:\n        ax.text(2003, 85, 'Women')\n        ax.text(2005, 10, 'Men')\n    elif cat_index == 5:\n        ax.text(2005, 87, 'Men')\n        ax.text(2003, 7, 'Women')\n        ax.tick_params(labelbottom='on')\n\nfor sp in range(1,16,3):\n    cat_index = int((sp-1)/3)\n    ax = fig.add_subplot(6,3,sp+1)\n    ax.plot(women_degrees['Year'], women_degrees[lib_arts_cats[cat_index]], c=cb_dark_blue, label='Women', linewidth=3)\n    ax.plot(women_degrees['Year'], 100-women_degrees[lib_arts_cats[cat_index]], c=cb_orange, label='Men', linewidth=3)\n    for key,spine in ax.spines.items():\n        spine.set_visible(False)\n    ax.set_xlim(1968, 2011)\n    ax.set_ylim(0,100)\n    ax.set_title(lib_arts_cats[cat_index])\n    ax.tick_params(bottom=\"off\", top=\"off\", left=\"off\", right=\"off\")\n    ax.set_yticks([0,100])\n    ax.axhline(50, c=(171/255, 171/255, 171/255), alpha=0.3)\n    \n    if cat_index == 0:\n        ax.text(2003, 78, 'Women')\n        ax.text(2005, 18, 'Men')\n    elif cat_index == 4:\n        ax.tick_params(labelbottom='on')\n        \nfor sp in range(2,20,3):\n    cat_index = int((sp-2)/3)\n    ax = fig.add_subplot(6,3,sp+1)\n    ax.plot(women_degrees['Year'], women_degrees[other_cats[cat_index]], c=cb_dark_blue, label='Women', linewidth=3)\n    ax.plot(women_degrees['Year'], 100-women_degrees[other_cats[cat_index]], c=cb_orange, label='Men', linewidth=3)\n    for key,spine in ax.spines.items():\n        spine.set_visible(False)\n    ax.set_xlim(1968, 2011)\n    ax.set_ylim(0,100)\n    ax.set_title(other_cats[cat_index])\n    ax.tick_params(bottom=\"off\", top=\"off\", left=\"off\", right=\"off\")\n    ax.set_yticks([0,100])\n    ax.axhline(50, c=(171/255, 171/255, 171/255), alpha=0.3)\n    \n    if cat_index == 0:\n        ax.text(2003, 90, 'Women')\n        ax.text(2005, 5, 'Men')\n    elif cat_index == 5:\n        ax.text(2005, 62, 'Men')\n        ax.text(2003, 30, 'Women')\n        ax.tick_params(labelbottom='on')\n        \nplt.show()"
  },
  {
    "objectID": "posts/2020-08-25-visualizing_women_in_usa_degrees.html",
    "href": "posts/2020-08-25-visualizing_women_in_usa_degrees.html",
    "title": "Visualizing Women in USA Degrees",
    "section": "",
    "text": "The Department of Education Statistics releases a data set annually containing the percentage of bachelor’s degrees granted to women from 1970 to 2012. The data set is broken up into 17 categories of degrees, with each column as a separate category.\nRandal Olson, a data scientist at University of Pennsylvania, has cleaned the data set and made it available on his personal website. You can download the dataset Randal compiled here.\n\n\nExplore how we can communicate the nuanced narrative of gender gap using effective data visualization.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n\nwomen_degrees = pd.read_csv('../data/percent-bachelors-degrees-women-usa.csv')\nwomen_degrees.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      Agriculture\n      Architecture\n      Art and Performance\n      Biology\n      Business\n      Communications and Journalism\n      Computer Science\n      Education\n      Engineering\n      English\n      Foreign Languages\n      Health Professions\n      Math and Statistics\n      Physical Sciences\n      Psychology\n      Public Administration\n      Social Sciences and History\n    \n  \n  \n    \n      0\n      1970\n      4.229798\n      11.921005\n      59.7\n      29.088363\n      9.064439\n      35.3\n      13.6\n      74.535328\n      0.8\n      65.570923\n      73.8\n      77.1\n      38.0\n      13.8\n      44.4\n      68.4\n      36.8\n    \n    \n      1\n      1971\n      5.452797\n      12.003106\n      59.9\n      29.394403\n      9.503187\n      35.5\n      13.6\n      74.149204\n      1.0\n      64.556485\n      73.9\n      75.5\n      39.0\n      14.9\n      46.2\n      65.5\n      36.2\n    \n    \n      2\n      1972\n      7.420710\n      13.214594\n      60.4\n      29.810221\n      10.558962\n      36.6\n      14.9\n      73.554520\n      1.2\n      63.664263\n      74.6\n      76.9\n      40.2\n      14.8\n      47.6\n      62.6\n      36.1\n    \n    \n      3\n      1973\n      9.653602\n      14.791613\n      60.2\n      31.147915\n      12.804602\n      38.4\n      16.4\n      73.501814\n      1.6\n      62.941502\n      74.9\n      77.4\n      40.9\n      16.5\n      50.4\n      64.3\n      36.4\n    \n    \n      4\n      1974\n      14.074623\n      17.444688\n      61.9\n      32.996183\n      16.204850\n      40.5\n      18.9\n      73.336811\n      2.2\n      62.413412\n      75.3\n      77.9\n      41.8\n      18.2\n      52.6\n      66.1\n      37.3"
  },
  {
    "objectID": "posts/2020-08-25-visualizing_women_in_usa_degrees.html#observation",
    "href": "posts/2020-08-25-visualizing_women_in_usa_degrees.html#observation",
    "title": "Visualizing Women in USA Degrees",
    "section": "Observation",
    "text": "Observation\nFrom the plot, we can tell that Biology degrees increased steadily from 1970 and peaked in the early 2000’s. We can also tell that the percentage has stayed above 50% since around 1987."
  },
  {
    "objectID": "posts/2020-08-25-visualizing_women_in_usa_degrees.html#visualizing-the-gender-gap",
    "href": "posts/2020-08-25-visualizing_women_in_usa_degrees.html#visualizing-the-gender-gap",
    "title": "Visualizing Women in USA Degrees",
    "section": "Visualizing the gender gap",
    "text": "Visualizing the gender gap\n\nfig, ax = plt.subplots(figsize=(20,10))\nax.plot(women_degrees.Year, women_degrees.Biology, label='Women')\nax.plot(women_degrees.Year, 100-women_degrees.Biology, label='Men')\nax.set_title('Percentage of Biology Degrees Awarded By Gender')\nax.set_ylabel('Biology')\nax.set_xlabel('Year')\nax.set_xlim(left=1970)\nax.legend()\nplt.show()\n\n\n\n\nIn the first period, from 1970 to around 1987, women were a minority when it came to majoring in Biology while in the second period, from around 1987 to around 2012, women became a majority"
  },
  {
    "objectID": "posts/2020-08-25-visualizing_women_in_usa_degrees.html#observation-1",
    "href": "posts/2020-08-25-visualizing_women_in_usa_degrees.html#observation-1",
    "title": "Visualizing Women in USA Degrees",
    "section": "Observation",
    "text": "Observation\nwe can conclude that the gender gap in Computer Science and Engineering have big gender gaps while the gap in Biology and Math and Statistics is quite small. In addition, the first two degree categories are dominated by men while the latter degree categories are much more balanced.\n\nstem_cats = ['Engineering', 'Computer Science', 'Psychology', 'Biology', 'Physical Sciences', 'Math and Statistics']\n\nfig = plt.figure(figsize=(20, 3))\n\nfor sp in range(0,6):\n    ax = fig.add_subplot(1,6,sp+1)\n    ax.plot(women_degrees['Year'], women_degrees[stem_cats[sp]], c=(0/255, 107/255, 164/255), label='Women', linewidth=3)\n    ax.plot(women_degrees['Year'], 100-women_degrees[stem_cats[sp]], c=(255/255, 128/255, 14/255), label='Men', linewidth=3)\n    for key,spine in ax.spines.items():\n        spine.set_visible(False)\n    ax.set_xlim(1968, 2011)\n    ax.set_ylim(0,100)\n    ax.set_title(stem_cats[sp])\n    ax.tick_params(bottom=False, top=False, left=False, right=False)\n    if sp==0:\n        ax.text(2005,87,\"Men\")\n        ax.text(2002,8, \"Women\")\n    if sp==5:\n        ax.text(2005,62,\"Men\")\n        ax.text(2001,35, \"Women\")\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-19-cleaning data in r.html",
    "href": "posts/2020-10-19-cleaning data in r.html",
    "title": "Introduction and Exproling Raw data",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(installr)\n\nWarning message:\n\"package 'stringr' was built under R version 3.6.3\"Warning message:\n\"package 'installr' was built under R version 3.6.3\"\nWelcome to installr version 0.22.0\n\nMore information is available on the installr project website:\nhttps://github.com/talgalili/installr/\n\nContact: <tal.galili@gmail.com>\nSuggestions and bug-reports can be submitted at: https://github.com/talgalili/installr/issues\n\n            To suppress this message use:\n            suppressPackageStartupMessages(library(installr))\n\n\n\n\nweather = readRDS(gzcon(url('https://assets.datacamp.com/production/repositories/34/datasets/b3c1036d9a60a9dfe0f99051d2474a54f76055ea/weather.rds')))\n\n\nclass(weather)\n\n'data.frame'\n\n\n\nhead(weather)\n\n\n\nXyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31\n\n    1                2014             12               Max.TemperatureF 64               42               51               43               42               45               ...              44               47               46               59               50               52               52               41               30               30               \n    2                2014             12               Mean.TemperatureF52               38               44               37               34               42               ...              39               45               44               52               44               45               46               36               26               25               \n    3                2014             12               Min.TemperatureF 39               33               37               30               26               38               ...              33               42               41               44               37               38               40               30               22               20               \n    4                2014             12               Max.Dew.PointF   46               40               49               24               37               45               ...              39               45               46               58               31               34               42               26               10               8                \n    5                2014             12               MeanDew.PointF   40               27               42               21               25               40               ...              34               42               44               43               29               31               35               20               4                5                \n    6                2014             12               Min.DewpointF    26               17               24               13               12               36               ...              25               37               41               29               28               29               27               10               -6               1                \n\n\n\n\n\ntail(weather)\n\n\n\nXyearmonthmeasureX1X2X3X4X5X6...X22X23X24X25X26X27X28X29X30X31\n\n    281281               2015              12                Mean.Wind.SpeedMPH6                 NA                NA                NA                NA                NA                ...               NA                NA                NA                NA                NA                NA                NA                NA                NA                NA                \n    282282               2015              12                Max.Gust.SpeedMPH 17                NA                NA                NA                NA                NA                ...               NA                NA                NA                NA                NA                NA                NA                NA                NA                NA                \n    283283               2015              12                PrecipitationIn   0.14              NA                NA                NA                NA                NA                ...               NA                NA                NA                NA                NA                NA                NA                NA                NA                NA                \n    284284               2015              12                CloudCover        7                 NA                NA                NA                NA                NA                ...               NA                NA                NA                NA                NA                NA                NA                NA                NA                NA                \n    285285               2015              12                Events            Rain              NA                NA                NA                NA                NA                ...               NA                NA                NA                NA                NA                NA                NA                NA                NA                NA                \n    286286               2015              12                WindDirDegrees    109               NA                NA                NA                NA                NA                ...               NA                NA                NA                NA                NA                NA                NA                NA                NA                NA                \n\n\n\n\n\nstr(weather)\n\n'data.frame':   286 obs. of  35 variables:\n $ X      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ year   : int  2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ...\n $ month  : int  12 12 12 12 12 12 12 12 12 12 ...\n $ measure: chr  \"Max.TemperatureF\" \"Mean.TemperatureF\" \"Min.TemperatureF\" \"Max.Dew.PointF\" ...\n $ X1     : chr  \"64\" \"52\" \"39\" \"46\" ...\n $ X2     : chr  \"42\" \"38\" \"33\" \"40\" ...\n $ X3     : chr  \"51\" \"44\" \"37\" \"49\" ...\n $ X4     : chr  \"43\" \"37\" \"30\" \"24\" ...\n $ X5     : chr  \"42\" \"34\" \"26\" \"37\" ...\n $ X6     : chr  \"45\" \"42\" \"38\" \"45\" ...\n $ X7     : chr  \"38\" \"30\" \"21\" \"36\" ...\n $ X8     : chr  \"29\" \"24\" \"18\" \"28\" ...\n $ X9     : chr  \"49\" \"39\" \"29\" \"49\" ...\n $ X10    : chr  \"48\" \"43\" \"38\" \"45\" ...\n $ X11    : chr  \"39\" \"36\" \"32\" \"37\" ...\n $ X12    : chr  \"39\" \"35\" \"31\" \"28\" ...\n $ X13    : chr  \"42\" \"37\" \"32\" \"28\" ...\n $ X14    : chr  \"45\" \"39\" \"33\" \"29\" ...\n $ X15    : chr  \"42\" \"37\" \"32\" \"33\" ...\n $ X16    : chr  \"44\" \"40\" \"35\" \"42\" ...\n $ X17    : chr  \"49\" \"45\" \"41\" \"46\" ...\n $ X18    : chr  \"44\" \"40\" \"36\" \"34\" ...\n $ X19    : chr  \"37\" \"33\" \"29\" \"25\" ...\n $ X20    : chr  \"36\" \"32\" \"27\" \"30\" ...\n $ X21    : chr  \"36\" \"33\" \"30\" \"30\" ...\n $ X22    : chr  \"44\" \"39\" \"33\" \"39\" ...\n $ X23    : chr  \"47\" \"45\" \"42\" \"45\" ...\n $ X24    : chr  \"46\" \"44\" \"41\" \"46\" ...\n $ X25    : chr  \"59\" \"52\" \"44\" \"58\" ...\n $ X26    : chr  \"50\" \"44\" \"37\" \"31\" ...\n $ X27    : chr  \"52\" \"45\" \"38\" \"34\" ...\n $ X28    : chr  \"52\" \"46\" \"40\" \"42\" ...\n $ X29    : chr  \"41\" \"36\" \"30\" \"26\" ...\n $ X30    : chr  \"30\" \"26\" \"22\" \"10\" ...\n $ X31    : chr  \"30\" \"25\" \"20\" \"8\" ...\n\n\n\ndim(weather)\n\n\n    286\n    35\n\n\n\n\nbmi=read_csv('https://assets.datacamp.com/production/repositories/34/datasets/a0a569ebbb34500d11979eba95360125127e6434/bmi_clean.csv')\n\nParsed with column specification:\ncols(\n  .default = col_double(),\n  Country = col_character()\n)\nSee spec(...) for full column specifications.\n\n\n\nclass(bmi)\n\n\n    'spec_tbl_df'\n    'tbl_df'\n    'tbl'\n    'data.frame'\n\n\n\n\ndim(bmi)\n\n\n    199\n    30\n\n\n\n\nhead(bmi)\n\n\n\nCountryY1980Y1981Y1982Y1983Y1984Y1985Y1986Y1987Y1988...Y1999Y2000Y2001Y2002Y2003Y2004Y2005Y2006Y2007Y2008\n\n    Afghanistan        21.48678           21.46552           21.45145           21.43822           21.42734           21.41222           21.40132           21.37679           21.34018           ...                20.75469           20.69521           20.62643           20.59848           20.58706           20.57759           20.58084           20.58749           20.60246           20.62058           \n    Albania            25.22533           25.23981           25.25636           25.27176           25.27901           25.28669           25.29451           25.30217           25.30450           ...                25.46555           25.55835           25.66701           25.77167           25.87274           25.98136           26.08939           26.20867           26.32753           26.44657           \n    Algeria            22.25703           22.34745           22.43647           22.52105           22.60633           22.69501           22.76979           22.84096           22.90644           ...                23.69486           23.77659           23.86256           23.95294           24.05243           24.15957           24.27001           24.38270           24.48846           24.59620           \n    Andorra            25.66652           25.70868           25.74681           25.78250           25.81874           25.85236           25.89089           25.93414           25.98477           ...                26.75078           26.83179           26.92373           27.02525           27.12481           27.23107           27.32827           27.43588           27.53363           27.63048           \n    Angola             20.94876           20.94371           20.93754           20.93187           20.93569           20.94857           20.96030           20.98025           21.01375           ...                21.31954           21.37480           21.43664           21.51765           21.59924           21.69218           21.80564           21.93881           22.08962           22.25083           \n    Antigua and Barbuda23.31424           23.39054           23.45883           23.53735           23.63584           23.73109           23.83449           23.93649           24.05364           ...                24.91721           24.99158           25.05857           25.13039           25.20713           25.29898           25.39965           25.51382           25.64247           25.76602           \n\n\n\n\n\ntail(bmi)\n\n\n\nCountryY1980Y1981Y1982Y1983Y1984Y1985Y1986Y1987Y1988...Y1999Y2000Y2001Y2002Y2003Y2004Y2005Y2006Y2007Y2008\n\n    Venezuela         24.58052          24.69666          24.80082          24.89208          24.98440          25.07104          25.15587          25.24624          25.35274          ...               26.50035          26.61021          26.71688          26.79210          26.85498          26.95162          27.05633          27.17698          27.30849          27.44500          \n    Vietnam           19.01394          19.03902          19.06804          19.09675          19.13046          19.16397          19.19740          19.23481          19.27090          ...               20.02081          20.10343          20.18623          20.27145          20.36402          20.46585          20.57277          20.68655          20.80189          20.91630          \n    West Bank and Gaza24.31624          24.40192          24.48713          24.57107          24.65582          24.74148          24.82984          24.91615          25.00108          ...               26.28240          26.39074          26.45700          26.48925          26.51152          26.52924          26.54329          26.54449          26.55460          26.57750          \n    Yemen, Rep.       22.90384          22.96813          23.02669          23.07279          23.12566          23.16944          23.20933          23.25043          23.29401          ...               23.85482          23.92467          23.99129          24.05692          24.12459          24.19204          24.25638          24.32120          24.37949          24.44157          \n    Zambia            19.66295          19.69512          19.72538          19.75420          19.78070          19.80335          19.82396          19.85065          19.88320          ...               20.15094          20.17261          20.20266          20.24298          20.29474          20.35966          20.43398          20.51422          20.59770          20.68321          \n    Zimbabwe          21.46989          21.48867          21.50738          21.52936          21.53383          21.54341          21.54859          21.54590          21.55396          ...               21.68873          21.72652          21.76514          21.79645          21.82499          21.85806          21.89495          21.93371          21.97405          22.02660          \n\n\n\n\n\nstr(bmi)\n\ntibble [199 x 30] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Country: chr [1:199] \"Afghanistan\" \"Albania\" \"Algeria\" \"Andorra\" ...\n $ Y1980  : num [1:199] 21.5 25.2 22.3 25.7 20.9 ...\n $ Y1981  : num [1:199] 21.5 25.2 22.3 25.7 20.9 ...\n $ Y1982  : num [1:199] 21.5 25.3 22.4 25.7 20.9 ...\n $ Y1983  : num [1:199] 21.4 25.3 22.5 25.8 20.9 ...\n $ Y1984  : num [1:199] 21.4 25.3 22.6 25.8 20.9 ...\n $ Y1985  : num [1:199] 21.4 25.3 22.7 25.9 20.9 ...\n $ Y1986  : num [1:199] 21.4 25.3 22.8 25.9 21 ...\n $ Y1987  : num [1:199] 21.4 25.3 22.8 25.9 21 ...\n $ Y1988  : num [1:199] 21.3 25.3 22.9 26 21 ...\n $ Y1989  : num [1:199] 21.3 25.3 23 26 21.1 ...\n $ Y1990  : num [1:199] 21.2 25.3 23 26.1 21.1 ...\n $ Y1991  : num [1:199] 21.2 25.3 23.1 26.2 21.1 ...\n $ Y1992  : num [1:199] 21.1 25.2 23.2 26.2 21.1 ...\n $ Y1993  : num [1:199] 21.1 25.2 23.3 26.3 21.1 ...\n $ Y1994  : num [1:199] 21 25.2 23.3 26.4 21.1 ...\n $ Y1995  : num [1:199] 20.9 25.3 23.4 26.4 21.2 ...\n $ Y1996  : num [1:199] 20.9 25.3 23.5 26.5 21.2 ...\n $ Y1997  : num [1:199] 20.8 25.3 23.5 26.6 21.2 ...\n $ Y1998  : num [1:199] 20.8 25.4 23.6 26.7 21.3 ...\n $ Y1999  : num [1:199] 20.8 25.5 23.7 26.8 21.3 ...\n $ Y2000  : num [1:199] 20.7 25.6 23.8 26.8 21.4 ...\n $ Y2001  : num [1:199] 20.6 25.7 23.9 26.9 21.4 ...\n $ Y2002  : num [1:199] 20.6 25.8 24 27 21.5 ...\n $ Y2003  : num [1:199] 20.6 25.9 24.1 27.1 21.6 ...\n $ Y2004  : num [1:199] 20.6 26 24.2 27.2 21.7 ...\n $ Y2005  : num [1:199] 20.6 26.1 24.3 27.3 21.8 ...\n $ Y2006  : num [1:199] 20.6 26.2 24.4 27.4 21.9 ...\n $ Y2007  : num [1:199] 20.6 26.3 24.5 27.5 22.1 ...\n $ Y2008  : num [1:199] 20.6 26.4 24.6 27.6 22.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Country = col_character(),\n  ..   Y1980 = col_double(),\n  ..   Y1981 = col_double(),\n  ..   Y1982 = col_double(),\n  ..   Y1983 = col_double(),\n  ..   Y1984 = col_double(),\n  ..   Y1985 = col_double(),\n  ..   Y1986 = col_double(),\n  ..   Y1987 = col_double(),\n  ..   Y1988 = col_double(),\n  ..   Y1989 = col_double(),\n  ..   Y1990 = col_double(),\n  ..   Y1991 = col_double(),\n  ..   Y1992 = col_double(),\n  ..   Y1993 = col_double(),\n  ..   Y1994 = col_double(),\n  ..   Y1995 = col_double(),\n  ..   Y1996 = col_double(),\n  ..   Y1997 = col_double(),\n  ..   Y1998 = col_double(),\n  ..   Y1999 = col_double(),\n  ..   Y2000 = col_double(),\n  ..   Y2001 = col_double(),\n  ..   Y2002 = col_double(),\n  ..   Y2003 = col_double(),\n  ..   Y2004 = col_double(),\n  ..   Y2005 = col_double(),\n  ..   Y2006 = col_double(),\n  ..   Y2007 = col_double(),\n  ..   Y2008 = col_double()\n  .. )\n\n\n\nglimpse(bmi)\n\nRows: 199\nColumns: 30\n$ Country <chr> \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"...\n$ Y1980   <dbl> 21.48678, 25.22533, 22.25703, 25.66652, 20.94876, 23.31424,...\n$ Y1981   <dbl> 21.46552, 25.23981, 22.34745, 25.70868, 20.94371, 23.39054,...\n$ Y1982   <dbl> 21.45145, 25.25636, 22.43647, 25.74681, 20.93754, 23.45883,...\n$ Y1983   <dbl> 21.43822, 25.27176, 22.52105, 25.78250, 20.93187, 23.53735,...\n$ Y1984   <dbl> 21.42734, 25.27901, 22.60633, 25.81874, 20.93569, 23.63584,...\n$ Y1985   <dbl> 21.41222, 25.28669, 22.69501, 25.85236, 20.94857, 23.73109,...\n$ Y1986   <dbl> 21.40132, 25.29451, 22.76979, 25.89089, 20.96030, 23.83449,...\n$ Y1987   <dbl> 21.37679, 25.30217, 22.84096, 25.93414, 20.98025, 23.93649,...\n$ Y1988   <dbl> 21.34018, 25.30450, 22.90644, 25.98477, 21.01375, 24.05364,...\n$ Y1989   <dbl> 21.29845, 25.31944, 22.97931, 26.04450, 21.05269, 24.16347,...\n$ Y1990   <dbl> 21.24818, 25.32357, 23.04600, 26.10936, 21.09007, 24.26782,...\n$ Y1991   <dbl> 21.20269, 25.28452, 23.11333, 26.17912, 21.12136, 24.36568,...\n$ Y1992   <dbl> 21.14238, 25.23077, 23.18776, 26.24017, 21.14987, 24.45644,...\n$ Y1993   <dbl> 21.06376, 25.21192, 23.25764, 26.30356, 21.13938, 24.54096,...\n$ Y1994   <dbl> 20.97987, 25.22115, 23.32273, 26.36793, 21.14186, 24.60945,...\n$ Y1995   <dbl> 20.91132, 25.25874, 23.39526, 26.43569, 21.16022, 24.66461,...\n$ Y1996   <dbl> 20.85155, 25.31097, 23.46811, 26.50769, 21.19076, 24.72544,...\n$ Y1997   <dbl> 20.81307, 25.33988, 23.54160, 26.58255, 21.22621, 24.78714,...\n$ Y1998   <dbl> 20.78591, 25.39116, 23.61592, 26.66337, 21.27082, 24.84936,...\n$ Y1999   <dbl> 20.75469, 25.46555, 23.69486, 26.75078, 21.31954, 24.91721,...\n$ Y2000   <dbl> 20.69521, 25.55835, 23.77659, 26.83179, 21.37480, 24.99158,...\n$ Y2001   <dbl> 20.62643, 25.66701, 23.86256, 26.92373, 21.43664, 25.05857,...\n$ Y2002   <dbl> 20.59848, 25.77167, 23.95294, 27.02525, 21.51765, 25.13039,...\n$ Y2003   <dbl> 20.58706, 25.87274, 24.05243, 27.12481, 21.59924, 25.20713,...\n$ Y2004   <dbl> 20.57759, 25.98136, 24.15957, 27.23107, 21.69218, 25.29898,...\n$ Y2005   <dbl> 20.58084, 26.08939, 24.27001, 27.32827, 21.80564, 25.39965,...\n$ Y2006   <dbl> 20.58749, 26.20867, 24.38270, 27.43588, 21.93881, 25.51382,...\n$ Y2007   <dbl> 20.60246, 26.32753, 24.48846, 27.53363, 22.08962, 25.64247,...\n$ Y2008   <dbl> 20.62058, 26.44657, 24.59620, 27.63048, 22.25083, 25.76602,...\n\n\n\n# View the first 6 rows\nhead(bmi, n=6)\n\n# View the first 15 rows\nhead(bmi, n=15)\n\n# View the last 6 rows\ntail(bmi, n=6)\n\n# View the last 10 rows\ntail(bmi, n=10)\n\n\n\nCountryY1980Y1981Y1982Y1983Y1984Y1985Y1986Y1987Y1988...Y1999Y2000Y2001Y2002Y2003Y2004Y2005Y2006Y2007Y2008\n\n    Afghanistan        21.48678           21.46552           21.45145           21.43822           21.42734           21.41222           21.40132           21.37679           21.34018           ...                20.75469           20.69521           20.62643           20.59848           20.58706           20.57759           20.58084           20.58749           20.60246           20.62058           \n    Albania            25.22533           25.23981           25.25636           25.27176           25.27901           25.28669           25.29451           25.30217           25.30450           ...                25.46555           25.55835           25.66701           25.77167           25.87274           25.98136           26.08939           26.20867           26.32753           26.44657           \n    Algeria            22.25703           22.34745           22.43647           22.52105           22.60633           22.69501           22.76979           22.84096           22.90644           ...                23.69486           23.77659           23.86256           23.95294           24.05243           24.15957           24.27001           24.38270           24.48846           24.59620           \n    Andorra            25.66652           25.70868           25.74681           25.78250           25.81874           25.85236           25.89089           25.93414           25.98477           ...                26.75078           26.83179           26.92373           27.02525           27.12481           27.23107           27.32827           27.43588           27.53363           27.63048           \n    Angola             20.94876           20.94371           20.93754           20.93187           20.93569           20.94857           20.96030           20.98025           21.01375           ...                21.31954           21.37480           21.43664           21.51765           21.59924           21.69218           21.80564           21.93881           22.08962           22.25083           \n    Antigua and Barbuda23.31424           23.39054           23.45883           23.53735           23.63584           23.73109           23.83449           23.93649           24.05364           ...                24.91721           24.99158           25.05857           25.13039           25.20713           25.29898           25.39965           25.51382           25.64247           25.76602           \n\n\n\n\n\n\nCountryY1980Y1981Y1982Y1983Y1984Y1985Y1986Y1987Y1988...Y1999Y2000Y2001Y2002Y2003Y2004Y2005Y2006Y2007Y2008\n\n    Afghanistan        21.48678           21.46552           21.45145           21.43822           21.42734           21.41222           21.40132           21.37679           21.34018           ...                20.75469           20.69521           20.62643           20.59848           20.58706           20.57759           20.58084           20.58749           20.60246           20.62058           \n    Albania            25.22533           25.23981           25.25636           25.27176           25.27901           25.28669           25.29451           25.30217           25.30450           ...                25.46555           25.55835           25.66701           25.77167           25.87274           25.98136           26.08939           26.20867           26.32753           26.44657           \n    Algeria            22.25703           22.34745           22.43647           22.52105           22.60633           22.69501           22.76979           22.84096           22.90644           ...                23.69486           23.77659           23.86256           23.95294           24.05243           24.15957           24.27001           24.38270           24.48846           24.59620           \n    Andorra            25.66652           25.70868           25.74681           25.78250           25.81874           25.85236           25.89089           25.93414           25.98477           ...                26.75078           26.83179           26.92373           27.02525           27.12481           27.23107           27.32827           27.43588           27.53363           27.63048           \n    Angola             20.94876           20.94371           20.93754           20.93187           20.93569           20.94857           20.96030           20.98025           21.01375           ...                21.31954           21.37480           21.43664           21.51765           21.59924           21.69218           21.80564           21.93881           22.08962           22.25083           \n    Antigua and Barbuda23.31424           23.39054           23.45883           23.53735           23.63584           23.73109           23.83449           23.93649           24.05364           ...                24.91721           24.99158           25.05857           25.13039           25.20713           25.29898           25.39965           25.51382           25.64247           25.76602           \n    Argentina          25.37913           25.44951           25.50242           25.55644           25.61271           25.66593           25.72364           25.78529           25.84428           ...                26.79005           26.88103           26.96067           26.99882           27.04738           27.11001           27.18941           27.28179           27.38889           27.50170           \n    Armenia            23.82469           23.86401           23.91023           23.95649           24.00181           24.04083           24.08736           24.13334           24.17219           ...                24.11699           24.18045           24.26670           24.37698           24.50332           24.64178           24.81447           24.99160           25.17590           25.35542           \n    Australia          24.92729           25.00216           25.07660           25.14938           25.22894           25.31849           25.41017           25.50528           25.60001           ...                26.65506           26.74486           26.84397           26.93858           27.03801           27.13871           27.24614           27.35267           27.45878           27.56373           \n    Austria            24.84097           24.88110           24.93482           24.98118           25.02208           25.06015           25.10680           25.14747           25.19333           ...                25.81773           25.87471           25.93806           25.99583           26.06356           26.14360           26.21107           26.29374           26.38136           26.46741           \n    Azerbaijan         24.49375           24.52584           24.56064           24.60150           24.64121           24.67566           24.71906           24.75799           24.78894           ...                24.47842           24.51287           24.57202           24.66021           24.77164           24.89376           25.06256           25.25706           25.45513           25.65117           \n    Bahamas            24.21064           24.30814           24.42750           24.54415           24.66558           24.78408           24.90724           25.03166           25.14778           ...                26.12080           26.25748           26.38653           26.51184           26.62607           26.75612           26.88517           27.00715           27.12653           27.24594           \n    Bahrain            23.97588           24.09045           24.20617           24.32335           24.43174           24.53684           24.63328           24.74914           24.86604           ...                26.50245           26.65409           26.80388           26.94923           27.09298           27.23908           27.38693           27.53868           27.68865           27.83721           \n    Bangladesh         20.51918           20.47766           20.43741           20.40075           20.36524           20.32983           20.29654           20.26401           20.23497           ...                20.13361           20.14774           20.16802           20.18621           20.20948           20.23957           20.27648           20.31554           20.35493           20.39742           \n    Barbados           24.36372           24.43455           24.49314           24.54713           24.59913           24.64998           24.71728           24.77976           24.84265           ...                25.51681           25.60292           25.68910           25.77615           25.87020           25.95660           26.06074           26.16874           26.27575           26.38439           \n\n\n\n\n\n\nCountryY1980Y1981Y1982Y1983Y1984Y1985Y1986Y1987Y1988...Y1999Y2000Y2001Y2002Y2003Y2004Y2005Y2006Y2007Y2008\n\n    Venezuela         24.58052          24.69666          24.80082          24.89208          24.98440          25.07104          25.15587          25.24624          25.35274          ...               26.50035          26.61021          26.71688          26.79210          26.85498          26.95162          27.05633          27.17698          27.30849          27.44500          \n    Vietnam           19.01394          19.03902          19.06804          19.09675          19.13046          19.16397          19.19740          19.23481          19.27090          ...               20.02081          20.10343          20.18623          20.27145          20.36402          20.46585          20.57277          20.68655          20.80189          20.91630          \n    West Bank and Gaza24.31624          24.40192          24.48713          24.57107          24.65582          24.74148          24.82984          24.91615          25.00108          ...               26.28240          26.39074          26.45700          26.48925          26.51152          26.52924          26.54329          26.54449          26.55460          26.57750          \n    Yemen, Rep.       22.90384          22.96813          23.02669          23.07279          23.12566          23.16944          23.20933          23.25043          23.29401          ...               23.85482          23.92467          23.99129          24.05692          24.12459          24.19204          24.25638          24.32120          24.37949          24.44157          \n    Zambia            19.66295          19.69512          19.72538          19.75420          19.78070          19.80335          19.82396          19.85065          19.88320          ...               20.15094          20.17261          20.20266          20.24298          20.29474          20.35966          20.43398          20.51422          20.59770          20.68321          \n    Zimbabwe          21.46989          21.48867          21.50738          21.52936          21.53383          21.54341          21.54859          21.54590          21.55396          ...               21.68873          21.72652          21.76514          21.79645          21.82499          21.85806          21.89495          21.93371          21.97405          22.02660          \n\n\n\n\n\n\nCountryY1980Y1981Y1982Y1983Y1984Y1985Y1986Y1987Y1988...Y1999Y2000Y2001Y2002Y2003Y2004Y2005Y2006Y2007Y2008\n\n    United States     25.46406          25.57524          25.67883          25.78812          25.90690          26.02568          26.13740          26.25939          26.37687          ...               27.60386          27.71039          27.80569          27.90479          28.00041          28.10039          28.19703          28.28959          28.37574          28.45698          \n    Uruguay           24.24001          24.31948          24.39260          24.44209          24.49525          24.54516          24.59804          24.67024          24.73972          ...               25.78625          25.86898          25.93469          25.96627          26.00585          26.06073          26.13136          26.20624          26.29256          26.39123          \n    Uzbekistan        24.56500          24.60077          24.62187          24.64780          24.66890          24.69832          24.72305          24.74603          24.77115          ...               24.72082          24.75326          24.79418          24.83998          24.88965          24.95455          25.03331          25.12717          25.22226          25.32054          \n    Vanuatu           23.20701          23.32990          23.46016          23.60431          23.75134          23.89466          24.03171          24.15571          24.27529          ...               25.72398          25.85208          25.96032          26.05661          26.16060          26.27087          26.38887          26.51376          26.64903          26.78926          \n    Venezuela         24.58052          24.69666          24.80082          24.89208          24.98440          25.07104          25.15587          25.24624          25.35274          ...               26.50035          26.61021          26.71688          26.79210          26.85498          26.95162          27.05633          27.17698          27.30849          27.44500          \n    Vietnam           19.01394          19.03902          19.06804          19.09675          19.13046          19.16397          19.19740          19.23481          19.27090          ...               20.02081          20.10343          20.18623          20.27145          20.36402          20.46585          20.57277          20.68655          20.80189          20.91630          \n    West Bank and Gaza24.31624          24.40192          24.48713          24.57107          24.65582          24.74148          24.82984          24.91615          25.00108          ...               26.28240          26.39074          26.45700          26.48925          26.51152          26.52924          26.54329          26.54449          26.55460          26.57750          \n    Yemen, Rep.       22.90384          22.96813          23.02669          23.07279          23.12566          23.16944          23.20933          23.25043          23.29401          ...               23.85482          23.92467          23.99129          24.05692          24.12459          24.19204          24.25638          24.32120          24.37949          24.44157          \n    Zambia            19.66295          19.69512          19.72538          19.75420          19.78070          19.80335          19.82396          19.85065          19.88320          ...               20.15094          20.17261          20.20266          20.24298          20.29474          20.35966          20.43398          20.51422          20.59770          20.68321          \n    Zimbabwe          21.46989          21.48867          21.50738          21.52936          21.53383          21.54341          21.54859          21.54590          21.55396          ...               21.68873          21.72652          21.76514          21.79645          21.82499          21.85806          21.89495          21.93371          21.97405          22.02660          \n\n\n\n\n\n\n\n\n# Histogram of BMIs from 2008\nhist(bmi$Y2008)\n\n# Scatter plot comparing BMIs from 1980 to those from 2008\nplot(bmi$Y1980, bmi$Y2008)"
  },
  {
    "objectID": "posts/2020-10-19-cleaning data in r.html#introduction-to-tidy-data",
    "href": "posts/2020-10-19-cleaning data in r.html#introduction-to-tidy-data",
    "title": "Introduction and Exproling Raw data",
    "section": "Introduction to tidy data",
    "text": "Introduction to tidy data\nin bmi, All of the year column names could be expressed as values of a new variable called year.\n\nGathering columns into key-value pairs\nThe most important function in tidyr is gather(). It should be used when you have columns that are not variables and you want to collapse them into key-value pairs.\nThe easiest way to visualize the effect of gather() is that it makes wide datasets long. Running the following command on wide_df will make it long:\ngather(wide_df, my_key, my_val, -col)\n\n\nSpreading key-value pairs into columns\nThe opposite of gather() is spread(), which takes key-values pairs and spreads them across multiple columns. This is useful when values in a column should actually be column names (i.e. variables). It can also make data more compact and easier to read.\nThe easiest way to visualize the effect of spread() is that it makes long datasets wide. As you saw in the video, running the following command will make long_df wide:\nspread(long_df, my_key, my_val)\n\n\nSeparating columns\nThe separate() function allows you to separate one column into multiple columns. Unless you tell it otherwise, it will attempt to separate on any character that is not a letter or number. You can also specify a specific separator using the sep argument.\ntreatments dataset obeys the principles of tidy data, but we’d like to split the treatment dates into two separate columns: year and month. This can be accomplished with the following:\nseparate(treatments, year_mo, c(\"year\", \"month\"))\n\n\nUniting columns\nThe opposite of separate() is unite(), which takes multiple columns and pastes them together. By default, the contents of the columns will be separated by underscores in the new column, but this behavior can be altered via the sep argument.\ntreatments but this time the year_mo column has been separated into year and month. The original column can be recreated by putting year and month back together:\nunite(treatments, year_mo, year, month)\n\n\nColumn headers are values, not variable names\nYou saw earlier, how we sometimes come across datasets where column names are actually values of a variable (e.g. months of the year). This is often the case when working with repeated measures data, where measurements are taken on subjects of interest on multiple occasions over time. The gather() function is helpful in these situations.\n\n\nVariables are stored in both rows and columns\nSometimes you’ll run into situations where variables are stored in both rows and columns. Although it may not be immediately obvious, if we treat the values in the type column as variables and create a separate column for each of them, we can set things straight. To do this, we use the spread() function.\n\n\nMultiple values are stored in one column\nIt’s also fairly common that you will find two variables stored in a single column of data. These variables may be joined by a separator like a dash, underscore, space, or forward slash.\nThe separate() function comes in handy in these situations. Keep in mind that the into argument, which specifies the names of the 2 new columns being formed, must be given as a character vector (e.g. c(\"column1\", \"column2\"))."
  },
  {
    "objectID": "posts/2020-10-19-cleaning data in r.html#string-manipulation",
    "href": "posts/2020-10-19-cleaning data in r.html#string-manipulation",
    "title": "Introduction and Exproling Raw data",
    "section": "String manipulation",
    "text": "String manipulation\n\nTrimming and padding strings\nOne common issue that comes up when cleaning data is the need to remove leading and/or trailing white space. The str_trim() function from stringr makes it easy to do this while leaving intact the part of the string that you actually want.\n> str_trim(\"  this is a test     \")\n[1] \"this is a test\"\nA similar issue is when you need to pad strings to make them a certain number of characters wide. One example is if you had a bunch of employee ID numbers, some of which begin with one or more zeros. When reading these data in, you find that the leading zeros have been dropped somewhere along the way (probably because the variable was thought to be numeric and in that case, leading zeros would be unnecessary.)\n> str_pad(\"24493\", width = 7, side = \"left\", pad = \"0\")\n[1] \"0024493\"\n\n# Load the stringr package\nlibrary(stringr)\n\n# Trim all leading and trailing whitespace\nstr_trim(c(\"   Filip \", \"Nick  \", \" Jonathan\"))\n\n# Pad these strings with leading zeros\nstr_pad(c(\"23485W\", \"8823453Q\", \"994Z\"), width=9, side='left', pad='0')\n\n\n    'Filip'\n    'Nick'\n    'Jonathan'\n\n\n\n\n    '00023485W'\n    '08823453Q'\n    '00000994Z'\n\n\n\n\nExamples like this are certainly handy in R. For example, the str_pad() function is useful when importing a dataset with US zip codes. Occasionally R will drop the leading 0 in a zipcode, thinking it’s numeric.\n\n\n\nUpper and lower case\nIn addition to trimming and padding strings, you may need to adjust their case from time to time. Making strings uppercase or lowercase is very straightforward in (base) R thanks to toupper() and tolower(). Each function takes exactly one argument: the character string (or vector/column of strings) to be converted to the desired case.\n\nstates <- c(\"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"de\", \"fl\", \"ga\", \"hi\", \"id\", \"il\", \"in\", \"ia\", \"ks\", \"ky\", \"la\", \"me\", \"md\", \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\", \"ne\", \"nv\", \"nh\", \"nj\", \"nm\", \"ny\", \"nc\", \"nd\", \"oh\", \"ok\", \"or\", \"pa\", \"ri\", \"sc\", \"sd\", \"tn\", \"tx\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\")\n# Print state abbreviations\nstates\n\n# Make states all uppercase and save result to states_upper\nstates_upper <- toupper(states)\nstates_upper \n\n# Make states_upper all lowercase again\ntolower(states_upper)\n\n\n    'al'\n    'ak'\n    'az'\n    'ar'\n    'ca'\n    'co'\n    'ct'\n    'de'\n    'fl'\n    'ga'\n    'hi'\n    'id'\n    'il'\n    'in'\n    'ia'\n    'ks'\n    'ky'\n    'la'\n    'me'\n    'md'\n    'ma'\n    'mi'\n    'mn'\n    'ms'\n    'mo'\n    'mt'\n    'ne'\n    'nv'\n    'nh'\n    'nj'\n    'nm'\n    'ny'\n    'nc'\n    'nd'\n    'oh'\n    'ok'\n    'or'\n    'pa'\n    'ri'\n    'sc'\n    'sd'\n    'tn'\n    'tx'\n    'ut'\n    'vt'\n    'va'\n    'wa'\n    'wv'\n    'wi'\n    'wy'\n\n\n\n\n    'AL'\n    'AK'\n    'AZ'\n    'AR'\n    'CA'\n    'CO'\n    'CT'\n    'DE'\n    'FL'\n    'GA'\n    'HI'\n    'ID'\n    'IL'\n    'IN'\n    'IA'\n    'KS'\n    'KY'\n    'LA'\n    'ME'\n    'MD'\n    'MA'\n    'MI'\n    'MN'\n    'MS'\n    'MO'\n    'MT'\n    'NE'\n    'NV'\n    'NH'\n    'NJ'\n    'NM'\n    'NY'\n    'NC'\n    'ND'\n    'OH'\n    'OK'\n    'OR'\n    'PA'\n    'RI'\n    'SC'\n    'SD'\n    'TN'\n    'TX'\n    'UT'\n    'VT'\n    'VA'\n    'WA'\n    'WV'\n    'WI'\n    'WY'\n\n\n\n\n    'al'\n    'ak'\n    'az'\n    'ar'\n    'ca'\n    'co'\n    'ct'\n    'de'\n    'fl'\n    'ga'\n    'hi'\n    'id'\n    'il'\n    'in'\n    'ia'\n    'ks'\n    'ky'\n    'la'\n    'me'\n    'md'\n    'ma'\n    'mi'\n    'mn'\n    'ms'\n    'mo'\n    'mt'\n    'ne'\n    'nv'\n    'nh'\n    'nj'\n    'nm'\n    'ny'\n    'nc'\n    'nd'\n    'oh'\n    'ok'\n    'or'\n    'pa'\n    'ri'\n    'sc'\n    'sd'\n    'tn'\n    'tx'\n    'ut'\n    'vt'\n    'va'\n    'wa'\n    'wv'\n    'wi'\n    'wy'\n\n\n\n\n\nFinding and replacing strings\nThe stringr package provides two functions that are very useful for finding and/or replacing patterns in strings: str_detect() and str_replace().\nLike all functions in stringr, the first argument of each is the string of interest. The second argument of each is the pattern of interest. In the case of str_detect(), this is the pattern we are searching for. In the case of str_replace(), this is the pattern we want to replace. Finally, str_replace() has a third argument, which is the string to replace with.\n> str_detect(c(\"banana\", \"kiwi\"), \"a\")\n[1]  TRUE FALSE\n> str_replace(c(\"banana\", \"kiwi\"), \"a\", \"o\")\n[1] \"bonana\" \"kiwi\"\n\n# Copy of students2: students3\nstudents3 <- students\n\n# Look at the head of students3\nhead(students3)\n\n# Detect all dates of birth (dob) in 1997\nstr_detect(students3$dob, \"1997\")\n\n# In the sex column, replace \"F\" with \"Female\" ...\nstudents3$sex <- str_replace(students3$sex, \"F\", \"Female\")\n\n# ... and \"M\" with \"Male\"\nstudents3$sex <- str_replace(students3$sex, \"M\", \"Male\")\n\n# View the head of students3\nhead(students3)\n\n\n\nX1schoolsexdobaddressfamsizePstatusMeduFeduMjob...romanticfamrelfreetimegooutDalcWalchealthnurse_visitabsencesGrades\n\n    1                  GP                 F                  2000-06-05         U                  GT3                A                  4                  4                  at_home            ...                no                 4                  3                  4                  1                  1                  3                  2014-04-10 14:59:54 6                 5/6/6              \n    2                  GP                 F                  1999-11-25         U                  GT3                T                  1                  1                  at_home            ...                no                 5                  3                  3                  1                  1                  3                  2015-03-12 14:59:54 4                 5/5/6              \n    3                  GP                 F                  1998-02-02         U                  LE3                T                  1                  1                  at_home            ...                no                 4                  3                  2                  2                  3                  3                  2015-09-21 14:59:5410                 7/8/10             \n    4                  GP                 F                  1997-12-20         U                  GT3                T                  4                  2                  health             ...                yes                3                  2                  2                  1                  1                  5                  2015-09-03 14:59:54 2                 15/14/15           \n    5                  GP                 F                  1998-10-04         U                  GT3                T                  3                  3                  other              ...                no                 4                  3                  2                  1                  2                  5                  2015-04-07 14:59:54 4                 6/10/10            \n    6                  GP                 M                  1999-06-16         U                  LE3                T                  4                  3                  services           ...                no                 5                  4                  2                  1                  2                  5                  2013-11-15 14:59:5410                 15/15/15           \n\n\n\n\n\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    TRUE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    TRUE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    FALSE\n    TRUE\n    FALSE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    TRUE\n    TRUE\n    TRUE\n    TRUE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n    FALSE\n\n\n\n\n\nX1schoolsexdobaddressfamsizePstatusMeduFeduMjob...romanticfamrelfreetimegooutDalcWalchealthnurse_visitabsencesGrades\n\n    1                  GP                 Female             2000-06-05         U                  GT3                A                  4                  4                  at_home            ...                no                 4                  3                  4                  1                  1                  3                  2014-04-10 14:59:54 6                 5/6/6              \n    2                  GP                 Female             1999-11-25         U                  GT3                T                  1                  1                  at_home            ...                no                 5                  3                  3                  1                  1                  3                  2015-03-12 14:59:54 4                 5/5/6              \n    3                  GP                 Female             1998-02-02         U                  LE3                T                  1                  1                  at_home            ...                no                 4                  3                  2                  2                  3                  3                  2015-09-21 14:59:5410                 7/8/10             \n    4                  GP                 Female             1997-12-20         U                  GT3                T                  4                  2                  health             ...                yes                3                  2                  2                  1                  1                  5                  2015-09-03 14:59:54 2                 15/14/15           \n    5                  GP                 Female             1998-10-04         U                  GT3                T                  3                  3                  other              ...                no                 4                  3                  2                  1                  2                  5                  2015-04-07 14:59:54 4                 6/10/10            \n    6                  GP                 Male               1999-06-16         U                  LE3                T                  4                  3                  services           ...                no                 5                  4                  2                  1                  2                  5                  2013-11-15 14:59:5410                 15/15/15"
  },
  {
    "objectID": "posts/2020-10-19-cleaning data in r.html#missing-and-special-values",
    "href": "posts/2020-10-19-cleaning data in r.html#missing-and-special-values",
    "title": "Introduction and Exproling Raw data",
    "section": "Missing and special values",
    "text": "Missing and special values\n\nFinding missing values\n\nsocial_df = data.frame(name=c('Sarah', 'Tom', 'David', 'Alice'), n_friends=c(244,NA,145,43),status=c('going out', \"\",'Movie Night', \"\"))\n\n\n# Call is.na() on the full social_df to spot all NAs\nis.na(social_df)\n\n# Use the any() function to ask whether there are any NAs in the data\nany(is.na(social_df))\n\n# View a summary() of the dataset\nsummary(social_df)\n\n# Call table() on the status column\ntable(social_df$status)\n\n\n\nnamen_friendsstatus\n\n    FALSEFALSEFALSE\n    FALSE TRUEFALSE\n    FALSEFALSEFALSE\n    FALSEFALSEFALSE\n\n\n\n\nTRUE\n\n\n    name     n_friends             status \n Alice:1   Min.   : 43.0              :2  \n David:1   1st Qu.: 94.0   going out  :1  \n Sarah:1   Median :145.0   Movie Night:1  \n Tom  :1   Mean   :144.0                  \n           3rd Qu.:194.5                  \n           Max.   :244.0                  \n           NA's   :1                      \n\n\n\n              going out Movie Night \n          2           1           1 \n\n\n\n\nDealing with missing values\nMissing values can be a rather complex subject, but here we’ll only look at the simple case where you are simply interested in normalizing and/or removing all missing values from your data. For more information on why this is not always the best strategy, search online for “missing not at random.”\nLooking at the social_df dataset again, we asked around a bit and figured out what’s causing the missing values that you saw in the last exercise. Tom doesn’t have a social media account on this particular platform, which explains why his number of friends and current status are missing (although coded in two different ways). Alice is on the platform, but is a passive user and never sets her status, hence the reason it’s missing for her.\n\n# Replace all empty strings in status with NA\nsocial_df$status[social_df$status == \"\"] <- NA\n\n# Print social_df to the console\nsocial_df\n\n# Use complete.cases() to see which rows have no missing values\ncomplete.cases(social_df)\n\n# Use na.omit() to remove all rows with any missing values\nna.omit(social_df)\n\n\n\nnamen_friendsstatus\n\n    Sarah      244        going out  \n    Tom         NA        NA         \n    David      145        Movie Night\n    Alice       43        NA         \n\n\n\n\n\n    TRUE\n    FALSE\n    TRUE\n    FALSE\n\n\n\n\n\nnamen_friendsstatus\n\n    1Sarah      244        going out  \n    3David      145        Movie Night"
  },
  {
    "objectID": "posts/2020-10-19-cleaning data in r.html#outliers-and-obvious-errors",
    "href": "posts/2020-10-19-cleaning data in r.html#outliers-and-obvious-errors",
    "title": "Introduction and Exproling Raw data",
    "section": "Outliers and obvious errors",
    "text": "Outliers and obvious errors\n\nDealing with outliers and obvious errors\nWhen dealing with strange values in your data, you often must decide whether they are just extreme or actually erroneous. Extreme values show up all over the place, but you, the data analyst, must figure out when they are plausible and when they are not.\n\n# Look at a summary() of students3\nsummary(students3)\n\n# View a histogram of the studytime variable\nhist(students3$studytime)\n\n# View a histogram of the failures variable\nhist(students3$failures)\n\n# View a histogram of absences, but force zeros to be bucketed to the right of zero\nhist(students3$failures, right=FALSE)\n\n       X1           school              sex                 dob            \n Min.   :  1.0   Length:395         Length:395         Min.   :1996-11-02  \n 1st Qu.: 99.5   Class :character   Class :character   1st Qu.:1997-11-04  \n Median :198.0   Mode  :character   Mode  :character   Median :1998-12-16  \n Mean   :198.0                                         Mean   :1998-10-30  \n 3rd Qu.:296.5                                         3rd Qu.:1999-10-29  \n Max.   :395.0                                         Max.   :2000-10-25  \n   address            famsize            Pstatus          Medu    Fedu   \n Length:395         Length:395         Length:395         0:  3   0:  2  \n Class :character   Class :character   Class :character   1: 59   1: 82  \n Mode  :character   Mode  :character   Mode  :character   2:103   2:115  \n                                                          3: 99   3:100  \n                                                          4:131   4: 96  \n                                                                         \n     Mjob               Fjob              reason            guardian        \n Length:395         Length:395         Length:395         Length:395        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   traveltime      studytime        failures       schoolsup        \n Min.   :1.000   Min.   :1.000   Min.   :0.0000   Length:395        \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:0.0000   Class :character  \n Median :1.000   Median :2.000   Median :0.0000   Mode  :character  \n Mean   :1.448   Mean   :2.035   Mean   :0.3342                     \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:0.0000                     \n Max.   :4.000   Max.   :4.000   Max.   :3.0000                     \n    famsup              paid            activities          nursery         \n Length:395         Length:395         Length:395         Length:395        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    higher            internet           romantic             famrel     \n Length:395         Length:395         Length:395         Min.   :1.000  \n Class :character   Class :character   Class :character   1st Qu.:4.000  \n Mode  :character   Mode  :character   Mode  :character   Median :4.000  \n                                                          Mean   :3.944  \n                                                          3rd Qu.:5.000  \n                                                          Max.   :5.000  \n    freetime         goout            Dalc            Walc      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :3.000   Median :3.000   Median :1.000   Median :2.000  \n Mean   :3.235   Mean   :3.109   Mean   :1.481   Mean   :2.291  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:2.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n     health       nurse_visit                     absences     \n Min.   :1.000   Min.   :2013-10-28 14:59:54   Min.   : 0.000  \n 1st Qu.:3.000   1st Qu.:2014-04-07 02:59:54   1st Qu.: 0.000  \n Median :4.000   Median :2014-09-15 14:59:54   Median : 4.000  \n Mean   :3.554   Mean   :2014-10-10 05:31:11   Mean   : 5.709  \n 3rd Qu.:5.000   3rd Qu.:2015-04-08 02:59:54   3rd Qu.: 8.000  \n Max.   :5.000   Max.   :2015-10-15 14:59:54   Max.   :75.000  \n    Grades         \n Length:395        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother look at strange values\nAnother useful way of looking at strange values is with boxplots. Simply put, boxplots draw a box around the middle 50% of values for a given variable, with a bolded horizontal line drawn at the median. Values that fall far from the bulk of the data points (i.e. outliers) are denoted by open circles.\n\nboxplot(students3$studytime)"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html",
    "href": "posts/2020-07-26-working with dates and times in r.html",
    "title": "Working with Dates and Times in R",
    "section": "",
    "text": "Dates and times are abundant in data and essential for answering questions that start with when, how long, or how often. However, they can be tricky, as they come in a variety of formats and can behave in unintuitive ways. This article teaches you the essentials of parsing, manipulating, and computing with dates and times in R. By the end, we’ll have mastered the lubridate package, a member of the tidyverse, specifically designed to handle dates and times. we’ll also have applied the new skills to explore how often R versions are released, when the weather is good in Auckland (the birthplace of R), and how long monarchs ruled in Britain."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#introduction-to-dates",
    "href": "posts/2020-07-26-working with dates and times in r.html#introduction-to-dates",
    "title": "Working with Dates and Times in R",
    "section": "Introduction to dates",
    "text": "Introduction to dates\n\nDates\n\nDifferent conventions in different places\n27th Feb 2013\nNZ: 27/2/2013\nUSA: 2/27/2013\n\n\n\nThe global standard numeric date format\n{% fn 1%}\n\n\nISO 8601 YYYY-MM-DD\n\nValues ordered from the largest to smallest unit of time\nEach has a xed number of digits, must be padded with leadingzeros\nEither, no separators for computers, or - in dates\n1st of January 2011 -> 2011-01-01\n\n\n\nDates in R - Packages that importd ates:readr,anytime\n\n\n2003-02-27\n\n1974\n\n\n\nas.Date(\"2003-02-27\")\n\n2003-02-27\n\n\n\n\"2003-02-27\"\n\n'2003-02-27'\n\n\n\nstr(\"2003-02-27\")\n\n chr \"2003-02-27\"\n\n\n\nstr(as.Date(\"2003-02-27\"))\n\n Date[1:1], format: \"2003-02-27\"\n\n\n\nSpecifying dates\nR doesn’t know something is a date unless you tell it. If you have a character string that represents a date in the ISO 8601 standard you can turn it into a Date using the as.Date() function. Just pass the character string (or a vector of character strings) as the first argument.\nWe’ll convert a character string representation of a date to a Date object.\n\n# The date R 3.0.0 was released\nx <- \"2013-04-03\"\n\n# Examine structure of x\nstr(x)\n\n# Use as.Date() to interpret x as a date\nx_date <- as.Date(x)\n\n# Examine structure of x_date\nstr(x_date)\n\n# Store April 10 2014 as a Date\napril_10_2014 <- as.Date(\"2014-04-10\")\napril_10_2014\n\n chr \"2013-04-03\"\n Date[1:1], format: \"2013-04-03\"\n\n\n2014-04-10\n\n\n\n\nAutomatic import\nSometimes we’ll need to input a couple of dates by hand using as.Date() but it’s much more common to have a column of dates in a data file.\nSome functions that read in data will automatically recognize and parse dates in a variety of formats. In particular the import functions, like read_csv(), in the readr package will recognize dates in a few common formats.\nThere is also the anytime() function in the anytime package whose sole goal is to automatically parse strings as dates regardless of the format.\n\n# Use read_csv() to import rversions.csv\nreleases <- read_csv('datasets/rversions.csv')\n\n# Examine the structure of the date column\nstr(releases$date)\n\n# Various ways of writing Sep 10 2009\nsep_10_2009 <- c(\"September 10 2009\", \"2009-09-10\", \"10 Sep 2009\", \"09-10-2009\")\n\n# Use anytime() to parse sep_10_2009\nanytime(sep_10_2009)\n\nParsed with column specification:\ncols(\n  major = col_double(),\n  minor = col_double(),\n  patch = col_double(),\n  date = col_date(format = \"\"),\n  datetime = col_datetime(format = \"\"),\n  time = col_time(format = \"\"),\n  type = col_character()\n)\n\n\n Date[1:105], format: \"1997-12-04\" \"1997-12-21\" \"1998-01-10\" \"1998-03-14\" \"1998-05-02\" ...\n\n\n[1] \"2009-09-10 EAT\" \"2009-09-10 EAT\" \"2009-09-10 EAT\" \"2009-09-10 EAT\""
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#why-use-dates",
    "href": "posts/2020-07-26-working with dates and times in r.html#why-use-dates",
    "title": "Working with Dates and Times in R",
    "section": "Why use dates?",
    "text": "Why use dates?\n\nDates act like numbers\n\nDate objects are stored as days since 1970-01-01\n\n\n\nas.Date(\"2020-07-13\") > as.Date(\"2020-01-01\")\n\nTRUE\n\n\n\nas.Date(\"2020-07-10\") + 3\n\n2020-07-13\n\n\n\nas.Date(\"2020-07-13\") - as.Date(\"2019-07-13\")\n\nTime difference of 366 days\n\n\n\nPlotting with dates\n\n\nfirst_days = c(as.Date(\"2020-01-01\"), as.Date(\"2020-02-01\"), \n               as.Date(\"2020-03-01\"), as.Date(\"2020-04-01\"), \n               as.Date(\"2020-05-01\"), as.Date(\"2020-06-01\"), \n               as.Date(\"2020-07-01\"), as.Date(\"2020-08-01\"))\nplot(first_days, 1:8)\n\n\n\n\n\nggplot() + geom_point(aes(x=first_days, y=1:8))\n\n\n\n\n\nR releases\n\n\nhead(releases)\n\n\n\nmajorminorpatchdatedatetimetimetype\n\n    0                  60                 NA                 1997-12-04         1997-12-04 08:47:5808:47:58           patch              \n    0                  61                 NA                 1997-12-21         1997-12-21 13:09:2213:09:22           minor              \n    0                  61                  1                 1998-01-10         1998-01-10 00:31:5500:31:55           patch              \n    0                  61                  2                 1998-03-14         1998-03-14 19:25:5519:25:55           patch              \n    0                  61                  3                 1998-05-02         1998-05-02 07:58:1707:58:17           patch              \n    0                  62                 NA                 1998-06-14         1998-06-14 12:56:2012:56:20           minor              \n\n\n\n\n\nPlotting\nIf you plot a Date on the axis of a plot, you expect the dates to be in calendar order, and that’s exactly what happens with plot() or ggplot().\nWe’ll make some plots with the R version releases data using ggplot2. There are two big differences when a Date is on an axis:\n\nIf you specify limits they must be Date objects.\nTo control the behavior of the scale we’ll use the scale_x_date() function.\n\n\n# Set the x axis to the date column\nggplot(releases, aes(x = date, y = type)) +\n  geom_line(aes(group = 1, color = factor(major)))\n\n# Limit the axis to between 2010-01-01 and 2014-01-01\nggplot(releases, aes(x = date, y = type)) +\n  geom_line(aes(group = 1, color = factor(major))) +\n  xlim(as.Date(\"2010-01-01\"), as.Date(\"2014-01-01\"))\n\n# Specify breaks every ten years and labels with \"%Y\"\nggplot(releases, aes(x = date, y = type)) +\n  geom_line(aes(group = 1, color = factor(major))) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\")\n\nWarning message:\n\"Removed 87 row(s) containing missing values (geom_path).\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nArithmetic and logical operators\nSince Date objects are internally represented as the number of days since 1970-01-01 we can do basic math and comparisons with dates. We can compare dates with the usual logical operators (<, ==, > etc.), find extremes with min() and max(), and even subtract two dates to find out the time between them.\n\n# Find the largest date\nlast_release_date <- max(releases$date)\n\n# Filter row for last release\nlast_release <- filter(releases, date==last_release_date)\n\n# Print last_release\nlast_release\n\n# How long since last release?\nSys.Date() - last_release_date\n\n\n\nmajorminorpatchdatedatetimetimetype\n\n    3                  4                  1                  2017-06-30         2017-06-30 07:04:1107:04:11           patch              \n\n\n\n\nTime difference of 1110 days\n\n\nSys.date()- in the code, it simply returns today’s date."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#what-about-times",
    "href": "posts/2020-07-26-working with dates and times in r.html#what-about-times",
    "title": "Working with Dates and Times in R",
    "section": "What about times?",
    "text": "What about times?\n\nISO 8601\n\nHH:MM:SS\nLargest unit to smallest\nFixed digits\nHours: 00 – 24\nMinutes: 00 – 59\nSeconds: 00 – 60 (60 only for leap seconds)\nNo separator or :\n\n\n\nDatetimes in R\n\nTwo objects types:\nPOSIXlt - list with named components\nPOSIXct - seconds since 1970-01-01 00:00:00\nPOSIXct will go in a data frame\nas.POSIXct() turns a string into a POSIXct object\n\n\n\nstr(as.POSIXct(\"1997-06-15 00:01:00\"))\n\n POSIXct[1:1], format: \"1997-06-15 00:01:00\"\n\n\n\nTimezones\n\n\"2013-02-27T18:00:00\"-6pm localtime\n\"2013-02-27T18:00:00Z\"-6pm UTC\n\"2013-02-27T18:00:00-08:00\"-6pmin Oregon\n\n\n\nas.POSIXct(\"1997-06-15T18:00:59Z\")\n\n[1] \"1997-06-15 EAT\"\n\n\n\nas.POSIXct(\"1997-06-15T18:00:59Z\", tz=\"UTC\")\n\n[1] \"1997-06-15 UTC\"\n\n\n\nDatetimes behave nicely too\n\nOnce a POSIXct object,datetimes can be:\nCompared\nSubtracted\nPlotted\n\n\n\nGetting datetimes into R\nJust like dates without times, if you want R to recognize a string as a datetime you need to convert it, although now you use as.POSIXct(). as.POSIXct() expects strings to be in the format YYYY-MM-DD HH:MM:SS.\nThe only tricky thing is that times will be interpreted in local time based on your machine’s set up. You can check your timezone with Sys.timezone(). If you want the time to be interpreted in a different timezone, you just set the tz argument of as.POSIXct().\n\nSys.timezone()\n\n'Africa/Nairobi'\n\n\n\n# Use as.POSIXct to enter the datetime \nas.POSIXct(\"2010-10-01 12:12:00\")\n\n# Use as.POSIXct again but set the timezone to `\"America/Los_Angeles\"`\nas.POSIXct(\"2010-10-01 12:12:00\", tz = \"America/Los_Angeles\")\n\n# Examine structure of datetime column\nstr(releases$datetime)\n\n[1] \"2010-10-01 12:12:00 EAT\"\n\n\n[1] \"2010-10-01 12:12:00 PDT\"\n\n\n POSIXct[1:105], format: \"1997-12-04 08:47:58\" \"1997-12-21 13:09:22\" \"1998-01-10 00:31:55\" ...\n\n\n\n\nDatetimes behave nicely too\nJust like Date objects, you can plot and do math with POSIXct objects. We’ll see how quickly people download new versions of R, by examining the download logs from the RStudio CRAN mirror. R 3.2.0 was released at “2015-04-16 07:13:33” so cran-logs_2015-04-17.csv contains a random sample of downloads on the 16th, 17th and 18th.\n\n# Import \"cran-logs_2015-04-17.csv\" with read_csv()\nlogs <- read_csv(\"datasets/cran-logs_2015-04-17.csv\")\n\n# Print logs\nhead(logs)\n\nParsed with column specification:\ncols(\n  datetime = col_datetime(format = \"\"),\n  r_version = col_character(),\n  country = col_character()\n)\n\n\n\n\ndatetimer_versioncountry\n\n    2015-04-16 22:40:193.1.3              CO                 \n    2015-04-16 09:11:043.1.3              GB                 \n    2015-04-16 17:12:373.1.3              DE                 \n    2015-04-18 12:34:433.2.0              GB                 \n    2015-04-16 04:49:183.1.3              PE                 \n    2015-04-16 06:40:443.1.3              TW                 \n\n\n\n\n\n# Store the release time as a POSIXct object\nrelease_time <- as.POSIXct(\"2015-04-16 07:13:33\", tz = \"UTC\")\n\n# When is the first download of 3.2.0?\nlogs %>% \n  filter(datetime==release_time,\n    r_version == \"3.2.0\")\n\n# Examine histograms of downloads by version\nggplot(logs, aes(x = datetime)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = as.numeric(release_time)))+\n  facet_wrap(~ r_version, ncol = 1)\n\n\n\ndatetimer_versioncountry\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nit takes about two days for downloads of the new version (3.2.0) to overtake downloads of the old version (3.1.3)"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#why-lubridate",
    "href": "posts/2020-07-26-working with dates and times in r.html#why-lubridate",
    "title": "Working with Dates and Times in R",
    "section": "Why lubridate?",
    "text": "Why lubridate?\n\nlubridate\n\nMake working with dates and times in R easy!\ntidyverse package\nPlays nicely with builtin datetime objects\nDesigned for humans not computers\nPlays nicely with other tidyverse packages\nConsistent behaviour regardless of underlying object\n\n\n\nParsing a wide range of formats\n\n\nymd(\"1997-06-15\")\n\n1997-06-15\n\n\n\ndmy(\"15/06/97\")\n\n1997-06-15\n\n\n\nparse_date_time(c(\"June 15th, 1997\", \"15th June, 1997\"), order=c(\"mdy\", \"dmy\"))\n\n[1] \"1997-06-15 UTC\" \"1997-06-15 UTC\"\n\n\n\nManipulating datetimes\n\n\nakl_daily = read_csv(\"datasets/akl_weather_daily.csv\")\nhead(akl_daily)\n\nParsed with column specification:\ncols(\n  date = col_character(),\n  max_temp = col_double(),\n  min_temp = col_double(),\n  mean_temp = col_double(),\n  mean_rh = col_double(),\n  events = col_character(),\n  cloud_cover = col_double()\n)\n\n\n\n\ndatemax_tempmin_tempmean_tempmean_rheventscloud_cover\n\n    2007-9-160      51      56      75      NA      4       \n    2007-9-260      53      56      82      Rain    4       \n    2007-9-357      51      54      78      NA      6       \n    2007-9-464      50      57      80      Rain    6       \n    2007-9-553      48      50      90      Rain    7       \n    2007-9-657      42      50      69      NA      1       \n\n\n\n\n\nakl_daily_m <- akl_daily %>%\n    mutate(\n        year = year(date),\n        yday = yday(date),\n        month = month(date, label=TRUE)\n    )\nhead(akl_daily_m)\n\n\n\ndatemax_tempmin_tempmean_tempmean_rheventscloud_coveryearydaymonth\n\n    2007-9-160      51      56      75      NA      4       2007    244     Sep     \n    2007-9-260      53      56      82      Rain    4       2007    245     Sep     \n    2007-9-357      51      54      78      NA      6       2007    246     Sep     \n    2007-9-464      50      57      80      Rain    6       2007    247     Sep     \n    2007-9-553      48      50      90      Rain    7       2007    248     Sep     \n    2007-9-657      42      50      69      NA      1       2007    249     Sep     \n\n\n\n\n\nOther lubridate features\n\nHandling timezones\nFast parsing of standard formats\nOutputting datetimes"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#parsing-dates-with-lubridate",
    "href": "posts/2020-07-26-working with dates and times in r.html#parsing-dates-with-lubridate",
    "title": "Working with Dates and Times in R",
    "section": "Parsing dates with lubridate",
    "text": "Parsing dates with lubridate\n\nymd()\n\n27th of February 2013\nymd() - year, then month, then day\n\n\n\nymd(\"1997-06-15\")\n\n1997-06-15\n\n\n\nymd(\"97/6/15\")\n\n1997-06-15\n\n\n\nymd(\"97t6t15\")\n\n1997-06-15\n\n\n\nFriends of ymd()\n\nymd(),ydm(),mdy(),myd(),dmy(),dym(), dmy_hm()\nparse_date_time(x = ___, order = ___)\n\n\n\nFormatting characters\n\n\n\nCharacter\nMeaning\n\nCharacter\nMeaning\n\n\n\n\nd\nnumeric day of the month\n\na\nAbbreviated weekday\n\n\nm\nmonth of the year\n\nA\nFull weekday\n\n\ny\nYear with century\n\nb\nAbbreviated month name\n\n\nY\nyear without century\n\nB\nfull month name\n\n\nH\nhours (24 hours)\n\nI\nhours (12 hour)\n\n\nM\nminutes\n\nP\nAM/PM\n\n\nz\nTimezone, offset\n\n\n\n\n\n\n\n\nSelecting the right parsing function\nlubridate provides a set of functions for parsing dates of a known order. For example, ymd() will parse dates with year first, followed by month and then day. The parsing is flexible, for example, it will parse the m whether it is numeric (e.g. 9 or 09), a full month name (e.g. September), or an abbreviated month name (e.g. Sep).\nAll the functions with y, m and d in any order exist. If the dates have times as well, you can use the functions that start with ymd, dmy, mdy or ydm and are followed by any of _h, _hm or _hms.\n\n# Parse x \nx <- \"2010 September 20th\" # 2010-09-20\nymd(x)\n\n# Parse y \ny <- \"02.01.2010\"  # 2010-01-02\ndmy(y)\n\n# Parse z \nz <- \"Sep, 12th 2010 14:00\"  # 2010-09-12T14:00\nmdy_hm(z)\n\n2010-09-20\n\n\n2010-01-02\n\n\n[1] \"2010-09-12 14:00:00 UTC\"\n\n\n\n\nSpecifying an order with parse_date_time()\nWhat about if you have something in a really weird order like dym_msh?There’s no named function just for that order, but that is where parse_date_time() comes in. parse_date_time() takes an additional argument, orders, where you can specify the order of the components in the date.\nFor example, to parse \"2010 September 20th\" you could say parse_date_time(\"2010 September 20th\", orders = \"ymd\") and that would be equivalent to using the ymd() function from the previous exercise.\nOne advantage of parse_date_time() is that you can use more format characters. For example, you can specify weekday names with A, I for 12 hour time, am/pm indicators with p and many others. Another big advantage is that you can specify a vector of orders, and that allows parsing of dates where multiple formats might be used.\n\n# Specify an order string to parse x\nx <- \"Monday June 1st 2010 at 4pm\"\nparse_date_time(x, orders = \"ABdyIp\")\n\n# Specify order to include both \"mdy\" and \"dmy\"\ntwo_orders <- c(\"October 7, 2001\", \"October 13, 2002\", \"April 13, 2003\", \n  \"17 April 2005\", \"23 April 2017\")\nparse_date_time(two_orders, orders = c(\"Bdy\", \"Bdy\", \"Bdy\", \"dBy\", \"dBy\"))\n\n# Specify order to include \"dOmY\", \"OmY\" and \"Y\"\nshort_dates <- c(\"11 December 1282\", \"May 1372\", \"1253\")\nparse_date_time(short_dates, orders = c(\"d0mY\", \"0mY\", \"Y\"))\n\n[1] \"2010-06-01 16:00:00 UTC\"\n\n\n[1] \"2001-10-07 UTC\" \"2002-10-13 UTC\" \"2003-04-13 UTC\" \"2005-04-17 UTC\"\n[5] \"2017-04-23 UTC\"\n\n\n[1] \"1282-12-11 UTC\" \"1372-05-01 UTC\" \"1253-01-01 UTC\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nwhen a date component is missing, it’s just set to 1? For example, the input 1253 resulted in the date 1253-01-01."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#weather-in-auckland",
    "href": "posts/2020-07-26-working with dates and times in r.html#weather-in-auckland",
    "title": "Working with Dates and Times in R",
    "section": "Weather in Auckland",
    "text": "Weather in Auckland\n\nmake_date(year, month, day)\n\n\nmake_date(year=1997, month=6, day=15)\n\n1997-06-15\n\n\nmake_datetime(year, month, day, hour, min, sec) for datetimes\n\ndplyr Review\n\nmutate() - add new columns (or overwrite old ones)\nfilter() - subset rows\nselect() - subset columns\narrange() - order rows\nsummarise() - summarise rows\ngroup_by() - useful in conjuction with summarise()\n\n\n\nImport daily weather data\nIn practice you won’t be parsing isolated dates and times, they’ll be part of a larger dataset. We’ll be working with weather data from Auckland NZ.\nThere are two data sets: - akl_weather_daily.csv a set of once daily summaries for 10 years, and - akl_weather_hourly_2016.csv observations every half hour for 2016.\n\n# Import CSV with read_csv()\nakl_daily_raw <- read_csv(\"datasets/akl_weather_daily.csv\")\n\n# Print akl_daily_raw\nhead(akl_daily_raw)\n\nParsed with column specification:\ncols(\n  date = col_character(),\n  max_temp = col_double(),\n  min_temp = col_double(),\n  mean_temp = col_double(),\n  mean_rh = col_double(),\n  events = col_character(),\n  cloud_cover = col_double()\n)\n\n\n\n\ndatemax_tempmin_tempmean_tempmean_rheventscloud_cover\n\n    2007-9-160      51      56      75      NA      4       \n    2007-9-260      53      56      82      Rain    4       \n    2007-9-357      51      54      78      NA      6       \n    2007-9-464      50      57      80      Rain    6       \n    2007-9-553      48      50      90      Rain    7       \n    2007-9-657      42      50      69      NA      1       \n\n\n\n\n\n# Parse date \nakl_daily <- akl_daily_raw %>%\n  mutate(date = ymd(date))\n\n# Print akl_daily\nhead(akl_daily)\n\n\n\ndatemax_tempmin_tempmean_tempmean_rheventscloud_cover\n\n    2007-09-0160        51        56        75        NA        4         \n    2007-09-0260        53        56        82        Rain      4         \n    2007-09-0357        51        54        78        NA        6         \n    2007-09-0464        50        57        80        Rain      6         \n    2007-09-0553        48        50        90        Rain      7         \n    2007-09-0657        42        50        69        NA        1         \n\n\n\n\n\n# Plot to check work\nakl_daily %>%\n    ggplot(aes(x = date, y = max_temp)) +\n        geom_line() \n\nWarning message:\n\"Removed 1 row(s) containing missing values (geom_path).\"\n\n\n\n\n\nThe temperatures are in farenheit. Yup, summer falls in Dec-Jan-Feb.\n\n\nImport hourly weather data\nThe hourly data is a little different. The date information is spread over three columns year, month and mday, so we’ll need to use make_date() to combine them.\n\nakl_hourly_raw <- read_csv(\"datasets/akl_weather_hourly_2016.csv\")\nhead(akl_hourly_raw)\n\nParsed with column specification:\ncols(\n  year = col_double(),\n  month = col_double(),\n  mday = col_double(),\n  time = col_time(format = \"\"),\n  temperature = col_double(),\n  weather = col_character(),\n  conditions = col_character(),\n  events = col_character(),\n  humidity = col_double(),\n  date_utc = col_datetime(format = \"\")\n)\n\n\n\n\nyearmonthmdaytimetemperatureweatherconditionseventshumiditydate_utc\n\n    2016               1                  1                  00:00:00           68                 Clear              Clear              NA                 68                 2015-12-31 11:00:00\n    2016               1                  1                  00:30:00           68                 Clear              Clear              NA                 68                 2015-12-31 11:30:00\n    2016               1                  1                  01:00:00           68                 Clear              Clear              NA                 73                 2015-12-31 12:00:00\n    2016               1                  1                  01:30:00           68                 Clear              Clear              NA                 68                 2015-12-31 12:30:00\n    2016               1                  1                  02:00:00           68                 Clear              Clear              NA                 68                 2015-12-31 13:00:00\n    2016               1                  1                  02:30:00           68                 Clear              Clear              NA                 68                 2015-12-31 13:30:00\n\n\n\n\nThen the time information is in a separate column again, time. It’s quite common to find date and time split across different variables. One way to construct the datetimes is to paste the date and time together and then parse them.\n\n# Use make_date() to combine year, month and mday \nakl_hourly  <- akl_hourly_raw  %>% \n  mutate(date = make_date(year = year, month = month, day = mday))\n\n# Parse datetime_string \nakl_hourly <- akl_hourly  %>% \n  mutate(\n    datetime_string = paste(date, time, sep = \"T\"),\n    datetime = ymd_hms(datetime_string)\n  )\n\n# Print date, time and datetime columns of akl_hourly\nakl_hourly %>% \n    select(date, time, datetime) %>%\n        head()\n\n\n\ndatetimedatetime\n\n    2016-01-01         00:00:00           2016-01-01 00:00:00\n    2016-01-01         00:30:00           2016-01-01 00:30:00\n    2016-01-01         01:00:00           2016-01-01 01:00:00\n    2016-01-01         01:30:00           2016-01-01 01:30:00\n    2016-01-01         02:00:00           2016-01-01 02:00:00\n    2016-01-01         02:30:00           2016-01-01 02:30:00\n\n\n\n\n\n# Plot to check work\nakl_hourly %>%\n    ggplot(aes(x = datetime, y = temperature)) +\n        geom_line()\n\n\n\n\nIt’s interesting how the day to day variation is about half the size of the yearly variation."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#extracting-parts-of-a-datetime",
    "href": "posts/2020-07-26-working with dates and times in r.html#extracting-parts-of-a-datetime",
    "title": "Working with Dates and Times in R",
    "section": "Extracting parts of a datetime",
    "text": "Extracting parts of a datetime\n\nExtracting parts of a datetime\n\n\nyob = ymd(\"1997-06-15\")\nyob\n\n1997-06-15\n\n\n\nyear(yob)\n\n1997\n\n\n\nmonth(yob)\n\n6\n\n\n\nday(yob)\n\n15\n\n\n\nExtracting parts of a datetime\n\n\n\nFunction\nExtracts\n\n\n\n\nyear()\nYear with century\n\n\nmonth()\nMonth (1-12)\n\n\nday()\nDay of month (1-31)\n\n\nhour()\nHour (0-23)\n\n\nmin()\nMinute (0-59)\n\n\nsecond()\nSecond (0-59)\n\n\nwday()\nWeekday (1-7)\n\n\n\n\n\nSetting parts of a datetime\n\n\nyob\n\n1997-06-15\n\n\n\nyear(yob) <- 2020\n\n\nyob\n\n2020-06-15\n\n\n\nOther useful functions\n\n\n\nFunction\nExtracts\n\n\n\n\nleap_year()\nIn leap year (TRUE or FALSE)\n\n\nam()\nIn morning (TRUE or FALSE)\n\n\npm()\nIn afternoon (TRUE or FALSE)\n\n\ndst()\nDuring daylight savings (TRUE or FALSE)\n\n\nquarter()\nQuarter of year (1-4)\n\n\nsemester()\nHalf of year (1-2)\n\n\n\n\n\n# Examine the head() of release_time\nhead(releases$datetime)\n\n[1] \"1997-12-04 08:47:58 UTC\" \"1997-12-21 13:09:22 UTC\"\n[3] \"1998-01-10 00:31:55 UTC\" \"1998-03-14 19:25:55 UTC\"\n[5] \"1998-05-02 07:58:17 UTC\" \"1998-06-14 12:56:20 UTC\"\n\n\n\n# Examine the head() of the months of release_time\nhead(month(releases$datetime))\n\n\n    12\n    12\n    1\n    3\n    5\n    6\n\n\n\n\n# Extract the month of releases \nmonth(releases$datetime) %>% table()\n\n.\n 1  2  3  4  5  6  7  8  9 10 11 12 \n 5  6  8 18  5 16  4  7  2 15  6 13 \n\n\n\n# Extract the year of releases\nyear(releases$datetime) %>% table()\n\n.\n1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 \n   2   10    9    6    6    5    5    4    4    4    4    6    5    4    6    4 \n2013 2014 2015 2016 2017 \n   4    4    5    5    3 \n\n\n\n# How often is the hour before 12 (noon)?\nmean(hour(releases$datetime) < 12)\n\n0.752380952380952\n\n\n\n# How often is the release in am?\nmean(am(releases$datetime))\n\n0.752380952380952\n\n\nR versions have historically been released most in April, June, October and December, 1998 saw 10 releases and about 75% of releases happen in the morning (at least according to UTC).\n\nAdding useful labels\nSometimes it’s nicer (especially for plotting or tables) to have named months. Both the month() and wday() (day of the week) functions have additional arguments label and abbr to achieve just that. Set label = TRUE to have the output labelled with month (or weekday) names, and abbr = FALSE for those names to be written in full rather than abbreviated.\n\n# Use wday() to tabulate release by day of the week\nwday(releases$datetime) %>% table()\n\n.\n 1  2  3  4  5  6  7 \n 3 29  9 12 18 31  3 \n\n\n\n# Add label = TRUE to make table more readable\nwday(releases$datetime, label=TRUE) %>% table()\n\n.\nSun Mon Tue Wed Thu Fri Sat \n  3  29   9  12  18  31   3 \n\n\n\n# Create column wday to hold labelled week days\nreleases$wday <- wday(releases$datetime, label=T)\n\n# Plot barchart of weekday by type of release\nreleases %>%\n    ggplot(aes(wday)) +\n        geom_bar() +\n        facet_wrap(~ type, ncol = 1, scale = \"free_y\")\n\n\n\n\nLooks like not too many releases occur on the weekends, and there is quite a different weekday pattern between minor and patch releases.\n\n\nExtracting for plotting\nExtracting components from a datetime is particularly useful when exploring data. Earlier we imported daily data for weather in Auckland, and created a time series plot of ten years of daily maximum temperature. While that plot gives a good overview of the whole ten years, it’s hard to see the annual pattern.\nWe’ll use components of the dates to help explore the pattern of maximum temperature over the year. The first step is to create some new columns to hold the extracted pieces, then we’ll use them in a couple of plots.\n\n# Add columns for year, yday and month\nakl_daily <- akl_daily %>%\n  mutate(\n    year = year(date),\n    yday = yday(date),\n    month = month(date, label=T))\n\n# Plot max_temp by yday for all years\nakl_daily %>%\n    ggplot(aes(x = yday, y = max_temp)) +\n    geom_line(aes(group = year), alpha = 0.5)\n\nWarning message:\n\"Removed 1 row(s) containing missing values (geom_path).\"\n\n\n\n\n\n\n# Examine distribution of max_temp by month\nakl_daily %>%\n    ggplot(aes(x = max_temp, y = month, height = ..density..)) +\n    geom_density_ridges(stat = \"density\")\n\nWarning message:\n\"Removed 10 rows containing non-finite values (stat_density).\"\n\n\n\n\n\nBoth plots give a great view into both the expected temperatures and how much they vary. Looks like Jan, Feb and Mar are great months to visit if you want warm temperatures. Did you notice the warning messages? These are a consequence of some missing values in the max_temp column. They are a reminder to think carefully about what you might miss by ignoring missing values.\n\n\nExtracting for filtering and summarizing\nAnother reason to extract components is to help with filtering observations or creating summaries. For example, if you are only interested in observations made on weekdays (i.e. not on weekends) you could extract the weekdays then filter out weekends, e.g. wday(date) %in% 2:6.\nWe saw that January, February and March were great times to visit Auckland for warm temperatures, but will you need a raincoat?\nWe’ll use the hourly data to calculate how many days in each month there was any rain during the day.\n\n# Create new columns hour, month and rainy\nakl_hourly <- akl_hourly %>%\n  mutate(\n    hour = hour(datetime),\n    month = month(datetime, label=T),\n    rainy = weather == \"Precipitation\"\n  )\n\n# Filter for hours between 8am and 10pm (inclusive)\nakl_day <- akl_hourly %>% \n  filter(hour>=8, hour<=22)\n\n# Summarise for each date if there is any rain\nrainy_days <- akl_day %>% \n  group_by(month, date) %>%\n  summarise(\n    any_rain = any(rainy)\n  )\n\n# Summarise for each month, the number of days with rain\nrainy_days<- rainy_days %>% \n    summarise(\n        days_rainy = sum(any_rain)\n    )\nrainy_days\n\n`summarise()` regrouping output by 'month' (override with `.groups` argument)\n`summarise()` ungrouping output (override with `.groups` argument)\n\n\n\n\nmonthdays_rainy\n\n    Jan15 \n    Feb13 \n    Mar12 \n    Apr15 \n    May21 \n    Jun19 \n    Jul22 \n    Aug16 \n    Sep25 \n    Oct20 \n    Nov19 \n    Dec11 \n\n\n\n\nAt least in 2016, it looks like you’ll still need to pack a raincoat if you visit in Jan, Feb or March. Months of course are different lengths so we should really correct for that, take a look at days_in_month() for helping with that."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#rounding-datetimes",
    "href": "posts/2020-07-26-working with dates and times in r.html#rounding-datetimes",
    "title": "Working with Dates and Times in R",
    "section": "Rounding datetimes",
    "text": "Rounding datetimes\n\nRounding versus extracting\n\n\nrelease_time = releases$datetime\nhead(release_time)\n\n[1] \"1997-12-04 08:47:58 UTC\" \"1997-12-21 13:09:22 UTC\"\n[3] \"1998-01-10 00:31:55 UTC\" \"1998-03-14 19:25:55 UTC\"\n[5] \"1998-05-02 07:58:17 UTC\" \"1998-06-14 12:56:20 UTC\"\n\n\n\nrelease_time %>%\n    head() %>%\n        hour()\n\n\n    8\n    13\n    0\n    19\n    7\n    12\n\n\n\n\nrelease_time %>%\n    head() %>%\n        floor_date(unit=\"hour\")\n\n[1] \"1997-12-04 08:00:00 UTC\" \"1997-12-21 13:00:00 UTC\"\n[3] \"1998-01-10 00:00:00 UTC\" \"1998-03-14 19:00:00 UTC\"\n[5] \"1998-05-02 07:00:00 UTC\" \"1998-06-14 12:00:00 UTC\"\n\n\n\nRounding in lubridate\n\nround_date() - round to nearest\nceiling_date() - round up\nfloor_date() - round to down\nPossible values of unit:\n\"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"bimonth\", \"quarter\", \"halfyear\", or \"year\".\nOr multiples, e.g \"2 years\", \"5 minutes\"\n\n\n\nr_3_4_1 <- ymd_hms(\"2016-05-03 07:13:28 UTC\")\n\n# Round down to day\nfloor_date(r_3_4_1, unit = \"day\")\n\n[1] \"2016-05-03 UTC\"\n\n\n\n# Round to nearest 5 minutes\nround_date(r_3_4_1, unit = \"5 minutes\")\n\n[1] \"2016-05-03 07:15:00 UTC\"\n\n\n\n# Round up to week \nceiling_date(r_3_4_1, unit = \"week\")\n\n[1] \"2016-05-08 UTC\"\n\n\n\n# Subtract r_3_4_1 rounded down to day\nr_3_4_1 - floor_date(r_3_4_1, unit = \"day\")\n\nTime difference of 7.224444 hours\n\n\nThat last technique of subtracting a rounded datetime from an unrounded one is a really useful trick to remember.\n\nRounding with the weather data\nWhen is rounding useful? In a lot of the same situations extracting date components is useful. The advantage of rounding over extracting is that it maintains the context of the unit. For example, extracting the hour gives you the hour the datetime occurred, but you lose the day that hour occurred on (unless you extract that too), on the other hand, rounding to the nearest hour maintains the day, month and year.\nWe’ll explore how many observations per hour there really are in the hourly Auckland weather data.\n\n# Create day_hour, datetime rounded down to hour\nakl_hourly <- akl_hourly %>%\n  mutate(\n    day_hour = floor_date(datetime, unit = \"hour\")\n  )\n\n# Count observations per hour  \nakl_hourly %>% \n    count(day_hour) %>%\n    head()\n\n\n\nday_hourn\n\n    2016-01-01 00:00:002                  \n    2016-01-01 01:00:002                  \n    2016-01-01 02:00:002                  \n    2016-01-01 03:00:002                  \n    2016-01-01 04:00:002                  \n    2016-01-01 05:00:002                  \n\n\n\n\n\n# Find day_hours with n != 2  \nakl_hourly %>% \n    count(day_hour) %>%\n        filter(n!=2) %>% \n            arrange(desc(n)) %>%\n                head()\n\n\n\nday_hourn\n\n    2016-04-03 02:00:004                  \n    2016-09-25 00:00:004                  \n    2016-06-26 09:00:001                  \n    2016-09-01 23:00:001                  \n    2016-09-02 01:00:001                  \n    2016-09-04 11:00:001                  \n\n\n\n\nInterestingly there are four measurements on 2016-04-03 and 2016-09-25, they happen to be the days Daylight Saving starts and ends."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#taking-differences-of-datetimes",
    "href": "posts/2020-07-26-working with dates and times in r.html#taking-differences-of-datetimes",
    "title": "Working with Dates and Times in R",
    "section": "Taking differences of datetimes",
    "text": "Taking differences of datetimes\n\nArithmetic for datetimes\n\ndatetime_1 - datetime2: Subtraction for time elapsed\ndatetime_1 + (2 * timespan): Addition and multiplication for generating new datetimes in the past or future\ntimespan1 / timespan2: Division for change of units\n\n\n\nSubtraction of datetimes\n\n\nlast_release_date\n\n2017-06-30\n\n\n\nSys.Date() - last_release_date\n\nTime difference of 1110 days\n\n\n\ndifftime(Sys.Date(), last_release_date)\n\nTime difference of 1110 days\n\n\n\ndifftime()\n\nunits =\"secs\",\"mins\",\"hours\",\"days\", or \"weeks\"\n\n\n\nyear(yob) <- 1997\nyob\n\n1997-06-15\n\n\n\ndifftime(Sys.Date(), yob, units=\"weeks\")\n\nTime difference of 1204.286 weeks\n\n\n\ndifftime(Sys.Date(), yob, units=\"days\")\n\nTime difference of 8430 days\n\n\n\ndifftime(Sys.Date(), yob, units=\"secs\")\n\nTime difference of 728352000 secs\n\n\n\nnow() and today()\n\n\nnow()\n\n[1] \"2020-07-14 09:07:16 EAT\"\n\n\n\ntoday()\n\n2020-07-14\n\n\n\nstr(today())\n\n Date[1:1], format: \"2020-07-14\"\n\n\n\nHow long has it been?\nTo get finer control over a difference between datetimes use the base function difftime(). For example instead of time1 - time2, we can use difftime(time1, time2).\ndifftime() takes an argument units which specifies the units for the difference. Your options are \"secs\", \"mins\", \"hours\", \"days\", or \"weeks\".\nWe’ll find the time since the first man stepped on the moon.\n\n# The date of landing and moment of step\ndate_landing <- mdy(\"July 20, 1969\")\nmoment_step <- mdy_hms(\"July 20, 1969, 02:56:15\", tz = \"UTC\")\n\n# How many days since the first man on the moon?\ndifftime(today(), date_landing, units = \"days\")\n\nTime difference of 18622 days\n\n\n\n# How many seconds since the first man on the moon?\ndifftime(now(), moment_step, units=\"secs\")\n\nTime difference of 1608952266 secs\n\n\n\n\nHow many seconds are in a day?\nHow many seconds are in a day? There are 24 hours in a day, 60 minutes in an hour, and 60 seconds in a minute, so there should be 246060 = 86400 seconds, right?\nNot always! We’ll see a counter example.\n\n# Three dates\nmar_11 <- ymd_hms(\"2017-03-11 12:00:00\", \n  tz = \"America/Los_Angeles\")\nmar_12 <- ymd_hms(\"2017-03-12 12:00:00\", \n  tz = \"America/Los_Angeles\")\nmar_13 <- ymd_hms(\"2017-03-13 12:00:00\", \n  tz = \"America/Los_Angeles\")\n\n# Difference between mar_13 and mar_12 in seconds\ndifftime(mar_13, mar_12, units = \"secs\")\n\nTime difference of 86400 secs\n\n\n\n# Difference between mar_12 and mar_11 in seconds\ndifftime(mar_12, mar_11, units = \"secs\")\n\nTime difference of 82800 secs\n\n\nWhy would a day only have 82800 seconds? At 2am on Mar 12th 2017, Daylight Savings started in the Pacific timezone. That means a whole hour of seconds gets skipped between noon on the 11th and noon on the 12th."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#time-spans.",
    "href": "posts/2020-07-26-working with dates and times in r.html#time-spans.",
    "title": "Working with Dates and Times in R",
    "section": "Time spans.",
    "text": "Time spans.\n\nTime spans in lubridate\n\n\n\n\n\n\n\nperiod\nDuration\n\n\n\n\nHuman concept of a timespan\nStopwatch concept of a time span\n\n\ndatetime + period of oneday = same time on the next date\ndatetime + duration of oneday = datetime + 86400seconds\n\n\nvariable length\nfixed number of seconds\n\n\n\n\n\nCreating a time span\n\n\ndays()\n\n1d 0H 0M 0S\n\n\n\ndays(x=2)\n\n2d 0H 0M 0S\n\n\n\nddays(2)\n\n172800s (~2 days)\n\n\n\nArithmetic with time spans\n\n\n2 * days()\n\n2d 0H 0M 0S\n\n\n\ndays() + days()\n\n2d 0H 0M 0S\n\n\n\nymd(\"2020-07-13\") + days()\n\n2020-07-14\n\n\n\nFunctions to create time spans\n\n\n\nTime span\nDuration\nPeriod\n\n\n\n\nSeconds\ndseconds()\nseconds()\n\n\nMinutes\ndminutes()\nminutes()\n\n\nHours\ndhours()\nhours()\n\n\nDays\nddays()\ndays()\n\n\nWeeks\ndweeks()\nweeks()\n\n\nMonths\n-\nmonths()\n\n\nYears\ndyears()\nyears()\n\n\n\n\n\nddays()\n\n86400s (~1 days)\n\n\n\nAdding or subtracting a time span to a datetime\nA common use of time spans is to add or subtract them from a moment in time. For, example to calculate the time one day in the future from mar_11, you could do either of:\n\nmar_11+days()\n\n[1] \"2017-03-12 12:00:00 PDT\"\n\n\n\nmar_11+ddays()\n\n[1] \"2017-03-12 13:00:00 PDT\"\n\n\nBut which one is the right one? It depends on your intent. If you want to account for the fact that time units, in this case days, have different lengths (i.e. due to daylight savings), you want a period days(). If you want the time 86400 seconds in the future you use a duration ddays().\nWe’ll add and subtract timespans from dates and datetimes.\n\n# Add a period of one week to mon_2pm\nmon_2pm <- dmy_hm(\"27 Aug 2018 14:00\")\nmon_2pm + weeks()\n\n[1] \"2018-09-03 14:00:00 UTC\"\n\n\n\n# Add a duration of 81 hours to tue_9am\ntue_9am <- dmy_hm(\"28 Aug 2018 9:00\")\ntue_9am + dhours(81)\n\n[1] \"2018-08-31 18:00:00 UTC\"\n\n\n\n# Subtract a period of five years from today()\ntoday() - years(5)\n\n2015-07-14\n\n\n\n# Subtract a duration of five years from today()\ntoday() - dyears(5)\n\n[1] \"2015-07-14 18:00:00 UTC\"\n\n\nWhy did subtracting a duration of five years from today, give a different answer to subtracting a period of five years? Periods know about leap years, and since five years ago includes at least one leap year, the period of five years is longer than the duration of 365*5 days.\n\n\n\n\n\n\nNote\n\n\n\nwhen dealing with human interpretaions of dates and time you want to use periods.\n\n\n\n\nArithmetic with timespans\nYou can add and subtract timespans to create different length timespans, and even multiply them by numbers. For example, to create a duration of three days and three hours you could do:\n\nddays(3) + dhours(3)\n\n270000s (~3.12 days)\n\n\nor\n\n3*ddays(1) + 3*dhours(1)\n\n270000s (~3.12 days)\n\n\nor even\n\n3*(ddays(1) + dhours(1))\n\n270000s (~3.12 days)\n\n\nThere was an eclipse over North America on 2017-08-21 at 18:26:40. It’s possible to predict the next eclipse with similar geometry by calculating the time and date one Saros in the future. A Saros is a length of time that corresponds to 223 Synodic months, a Synodic month being the period of the Moon’s phases, a duration of 29 days, 12 hours, 44 minutes and 3 seconds.\n\n# Time of North American Eclipse 2017\neclipse_2017 <- ymd_hms(\"2017-08-21 18:26:40\")\n\n# Duration of 29 days, 12 hours, 44 mins and 3 secs\nsynodic <- ddays(29) + dhours(12) + dminutes(44) + dseconds(3)\n\n# 223 synodic months\nsaros <- 223 * synodic\n\n# Add saros to eclipse_2017\nsaros + eclipse_2017\n\n[1] \"2035-09-02 02:09:49 UTC\"\n\n\n2035 is a long way away for an eclipse, but luckily there are eclipses on different Saros cycles, so you can see one much sooner.\n\n\nGenerating sequences of datetimes\nBy combining addition and multiplication with sequences you can generate sequences of datetimes. For example, you can generate a sequence of periods from 1 day up to 10 days with,\n\n1:10 * days()\n\n\n    1d 0H 0M 0S\n    2d 0H 0M 0S\n    3d 0H 0M 0S\n    4d 0H 0M 0S\n    5d 0H 0M 0S\n    6d 0H 0M 0S\n    7d 0H 0M 0S\n    8d 0H 0M 0S\n    9d 0H 0M 0S\n    10d 0H 0M 0S\n\n\n\nThen by adding this sequence to a specific datetime, you can construct a sequence of datetimes from 1 day up to 10 days into the future\n\ntoday() + 1:10 * days()\n\n\n    2020-07-15\n    2020-07-16\n    2020-07-17\n    2020-07-18\n    2020-07-19\n    2020-07-20\n    2020-07-21\n    2020-07-22\n    2020-07-23\n    2020-07-24\n\n\n\nWe had a meeting this morning at 8am and we’d like to have that meeting at the same time and day every two weeks for a year.\n\n# Add a period of 8 hours to today\ntoday_8am <- today() + hours(8)\n\n# Sequence of two weeks from 1 to 26\nevery_two_weeks <- 1:26 * weeks()\n\n# Create datetime for every two weeks for a year\n\nevery_two_weeks + today_8am\n\n [1] \"2020-07-21 08:00:00 UTC\" \"2020-07-28 08:00:00 UTC\"\n [3] \"2020-08-04 08:00:00 UTC\" \"2020-08-11 08:00:00 UTC\"\n [5] \"2020-08-18 08:00:00 UTC\" \"2020-08-25 08:00:00 UTC\"\n [7] \"2020-09-01 08:00:00 UTC\" \"2020-09-08 08:00:00 UTC\"\n [9] \"2020-09-15 08:00:00 UTC\" \"2020-09-22 08:00:00 UTC\"\n[11] \"2020-09-29 08:00:00 UTC\" \"2020-10-06 08:00:00 UTC\"\n[13] \"2020-10-13 08:00:00 UTC\" \"2020-10-20 08:00:00 UTC\"\n[15] \"2020-10-27 08:00:00 UTC\" \"2020-11-03 08:00:00 UTC\"\n[17] \"2020-11-10 08:00:00 UTC\" \"2020-11-17 08:00:00 UTC\"\n[19] \"2020-11-24 08:00:00 UTC\" \"2020-12-01 08:00:00 UTC\"\n[21] \"2020-12-08 08:00:00 UTC\" \"2020-12-15 08:00:00 UTC\"\n[23] \"2020-12-22 08:00:00 UTC\" \"2020-12-29 08:00:00 UTC\"\n[25] \"2021-01-05 08:00:00 UTC\" \"2021-01-12 08:00:00 UTC\"\n\n\n\n\nThe tricky thing about months\nWhat should ymd(“2018-01-31”) + months(1) return?\n\nymd(\"2018-01-31\") + months(1)\n\n<NA>\n\n\nIn general lubridate returns the same day of the month in the next month, but since the 31st of February doesn’t exist lubridate returns a missing value, NA.\nThere are alternative addition and subtraction operators: %m+% and %m-% that have different behavior. Rather than returning an NA for a non-existent date, they roll back to the last existing date\n\nymd(\"2018-01-31\") %m+% months(1)\n\n2018-02-28\n\n\n\njan_31 = ymd(\"2020-01-31\")\n# A sequence of 1 to 12 periods of 1 month\nmonth_seq <- 1:12 * months(1)\n\n# Add 1 to 12 months to jan_31\nmonth_seq + jan_31\n\n\n    <NA>\n    2020-03-31\n    <NA>\n    2020-05-31\n    <NA>\n    2020-07-31\n    2020-08-31\n    <NA>\n    2020-10-31\n    <NA>\n    2020-12-31\n    2021-01-31\n\n\n\n\n# Replace + with %m+%\nmonth_seq %m+% jan_31\n\n\n    2020-02-29\n    2020-03-31\n    2020-04-30\n    2020-05-31\n    2020-06-30\n    2020-07-31\n    2020-08-31\n    2020-09-30\n    2020-10-31\n    2020-11-30\n    2020-12-31\n    2021-01-31\n\n\n\n\n# Replace + with %m-%\njan_31 %m-% month_seq\n\n\n    2019-12-31\n    2019-11-30\n    2019-10-31\n    2019-09-30\n    2019-08-31\n    2019-07-31\n    2019-06-30\n    2019-05-31\n    2019-04-30\n    2019-03-31\n    2019-02-28\n    2019-01-31\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nuse these operators with caution, unlike + and -, you might not get x back from x %m+% months(1) %m-% months(1). If you’d prefer that the date was rolled forward check out add_with_rollback() which has roll_to_first argument."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#intervals",
    "href": "posts/2020-07-26-working with dates and times in r.html#intervals",
    "title": "Working with Dates and Times in R",
    "section": "Intervals",
    "text": "Intervals\n\nCreating intervals\n\ndatetime1%--%datetime2,or\ninterval(datetime1, datetime2)\n\n\n\nyob %--% today()\n\n1997-06-15 UTC--2020-07-14 UTC\n\n\n\ninterval(yob, today())\n\n1997-06-15 UTC--2020-07-14 UTC\n\n\n\nOperating on an interval\n\n\nyob_interval = interval(yob, today())\nint_start(yob_interval)\n\n[1] \"1997-06-15 UTC\"\n\n\n\nint_end(yob_interval)\n\n[1] \"2020-07-14 UTC\"\n\n\n\nOperating on an interval\n\n\nint_length(yob_interval)\n\n728352000\n\n\n\nas.period(yob_interval)\n\n23y 0m 29d 0H 0M 0S\n\n\n\nas.duration(yob_interval)\n\n728352000s (~23.08 years)\n\n\n\nComparing intervals\n\n\nenter_mmu <- ymd(\"2017-07-30\")\nenter_mmu %within% yob_interval\n\nTRUE\n\n\n\nbeen_mmu <- ymd(\"2017-07-30\") %--% today()\n\n\nint_overlaps(been_mmu, yob_interval)\n\nTRUE\n\n\n\nWhich kind of time span?\n\nUse:\nIntervals when you have a start and end\nPeriods when you are interested in human units\nDurations if you are interested in seconds elapsed\n\n\n\nMonarchs of England {% fn 2 %}\nHalley’s cometHalley’scomet:{% fn 3 %}\n\n\nExamining intervals. Reigns of kings and queens\n\n# Print monarchs\nmonarchs\n\n# Create an interval for reign\nmonarchs <- monarchs %>%\n  mutate(reign = from %--% to) \n\n# Find the length of reign, and arrange\nmonarchs %>%\n  mutate(length = int_length(reign)) %>% \n  arrange(desc(length)) %>%\n  select(name, length, dominion)\n\nThe current queen, Elizabeth II, has ruled for 2070144000 seconds…we’ll see a better way to display the length later. If you know your British monarchs, you might notice George III doesn’t appear in the the top 5. In this data, his reign is spread over two rows for U.K. And Great Britain and you would need to add their lengths to see his total reign.\n\n\nComparing intervals and datetimes\nA common task with intervals is to ask if a certain time is inside the interval or whether it overlaps with another interval.\nThe operator %within% tests if the datetime (or interval) on the left hand side is within the interval of the right hand side. For example, if y2001 is the interval covering the year 2001,\n\ny2001 <- ymd(\"2001-01-01\") %--% ymd(\"2001-12-31\")\nymd(\"2001-03-30\") %within% y2001\n\nTRUE\n\n\n\nymd(\"2002-03-30\") %within% y2001\n\nFALSE\n\n\nint_overlaps() performs a similar test, but will return true if two intervals overlap at all.\n\n# Print halleys\nhalleys\n\n# New column for interval from start to end date\nhalleys <- halleys %>% \n  mutate(visible = interval(start_date, end_date))\n\n# The visitation of 1066\nhalleys_1066 <- halleys[14, ] \n\n# Monarchs in power on perihelion date\nmonarchs %>% \n  filter(halleys_1066$perihelion_date %within%reign) %>%\n  select(name, from, to, dominion)\n\n# Monarchs whose reign overlaps visible time\nmonarchs %>% \n  filter(int_overlaps(halleys_1066$visible,reign)) %>%\n  select(name, from, to, dominion)\n\nLooks like the Kings of England Edward the Confessor and Harold II would have been able to see the comet. It may have been a bad omen, neither were in power by 1067.\n\n\nConverting to durations and periods\nIntervals are the most specific way to represent a span of time since they retain information about the exact start and end moments. They can be converted to periods and durations exactly: it’s possible to calculate both the exact number of seconds elapsed between the start and end date, as well as the perceived change in clock time\n\n# New columns for duration and period\nmonarchs <- monarchs %>%\n  mutate(\n    duration = as.duration(reign),\n    period = as.period(reign)) \n    \n# Examine results    \nmonarchs %>%\n  select(name, duration, period)"
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#time-zones",
    "href": "posts/2020-07-26-working with dates and times in r.html#time-zones",
    "title": "Working with Dates and Times in R",
    "section": "Time zones",
    "text": "Time zones\n\nSys.timezone()\n\n'Africa/Nairobi'\n\n\n\nIANA Timezones\n\n\nOlsonNames() %>%\n    head(10)\n\n\n    'Africa/Abidjan'\n    'Africa/Accra'\n    'Africa/Addis_Ababa'\n    'Africa/Algiers'\n    'Africa/Asmara'\n    'Africa/Asmera'\n    'Africa/Bamako'\n    'Africa/Bangui'\n    'Africa/Banjul'\n    'Africa/Bissau'\n\n\n\n\nlength(OlsonNames())\n\n593\n\n\n\nSetting and extracting\n\n\nmar_11\n\n[1] \"2017-03-11 12:00:00 PST\"\n\n\n\ntz(mar_11)\n\n'America/Los_Angeles'\n\n\n\nManipulating timezones\n\nforce_tz() - change thetimezone without changing the clock time\nwith_tz() - view the sameinstant in a different timezone\n\n\n\nwith_tz(mar_11, tzone=\"Africa/Nairobi\")\n\n[1] \"2017-03-11 23:00:00 EAT\"\n\n\n\nforce_tz(mar_11, tzone=\"Africa/Nairobi\")\n\n[1] \"2017-03-11 12:00:00 EAT\"\n\n\n\nSetting the timezone\nIf you import a datetime and it has the wrong timezone, you can set it with force_tz(). Pass in the datetime as the first argument and the appropriate timezone to the tzone argument. Remember the timezone needs to be one from OlsonNames().\nI wanted to watch New Zealand in the Women’s World Cup Soccer games in 2015, but the times listed on the FIFA website were all in times local to the venues. We’ll set the timezones, then figure out what time I needed to tune in to watch them.\n\n# Game2: CAN vs NZL in Edmonton\ngame2 <- mdy_hm(\"June 11 2015 19:00\")\n\n# Game3: CHN vs NZL in Winnipeg\ngame3 <- mdy_hm(\"June 15 2015 18:30\")\n\n# Set the timezone to \"America/Edmonton\"\ngame2_local <- force_tz(game2, tzone = \"America/Edmonton\")\ngame2_local\n\n[1] \"2015-06-11 19:00:00 MDT\"\n\n\n\n# Set the timezone to \"America/Winnipeg\"\ngame3_local <- force_tz(game3, tzone = \"America/Winnipeg\")\ngame3_local\n\n[1] \"2015-06-15 18:30:00 CDT\"\n\n\n\n# How long does the team have to rest?\nas.period(interval(game2_local, game3_local))\n\n3d 22H 30M 0S\n\n\nEdmonton and Winnipeg are in different timezones, so even though the start times of the games only look 30 minutes apart, they are in fact 1 hour and 30 minutes apart, and the team only has 3 days, 22 hours and 30 minutes to prepare.\n\n\nViewing in a timezone\nTo view a datetime in another timezone use with_tz(). The syntax of with_tz() is the same as force_tz(), passing a datetime and set the tzone argument to the desired timezone. Unlike force_tz(), with_tz() isn’t changing the underlying moment of time, just how it is displayed.\n\n# What time is game2_local in NZ?\nwith_tz(game2_local, tzone = \"Pacific/Auckland\")\n\n[1] \"2015-06-12 13:00:00 NZST\"\n\n\n\n# What time is game2_local in Corvallis, Oregon?\nwith_tz(game2_local, tzone = \"America/Los_Angeles\")\n\n[1] \"2015-06-11 18:00:00 PDT\"\n\n\n\n# What time is game3_local in NZ?\nwith_tz(game3_local, tzone=\"Pacific/Auckland\")\n\n[1] \"2015-06-16 11:30:00 NZST\"\n\n\n\n\nTimezones in the weather data\n\nhead(akl_hourly)\n\n\n\nyearmonthmdaytimetemperatureweatherconditionseventshumiditydate_utcdatedatetime_stringdatetimehourrainyday_hour\n\n    2016               Jan                1                  00:00:00           68                 Clear              Clear              NA                 68                 2015-12-31 11:00:002016-01-01         2016-01-01T00:00:002016-01-01 00:00:000                  FALSE              2016-01-01 00:00:00\n    2016               Jan                1                  00:30:00           68                 Clear              Clear              NA                 68                 2015-12-31 11:30:002016-01-01         2016-01-01T00:30:002016-01-01 00:30:000                  FALSE              2016-01-01 00:00:00\n    2016               Jan                1                  01:00:00           68                 Clear              Clear              NA                 73                 2015-12-31 12:00:002016-01-01         2016-01-01T01:00:002016-01-01 01:00:001                  FALSE              2016-01-01 01:00:00\n    2016               Jan                1                  01:30:00           68                 Clear              Clear              NA                 68                 2015-12-31 12:30:002016-01-01         2016-01-01T01:30:002016-01-01 01:30:001                  FALSE              2016-01-01 01:00:00\n    2016               Jan                1                  02:00:00           68                 Clear              Clear              NA                 68                 2015-12-31 13:00:002016-01-01         2016-01-01T02:00:002016-01-01 02:00:002                  FALSE              2016-01-01 02:00:00\n    2016               Jan                1                  02:30:00           68                 Clear              Clear              NA                 68                 2015-12-31 13:30:002016-01-01         2016-01-01T02:30:002016-01-01 02:30:002                  FALSE              2016-01-01 02:00:00\n\n\n\n\nThe datetime column we created represented local time in Auckland, NZ. I suspect this additional column, date_utc represents the observation time in UTC (the name seems a big clue). But does it really?\n\n# Examine datetime and date_utc columns\nhead(akl_hourly$datetime)\n\n[1] \"2016-01-01 00:00:00 UTC\" \"2016-01-01 00:30:00 UTC\"\n[3] \"2016-01-01 01:00:00 UTC\" \"2016-01-01 01:30:00 UTC\"\n[5] \"2016-01-01 02:00:00 UTC\" \"2016-01-01 02:30:00 UTC\"\n\n\n\nhead(akl_hourly$date_utc)  \n\n[1] \"2015-12-31 11:00:00 UTC\" \"2015-12-31 11:30:00 UTC\"\n[3] \"2015-12-31 12:00:00 UTC\" \"2015-12-31 12:30:00 UTC\"\n[5] \"2015-12-31 13:00:00 UTC\" \"2015-12-31 13:30:00 UTC\"\n\n\n\n# Force datetime to Pacific/Auckland\nakl_hourly <- akl_hourly %>%\n  mutate(\n    datetime = force_tz(datetime, tzone = \"Pacific/Auckland\"))\n\n# Reexamine datetime\nhead(akl_hourly$datetime)\n\n[1] \"2016-01-01 00:00:00 NZDT\" \"2016-01-01 00:30:00 NZDT\"\n[3] \"2016-01-01 01:00:00 NZDT\" \"2016-01-01 01:30:00 NZDT\"\n[5] \"2016-01-01 02:00:00 NZDT\" \"2016-01-01 02:30:00 NZDT\"\n\n\n\n# Are datetime and date_utc the same moments\ntable(akl_hourly$datetime - akl_hourly$date_utc)\n\n\n-82800      0   3600 \n     2  17450      2 \n\n\nLooks like for 17,450 rows datetime and date_utc describe the same moment, but for 4 rows they are different. Can you guess which? Yup, the times where DST kicks in.\n\n\nTimes without dates\n\n# Examine structure of time column\nstr(akl_hourly$time)\n\n 'hms' num [1:17454] 00:00:00 00:30:00 01:00:00 01:30:00 ...\n - attr(*, \"units\")= chr \"secs\"\n\n\n\n# Examine head of time column\nhead(akl_hourly$time)\n\n00:00:00\n00:30:00\n01:00:00\n01:30:00\n02:00:00\n02:30:00\n\n\n\n# A plot using just time\nakl_hourly %>% \n    ggplot(aes(x = time, y = temperature)) +\n    geom_line(aes(group = make_date(year, month, mday)), alpha = 0.2)\n\n\n\n\nUsing time without date is a great way to examine daily patterns."
  },
  {
    "objectID": "posts/2020-07-26-working with dates and times in r.html#more-on-importing-and-exporting-datetimes",
    "href": "posts/2020-07-26-working with dates and times in r.html#more-on-importing-and-exporting-datetimes",
    "title": "Working with Dates and Times in R",
    "section": "More on importing and exporting datetimes",
    "text": "More on importing and exporting datetimes\n\nFast parsing\n\nparse_date_time() can be slow because it’s designed to be forgiving and flexible.\n\n\n\nfastPOSIXct(\"1997-06-16\")\n\n[1] \"1997-06-16 03:00:00 EAT\"\n\n\n\nfast_strptime()\n\n\nyob_str <- \"1997-06-15\"\nparse_date_time(yob_str, order=\"ymd\")\n\n[1] \"1997-06-15 UTC\"\n\n\n\nfast_strptime(yob_str, format = \"%Y-%m-%d\")\n\n[1] \"1997-06-15 UTC\"\n\n\n\nExporting datetimes\n\n\nakl_hourly %>%\n    select(datetime) %>%\n    write_csv(\"datasets/tmp.csv\")\n\n\nFormatting datetimes\n\n\nyob_stamp <- stamp(\"Sunday June 15 1997\")\n\nMultiple formats matched: \"%A %B %d %Y\"(1), \"Sunday %Om %d %Y\"(1), \"Sunday %B %d %Y\"(1), \"%A %Om %d %Y\"(0)\nUsing: \"%A %B %d %Y\"\n\n\n\nyob_stamp(ymd(\"1997-06-15\"))\n\n'Sunday June 15 1997'\n\n\n\nyob_stamp \n\nfunction (x, locale = \"English_United States.1252\") \n{\n    {\n        old_lc_time <- Sys.getlocale(\"LC_TIME\")\n        if (old_lc_time != locale) {\n            on.exit(Sys.setlocale(\"LC_TIME\", old_lc_time))\n            Sys.setlocale(\"LC_TIME\", locale)\n        }\n    }\n    format(x, format = \"%A %B %d %Y\")\n}\n\n\n\nFast parsing with fasttime\nThe fasttime package provides a single function fastPOSIXct(), designed to read in datetimes formatted according to ISO 8601. Because it only reads in one format, and doesn’t have to guess a format, it is really fast!\nWe’ll see how fast by comparing how fast it reads in the dates from the Auckland hourly weather data (over 17,000 dates) to lubridates ymd_hms().\n\n# Examine structure of dates\nstr(akl_hourly$datetime_string)\n\n chr [1:17454] \"2016-01-01T00:00:00\" \"2016-01-01T00:30:00\" ...\n\n\n\n# Use fastPOSIXct() to parse dates\nfastPOSIXct(akl_hourly$datetime_string) %>% str()\n\n POSIXct[1:17454], format: \"2016-01-01 03:00:00\" \"2016-01-01 03:30:00\" \"2016-01-01 04:00:00\" ...\n\n\n\n# Compare speed of fastPOSIXct() to ymd_hms()\nmicrobenchmark(\n  ymd_hms = ymd_hms(akl_hourly$datetime_string),\n  fasttime = fastPOSIXct(akl_hourly$datetime_string),\n  times = 20)\n\n\n\nexprtime\n\n    fasttime 2455100\n    ymd_hms 30602000\n    fasttime 2577700\n    ymd_hms 31090200\n    fasttime 2762600\n    fasttime 2779400\n    fasttime 2796400\n    fasttime 2786900\n    ymd_hms 35160800\n    fasttime 2590900\n    ymd_hms 32037000\n    ymd_hms 31251200\n    ymd_hms 32154400\n    fasttime 2638200\n    ymd_hms 32626700\n    ymd_hms 31518000\n    fasttime 2517700\n    ymd_hms 32616500\n    fasttime 2396500\n    ymd_hms 33101100\n    fasttime 2503900\n    fasttime 2429800\n    ymd_hms 38136000\n    ymd_hms 28758100\n    fasttime 2131100\n    ymd_hms 26896600\n    fasttime 2219700\n    ymd_hms 26722900\n    ymd_hms 26774200\n    ymd_hms 28291400\n    ymd_hms 35462800\n    ymd_hms 35749400\n    fasttime 2591500\n    fasttime 2532400\n    ymd_hms 32089700\n    fasttime 2776700\n    fasttime 2697700\n    fasttime 2550500\n    ymd_hms 33218200\n    fasttime 2475800\n\n\n\n\nfasttime is about 20 times faster than ymd_hms().\n\n\nFast parsing with lubridate::fast_strptime\nlubridate provides its own fast datetime parser: fast_strptime(). Instead of taking an order argument like parse_date_time() it takes a format argument and the format must comply with the strptime() style.\n\n# Head of dates\nhead(akl_hourly$datetime_string)\n\n\n    '2016-01-01T00:00:00'\n    '2016-01-01T00:30:00'\n    '2016-01-01T01:00:00'\n    '2016-01-01T01:30:00'\n    '2016-01-01T02:00:00'\n    '2016-01-01T02:30:00'\n\n\n\n\n# Parse dates with fast_strptime\nfast_strptime(akl_hourly$datetime_string, \n    format = \"%Y-%m-%dT%H:%M:%S\") %>% str()\n\n POSIXlt[1:17454], format: \"2016-01-01 00:00:00\" \"2016-01-01 00:30:00\" \"2016-01-01 01:00:00\" ...\n\n\n\n# Comparse speed to ymd_hms() and fasttime\nmicrobenchmark(\n  ymd_hms = ymd_hms(akl_hourly$datetime_string),\n  fasttime = fastPOSIXct(akl_hourly$datetime_string),\n  fast_strptime = fast_strptime(akl_hourly$datetime_string, \n    format = \"%Y-%m-%dT%H:%M:%S\"),\n  times = 20)\n\n\n\nexprtime\n\n    ymd_hms      42620600     \n    ymd_hms      46698300     \n    fast_strptime 3006900     \n    ymd_hms      29005100     \n    ymd_hms      28695300     \n    fasttime      2406000     \n    fasttime      2340100     \n    fasttime      2312100     \n    fasttime      2310500     \n    ymd_hms      31183400     \n    fast_strptime 3118800     \n    fasttime      2325100     \n    ymd_hms      29844300     \n    fast_strptime 3363600     \n    fast_strptime 3071200     \n    ymd_hms      30061000     \n    fasttime      2251400     \n    ymd_hms      36800700     \n    fast_strptime 3555000     \n    ymd_hms      34068400     \n    fasttime      2512300     \n    fast_strptime 3111700     \n    fast_strptime 3095400     \n    fasttime      2264900     \n    fast_strptime 3090500     \n    ymd_hms      36769000     \n    ymd_hms      27374000     \n    fast_strptime 2777400     \n    ymd_hms      27985100     \n    fasttime      2200400     \n    fasttime      1994100     \n    fast_strptime 2602500     \n    fast_strptime 2617500     \n    ymd_hms      26029600     \n    ymd_hms      26985200     \n    fast_strptime 3005900     \n    fasttime      2201400     \n    fasttime      2131400     \n    ymd_hms      29039200     \n    fasttime      2134500     \n    fasttime      2064700     \n    fast_strptime 2821100     \n    fasttime      2143200     \n    fasttime      2044300     \n    ymd_hms      29941500     \n    fast_strptime 3029800     \n    ymd_hms      41443300     \n    fasttime      2978200     \n    fasttime      3032300     \n    fast_strptime 4292500     \n    fast_strptime 3543800     \n    ymd_hms      35316000     \n    ymd_hms      37035500     \n    fast_strptime 3009300     \n    ymd_hms      27633800     \n    fasttime      2117500     \n    fast_strptime 2832600     \n    fasttime      2035900     \n    fast_strptime 2714100     \n    fast_strptime 2799400     \n\n\n\n\n\n\nOutputting pretty dates and times\nAn easy way to output dates is to use the stamp() function in lubridate. stamp() takes a string which should be an example of how the date should be formatted, and returns a function that can be used to format dates.\n\n# Create a stamp based on \"Saturday, Jan 1, 2000\"\ndate_stamp <- stamp(\"Saturday, Jan 1, 2000\")\n\n# Print date_stamp\ndate_stamp\n\nMultiple formats matched: \"%A, %b %d, %Y\"(1), \"Saturday, Jan %Om, %Y\"(1), \"Saturday, %Om %d, %Y\"(1), \"Saturday, %b %d, %Y\"(1), \"Saturday, Jan %m, %Y\"(1), \"%A, Jan %Om, %Y\"(0), \"%A, %Om %d, %Y\"(0), \"%A, Jan %m, %Y\"(0)\nUsing: \"%A, %b %d, %Y\"\n\n\nfunction (x, locale = \"English_United States.1252\") \n{\n    {\n        old_lc_time <- Sys.getlocale(\"LC_TIME\")\n        if (old_lc_time != locale) {\n            on.exit(Sys.setlocale(\"LC_TIME\", old_lc_time))\n            Sys.setlocale(\"LC_TIME\", locale)\n        }\n    }\n    format(x, format = \"%A, %b %d, %Y\")\n}\n\n\n\n# Call date_stamp on today()\ndate_stamp(today())\n\n'Tuesday, Jul 14, 2020'\n\n\n\n# Create and call a stamp based on \"12/31/1999\"\nstamp(\"12/31/1999\")(today())\n\nMultiple formats matched: \"%Om/%d/%Y\"(1), \"%m/%d/%Y\"(1)\nUsing: \"%Om/%d/%Y\"\n\n\n'07/14/2020'\n\n\n\nfinished = \"I finished 'Dates and Times in R' on Thursday, September 4, 2017!\"\n\n\n# Use string finished for stamp()\nstamp(finished)(today())\n\nMultiple formats matched: \"I finished 'Dates and Times in R' on %A, %B %d, %Y!\"(1), \"I finished 'Dates and Times in R' on Thursday, September %Om, %Y!\"(1), \"I finished 'Dates and Times in R' on Thursday, %Om %d, %Y!\"(1), \"I finished 'Dates and Times in R' on Thursday, %B %d, %Y!\"(1), \"I finished 'Dates and Times in R' on Thursday, September %m, %Y!\"(1), \"I finished 'Dates and Times in R' on %A, September %Om, %Y!\"(0), \"I finished 'Dates and Times in R' on %A, %Om %d, %Y!\"(0), \"I finished 'Dates and Times in R' on %A, September %m, %Y!\"(0)\nUsing: \"I finished 'Dates and Times in R' on %A, %B %d, %Y!\"\n\n\n'I finished \\'Dates and Times in R\\' on Tuesday, July 14, 2020!'"
  },
  {
    "objectID": "posts/datasets/Wikipedia_articles/preprocessing.html",
    "href": "posts/datasets/Wikipedia_articles/preprocessing.html",
    "title": "blog",
    "section": "",
    "text": "To preprocess wikipedia-vectors.csv into the format in which you used it in the exercises, you have to take its transpose:\n\nimport pandas as pd\nfrom scipy.sparse import csr_matrix\n\ndf = pd.read_csv('wikipedia-vectors.csv', index_col=0)\narticles = csr_matrix(df.transpose())\ntitles = list(df.columns)\n\nThe reason for taking this transpose is that without it, there would be 13,000 columns (corresponding to the 13,000 words in the file), which is a lot of columns for a CSV to have."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-08-30-analyzing hacker news dataset.html",
    "href": "posts/2020-08-30-analyzing hacker news dataset.html",
    "title": "Analyzing Hacker News Dataset",
    "section": "",
    "text": "import pandas as pd\nimport re"
  },
  {
    "objectID": "posts/2020-08-30-analyzing hacker news dataset.html#dataset-shape",
    "href": "posts/2020-08-30-analyzing hacker news dataset.html#dataset-shape",
    "title": "Analyzing Hacker News Dataset",
    "section": "Dataset Shape",
    "text": "Dataset Shape\n\nhn.shape\n\n(20099, 7)\n\n\n\nhn.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20099 entries, 0 to 20098\nData columns (total 7 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   id            20099 non-null  int64 \n 1   title         20099 non-null  object\n 2   url           17659 non-null  object\n 3   num_points    20099 non-null  int64 \n 4   num_comments  20099 non-null  int64 \n 5   author        20099 non-null  object\n 6   created_at    20099 non-null  object\ndtypes: int64(3), object(4)\nmemory usage: 1.1+ MB\n\n\n\nhn.isnull().sum()\n\nid                 0\ntitle              0\nurl             2440\nnum_points         0\nnum_comments       0\nauthor             0\ncreated_at         0\ndtype: int64\n\n\n\nhn.describe().T\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      id\n      20099.0\n      1.131755e+07\n      696453.087424\n      10176908.0\n      10701720.0\n      11284523.0\n      11926127.0\n      12578975.0\n    \n    \n      num_points\n      20099.0\n      5.029663e+01\n      107.110322\n      1.0\n      3.0\n      9.0\n      54.0\n      2553.0\n    \n    \n      num_comments\n      20099.0\n      2.480303e+01\n      56.108639\n      1.0\n      1.0\n      3.0\n      21.0\n      1733.0"
  },
  {
    "objectID": "posts/2020-10-19-2016 olympic games.html",
    "href": "posts/2020-10-19-2016 olympic games.html",
    "title": "Weight",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nThe distribution of weights of medalists in gymnastics and in rowing in the 2016 Olympic games for a comparison between them\n\nmens_rowing = pd.read_csv('../data/mens-rowing.csv')\nmens_gymnastics = pd.read_csv('../data/mens_gymnastics.csv')\n\n\nmens_rowing.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      158\n      62\n      Giovanni Abagnale\n      M\n      21.0\n      198.0\n      90.0\n      Italy\n      ITA\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Rowing\n      Rowing Men's Coxless Pairs\n      Bronze\n    \n    \n      1\n      11648\n      6346\n      Jrmie Azou\n      M\n      27.0\n      178.0\n      71.0\n      France\n      FRA\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Rowing\n      Rowing Men's Lightweight Double Sculls\n      Gold\n    \n    \n      2\n      14871\n      8025\n      Thomas Gabriel Jrmie Baroukh\n      M\n      28.0\n      183.0\n      70.0\n      France\n      FRA\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Rowing\n      Rowing Men's Lightweight Coxless Fours\n      Bronze\n    \n    \n      3\n      15215\n      8214\n      Jacob Jepsen Barse\n      M\n      27.0\n      188.0\n      73.0\n      Denmark\n      DEN\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Rowing\n      Rowing Men's Lightweight Coxless Fours\n      Silver\n    \n    \n      4\n      18441\n      9764\n      Alexander Belonogoff\n      M\n      26.0\n      187.0\n      90.0\n      Australia\n      AUS\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Rowing\n      Rowing Men's Quadruple Sculls\n      Silver\n    \n  \n\n\n\n\n\nmens_gymnastics.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      794\n      455\n      Denis Mikhaylovich Ablyazin\n      M\n      24.0\n      161.0\n      62.0\n      Russia\n      RUS\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Gymnastics\n      Gymnastics Men's Team All-Around\n      Silver\n    \n    \n      1\n      796\n      455\n      Denis Mikhaylovich Ablyazin\n      M\n      24.0\n      161.0\n      62.0\n      Russia\n      RUS\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Gymnastics\n      Gymnastics Men's Horse Vault\n      Silver\n    \n    \n      2\n      797\n      455\n      Denis Mikhaylovich Ablyazin\n      M\n      24.0\n      161.0\n      62.0\n      Russia\n      RUS\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Gymnastics\n      Gymnastics Men's Rings\n      Bronze\n    \n    \n      3\n      18577\n      9829\n      David Sagitovich Belyavsky\n      M\n      24.0\n      165.0\n      55.0\n      Russia\n      RUS\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Gymnastics\n      Gymnastics Men's Team All-Around\n      Silver\n    \n    \n      4\n      18579\n      9829\n      David Sagitovich Belyavsky\n      M\n      24.0\n      165.0\n      55.0\n      Russia\n      RUS\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Gymnastics\n      Gymnastics Men's Parallel Bars\n      Bronze\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots()\nax.hist(mens_rowing.Weight, histtype='step', bins=5, label='Rowing')\nax.hist(mens_gymnastics.Weight, histtype='step', bins=5, label='Gymnastics')\nax.set_xlabel('Weight (kg)')\nax.set_ylabel('# of observations')\nax.legend()\nplt.show()\n\n\n\n\nSelecting groups from the Summer 2016 Olympic Games medalist dataset to compare the height of medalist athletes in two different sports.\n\nAdding error bars\n\nfig, ax = plt.subplots()\nax.bar('Rowing', mens_rowing.Height.mean(), yerr=mens_rowing.Height.std())\nax.bar('Gymnastics', mens_gymnastics.Height.mean(), yerr=mens_gymnastics.Height.std())\nax.set_ylabel(\"Height (cm)\")\n\nplt.show()\n\n\n\n\n\n\nBoxplot\n\nfig, ax = plt.subplots(figsize=(10,6))\nax.boxplot([mens_rowing.Height, mens_gymnastics.Height])\nax.set_xticklabels(['Rowing', 'Gymnastics'])\nax.set_ylabel('Height (cm)')\nplt.show()\n\n\n\n\nwe can see how many individuals are outliers within their group.\n\nsummer_2016_medals = pd.read_csv('../data/summer_2016_medals.csv')\nsummer_2016_medals.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      158\n      Giovanni Abagnale\n      M\n      21.0\n      198.0\n      90.0\n      Italy\n      ITA\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Rowing\n      Rowing Men's Coxless Pairs\n      Bronze\n    \n    \n      1\n      161\n      Patimat Abakarova\n      F\n      21.0\n      165.0\n      49.0\n      Azerbaijan\n      AZE\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Taekwondo\n      Taekwondo Women's Flyweight\n      Bronze\n    \n    \n      2\n      175\n      Luc Abalo\n      M\n      31.0\n      182.0\n      86.0\n      France\n      FRA\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Handball\n      Handball Men's Handball\n      Silver\n    \n    \n      3\n      450\n      Saeid Morad Abdevali\n      M\n      26.0\n      170.0\n      80.0\n      Iran\n      IRI\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Wrestling\n      Wrestling Men's Middleweight, Greco-Roman\n      Bronze\n    \n    \n      4\n      794\n      Denis Mikhaylovich Ablyazin\n      M\n      24.0\n      161.0\n      62.0\n      Russia\n      RUS\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Gymnastics\n      Gymnastics Men's Team All-Around\n      Silver\n    \n  \n\n\n\n\n\nsports = summer_2016_medals.Sport.unique()\nsports\n\narray(['Rowing', 'Taekwondo', 'Handball', 'Wrestling', 'Gymnastics',\n       'Swimming', 'Basketball', 'Boxing', 'Volleyball', 'Athletics'],\n      dtype=object)\n\n\n\nfig, ax = plt.subplots(figsize=(14,5))\n\n# Loop over the different sports branches\nfor sport in sports:\n  # Extract the rows only for this sport\n  sport_df = summer_2016_medals[summer_2016_medals['Sport']==sport]\n  # Add a bar for the \"Weight\" mean with std y error bar\n  ax.bar(sport,sport_df['Weight'].mean(), yerr=sport_df['Weight'].std())\n\nax.set_ylabel(\"Weight\")\nax.set_xticklabels(sports, rotation=90)\n\n# Save the figure to file\nfig.savefig('../data/sports_weights.png')\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html",
    "href": "posts/2020-07-26-writing functions in python.html",
    "title": "Writing Functions in Python",
    "section": "",
    "text": "Best practices when writing functions. We’ll cover docstrings and why they matter and how to know when you need to turn a chunk of code into a function. We will also code how Python passes arguments to functions, as well as some common gotchas that can cause debugging headaches when calling functions.\n\n\n\n\nThe first function is count_letter(). It takes a string and a single letter and returns the number of times the letter appears in the string. We want the users of our open-source package to be able to understand how this function works easily, so we will need to give it a docstring.\n\nimport inspect\nimport pandas as pd\nimport numpy as np\nimport time\nimport contextlib\nimport os\n\n\ndef count_letter(content, letter):\n    \"\"\"Count the number of times `letter` appears in `content`.\n    \n    Args:\n        content (str): The string to search.\n        letter (str): The letter to search for.\n        \n    Returns:\n        int\n    Raises:\n        ValueError: If `letter` is not a one-character string.\n    \"\"\"\n    if (not isinstance(letter, str)) or len(letter) != 1:\n        raise ValueError('`letter` must be a single character string.')\n    return len([char for char in content if char == letter])\n\n\nhelp(count_letter)\n\nHelp on function count_letter in module __main__:\n\ncount_letter(content, letter)\n    Count the number of times `letter` appears in `content`.\n    \n    Args:\n        content (str): The string to search.\n        letter (str): The letter to search for.\n        \n    Returns:\n        int\n    Raises:\n        ValueError: If `letter` is not a one-character string.\n\n\n\n\n\n\nMy friends and I are working on building an amazing new Python IDE (integrated development environment – like PyCharm, Spyder, Eclipse, Visual Studio, etc.). Our team wants to add a feature that displays a tooltip with a function’s docstring whenever the user starts typing the function name. That way, the user doesn’t have to go elsewhere to look up the documentation for the function they are trying to use. We’ve been asked to complete the build_tooltip() function that retrieves a docstring from an arbitrary function.\n\n# Get the docstring with an attribute of count_letter()\ndocstring = count_letter.__doc__\n\nborder = '#' * 28\nprint('{}\\n{}\\n{}'.format(border, docstring, border))\n\n############################\nCount the number of times `letter` appears in `content`.\n    \n    Args:\n        content (str): The string to search.\n        letter (str): The letter to search for.\n        \n    Returns:\n        int\n    Raises:\n        ValueError: If `letter` is not a one-character string.\n    \n############################\n\n\n\n# Get the docstring with a function from the inspect module\ndocstring = inspect.getdoc(count_letter)\n\nborder = '#' * 28\nprint('{}\\n{}\\n{}'.format(border, docstring, border))\n\n############################\nCount the number of times `letter` appears in `content`.\n\nArgs:\n    content (str): The string to search.\n    letter (str): The letter to search for.\n    \nReturns:\n    int\nRaises:\n    ValueError: If `letter` is not a one-character string.\n############################\n\n\n\ndef build_tooltip(function):\n    \"\"\"Create a tooltip for any function that shows the function's docstring.\n    \n    Args:\n        function (callable): The function we want a tooltip for.\n    \n    Returns:\n        str\n    \"\"\"\n    # Use 'inspect' to get the docstring\n    docstring = inspect.getdoc(function)\n    border = '#' * 28\n    \n    return '{}\\n{}\\n{}'.format(border, docstring, border)\n\nprint(build_tooltip(count_letter))\nprint(build_tooltip(range))\nprint(build_tooltip(print))\n\n############################\nCount the number of times `letter` appears in `content`.\n\nArgs:\n    content (str): The string to search.\n    letter (str): The letter to search for.\n    \nReturns:\n    int\nRaises:\n    ValueError: If `letter` is not a one-character string.\n############################\n############################\nrange(stop) -> range object\nrange(start, stop[, step]) -> range object\n\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\n############################\n############################\nprint(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n\nPrints the values to a stream, or to sys.stdout by default.\nOptional keyword arguments:\nfile:  a file-like object (stream); defaults to the current sys.stdout.\nsep:   string inserted between values, default a space.\nend:   string appended after the last value, default a newline.\nflush: whether to forcibly flush the stream.\n############################\n\n\n\n\n\n\nWhile we were developing a model to predict the likelihood of a student graduating from college, we wrote this bit of code to get the z-scores of students’ yearly GPAs. Now we’re ready to turn it into a production-quality system, so we need to do something about the repetition. Writing a function to calculate the z-scores would improve this code.\n# Standardize the GPAs for each year\ndf['y1_z'] = (df.y1_gpa - df.y1_gpa.mean()) / df.y1_gpa.std()\ndf['y2_z'] = (df.y2_gpa - df.y2_gpa.mean()) / df.y2_gpa.std()\ndf['y3_z'] = (df.y3_gpa - df.y3_gpa.mean()) / df.y3_gpa.std()\ndf['y4_z'] = (df.y4_gpa - df.y4_gpa.mean()) / df.y4_gpa.std()\n\nNote: df is a pandas DataFrame where each row is a student with 4 columns of yearly student GPAs: y1_gpa, y2_gpa, y3_gpa, y4_gpa\n\n\ndf = pd.read_csv('students.csv', index_col=0)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      y1_gpa\n      y2_gpa\n      y3_gpa\n      y4_gpa\n    \n  \n  \n    \n      0\n      2.785877\n      2.052513\n      2.170544\n      0.065570\n    \n    \n      1\n      1.144557\n      2.666498\n      0.267098\n      2.884737\n    \n    \n      2\n      0.907406\n      0.423634\n      2.613459\n      0.030950\n    \n    \n      3\n      2.205259\n      0.523580\n      3.984345\n      0.339289\n    \n    \n      4\n      2.877876\n      1.287922\n      3.077589\n      0.901994\n    \n  \n\n\n\n\n\ndef standardize(column):\n    \"\"\"Standardize the values in a column.\n    \n    Args:\n        column (pandas Series): The data to standardize.\n        \n    Returns:\n        pandas Series: the values as z-scores\n    \"\"\"\n    # Finish the function so that it returns the z-scores\n    z_score = (df[column] - df[column].mean()) / df[column].std()\n    \n    return z_score\n\n# Use the standardize() function to calculate the z-scores\ndf['y1_z'] = standardize(\"y1_gpa\")\ndf['y2_z'] = standardize(\"y2_gpa\")\ndf['y3_z'] = standardize(\"y3_gpa\")\ndf['y4_z'] = standardize(\"y4_gpa\")\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      y1_gpa\n      y2_gpa\n      y3_gpa\n      y4_gpa\n      y1_z\n      y2_z\n      y3_z\n      y4_z\n    \n  \n  \n    \n      0\n      2.785877\n      2.052513\n      2.170544\n      0.065570\n      0.790863\n      0.028021\n      0.172322\n      -1.711179\n    \n    \n      1\n      1.144557\n      2.666498\n      0.267098\n      2.884737\n      -0.872971\n      0.564636\n      -1.347122\n      0.824431\n    \n    \n      2\n      0.907406\n      0.423634\n      2.613459\n      0.030950\n      -1.113376\n      -1.395595\n      0.525883\n      -1.742317\n    \n    \n      3\n      2.205259\n      0.523580\n      3.984345\n      0.339289\n      0.202281\n      -1.308243\n      1.620206\n      -1.464991\n    \n    \n      4\n      2.877876\n      1.287922\n      3.077589\n      0.901994\n      0.884124\n      -0.640219\n      0.896379\n      -0.958885\n    \n  \n\n\n\n\nstandardize() will probably be useful in other places in our code, and now it is easy to use, test, and update if we need to. It’s also easier to tell what the code is doing because of the docstring and the name of the function.\n\n\nAnother engineer on our team has written this function to calculate the mean and median of a list. We want to show them how to split it into two simpler functions: mean() and median()\ndef mean_and_median(values):\n  \"\"\"Get the mean and median of a list of `values`\n\n  Args:\n    values (iterable of float): A list of numbers\n\n  Returns:\n    tuple (float, float): The mean and median\n  \"\"\"\n  mean = sum(values) / len(values)\n  midpoint = int(len(values) / 2)\n  if len(values) % 2 == 0:\n    median = (values[midpoint - 1] + values[midpoint]) / 2\n  else:\n    median = values[midpoint]\n\n  return mean, median\n\ndef mean(values):\n    \"\"\"Get the mean of a list of values\n\n    Args:\n        values (iterable of float): A list of numbers\n\n    Returns:\n        float\n    \"\"\"\n    # Write the mean() function\n    mean = sum(values) / len(values)\n    \n    return mean\n\n\ndef median(values):\n    \"\"\"Get the median of a list of values\n\n    Args:\n        values (iterable of float): A list of numbers\n\n    Returns:\n        float\n    \"\"\"\n    # Write the median() function\n    midpoint = int(len(values) /2)\n    if len(values) % 2 == 0:\n        median = (values[midpoint-1] + values[midpoint]) / 2\n    else:\n        median = values[midpoint]\n    return median\n\nEach function does one thing and does it well. Using, testing, and maintaining these will be a breeze (although we’ll probably just use numpy.mean() and numpy.median() for this in real life).\n\n\n\n\n\n\nOne of our co-workers has written this function for adding a column to a panda’s DataFrame. Unfortunately, they used a mutable variable as a default argument value!\ndef add_column(values, df=pandas.DataFrame()):\n  \"\"\"Add a column of `values` to a DataFrame `df`.\n  The column will be named \"col_<n>\" where \"n\" is\n  the numerical index of the column.\n\n  Args:\n    values (iterable): The values of the new column\n    df (DataFrame, optional): The DataFrame to update.\n      If no DataFrame is passed, one is created by default.\n\n  Returns:\n    DataFrame\n  \"\"\"\n  df['col_{}'.format(len(df.columns))] = values\n  return df\n\n# Use an immutable variable for the default argument \ndef better_add_column(values, df=None):\n    \"\"\"Add a column of `values` to a DataFrame `df`. The column will be named \"col_<n>\" where \"n\" is the numerical index of the column.\n\n    Args:\n        values (iterable): The values of the new column\n        df (DataFrame, optional): The DataFrame to update.\n          If no DataFrame is passed, one is created by default.\n\n    Returns:\n        DataFrame\n    \"\"\"\n    # Update the function to create a default DataFrame\n    if df is None:\n        df = pandas.DataFrame()\n    df['col_{}'.format(len(df.columns))] = values\n    return df\n\n\nWhen you need to set a mutable variable as a default argument, always use None and then set the value in the body of the function. This prevents unexpected behavior like adding multiple columns if you call the function more than once."
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#using-context-managers",
    "href": "posts/2020-07-26-writing functions in python.html#using-context-managers",
    "title": "Writing Functions in Python",
    "section": "Using context managers",
    "text": "Using context managers\nWe are working on a natural language processing project to determine what makes great writers so great. Our current hypothesis is that great writers talk about cats a lot. To prove it, we will count the number of times the word \"cat\" appears in “Alice’s Adventures in Wonderland” by Lewis Carroll. We have already downloaded a text file, alice.txt, with the entire contents of this great book.\n\n# Open \"alice.txt\" and assign the file to \"file\"\nwith open('alice.txt', encoding=\"utf8\") as file:\n    text = file.read()\n\nn = 0\nfor word in text.split():\n    if word.lower() in ['cat', 'cats']:\n        n += 1\n\nprint('Lewis Carroll uses the word \"cat\" {} times'.format(n))\n\nLewis Carroll uses the word \"cat\" 24 times\n\n\n\nThe speed of cats\nWe’re working on a new web service that processes Instagram feeds to identify which pictures contain cats (don’t ask why – it’s the internet). The code that processes the data is slower than we would like it to be, so we are working on tuning it up to run faster. Given an image, image, we have two functions that can process it:\n\nprocess_with_numpy(image)\nprocess_with_pytorch(image)\n\nOur colleague wrote a context manager, timer(), that will print out how long the code inside the context block takes to run. She is suggesting we use it to see which of the two options is faster. Time each function to determine which one to use in your web service.\n\ndef get_image_from_instagram():\n    return np.random.rand(84, 84)"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#writing-context-managers",
    "href": "posts/2020-07-26-writing functions in python.html#writing-context-managers",
    "title": "Writing Functions in Python",
    "section": "Writing context managers",
    "text": "Writing context managers\n\nThe timer() context manager\nA colleague of ours is working on a web service that processes Instagram photos. Customers are complaining that the service takes too long to identify whether or not an image has a cat in it, so our colleague has come to us for help. We decided to write a context manager that they can use to time how long their functions take to run.\n\n@contextlib.contextmanager\ndef timer():\n    \"\"\"Time how long code in the context block takes to run.\"\"\"\n    t0 = time.time()\n    try:\n        yield\n    except:\n        raise\n    finally:\n        t1 = time.time()\n        print('Elapsed: {:.2f} seconds'.format(t1 - t0))\n\nour colleague can now use our timer() context manager to figure out which of their functions is running too slow. The three elements of a context manager are all here: * a function definition, * a yield statement, and the * @contextlib.contextmanager decorator. timer() is a context manager that does not return an explicit value, so yield is written by itself without specifying anything to return.\n\ndef process_with_numpy(p):\n    _process_pic(0.1521)\n\n\ndef _process_pic(n_sec):\n    print('Processing', end='', flush=True)\n    for i in range(10):\n        print('.', end='' if i < 9 else 'done!\\n', flush=True)\n        time.sleep(n_sec)\n\n\ndef process_with_pytorch(p):\n    _process_pic(0.0328)\n\n\nimage = get_image_from_instagram()\n\n# Time how long process_with_numpy(image) takes to run\nwith timer():\n    print('Numpy version')\n    process_with_numpy(image)\n\n# Time how long process_with_pytorch(image) takes to run\nwith timer():\n    print('Pytorch version')\n    process_with_pytorch(image)\n\nNumpy version\nProcessing..........done!\nElapsed: 1.66 seconds\nPytorch version\nProcessing..........done!\nElapsed: 0.49 seconds\n\n\nNow that we know the pytorch version is faster, we can use it in our web service to ensure our users get the rapid response time they expect.\n\nThere is no as <variable name> at the end of the with statement in timer() context manager. That is because timer() is a context manager that does not return a value, so the as <variable name> at the end of the with statement isn’t necessary.\n\n\n\nA read-only open() context manager\nWe have a bunch of data files for our next deep learning project that took us months to collect and clean. It would be terrible if we accidentally overwrote one of those files when trying to read it in for training, so we decided to create a read-only version of the open() context manager to use in our project.\nThe regular open() context manager:\n\ntakes a filename and a mode ('r' for read, 'w' for write, or 'a' for append)\nopens the file for reading, writing, or appending\nsends control back to the context, along with a reference to the file\nwaits for the context to finish\nand then closes the file before exiting\n\nOur context manager will do the same thing, except it will only take the filename as an argument and it will only open the file for reading.\n\n@contextlib.contextmanager\ndef open_read_only(filename):\n    \"\"\"Open a file in read-only mode.\n\n    Args:\n        filename (str): The location of the file to read\n\n    Yields:\n    file object\n    \"\"\"\n    read_only_file = open(filename, mode='r')\n    # Yield read_only_file so it can be assigned to my_file\n    yield read_only_file\n    # Close read_only_file\n    read_only_file.close()\n\nwith open_read_only('my_file.txt') as my_file:\n    print(my_file.read())\n\n    Congratulations! You wrote a context manager that acts like \"open()\" but operates in read-only mode!"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#functions-are-objects",
    "href": "posts/2020-07-26-writing functions in python.html#functions-are-objects",
    "title": "Writing Functions in Python",
    "section": "Functions are objects",
    "text": "Functions are objects\n\nBuilding a command line data app\nWe are building a command line tool that lets a user interactively explore a data set. We’ve defined four functions: mean(), std(), minimum(), and maximum() that users can call to analyze their data. users can call any of these functions by typing the function name at the input prompt.\n\nNote: The function get_user_input() in this exercise is a mock version of asking the user to enter a command. It randomly returns one of the four function names. In real life, we would ask for input and wait until the user entered a value.\n\n\nimport random\ndef get_user_input(prompt='Type a command: '):\n    command = random.choice(['mean', 'std', 'minimum', 'maximum'])\n    print(prompt)\n    print('> {}'.format(command))\n    return command\n\n\ndef mean(data):\n    print(data.mean())\n\n\ndef std(data):\n    print(data.std())\n\n\ndef minimum(data):\n    print(data.min())\n\n\ndef maximum(data):\n    print(data.max())\n\n\ndef load_data():\n    df = pd.DataFrame()\n    df['height'] = [72.1, 69.8, 63.2, 64.7]\n    df['weight'] = [198, 204, 164, 238]\n    return df\n\n\n# Add the missing function references to the function map\nfunction_map = {\n  'mean': mean,\n  'std': std,\n  'minimum': minimum,\n  'maximum': maximum\n}\n\ndata = load_data()\nprint(data)\n\nfunc_name = get_user_input()\n\n# Call the chosen function and pass \"data\" as an argument\nfunction_map[func_name](data)\n\n   height  weight\n0    72.1     198\n1    69.8     204\n2    63.2     164\n3    64.7     238\nType a command: \n> std\nheight     4.194043\nweight    30.309514\ndtype: float64\n\n\n\n\nReviewing our co-worker’s code\nOur co-worker is asking us to review some code that they’ve written and give them some tips on how to get it ready for production. We know that having a docstring is considered best practice for maintainable, reusable functions, so as a sanity check we decided to run this has_docstring() function on all of their functions.\n\ndef has_docstring(func):\n    \"\"\"Check to see if the function \n    `func` has a docstring.\n\n    Args:\n        func (callable): A function.\n\n    Returns:\n        bool\n    \"\"\"\n    return func.__doc__ is not None\n\n\ndef load_and_plot_data(filename):\n    \"\"\"Load a data frame and plot each column.\n  \n    Args:\n        filename (str): Path to a CSV file of data.\n  \n    Returns:\n        pandas.DataFrame\n    \"\"\"\n    df = pd.load_csv(filename, index_col=0)\n    df.hist()\n    return df\n\n\n# Call has_docstring() on the load_and_plot_data() function\nok = has_docstring(load_and_plot_data)\n\nif not ok:\n    print(\"load_and_plot_data() doesn't have a docstring!\")\nelse:\n    print(\"load_and_plot_data() looks ok\")\n\nload_and_plot_data() looks ok\n\n\n\ndef as_2D(arr):\n    \"\"\"Reshape an array to 2 dimensions\"\"\"\n    return np.array(arr).reshape(1, -1)\n\n\n# Call has_docstring() on the as_2D() function\nok = has_docstring(as_2D)\n\nif not ok:\n    print(\"as_2D() doesn't have a docstring!\")\nelse:\n    print(\"as_2D() looks ok\")\n\nas_2D() looks ok\n\n\n\ndef log_product(arr):\n    return np.exp(np.sum(np.log(arr)))\n\n\n# Call has_docstring() on the log_product() function\nok = has_docstring(log_product)\n\nif not ok:\n    print(\"log_product() doesn't have a docstring!\")\nelse:\n    print(\"log_product() looks ok\")\n\nlog_product() doesn't have a docstring!\n\n\n\n\nReturning functions for a math game\nWe are building an educational math game where the player enters a math term, and our program returns a function that matches that term. For instance, if the user types “add”, our program returns a function that adds two numbers. So far we’ve only implemented the “add” function. Now we want to include a “subtract” function.\n\ndef create_math_function(func_name):\n    if func_name == 'add':\n        def add(a, b):\n            return a + b\n        return add\n    elif func_name == 'subtract':\n        # Define the subtract() function\n        def subtract(a,b):\n            return a-b\n        return subtract\n    else:\n        print(\"I don't know that one\")\n    \nadd = create_math_function('add')\nprint('5 + 2 = {}'.format(add(5, 2)))\n\nsubtract = create_math_function('subtract')\nprint('5 - 2 = {}'.format(subtract(5, 2)))\n\n5 + 2 = 7\n5 - 2 = 3"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#scope",
    "href": "posts/2020-07-26-writing functions in python.html#scope",
    "title": "Writing Functions in Python",
    "section": "Scope",
    "text": "Scope\n\nModifying variables outside local scope\n\ncall_count = 0\n\ndef my_function():\n    # Use a keyword that lets us update call_count \n    global call_count\n    call_count += 1\n  \n    print(\"You've called my_function() {} times!\".format(\n        call_count\n    ))\n  \nfor _ in range(20):\n    my_function()\n\nYou've called my_function() 1 times!\nYou've called my_function() 2 times!\nYou've called my_function() 3 times!\nYou've called my_function() 4 times!\nYou've called my_function() 5 times!\nYou've called my_function() 6 times!\nYou've called my_function() 7 times!\nYou've called my_function() 8 times!\nYou've called my_function() 9 times!\nYou've called my_function() 10 times!\nYou've called my_function() 11 times!\nYou've called my_function() 12 times!\nYou've called my_function() 13 times!\nYou've called my_function() 14 times!\nYou've called my_function() 15 times!\nYou've called my_function() 16 times!\nYou've called my_function() 17 times!\nYou've called my_function() 18 times!\nYou've called my_function() 19 times!\nYou've called my_function() 20 times!\n\n\n\ndef read_files():\n    file_contents = None\n  \n    def save_contents(filename):\n        # Add a keyword that lets us modify file_contents\n        nonlocal file_contents\n        if file_contents is None:\n            file_contents = []\n        with open(filename) as fin:\n            file_contents.append(fin.read())\n      \n    for filename in ['1984.txt', 'MobyDick.txt', 'CatsEye.txt']:\n        save_contents(filename)\n    \n    return file_contents\n\nprint('\\n'.join(read_files()))\n\nIt was a bright day in April, and the clocks were striking thirteen.\nCall me Ishmael.\nTime is not a line but a dimension, like the dimensions of space.\n\n\n\ndef wait_until_done():\n    def check_is_done():\n        # Add a keyword so that wait_until_done() \n        # doesn't run forever\n        global done\n        if random.random() < 0.1:\n            done = True\n      \n    while not done:\n        check_is_done()\n\ndone = False\nwait_until_done()\n\nprint('Work done? {}'.format(done))\n\nWork done? True"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#closures",
    "href": "posts/2020-07-26-writing functions in python.html#closures",
    "title": "Writing Functions in Python",
    "section": "Closures",
    "text": "Closures\n\nChecking for closures\n\ndef return_a_func(arg1, arg2):\n    def new_func():\n        print('arg1 was {}'.format(arg1))\n        print('arg2 was {}'.format(arg2))\n    return new_func\n    \nmy_func = return_a_func(2, 17)\n\nprint(my_func.__closure__ is not None)\nprint(len(my_func.__closure__) == 2)\n\n# Get the values of the variables in the closure\nclosure_values = [\n  my_func.__closure__[i].cell_contents for i in range(2)\n]\nprint(closure_values == [2, 17])\n\nTrue\nTrue\nTrue\n\n\n\n\nClosures keep your values safe\n\ndef my_special_function():\n    print('You are running my_special_function()')\n\ndef get_new_func(func):\n    def call_func():\n        func()\n    return call_func\n\nnew_func = get_new_func(my_special_function)\n\n# Redefine my_special_function() to just print \"hello\"\ndef my_special_function():\n    print(\"hello\")\n\nnew_func()\n\nYou are running my_special_function()\n\n\n\ndef my_special_function():\n    print('You are running my_special_function()')\n  \ndef get_new_func(func):\n    def call_func():\n        func()\n    return call_func\n\nnew_func = get_new_func(my_special_function)\n\n# Delete my_special_function()\ndel(my_special_function)\n\nnew_func()\n\nYou are running my_special_function()\n\n\n\ndef my_special_function():\n    print('You are running my_special_function()')\n  \ndef get_new_func(func):\n    def call_func():\n        func()\n    return call_func\n\n# Overwrite `my_special_function` with the new function\nmy_special_function = get_new_func(my_special_function)\n\nmy_special_function()\n\nYou are running my_special_function()"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#decorators-1",
    "href": "posts/2020-07-26-writing functions in python.html#decorators-1",
    "title": "Writing Functions in Python",
    "section": "Decorators",
    "text": "Decorators\n\nUsing decorator syntax\nprint_args prints out all of the arguments and their values any time a function that it is decorating gets called.\n\ndef print_args(func):\n    sig = inspect.signature(func)\n    def wrapper(*args, **kwargs):\n        bound = sig.bind(*args, **kwargs).arguments\n        str_args = ', '.join(['{}={}'.format(k, v) for k, v in bound.items()])\n        print('{} was called with {}'.format(func.__name__, str_args))\n        return func(*args, **kwargs)\n    return wrapper\n\n\ndef my_function(a, b, c):\n    print(a + b + c)\n\n# Decorate my_function() with the print_args() decorator\nmy_function = print_args(my_function)\n\nmy_function(1, 2, 3)\n\nmy_function was called with a=1, b=2, c=3\n6\n\n\n\n# Decorate my_function() with the print_args() decorator\n@print_args\ndef my_function(a, b, c):\n  print(a + b + c)\n\nmy_function(1, 2, 3)\n\nmy_function was called with a=1, b=2, c=3\n6\n\n\neven though decorators are functions themselves, when you use decorator syntax with the@ symbol we do not include the parentheses after the decorator name.\n\n\nDefining a decorator\nOur buddy has been working on a decorator that prints a “before” message before the decorated function is called and prints an “after” message after the decorated function is called. They are having trouble remembering how wrapping the decorated function is supposed to work.\n\ndef print_before_and_after(func):\n  def wrapper(*args):\n    print('Before {}'.format(func.__name__))\n    # Call the function being decorated with *args\n    func(*args)\n    print('After {}'.format(func.__name__))\n  # Return the nested function\n  return wrapper\n\n@print_before_and_after\ndef multiply(a, b):\n  print(a * b)\n\nmultiply(5, 10)\n\nBefore multiply\n50\nAfter multiply\n\n\n\n\nPrint the return type\n\ndef print_return_type(func):\n  # Define wrapper(), the decorated function\n  def wrapper(*args, **kwargs):\n    # Call the function being decorated\n    result = func(*args, **kwargs)\n    print('{}() returned type {}'.format(\n      func.__name__, type(result)\n    ))\n    return result\n  # Return the decorated function\n  return wrapper\n  \n@print_return_type\ndef foo(value):\n  return value\n  \nprint(foo(42))\nprint(foo([1, 2, 3]))\nprint(foo({'a': 42}))\n\nfoo() returned type <class 'int'>\n42\nfoo() returned type <class 'list'>\n[1, 2, 3]\nfoo() returned type <class 'dict'>\n{'a': 42}\n\n\n\n\nCounter\nhow many times each of the functions in it gets called in a web app\n\ndef counter(func):\n  def wrapper(*args, **kwargs):\n    wrapper.count += 1\n    # Call the function being decorated and return the result\n    return func(*args, **kwargs)\n  wrapper.count = 0\n  # Return the new decorated function\n  return wrapper\n\n# Decorate foo() with the counter() decorator\n@counter\ndef foo():\n  print('calling foo()')\n  \nfoo()\nfoo()\n\nprint('foo() was called {} times.'.format(foo.count))\n\ncalling foo()\ncalling foo()\nfoo() was called 2 times."
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#decorators-and-metadata",
    "href": "posts/2020-07-26-writing functions in python.html#decorators-and-metadata",
    "title": "Writing Functions in Python",
    "section": "Decorators and metadata",
    "text": "Decorators and metadata\n\nPreserving docstrings when decorating functions\n\ndef add_hello(func):\n  def wrapper(*args, **kwargs):\n    print('Hello')\n    return func(*args, **kwargs)\n  return wrapper\n\n# Decorate print_sum() with the add_hello() decorator\n@add_hello\ndef print_sum(a, b):\n  \"\"\"Adds two numbers and prints the sum\"\"\"\n  print(a + b)\n  \nprint_sum(10, 20)\nprint(print_sum.__doc__)\n\nHello\n30\nNone\n\n\n\ndef add_hello(func):\n  # Add a docstring to wrapper\n  def wrapper(*args, **kwargs):\n    \"\"\"Print 'hello' and then call the decorated function.\"\"\"\n    print('Hello')\n    return func(*args, **kwargs)\n  return wrapper\n\n@add_hello\ndef print_sum(a, b):\n  \"\"\"Adds two numbers and prints the sum\"\"\"\n  print(a + b)\n  \nprint_sum(10, 20)\nprint(print_sum.__doc__)\n\nHello\n30\nPrint 'hello' and then call the decorated function.\n\n\n\nfrom functools import wraps\n\ndef add_hello(func):\n  # Decorate wrapper() so that it keeps func()'s metadata\n  @wraps(func)\n  def wrapper(*args, **kwargs):\n    \"\"\"Print 'hello' and then call the decorated function.\"\"\"\n    print('Hello')\n    return func(*args, **kwargs)\n  return wrapper\n  \n@add_hello\ndef print_sum(a, b):\n  \"\"\"Adds two numbers and prints the sum\"\"\"\n  print(a + b)\n  \nprint_sum(10, 20)\nprint(print_sum.__doc__)\n\nHello\n30\nAdds two numbers and prints the sum\n\n\n\n\nMeasuring decorator overhead\n\ndef check_inputs(a, *args, **kwargs):\n  for value in a:\n    time.sleep(0.01)\n  print('Finished checking inputs')\n\n\ndef check_outputs(a, *args, **kwargs):\n  for value in a:\n    time.sleep(0.01)\n  print('Finished checking outputs')\n\n\ndef check_everything(func):\n  @wraps(func)\n  def wrapper(*args, **kwargs):\n    check_inputs(*args, **kwargs)\n    result = func(*args, **kwargs)\n    check_outputs(result)\n    return result\n  return wrapper\n\n\n@check_everything\ndef duplicate(my_list):\n  \"\"\"Return a new list that repeats the input twice\"\"\"\n  return my_list + my_list\n\nt_start = time.time()\nduplicated_list = duplicate(list(range(50)))\nt_end = time.time()\ndecorated_time = t_end - t_start\n\nt_start = time.time()\n# Call the original function instead of the decorated one\nduplicated_list = duplicate.__wrapped__(list(range(50)))\nt_end = time.time()\nundecorated_time = t_end - t_start\n\nprint('Decorated time: {:.5f}s'.format(decorated_time))\nprint('Undecorated time: {:.5f}s'.format(undecorated_time))\n\nFinished checking inputs\nFinished checking outputs\nDecorated time: 2.40551s\nUndecorated time: 0.00000s"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#decorators-that-take-arguments",
    "href": "posts/2020-07-26-writing functions in python.html#decorators-that-take-arguments",
    "title": "Writing Functions in Python",
    "section": "Decorators that take arguments",
    "text": "Decorators that take arguments\n\nRun_n_times()\n\ndef run_n_times(n):\n  \"\"\"Define and return a decorator\"\"\"\n  def decorator(func):\n    def wrapper(*args, **kwargs):\n      for i in range(n):\n        func(*args, **kwargs)\n    return wrapper\n  return decorator\n\n\n# Make print_sum() run 10 times with the run_n_times() decorator\n@run_n_times(10)\ndef print_sum(a, b):\n  print(a + b)\n  \nprint_sum(15, 20)\n\n35\n35\n35\n35\n35\n35\n35\n35\n35\n35\n\n\n\n# Use run_n_times() to create the run_five_times() decorator\nrun_five_times = run_n_times(5)\n\n@run_five_times\ndef print_sum(a, b):\n  print(a + b)\n  \nprint_sum(4, 100)\n\n104\n104\n104\n104\n104\n\n\n\n# Modify the print() function to always run 20 times\nprint = run_n_times(20)(print)\n\nprint('What is happening?!?!')\n\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\nWhat is happening?!?!\n\n\n\nprint = run_n_times(1)(print)"
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#html-generator",
    "href": "posts/2020-07-26-writing functions in python.html#html-generator",
    "title": "Writing Functions in Python",
    "section": "HTML Generator",
    "text": "HTML Generator\na script that generates HTML for a webpage on the fly.\n\ndef html(open_tag, close_tag):\n  def decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n      msg = func(*args, **kwargs)\n      return '{}{}{}'.format(open_tag, msg, close_tag)\n    # Return the decorated function\n    return wrapper\n  # Return the decorator\n  return decorator\n\n\n# Make hello() return bolded text\n@html(\"<b>\", \"</b>\")\ndef hello(name):\n  return 'Hello {}!'.format(name)\n  \nprint(hello('Alice'))\n\n<b>Hello Alice!</b>\n\n\n\n%%html\n\n<b>Hello Alice!</b>\n\n\nHello Alice!\n\n\n\n# Make goodbye() return italicized text\n@html(\"<i>\", \"</i>\")\ndef goodbye(name):\n  return 'Goodbye {}.'.format(name)\n  \nprint(goodbye('Alice'))\n\n<i>Goodbye Alice.</i>\n\n\n\n%%html\n<i>Goodbye Alice.</i>\n\nGoodbye Alice.\n\n\n\n# Wrap the result of hello_goodbye() in <div> and </div>\n@html(\"<div>\", \"</div\")\ndef hello_goodbye(name):\n  return '\\n{}\\n{}\\n'.format(hello(name), goodbye(name))\n  \nprint(hello_goodbye('Alice'))\n\n<div>\n<b>Hello Alice!</b>\n<i>Goodbye Alice.</i>\n</div\n\n\n\n%%html\n<div>\n<b>Hello Alice!</b>\n<i>Goodbye Alice.</i>\n</div\n\n\nHello Alice!\nGoodbye Alice."
  },
  {
    "objectID": "posts/2020-07-26-writing functions in python.html#timeout-a-real-world-example",
    "href": "posts/2020-07-26-writing functions in python.html#timeout-a-real-world-example",
    "title": "Writing Functions in Python",
    "section": "Timeout(): a real world example",
    "text": "Timeout(): a real world example\n\nTag your functions\nTagging something means that you have given that thing one or more strings that act as labels. For instance, we often tag emails or photos so that we can search for them later. You’ve decided to write a decorator that will let you tag your functions with an arbitrary list of tags. You could use these tags for many things:\n\nAdding information about who has worked on the function, so a user can look up who to ask if they run into trouble using it.\nLabeling functions as “experimental” so that users know that the inputs and outputs might change in the future.\nMarking any functions that you plan to remove in a future version of the code.\nEtc.\n\n\ndef tag(*tags):\n  # Define a new decorator, named \"decorator\", to return\n  def decorator(func):\n    # Ensure the decorated function keeps its metadata\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n      # Call the function being decorated and return the result\n      return func(*args, **kwargs)\n    wrapper.tags = tags\n    return wrapper\n  # Return the new decorator\n  return decorator\n\n@tag('test', 'this is a tag')\ndef foo():\n  pass\n\nprint(foo.tags)\n\n('test', 'this is a tag')\n\n\n\n\nCheck the return type\n\ndef returns(return_type):\n  # Complete the returns() decorator\n  def decorator(func):\n    def wrapper(*args, **kwargs):\n      result = func(*args, **kwargs)\n      assert(type(result) == return_type)\n      return result\n    return wrapper\n  return decorator\n  \n@returns(dict)\ndef foo(value):\n  return value\n\ntry:\n  print(foo([1,2,3]))\nexcept AssertionError:\n  print('foo() did not return a dict!')\n\nfoo() did not return a dict!"
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html",
    "href": "posts/2020-07-14-unsupervised learning in python.html",
    "title": "Unsupervised Learning in Python",
    "section": "",
    "text": "Say we have a collection of customers with a variety of characteristics such as age, location, and financial history, and we wish to discover patterns and sort them into clusters. Or perhaps we have a set of texts, such as wikipedia pages, and we wish to segment them into categories based on their content. This is the world of unsupervised learning, called as such because we are not guiding, or supervising, the pattern discovery by some prediction task, but instead uncovering hidden structure from unlabeled data. Unsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension reduction to matrix factorization. We’ll explore the fundamentals of unsupervised learning and implement the essential algorithms using scikit-learn and scipy. We will explore how to cluster, transform, visualize, and extract insights from unlabeled datasets, and end the session by building a recommender system to recommend popular musical artists."
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#unsupervised-learning",
    "href": "posts/2020-07-14-unsupervised learning in python.html#unsupervised-learning",
    "title": "Unsupervised Learning in Python",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nUnsupervised learning\n\nUnsupervised learning finds patterns in data\nE.g. clustering customers by their purchases\nCompressing the data using purchase patterns (dimension reduction)\n\n\n\nSupervised vs unsupervised learning\n\nSupervised learning finds patterns for a prediction task\nE.g. classify tumors as benign or cancerous (labels)\nUnsupervised learning finds patterns in data\n… but without a specific prediction task in mind\n\n\n\nIris dataset\n\nMeasurements of many iris plants {% fn 1 %}\n3 species of iris: setosa, versicolor, virginica\nPetal length, petal width, sepal length, sepal width (the features of the dataset)\n\n\n\nIris data is 4-dimensional\n\nIris samples are points in 4 dimensional space\nDimension = number of features\nDimension too high to visualize!\n… but unsupervised learning gives insight\n\n\n\nk-means clustering\n\nFinds clusters of samples\nNumber of clusters must be specified\nImplemented in sklearn (“scikit-learn”)\n\n\n\nCluster labels for new samples\n\nNew samples can be assigned to existing clusters\nk-means remembers the mean of each cluster (the “centroids”)\nFinds the nearest centroid to each new sample\n\n\n\nScatter plots\n\nScatter plot of sepal length vs petal length\nEach point represents an iris sample\nColor points by cluster labels\nPyPlot (matplotlib.pyplot) TODO: add scatter plot\n\n\n\niris = datasets.load_iris()\niris.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n\n\n\nsamples = iris.data\nsamples[:5]\n\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2]])\n\n\n\nk-means clustering with scikit-learn\n\nmodel = KMeans(n_clusters=3)\nmodel.fit(samples)\n\nKMeans(n_clusters=3)\n\n\n\nlabels = model.predict([[5.8, 4. , 1.2, 0.2]])\nlabels\n\narray([1])\n\n\n\n\nCluster labels for new samples\n\nnew_samples = [[ 5.7,4.4,1.5,0.4] ,[ 6.5,3. ,5.5,1.8] ,[ 5.8,2.7,5.1,1.9]]\nmodel.predict(new_samples)\n\narray([1, 2, 0])\n\n\n\n\nScatter plots\n\nlabels_iris = model.predict(samples)\n\n\nxs_iris = samples[:,0]\nys_iris = samples[:,2]\n_ = sns.scatterplot(xs_iris, ys_iris, hue=labels_iris)\nplt.show()\n\n\n\n\n\npoints = pd.read_csv(\"datasets/points.csv\").values\npoints[:5]\n\narray([[ 0.06544649, -0.76866376],\n       [-1.52901547, -0.42953079],\n       [ 1.70993371,  0.69885253],\n       [ 1.16779145,  1.01262638],\n       [-1.80110088, -0.31861296]])\n\n\n\nxs_points = points[:,0]\nys_points = points[:,1]\n_ = sns.scatterplot(xs_points, ys_points)\nplt.show()\n\n\n\n\nThere are three clusters\n\n\nClustering 2D points\nFrom the scatter plot we saw that the points seem to separate into 3 clusters. we’ll now create a KMeans model to find 3 clusters, and fit it to the data points. After the model has been fit, we’ll obtain the cluster labels for some new points using the .predict() method.\n\nnew_points = pd.read_csv(\"datasets/new_points.csv\").values\nnew_points[:5]\n\narray([[ 0.40023333, -1.26544471],\n       [ 0.80323037,  1.28260167],\n       [-1.39507552,  0.05572929],\n       [-0.34119268, -1.07661994],\n       [ 1.54781747,  1.40250049]])\n\n\n\n# Create a KMeans instance with 3 clusters: model\nmodel_points = KMeans(n_clusters=3)\n\n# Fit model to points\nmodel_points.fit(points)\n\n# Determine the cluster labels of new_points: labels\nlabels_points = model_points.predict(new_points)\n\n# Print cluster labels of new_points\nprint(labels_points)\n\n[0 2 1 0 2 0 2 2 2 1 0 2 2 1 1 2 1 1 2 2 1 2 0 2 0 1 2 1 1 0 0 2 2 2 1 0 2\n 2 0 2 1 0 0 1 0 2 1 1 2 2 2 2 1 1 0 0 1 1 1 0 0 2 2 2 0 2 1 2 0 1 0 0 0 2\n 0 1 1 0 2 1 0 1 0 2 1 2 1 0 2 2 2 0 2 2 0 1 1 1 1 0 2 0 1 1 0 0 2 0 1 1 0\n 1 1 1 2 2 2 2 1 1 2 0 2 1 2 0 1 2 1 1 2 1 2 1 0 2 0 0 2 1 0 2 0 0 1 2 2 0\n 1 0 1 2 0 1 1 0 1 2 2 1 2 1 1 2 2 0 2 2 1 0 1 0 0 2 0 2 2 0 0 1 0 0 0 1 2\n 2 0 1 0 1 1 2 2 2 0 2 2 2 1 1 0 2 0 0 0 1 2 2 2 2 2 2 1 1 2 1 1 1 1 2 1 1\n 2 2 0 1 0 0 1 0 1 0 1 2 2 1 2 2 2 1 0 0 1 2 2 1 2 1 1 2 1 1 0 1 0 0 0 2 1\n 1 1 0 2 0 1 0 1 1 2 0 0 0 1 2 2 2 0 2 1 1 2 0 0 1 0 0 1 0 2 0 1 1 1 1 2 1\n 1 2 2 0]\n\n\nWe’ve successfully performed k-Means clustering and predicted the labels of new points. But it is not easy to inspect the clustering by just looking at the printed labels. A visualization would be far more useful. We’ll inspect the clustering with a scatter plot!\n\n\nInspect clustering\nLet’s now inspect the clustering we performed!\n\n# Assign the columns of new_points: xs and ys\nxs_np = new_points[:,0]\nys_np = new_points[:,1]\n\n# Make a scatter plot of xs and ys, using labels to define the colors\n_ = plt.scatter(xs_np, ys_np, c=labels_points, alpha=.5)\n\n# Assign the cluster centers: centroids\ncentroids_p = model_points.cluster_centers_\n\n# Assign the columns of centroids: centroids_x, centroids_y\ncentroids_x_p = centroids_p[:,0]\ncentroids_y_p = centroids_p[:,1]\n\n# Make a scatter plot of centroids_x and centroids_y\n_ = plt.scatter(centroids_x_p, centroids_y_p, marker=\"D\", s=50)\nplt.show()\n\n\n\n\nThe clustering looks great! But how can we be sure that 3 clusters is the correct choice? In other words, how can we evaluate the quality of a clustering?"
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#evaluating-a-clustering",
    "href": "posts/2020-07-14-unsupervised learning in python.html#evaluating-a-clustering",
    "title": "Unsupervised Learning in Python",
    "section": "Evaluating a clustering",
    "text": "Evaluating a clustering\n\nCan check correspondence with e.g. iris species\n… but what if there are no species to check against?\nMeasure quality of a clustering\nInforms choice of how many clusters to look for\n\n\nIris: clusters vs species\n\nk-means found 3 clusters amongst the iris samples\n\n\n\nCross tabulation with pandas\n\nClusters vs species is a “cross-tabulation”\n\n\n\niris_ct = pd.DataFrame({'labels':labels_iris, 'species':iris.target})\niris_ct.head()\n\n\n\n\n\n  \n    \n      \n      labels\n      species\n    \n  \n  \n    \n      0\n      1\n      0\n    \n    \n      1\n      1\n      0\n    \n    \n      2\n      1\n      0\n    \n    \n      3\n      1\n      0\n    \n    \n      4\n      1\n      0\n    \n  \n\n\n\n\n\nnp.unique(iris.target)\n\narray([0, 1, 2])\n\n\n\niris_ct.species.unique()\n\narray([0, 1, 2])\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n\n\n\niris_ct['species'] = iris_ct.species.map({0:'setosa', 1:'versicolor', 2:'virginica'})\niris_ct.head()\n\n\n\n\n\n  \n    \n      \n      labels\n      species\n    \n  \n  \n    \n      0\n      1\n      setosa\n    \n    \n      1\n      1\n      setosa\n    \n    \n      2\n      1\n      setosa\n    \n    \n      3\n      1\n      setosa\n    \n    \n      4\n      1\n      setosa\n    \n  \n\n\n\n\n\nCrosstab of labels and species\n\npd.crosstab(iris_ct.labels, iris_ct.species)\n\n\n\n\n\n  \n    \n      species\n      setosa\n      versicolor\n      virginica\n    \n    \n      labels\n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      48\n      14\n    \n    \n      1\n      50\n      0\n      0\n    \n    \n      2\n      0\n      2\n      36\n    \n  \n\n\n\n\n\n\nMeasuring clustering quality\n\nUsing only samples and their cluster labels\nA good clustering has tight clusters\n… and samples in each cluster bunched together\n\n\nInertia measures clustering quality\n\nMeasures how spread out the clusters are (lower is better)\nDistance from each sample to centroid of its cluster\nAfter fit(), available as attribute inertia_\nk-means attempts to minimize the inertia when choosing clusters\n\n\n\nmodel.inertia_\n\n78.851441426146\n\n\n\n\nThe number of clusters\n\nClusterings of the iris dataset with different numbers of clusters\nMore clusters means lower inertia\nWhat is the best number of clusters?\n\n\n\nHow many clusters to choose?\n\nA good clustering has tight clusters (so low inertia)\n… but not too many clusters!\nChoose an “elbow” in the inertia plot\nWhere inertia begins to decrease more slowly\nE.g. for iris dataset, 3 is a good choice\n\n\n\nHow many clusters of grain?\n\nsamples_grain = pd.read_csv(\"datasets/samples_grain.csv\").values\nsamples_grain[:5]\n\narray([[15.26  , 14.84  ,  0.871 ,  5.763 ,  3.312 ,  2.221 ,  5.22  ],\n       [14.88  , 14.57  ,  0.8811,  5.554 ,  3.333 ,  1.018 ,  4.956 ],\n       [14.29  , 14.09  ,  0.905 ,  5.291 ,  3.337 ,  2.699 ,  4.825 ],\n       [13.84  , 13.94  ,  0.8955,  5.324 ,  3.379 ,  2.259 ,  4.805 ],\n       [16.14  , 14.99  ,  0.9034,  5.658 ,  3.562 ,  1.355 ,  5.175 ]])\n\n\nan array samples contains the measurements (such as area, perimeter, length, and several others) of samples of grain. What’s a good number of clusters in this case?\n\nks_grain = range(1, 6)\ninertias_grain = []\n\nfor k in ks_grain:\n    # Create a KMeans instance with k clusters: model\n    model_grain = KMeans(n_clusters=k)\n    \n    # Fit model to samples\n    model_grain.fit(samples_grain)\n    \n    # Append the inertia to the list of inertias\n    inertias_grain.append(model_grain.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks_grain, inertias_grain, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks_grain)\nplt.show()\n\n\n\n\nThe inertia decreases very slowly from 3 clusters to 4, so it looks like 3 clusters would be a good choice for this data.\n\n\nEvaluating the grain clustering\nIn fact, the grain samples come from a mix of 3 different grain varieties: “Kama”, “Rosa” and “Canadian”. We will cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.\n\nvarieties = pd.read_csv(\"datasets/varieties.csv\")[\"0\"].to_list()\nvarieties[:5]\n\n['Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat', 'Kama wheat']\n\n\nlist varieties gives the grain variety for each sample.\n\n# Create a KMeans model with 3 clusters: model\nmodel_grain = KMeans(n_clusters=3)\n\n# Use fit_predict to fit model and obtain cluster labels: labels\nlabels_grain = model_grain.fit_predict(samples_grain)\n\n# Create a DataFrame with labels and varieties as columns: df\ngrain_df = pd.DataFrame({'labels': labels_grain, 'varieties': varieties})\n\n# Create crosstab: ct\nct_grain = pd.crosstab(grain_df.labels, grain_df.varieties)\n\n# Display ct\nct_grain\n\n\n\n\n\n  \n    \n      varieties\n      Canadian wheat\n      Kama wheat\n      Rosa wheat\n    \n    \n      labels\n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      1\n      60\n    \n    \n      1\n      2\n      60\n      10\n    \n    \n      2\n      68\n      9\n      0\n    \n  \n\n\n\n\nThe cross-tabulation shows that the 3 varieties of grain separate really well into 3 clusters. But depending on the type of data you are working with, the clustering may not always be this good. Is there anything we can do in such situations to improve the clustering?"
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#transforming-features-for-better-clusterings",
    "href": "posts/2020-07-14-unsupervised learning in python.html#transforming-features-for-better-clusterings",
    "title": "Unsupervised Learning in Python",
    "section": "Transforming features for better clusterings",
    "text": "Transforming features for better clusterings\n\nPiedmont wines dataset {% fn 2 %}\n\n178 samples from 3 distinct varieties of red wine: Barolo, Grignolino and Barbera\nFeatures measure chemical composition e.g. alcohol content\n… also visual properties like “color intensity”\n\n\n\nFeature variancesfeature\n\nThe wine features have very different variances!\nVariance of a feature measures spread of its values\n\n\n\nStandardScaler\n\nIn kmeans: feature variance = feature influence\nStandardScaler transforms each feature to have mean 0 and variance 1\nFeatures are said to be “standardized”\n\n\n\nSimilar methods\n\nStandardScaler and KMeans have similar methods\nUse fit() / transform() with StandardScaler\nUse fit() / predict() with KMeans\n\n\n\nStandardScaler, then KMeans\n\nNeed to perform two steps: StandardScaler, then KMeans\nUse sklearn pipeline to combine multiple steps\nData flows from one step into the next\n\n\n\nsklearn preprocessing steps\n\nStandardScaler is a “preprocessing” step\nMaxAbsScaler and Normalizer are other examples\n\n\n\nScaling fish data for clustering\n\nsamples_fish = pd.read_csv(\"datasets/samples_fish.csv\").values\nsamples_fish[:5]\n\narray([[242. ,  23.2,  25.4,  30. ,  38.4,  13.4],\n       [290. ,  24. ,  26.3,  31.2,  40. ,  13.8],\n       [340. ,  23.9,  26.5,  31.1,  39.8,  15.1],\n       [363. ,  26.3,  29. ,  33.5,  38. ,  13.3],\n       [430. ,  26.5,  29. ,  34. ,  36.6,  15.1]])\n\n\nan array samples_fish {% fn 3 %} gives measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, we’ll need to standardize these features first. We’ll build a pipeline to standardize and cluster the data.\n\n# Create scaler: scaler_fish\nscaler_fish = StandardScaler()\n\n# Create KMeans instance: kmeans_fish\nkmeans_fish = KMeans(n_clusters=4)\n\n# Create pipeline: pipeline_fish\npipeline_fish = make_pipeline(scaler_fish, kmeans_fish)\n\nNow that We’ve built the pipeline, we’ll use it to cluster the fish by their measurements.\n\n\nClustering the fish data\n\nspecies_fish = pd.read_csv(\"datasets/species_fish.csv\")[\"0\"].to_list()\nspecies_fish[:5]\n\n['Bream', 'Bream', 'Bream', 'Bream', 'Bream']\n\n\nWe’ll now use the standardization and clustering pipeline to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.\n\n# Fit the pipeline to samples\npipeline_fish.fit(samples_fish)\n\n# Calculate the cluster labels: labels_fish\nlabels_fish = pipeline_fish.predict(samples_fish)\n\n# Create a DataFrame with labels and species as columns: df\nfish_df = pd.DataFrame({'labels':labels_fish, 'species':species_fish})\n\n# Create crosstab: ct\nct_fish = pd.crosstab(fish_df.labels, fish_df.species)\n\n# Display ct\nct_fish\n\n\n\n\n\n  \n    \n      species\n      Bream\n      Pike\n      Roach\n      Smelt\n    \n    \n      labels\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      33\n      0\n      1\n      0\n    \n    \n      1\n      1\n      0\n      19\n      1\n    \n    \n      2\n      0\n      17\n      0\n      0\n    \n    \n      3\n      0\n      0\n      0\n      13\n    \n  \n\n\n\n\nIt looks like the fish data separates really well into 4 clusters!\n\n\nClustering stocks using KMeans\nWe’ll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day).\n\nmovements = pd.read_csv(\"datasets/movements.csv\").values\nmovements[:5]\n\narray([[  0.58    ,  -0.220005,  -3.409998, ...,  -5.359962,   0.840019,\n        -19.589981],\n       [ -0.640002,  -0.65    ,  -0.210001, ...,  -0.040001,  -0.400002,\n          0.66    ],\n       [ -2.350006,   1.260009,  -2.350006, ...,   4.790009,  -1.760009,\n          3.740021],\n       [  0.109997,   0.      ,   0.260002, ...,   1.849999,   0.040001,\n          0.540001],\n       [  0.459999,   1.77    ,   1.549999, ...,   1.940002,   1.130005,\n          0.309998]])\n\n\nA NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.\nSome stocks are more expensive than others. To account for this, we will include a Normalizer at the beginning of the pipeline. The Normalizer will separately transform each company’s stock price to a relative scale before the clustering begins.\n\n\n\n\n\n\nNote\n\n\n\nNormalizer() is different to StandardScaler(). While StandardScaler() standardizes features (such as the features of the fish data) by removing the mean and scaling to unit variance, Normalizer() rescales each sample - here, each company’s stock price - independently of the other.\n\n\n\n# Create a normalizer: normalizer_movements\nnormalizer_movements = Normalizer()\n\n# Create a KMeans model with 10 clusters: kmeans_movements\nkmeans_movements = KMeans(n_clusters=10)\n\n# Make a pipeline chaining normalizer and kmeans: pipeline_movements\npipeline_movements = make_pipeline(normalizer_movements, kmeans_movements)\n\n# Fit pipeline to the daily price movements\npipeline_movements.fit(movements)\n\nPipeline(steps=[('normalizer', Normalizer()),\n                ('kmeans', KMeans(n_clusters=10))])\n\n\nNow that the pipeline has been set up, we can find out which stocks move together\n\n\nWhich stocks move together?\nSo which company have stock prices that tend to change in the same way? We’ll now inspect the cluster labels from the clustering to find out.\n\ncompanies_movements=pd.read_csv(\"datasets/companies_movements.csv\")\ncompanies_movements.head()\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      Apple\n    \n    \n      1\n      AIG\n    \n    \n      2\n      Amazon\n    \n    \n      3\n      American express\n    \n    \n      4\n      Boeing\n    \n  \n\n\n\n\n\ncompanies_movements=companies_movements[\"0\"].to_list()\ncompanies_movements[:5]\n\n['Apple', 'AIG', 'Amazon', 'American express', 'Boeing']\n\n\na list companies_movements of the company names\n\n# Predict the cluster labels: labels_movements\nlabels_movements = pipeline_movements.predict(movements)\n\n# Create a DataFrame aligning labels and companies: df\nmovements_df = pd.DataFrame({'labels': labels_movements, 'companies': companies_movements})\n\n# Display df sorted by cluster label\nmovements_df.sort_values(\"labels\")\n\n\n\n\n\n  \n    \n      \n      labels\n      companies\n    \n  \n  \n    \n      18\n      0\n      Goldman Sachs\n    \n    \n      26\n      0\n      JPMorgan Chase\n    \n    \n      16\n      0\n      General Electrics\n    \n    \n      15\n      0\n      Ford\n    \n    \n      5\n      0\n      Bank of America\n    \n    \n      55\n      0\n      Wells Fargo\n    \n    \n      3\n      0\n      American express\n    \n    \n      1\n      0\n      AIG\n    \n    \n      22\n      1\n      HP\n    \n    \n      20\n      1\n      Home Depot\n    \n    \n      58\n      1\n      Xerox\n    \n    \n      30\n      1\n      MasterCard\n    \n    \n      23\n      1\n      IBM\n    \n    \n      14\n      1\n      Dell\n    \n    \n      54\n      1\n      Walgreen\n    \n    \n      32\n      1\n      3M\n    \n    \n      11\n      1\n      Cisco\n    \n    \n      33\n      1\n      Microsoft\n    \n    \n      47\n      1\n      Symantec\n    \n    \n      8\n      1\n      Caterpillar\n    \n    \n      13\n      1\n      DuPont de Nemours\n    \n    \n      39\n      2\n      Pfizer\n    \n    \n      53\n      2\n      Valero Energy\n    \n    \n      37\n      2\n      Novartis\n    \n    \n      42\n      2\n      Royal Dutch Shell\n    \n    \n      6\n      2\n      British American Tobacco\n    \n    \n      57\n      2\n      Exxon\n    \n    \n      19\n      2\n      GlaxoSmithKline\n    \n    \n      35\n      2\n      Navistar\n    \n    \n      46\n      2\n      Sanofi-Aventis\n    \n    \n      52\n      2\n      Unilever\n    \n    \n      12\n      2\n      Chevron\n    \n    \n      10\n      2\n      ConocoPhillips\n    \n    \n      49\n      2\n      Total\n    \n    \n      43\n      2\n      SAP\n    \n    \n      44\n      2\n      Schlumberger\n    \n    \n      40\n      3\n      Procter Gamble\n    \n    \n      27\n      3\n      Kimberly-Clark\n    \n    \n      56\n      3\n      Wal-Mart\n    \n    \n      9\n      3\n      Colgate-Palmolive\n    \n    \n      25\n      3\n      Johnson & Johnson\n    \n    \n      2\n      4\n      Amazon\n    \n    \n      59\n      4\n      Yahoo\n    \n    \n      21\n      5\n      Honda\n    \n    \n      45\n      5\n      Sony\n    \n    \n      34\n      5\n      Mitsubishi\n    \n    \n      48\n      5\n      Toyota\n    \n    \n      7\n      5\n      Canon\n    \n    \n      24\n      6\n      Intel\n    \n    \n      50\n      6\n      Taiwan Semiconductor Manufacturing\n    \n    \n      51\n      6\n      Texas instruments\n    \n    \n      28\n      7\n      Coca Cola\n    \n    \n      31\n      7\n      McDonalds\n    \n    \n      41\n      7\n      Philip Morris\n    \n    \n      38\n      7\n      Pepsi\n    \n    \n      29\n      8\n      Lookheed Martin\n    \n    \n      4\n      8\n      Boeing\n    \n    \n      36\n      8\n      Northrop Grumman\n    \n    \n      17\n      9\n      Google/Alphabet\n    \n    \n      0\n      9\n      Apple"
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#visualizing-hierarchies",
    "href": "posts/2020-07-14-unsupervised learning in python.html#visualizing-hierarchies",
    "title": "Unsupervised Learning in Python",
    "section": "Visualizing hierarchies",
    "text": "Visualizing hierarchies\n\nVisualisations communicate insight\n\n“t-SNE” : Creates a 2D map of a dataset (later)\n“Hierarchical clustering”\n\n\n\nA hierarchy of groups\n\nGroups of living things can form a hierarchy\nClusters are contained in one another\n\n\n\nEurovision scoring dataset {% fn 4 %}\n\nCountries gave scores to songs performed at the Eurovision 2016\n2D array of scores\nRows are countries, columns are songs\n\n\n\nHierarchical clustering\n\nEvery country begins in a separate cluster\nAt each step, the two closest clusters are merged\nContinue until all countries in a single cluster\nThis is “agglomerative” hierarchical clustering\n\n\n\nThe dendrogram of a hierarchical clustering\n\nRead from the bottom up\nVertical lines represent clusters\n\n\nWith 5 data samples, there would be 4 merge operations, and with 6 data samples, there would be 5 merges, and so on.\n\nHierarchical clustering of the grain data\nthe SciPy linkage() function performs hierarchical clustering on an array of samples. We will use the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to visualize the result.\n\n# Calculate the linkage: mergings_g\nmergings_g = linkage(samples_grain, method='complete')\n\n# Plot the dendrogram, using varieties as labels\nplt.figure(figsize=(20,7))\ndendrogram(mergings_g,\n           labels=varieties,\n           leaf_rotation=90,\n           leaf_font_size=8,\n)\nplt.show()\n\n\n\n\nDendrograms are a great way to illustrate the arrangement of the clusters produced by hierarchical clustering.\n\n\nHierarchies of stocks\nWe used k-means clustering to cluster companies according to their stock price movements. Now, we’ll perform hierarchical clustering of the companies. SciPy hierarchical clustering doesn’t fit into a sklearn pipeline, so we’ll need to use the normalize() function from sklearn.preprocessing instead of Normalizer.\n\n# Normalize the movements: normalized_movements\nnormalized_movements = normalize(movements)\n\n# Calculate the linkage: mergings\nmergings_m = linkage(normalized_movements, method=\"complete\")\n\n# Plot the dendrogram\nplt.figure(figsize=(20,10))\ndendrogram(mergings_m, labels=companies_movements, leaf_font_size=12, leaf_rotation=90)\nplt.show()\n\n\n\n\nYou can produce great visualizations such as this with hierarchical clustering, but it can be used for more than just visualizations."
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#cluster-labels-in-hierarchical-clustering",
    "href": "posts/2020-07-14-unsupervised learning in python.html#cluster-labels-in-hierarchical-clustering",
    "title": "Unsupervised Learning in Python",
    "section": "Cluster labels in hierarchical clustering",
    "text": "Cluster labels in hierarchical clustering\n\nCluster labels in hierarchical clustering\n\nNot only a visualisation tool!\nCluster labels at any intermediate stage can be recovered\nFor use in e.g. cross-tabulations\n\n\n\nIntermediate clusterings & height on dendrogram\n\nE.g. at height 15: Bulgaria, Cyprus, Greece are one cluster\nRussia and Moldova are another\nArmenia in a cluster on its own\n\n\n\nDendrograms show cluster distances\n\nHeight on dendrogram = distance between merging clusters\nE.g. clusters with only Cyprus and Greece had distance approx. 6\nThis new cluster distance approx. 12 from cluster with only Bulgaria\n\n\n\nIntermediate clusterings & height on dendrogram\n\nHeight on dendrogram specifies max. distance between merging clusters\nDon’t merge clusters further apart than this (e.g. 15)\n\n\n\nDistance between clusters\n\nDefined by a “linkage method”\nSpecified via method parameter, e.g. linkage(samples, method=“complete”)\nIn “complete” linkage: distance between clusters is max. distance between their samples\nDifferent linkage method, different hierarchical clustering!\n\n\n\nExtracting cluster labels\n\nUse the fcluster method\nReturns a NumPy array of cluster labels\n\n\nthe linkage method defines how the distance between clusters is measured. In complete linkage, the distance between clusters is the distance between the furthest points of the clusters. In single linkage, the distance between clusters is the distance between the closest points of the clusters.\n\nDifferent linkage, different hierarchical clustering!\nWe will perform a hierarchical clustering of the voting countries with ‘single’ linkage. Different linkage, different hierarchical clustering!\n\nsamples_eurovision = pd.read_csv(\"datasets/samples_eurovision.csv\").values\nsamples_eurovision[:5]\n\narray([[ 2., 12.,  0.,  0.,  0.,  8.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n         0., 10.,  0.,  4.,  0.,  5.,  7.,  0.,  0.,  3.,  0.,  6.,  0.],\n       [12.,  0.,  4.,  0.,  0.,  0.,  0.,  6.,  0.,  7.,  8.,  0.,  3.,\n         0.,  0.,  0.,  0.,  5.,  1., 12.,  0.,  0.,  2.,  0., 10.,  0.],\n       [ 0., 12.,  3.,  0., 12., 10.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,\n         0.,  0.,  0.,  1.,  6.,  0.,  5.,  0.,  2.,  0.,  0.,  8.,  4.],\n       [ 0.,  3., 12.,  0.,  0.,  5.,  0.,  0.,  0.,  1.,  0.,  2.,  0.,\n         0.,  0.,  0.,  0.,  0., 12.,  8.,  4.,  0.,  7.,  6., 10.,  0.],\n       [ 0.,  2.,  0., 12.,  0.,  8.,  0.,  0.,  0.,  4.,  1.,  0.,  7.,\n         6.,  0.,  0.,  0.,  5.,  3., 12.,  0.,  0.,  0.,  0., 10.,  0.]])\n\n\n\ncountry_names_eurovision = pd.read_csv(\"datasets/country_names_eurovision.csv\")[\"0\"].to_list()\ncountry_names_eurovision[:5]\n\n['Albania', 'Armenia', 'Australia', 'Austria', 'Azerbaijan']\n\n\n\nlen(country_names_eurovision)\n\n42\n\n\n\n# Calculate the linkage: mergings\nmergings_ev = linkage(samples_eurovision, method='single')\n\n# Plot the dendrogram\nplt.figure(figsize=(20,9))\ndendrogram(mergings_ev, labels=country_names_eurovision, leaf_rotation=90, leaf_font_size=12)\nplt.show()\n\n\n\n\n\n\nExtracting the cluster labels\nWe saw that the intermediate clustering of the grain samples at height 6 has 3 clusters. We will use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation.\n\n# Use fcluster to extract labels: labels_g\nlabels_g = fcluster(mergings_g,6, criterion='distance')\n\n# Create a DataFrame with labels and varieties as columns: df\ngrain_df = pd.DataFrame({'labels': labels_g, 'varieties': varieties})\n\n# Create crosstab: ct\ngrain_ct = pd.crosstab(grain_df.labels, grain_df.varieties)\n\n# Display ct\nprint(grain_ct)\n\nvarieties  Canadian wheat  Kama wheat  Rosa wheat\nlabels                                           \n1                       0           0          47\n2                       0          52          23\n3                      13           1           0\n4                      57          17           0\n\n\nWe’ve now mastered the fundamentals of k-Means and agglomerative hierarchical clustering. Next, we’ll explore t-SNE, which is a powerful tool for visualizing high dimensional data."
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#t-sne-for-2-dimensional-maps",
    "href": "posts/2020-07-14-unsupervised learning in python.html#t-sne-for-2-dimensional-maps",
    "title": "Unsupervised Learning in Python",
    "section": "t-SNE for 2-dimensional maps",
    "text": "t-SNE for 2-dimensional maps\n\nt-SNE for 2-dimensional maps\n\nt-SNE = “t-distributed stochastic neighbor embedding”\nMaps samples to 2D space (or 3D)\nMap approximately preserves nearness of samples\nGreat for inspecting datasets\n\n\n\nt-SNE on the iris dataset\n\nIris dataset has 4 measurements, so samples are 4-dimensional\nt-SNE maps samples to 2D space\nt-SNE didn’t know that there were different species\n… yet kept the species mostly separate\n\n\n\n\n\nimage.png\n\n\n\nInterpreting t-SNE scatter plots\n\n“versicolor” and “virginica” harder to distinguish from one another\nConsistent with k-means inertia plot: could argue for 2 clusters, or for 3\n\n\n\n\n\nimage.png\n\n\n\nt-SNE in sklearnIn\n\n2D NumPy array samples\nList species giving species of labels as number (0, 1, or 2)\n\n\n\nsamples[:5]\n\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2]])\n\n\n\niris.target[:5]\n\narray([0, 0, 0, 0, 0])\n\n\n\nt-SNE in sklearn\n\n\nmodel_i = TSNE(learning_rate=100)\ntransformed_i = model_i.fit_transform(samples)\nxs_i = transformed_i[:,0]\nys_i = transformed_i[:,1]\nplt.scatter(xs_i, ys_i, c=iris.target)\nplt.show()\n\n\n\n\n\nt-SNE has only fit_transform()\n\nHas a fit_transform() method\nSimultaneously fits the model and transforms the data\nHas no separate fit() or transform() methods\nCan’t extend the map to include new data samples\nMust start over each time!\n\n\n\nt-SNE learning rate\n\nChoose learning rate for the dataset\nWrong choice: points bunch together\nTry values between 50 and 200\n\n\n\nDifferent every time\n\nt-SNE features are different every time\nPiedmont wines, 3 runs, 3 different scatter plots!\n… however: The wine varieties (=colors) have same position relative to one another\n\n\n\n\n\nimage.png\n\n\n\nt-SNE visualization of grain dataset\nWe’ll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot.\n\nvariety_numbers_g = pd.read_csv(\"datasets/variety_numbers_grains.csv\")[\"0\"].to_list()\nvariety_numbers_g[:5]\n\n[1, 1, 1, 1, 1]\n\n\n\nsamples_grain[:5]\n\narray([[15.26  , 14.84  ,  0.871 ,  5.763 ,  3.312 ,  2.221 ,  5.22  ],\n       [14.88  , 14.57  ,  0.8811,  5.554 ,  3.333 ,  1.018 ,  4.956 ],\n       [14.29  , 14.09  ,  0.905 ,  5.291 ,  3.337 ,  2.699 ,  4.825 ],\n       [13.84  , 13.94  ,  0.8955,  5.324 ,  3.379 ,  2.259 ,  4.805 ],\n       [16.14  , 14.99  ,  0.9034,  5.658 ,  3.562 ,  1.355 ,  5.175 ]])\n\n\n\n# Create a TSNE instance: model\nmodel_g = TSNE(learning_rate=200)\n\n# Apply fit_transform to samples: tsne_features\ntsne_features_g = model_g.fit_transform(samples_grain)\n\n# Select the 0th feature: xs\nxs_g = tsne_features_g[:,0]\n\n# Select the 1st feature: ys\nys_g = tsne_features_g[:,1]\n\n# Scatter plot, coloring by variety_numbers_g\nplt.scatter(xs_g, ys_g, c=variety_numbers_g)\nplt.show()\n\n\n\n\nthe t-SNE visualization manages to separate the 3 varieties of grain samples. But how will it perform on the stock data?\n\n\nt-SNE map of the stock market\nt-SNE provides great visualizations when the individual samples can be labeled. We’ll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives a map of the stock market! The stock price movements for each company are available as the array normalized_movements. The list companies gives the name of each company.\n\n# Create a TSNE instance: model\nmodel_m = TSNE(learning_rate=50)\n\n# Apply fit_transform to normalized_movements: tsne_features\ntsne_features_m = model_m.fit_transform(normalized_movements)\n\n# Select the 0th feature: xs\nxs_m = tsne_features_m[:,0]\n\n# Select the 1th feature: ys\nys_m = tsne_features_m[:,1]\n\n# Scatter plot\nplt.figure(figsize=(20,14))\nplt.scatter(xs_m, ys_m)\n\n# Annotate the points\nfor x, y, company in zip(xs_m, ys_m, companies_movements):\n    plt.annotate(company, (x, y), fontsize=12)\nplt.show()\n\n\n\n\nIt’s visualizations such as this that make t-SNE such a powerful tool for extracting quick insights from high dimensional data."
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#visualizing-the-pca-transformation",
    "href": "posts/2020-07-14-unsupervised learning in python.html#visualizing-the-pca-transformation",
    "title": "Unsupervised Learning in Python",
    "section": "Visualizing the PCA transformation",
    "text": "Visualizing the PCA transformation\n\nDimension reduction\n\nMore efficient storage and computation\nRemove less-informative “noise” features\n… which cause problems for prediction tasks, e.g. classification, regression\n\n\n\nPrincipal Component Analysis\n\nPCA = “Principal Component Analysis”\nFundamental dimension reduction technique\nFirst step “decorrelation” (considered below)\nSecond step reduces dimension (considered later)\n\n\n\nPCA aligns data with axes\n\nRotates data samples to be aligned with axes\nShifts data samples so they have mean 0\nNo information is lostPCA\n\n\n\n\n\nimage.png\n\n\n\nPCA follows the fit/transform pattern\n\nPCA a scikit-learn component like KMeans or StandardScaler\nfit() learns the transformation from given data\ntransform() applies the learned transformation\ntransform() can also be applied to new data\n\n\n\nwine = pd.read_csv(\"datasets/wine.csv\")\nwine.head()\n\n\n\n\n\n  \n    \n      \n      class_label\n      class_name\n      alcohol\n      malic_acid\n      ash\n      alcalinity_of_ash\n      magnesium\n      total_phenols\n      flavanoids\n      nonflavanoid_phenols\n      proanthocyanins\n      color_intensity\n      hue\n      od280\n      proline\n    \n  \n  \n    \n      0\n      1\n      Barolo\n      14.23\n      1.71\n      2.43\n      15.6\n      127\n      2.80\n      3.06\n      0.28\n      2.29\n      5.64\n      1.04\n      3.92\n      1065\n    \n    \n      1\n      1\n      Barolo\n      13.20\n      1.78\n      2.14\n      11.2\n      100\n      2.65\n      2.76\n      0.26\n      1.28\n      4.38\n      1.05\n      3.40\n      1050\n    \n    \n      2\n      1\n      Barolo\n      13.16\n      2.36\n      2.67\n      18.6\n      101\n      2.80\n      3.24\n      0.30\n      2.81\n      5.68\n      1.03\n      3.17\n      1185\n    \n    \n      3\n      1\n      Barolo\n      14.37\n      1.95\n      2.50\n      16.8\n      113\n      3.85\n      3.49\n      0.24\n      2.18\n      7.80\n      0.86\n      3.45\n      1480\n    \n    \n      4\n      1\n      Barolo\n      13.24\n      2.59\n      2.87\n      21.0\n      118\n      2.80\n      2.69\n      0.39\n      1.82\n      4.32\n      1.04\n      2.93\n      735\n    \n  \n\n\n\n\n\nUsing scikit-learn PCA\n\nsamples_wine = array of two wine features (total_phenols & od280)\n\n\n\nsamples_wine = wine[['total_phenols', 'od280']].values\nsamples_wine[:5]\n\narray([[2.8 , 3.92],\n       [2.65, 3.4 ],\n       [2.8 , 3.17],\n       [3.85, 3.45],\n       [2.8 , 2.93]])\n\n\n\nmodel_w = PCA()\nmodel_w.fit(samples_wine)\n\nPCA()\n\n\n\ntransformed_w = model_w.transform(samples_wine)\n\n\nPCA features\n\nRows of transformed correspond to samples\nColumns of transformed are the “PCA features”\nRow gives PCA feature values of corresponding sample\n\n\n\ntransformed_w[:5]\n\narray([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]])\n\n\n\nPCA features are not correlated\n\nFeatures of dataset are often correlated, e.g. total_phenols and od280\nPCA aligns the data with axes\nResulting PCA features are not linearly correlated (“decorrelation”)PCA\n\n\n\n\n\npca\n\n\n\nPearson correlation\n\nMeasures linear correlation of features\nValue between -1 and 1\nValue of 0 means no linear correlation\n\n\n\n\n\nimage.png\n\n\n\nPrincipal components\n\n“Principal components” = directions of variance\nPCA aligns principal components with the axes\nAvailable as components_ attribute of PCA object\nEach row defines displacement from mean\n\n\n\nmodel_w.components_\n\narray([[nan, nan],\n       [nan, nan]])\n\n\n\nCorrelated data in nature\nWe have array grains giving the width and length of samples of grain. We suspect that width and length will be correlated. To confirm this, let’s make a scatter plot of width vs length and measure their Pearson correlation.\n\n# Assign the 0th column of grains: width\nwidth_g = samples_grain[:,4]\n\n# Assign the 1st column of grains: length\nlength_g = samples_grain[:,3]\n\n# Scatter plot width vs length\nplt.scatter(width_g, length_g)\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation\ncorrelation_g, pvalue_g = pearsonr(width_g, length_g)\n\n# Display the correlation\ncorrelation_g\n\n\n\n\n0.8604149377143469\n\n\nAs you would expect, the width and length of the grain samples are highly correlated.\n\n\nDecorrelating the grain measurements with PCA\nWe observed that the width and length measurements of the grain are correlated. Now, we’ll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation.\n\ngrains = pd.read_csv(\"datasets/grains.csv\").values\ngrains[:5]\n\narray([[3.312, 5.763],\n       [3.333, 5.554],\n       [3.337, 5.291],\n       [3.379, 5.324],\n       [3.562, 5.658]])\n\n\n\n# Create PCA instance: model_g\nmodel_g = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features_g = model_g.fit_transform(grains)\n\n# Assign 0th column of pca_features: xs\nxs_g = pca_features_g[:,0]\n\n# Assign 1st column of pca_features: ys\nys_g = pca_features_g[:,1]\n\n# Scatter plot xs vs ys\nplt.scatter(xs_g, ys_g)\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation of xs and ys\ncorrelation_g, pvalue_g = pearsonr(xs_g, ys_g)\n\n# Display the correlation\ncorrelation_g\n\n\n\n\n1.5634195327240974e-16\n\n\nprincipal components have to align with the axes of the point cloud."
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#intrinsic-dimension",
    "href": "posts/2020-07-14-unsupervised learning in python.html#intrinsic-dimension",
    "title": "Unsupervised Learning in Python",
    "section": "Intrinsic dimension",
    "text": "Intrinsic dimension\n\nIntrinsic dimension of a flight path\n\n2 features: longitude and latitude at points along a flight path\nDataset appears to be 2-dimensional\nBut can approximate using one feature: displacement along flight path\nIs intrinsically 1-dimensiona\n\n\n\n\n\nimage.png\n\n\n\nIntrinsic dimension\n\nIntrinsic dimension = number of features needed to approximate the dataset\nEssential idea behind dimension reduction\nWhat is the most compact representation of the samples?\nCan be detected with PCA\n\n\n\nVersicolor dataset\n\n“versicolor”, one of the iris species\nOnly 3 features: sepal length, sepal width, and petal width\nSamples are points in 3D space\n\n\n\nVersicolor dataset has intrinsic dimension 2\n\nSamples lie close to a flat 2-dimensional sheet\nSo can be approximated using 2 features\n\n\n\n\n\nimage.png\n\n\n\nPCA identifies intrinsic dimension\n\nScatter plots work only if samples have 2 or 3 features\nPCA identifies intrinsic dimension when samples have any number of features\nIntrinsic dimension = number of PCA features with significant variance\n\n\n\nPCA of the versicolor samples\n\n\n\n\nimage.png\n\n\n\nPCA features are ordered by variance descending\n\n\n\n\nimage.png\n\n\n\nVariance and intrinsic dimension\n\nIntrinsic dimension is number of PCA features with significant variance\nIn our example: the first two PCA features\nSo intrinsic dimension is 2\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n\n\n\nversicolor = pd.DataFrame(iris.data, columns=iris.feature_names)\nversicolor['target'] = iris.target\nversicolor = versicolor[versicolor.target==1]\nversicolor.head()\n\n\n\n\n\n  \n    \n      \n      sepal length (cm)\n      sepal width (cm)\n      petal length (cm)\n      petal width (cm)\n      target\n    \n  \n  \n    \n      50\n      7.0\n      3.2\n      4.7\n      1.4\n      1\n    \n    \n      51\n      6.4\n      3.2\n      4.5\n      1.5\n      1\n    \n    \n      52\n      6.9\n      3.1\n      4.9\n      1.5\n      1\n    \n    \n      53\n      5.5\n      2.3\n      4.0\n      1.3\n      1\n    \n    \n      54\n      6.5\n      2.8\n      4.6\n      1.5\n      1\n    \n  \n\n\n\n\n\nsamples_versicolor = versicolor[['sepal length (cm)', 'sepal width (cm)', 'petal width (cm)']].values\nsamples_versicolor[:5]\n\narray([[7. , 3.2, 1.4],\n       [6.4, 3.2, 1.5],\n       [6.9, 3.1, 1.5],\n       [5.5, 2.3, 1.3],\n       [6.5, 2.8, 1.5]])\n\n\n\nPlotting the variances of PCA features\n\nsamples_versicolor = array of versicolor samples\n\n\n\npca_versicolor = PCA()\npca_versicolor.fit(samples_versicolor)\n\nPCA()\n\n\n\nfeatures_versicolor = range(pca_versicolor.n_components_)\nplt.bar(features_versicolor, pca_versicolor.explained_variance_)\nplt.xticks(features_versicolor)\nplt.xlabel(\"PCA feature\")\nplt.ylabel(\"Variance\")\nplt.show()\n\n\n\n\n\nIntrinsic dimension can be ambiguous\n\nIntrinsic dimension is an idealization\n… there is not always one correct answer!\nPiedmont wines: could argue for 2, or for 3, or more\n\n\n\nThe first principal component\nThe first principal component of the data is the direction in which the data varies the most. We will use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot.\n\n# Make a scatter plot of the untransformed points\nplt.scatter(grains[:,0], grains[:,1])\n\n# Fit model to points\nmodel_g.fit(grains)\n\n# Get the mean of the grain samples: mean\nmean_g = model_g.mean_\n\n# Get the first principal component: first_pc\nfirst_pc_g = model_g.components_[0,:]\n\n# Plot first_pc as an arrow, starting at mean\nplt.arrow(mean_g[0], mean_g[1], first_pc_g[0], first_pc_g[1], color='blue', width=0.01)\n\n# Keep axes on same scale\nplt.axis('equal')\nplt.show()\n\n\n\n\nThis is the direction in which the grain data varies the most.\n\n\nVariance of the PCA features\nThe fish dataset is 6-dimensional. But what is its intrinsic dimension? We will make a plot of the variances of the PCA features to find out. We’ll need to standardize the features first.\n\n# Create scaler: scaler\nscaler_fish = StandardScaler()\n\n# Create a PCA instance: pca\npca_fish = PCA()\n\n# Create pipeline: pipeline\npipeline_fish = make_pipeline(scaler_fish, pca_fish)\n\n# Fit the pipeline to 'samples'\npipeline_fish.fit(samples_fish)\n\n# Plot the explained variances\nfeatures_fish = range(pca_fish.n_components_)\nplt.bar(features_fish, pca_fish.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features_fish)\nplt.show()\n\n\n\n\nIt looks like PCA features 0 and 1 have significant variance. Since PCA features 0 and 1 have significant variance, the intrinsic dimension of this dataset appears to be 2."
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#dimension-reduction-with-pca",
    "href": "posts/2020-07-14-unsupervised learning in python.html#dimension-reduction-with-pca",
    "title": "Unsupervised Learning in Python",
    "section": "Dimension reduction with PCA",
    "text": "Dimension reduction with PCA\n\nDimension reduction\n\nRepresents same data, using less features\nImportant part of machine-learning pipelines\nCan be performed using PCA\n\n\n\nDimension reduction with PCA\n\nPCA features are in decreasing order of variance\nAssumes the low variance features are “noise”\n… and high variance features are informative\n\n\n\nDimension reduction with PCA\n\nSpecify how many features to keep\nE.g. PCA(n_components=2)\nKeeps the first 2 PCA features\nIntrinsic dimension is a good choice\n\n\n\nDimension reduction of iris dataset\n\nsamples = array of iris measurements (4 features)\nspecies = list of iris species numbers\n\n\n\nDimension reduction with PCA\n\nDiscards low variance PCA features\nAssumes the high variance features are informative\nAssumption typically holds in practice (e.g. for iris)\n\n\n\nWord frequency arrays\n\nRows represent documents, columns represent words\nEntries measure presence of each word in each document\n… measure using “tf-idf” (more later)\n\n\n\nSparse arrays and csr_matrix\n\nArray is “sparse”: most entries are zero\nCan use scipy.sparse.csr_matrix instead of NumPy array\ncsr_matrix remembers only the non-zero entries (saves space!)\n\n\n\nTruncatedSVD and csr_matrix\n\nscikit-learn PCA doesn’t support csr_matrix\nUse scikit-learn TruncatedSVD instead\nPerforms same transformation\n\n\n\nDimension reduction of the fish measurements\nWe saw that 2 was a reasonable choice for the “intrinsic dimension” of the fish measurements. We will use PCA for dimensionality reduction of the fish measurements, retaining only the 2 most important components.\n\nscaled_samples_fish = pd.read_csv(\"datasets/scaled_samples_fish.csv\").values\nscaled_samples_fish[:5]\n\narray([[-0.50109735, -0.36878558, -0.34323399, -0.23781518,  1.0032125 ,\n         0.25373964],\n       [-0.37434344, -0.29750241, -0.26893461, -0.14634781,  1.15869615,\n         0.44376493],\n       [-0.24230812, -0.30641281, -0.25242364, -0.15397009,  1.13926069,\n         1.0613471 ],\n       [-0.18157187, -0.09256329, -0.04603648,  0.02896467,  0.96434159,\n         0.20623332],\n       [-0.00464454, -0.0747425 , -0.04603648,  0.06707608,  0.8282934 ,\n         1.0613471 ]])\n\n\n\n# Create a PCA model with 2 components: pca\npca_fish = PCA(n_components=2)\n\n# Fit the PCA instance to the scaled samples\npca_fish.fit(scaled_samples_fish)\n\n# Transform the scaled samples: pca_features\npca_features_fish = pca_fish.transform(scaled_samples_fish)\n\n# Print the shape of pca_features\npca_features_fish.shape\n\n(85, 2)\n\n\nWe’ve successfully reduced the dimensionality from 6 to 2.\n\n\nA tf-idf word-frequency array\nWe’ll create a tf-idf word frequency array for a toy collection of documents. For this, we will use the TfidfVectorizer from sklearn. It transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects.\n\ndocuments = ['cats say meow', 'dogs say woof', 'dogs chase cats']\n\n\n# Create a TfidfVectorizer: tfidf\ntfidf_d = TfidfVectorizer() \n\n# Apply fit_transform to document: csr_mat\ncsr_mat_d = tfidf_d.fit_transform(documents)\n\n# Print result of toarray() method\nprint(csr_mat_d.toarray())\n\n# Get the words: words\nwords_d = tfidf_d.get_feature_names()\n\n# Print words\nwords_d\n\n[[0.51785612 0.         0.         0.68091856 0.51785612 0.        ]\n [0.         0.         0.51785612 0.         0.51785612 0.68091856]\n [0.51785612 0.68091856 0.51785612 0.         0.         0.        ]]\n\n\n['cats', 'chase', 'dogs', 'meow', 'say', 'woof']\n\n\n\n\nClustering Wikipedia part I\nTruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. We will cluster some popular pages from Wikipedia {% fn 5 %}. We will build the pipeline and apply it to the word-frequency array of some Wikipedia articles.\nThe Pipeline object will be consisting of a TruncatedSVD followed by KMeans.\n\n# Create a TruncatedSVD instance: svd\nsvd_wp = TruncatedSVD(n_components=50)\n\n# Create a KMeans instance: kmeans\nkmeans_wp = KMeans(n_clusters=6)\n\n# Create a pipeline: pipeline\npipeline_wp = make_pipeline(svd_wp, kmeans_wp)\n\nNow that we have set up the pipeline, we will use to cluster the articles.\n\n\nClustering Wikipedia part II\n\nwv = pd.read_csv(\"datasets/Wikipedia_articles/wikipedia-vectors.csv\", index_col=0)\narticles = csr_matrix(wv.transpose())\narticles_titles = list(wv.columns)\n\n\n# Fit the pipeline to articles\npipeline_wp.fit(articles)\n\n# Calculate the cluster labels: labels\nlabels_wp = pipeline_wp.predict(articles)\n\n# Create a DataFrame aligning labels and titles: df\nwp = pd.DataFrame({'label': labels_wp, 'article': articles_titles})\n\n# Display df sorted by cluster label\nwp.sort_values(\"label\")\n\n\n\n\n\n  \n    \n      \n      label\n      article\n    \n  \n  \n    \n      47\n      0\n      Fever\n    \n    \n      40\n      0\n      Tonsillitis\n    \n    \n      41\n      0\n      Hepatitis B\n    \n    \n      42\n      0\n      Doxycycline\n    \n    \n      43\n      0\n      Leukemia\n    \n    \n      44\n      0\n      Gout\n    \n    \n      45\n      0\n      Hepatitis C\n    \n    \n      46\n      0\n      Prednisone\n    \n    \n      49\n      0\n      Lymphoma\n    \n    \n      48\n      0\n      Gabapentin\n    \n    \n      58\n      1\n      Sepsis\n    \n    \n      59\n      1\n      Adam Levine\n    \n    \n      50\n      1\n      Chad Kroeger\n    \n    \n      51\n      1\n      Nate Ruess\n    \n    \n      52\n      1\n      The Wanted\n    \n    \n      53\n      1\n      Stevie Nicks\n    \n    \n      54\n      1\n      Arctic Monkeys\n    \n    \n      55\n      1\n      Black Sabbath\n    \n    \n      56\n      1\n      Skrillex\n    \n    \n      57\n      1\n      Red Hot Chili Peppers\n    \n    \n      28\n      2\n      Anne Hathaway\n    \n    \n      27\n      2\n      Dakota Fanning\n    \n    \n      26\n      2\n      Mila Kunis\n    \n    \n      25\n      2\n      Russell Crowe\n    \n    \n      29\n      2\n      Jennifer Aniston\n    \n    \n      23\n      2\n      Catherine Zeta-Jones\n    \n    \n      22\n      2\n      Denzel Washington\n    \n    \n      21\n      2\n      Michael Fassbender\n    \n    \n      20\n      2\n      Angelina Jolie\n    \n    \n      24\n      2\n      Jessica Biel\n    \n    \n      10\n      3\n      Global warming\n    \n    \n      11\n      3\n      Nationally Appropriate Mitigation Action\n    \n    \n      12\n      3\n      Nigel Lawson\n    \n    \n      13\n      3\n      Connie Hedegaard\n    \n    \n      14\n      3\n      Climate change\n    \n    \n      15\n      3\n      Kyoto Protocol\n    \n    \n      17\n      3\n      Greenhouse gas emissions by the United States\n    \n    \n      18\n      3\n      2010 United Nations Climate Change Conference\n    \n    \n      16\n      3\n      350.org\n    \n    \n      19\n      3\n      2007 United Nations Climate Change Conference\n    \n    \n      9\n      4\n      LinkedIn\n    \n    \n      1\n      4\n      Alexa Internet\n    \n    \n      2\n      4\n      Internet Explorer\n    \n    \n      3\n      4\n      HTTP cookie\n    \n    \n      4\n      4\n      Google Search\n    \n    \n      5\n      4\n      Tumblr\n    \n    \n      6\n      4\n      Hypertext Transfer Protocol\n    \n    \n      7\n      4\n      Social search\n    \n    \n      8\n      4\n      Firefox\n    \n    \n      0\n      4\n      HTTP 404\n    \n    \n      30\n      5\n      France national football team\n    \n    \n      31\n      5\n      Cristiano Ronaldo\n    \n    \n      32\n      5\n      Arsenal F.C.\n    \n    \n      33\n      5\n      Radamel Falcao\n    \n    \n      34\n      5\n      Zlatan Ibrahimović\n    \n    \n      35\n      5\n      Colombia national football team\n    \n    \n      36\n      5\n      2014 FIFA World Cup qualification\n    \n    \n      37\n      5\n      Football\n    \n    \n      38\n      5\n      Neymar\n    \n    \n      39\n      5\n      Franck Ribéry"
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#non-negative-matrix-factorization-nmf",
    "href": "posts/2020-07-14-unsupervised learning in python.html#non-negative-matrix-factorization-nmf",
    "title": "Unsupervised Learning in Python",
    "section": "Non-negative matrix factorization (NMF)",
    "text": "Non-negative matrix factorization (NMF)\n\nNMF = “non-negative matrix factorization”\nDimension reduction technique\nNMF models are interpretable (unlike PCA)\nEasy to interpret means easy to explain!\nHowever, all sample features must be non-negative (>= 0)\n\n\nInterpretable parts\n\nNMF expresses documents as combinations of topics (or “themes”)\nNMF expresses images as combinations of patterns\n\n\n\nUsing scikit-learn NMF\n\nFollows fit() / transform() pattern\nMust specify number of components e.g. NMF(n_components=2)\nWorks with NumPy arrays and with csr_matrix\n\n\n\nExample word-frequency array\n\nWord frequency array, 4 words, many documents\nMeasure presence of words in each document using “tf-idf”\n“tf” = frequency of word in document\n“idf” reduces influence of frequent words\n\n\n\nNMF components\n\nNMF has components\n… just like PCA has principal components\nDimension of components = dimension of samples\nEntries are non-negative\n\n\n\nNMF features\n\nNMF feature values are non-negative\nCan be used to reconstruct the samples\n… combine feature values with components\n\n\n\nSample reconstruction\n\nMultiply components by feature values, and add up\nCan also be expressed as a product of matrices\nThis is the “Matrix Factorization” in “NMF”\n\n\n\nNMF fits to non-negative data, only\n\nWord frequencies in each document\nImages encoded as arrays\nAudio spectrograms\nPurchase histories on e-commerce sites\n… and many more!\n\n\n\nNon-negative data\n\nA tf-idf word-frequency array.\nAn array where rows are customers, columns are products and entries are 0 or 1, indicating whether a customer has purchased a product.\n\n\n\nNMF applied to Wikipedia articles\n\n# Create an NMF instance: model\nmodel_wp = NMF(n_components=6)\n\n# Fit the model to articles\nmodel_wp.fit(articles)\n\n# Transform the articles: nmf_features\nnmf_features_wp = model_wp.transform(articles)\n\n# Print the NMF features\nnmf_features_wp[:5]\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.44041429],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.56653905],\n       [0.00382072, 0.        , 0.        , 0.        , 0.        ,\n        0.39860038],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.3816956 ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.48546058]])\n\n\n\n\nNMF features of the Wikipedia articles\n\n\n\n\n\n\nNote\n\n\n\nWhen investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. We’ll see why: NMF components represent topics (for instance, acting!).\n\n\n\n# Create a pandas DataFrame: df\nwp_df = pd.DataFrame(nmf_features_wp, index=articles_titles)\n\n# Print the row for 'Anne Hathaway'\ndisplay(wp_df.loc[['Anne Hathaway']])\n\n# Print the row for 'Denzel Washington'\ndisplay(wp_df.loc[['Denzel Washington']])\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    \n      Anne Hathaway\n      0.003846\n      0.0\n      0.0\n      0.575735\n      0.0\n      0.0\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    \n      Denzel Washington\n      0.0\n      0.005601\n      0.0\n      0.422398\n      0.0\n      0.0"
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#nmf-learns-interpretable-parts",
    "href": "posts/2020-07-14-unsupervised learning in python.html#nmf-learns-interpretable-parts",
    "title": "Unsupervised Learning in Python",
    "section": "NMF learns interpretable parts",
    "text": "NMF learns interpretable parts\n\nExample: NMF learns interpretable parts\n\nWord-frequency array articles (tf-idf )\n20,000 scientific articles (rows)\n800 words (columns)\n\n\n\narticles.shape\n\n(60, 13125)\n\n\nNMF components are topics\n\nNMF components\n\nFor documents:\nNMF components represent topics\nNMF features combine topics into documents\nFor images, NMF components are parts of images\n\n\n\nGrayscale images\n\n“Grayscale” image = no colors, only shades of gray\nMeasure pixel brightness\nRepresent with value between 0 and 1 (0 is black)\nConvert to 2D array\n\n\n\nEncoding a collection of images\n\nCollection of images of the same size\nEncode as 2D array\nEach row corresponds to an image\nEach column corresponds to a pixel\n… can apply NMF!\n\n\n\nNMF learns topics of documents\nwhen NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. We will using the NMF model that we built earlier using the Wikipedia articles. 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. We will identify the topic of the corresponding NMF component.\n\nwords = pd.read_csv(\"datasets/Wikipedia_articles/words.csv\")[\"0\"].to_list()\nwords[:5]\n\n['aaron', 'abandon', 'abandoned', 'abandoning', 'abandonment']\n\n\n\n# Create a DataFrame: components_df\ncomponents_df = pd.DataFrame(model_wp.components_, columns=words)\n\n# Print the shape of the DataFrame\nprint(components_df.shape)\n\n# Select row 3: component\ncomponent = components_df.iloc[3]\n\n# Print result of nlargest\ncomponent.nlargest()\n\n(6, 13125)\n\n\nfilm       0.627850\naward      0.253121\nstarred    0.245274\nrole       0.211442\nactress    0.186390\nName: 3, dtype: float64\n\n\n\n\nExplore the LED digits dataset\nWe’ll use NMF to decompose grayscale images into their commonly occurring patterns. Firstly, we’ll explore the image dataset and see how it is encoded as an array.\n\nsamples_images = pd.read_csv(\"datasets/samples_images.csv\")\nx=samples_images.isnull().sum()\nx[x>0]\n\nSeries([], dtype: int64)\n\n\n\nsamples_images=samples_images.values\nnp.isinf(samples_images).any()\n\nFalse\n\n\n\nnp.isnan(samples_images).any()\n\nFalse\n\n\n\n# Select the 0th row: digit\ndigit_i = samples_images[0,:]\n\n# Print digit\nprint(digit_i)\n\n# Reshape digit to a 13x8 array: bitmap\nbitmap_i = digit_i.reshape(13,8)\n\n# Print bitmap\nprint(bitmap_i)\n\n# Use plt.imshow to display bitmap\nplt.imshow(bitmap_i, cmap='gray', interpolation='nearest')\nplt.colorbar()\nplt.show()\n\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.]\n[[0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 1. 1. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\n\n\n\n\n\nNMF learns the parts of images\n\ndef show_as_image(sample):\n    \"\"\"displays the image encoded by any 1D array\"\"\"\n    bitmap = sample.reshape((13, 8))\n    plt.figure()\n    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n    plt.colorbar()\n    plt.show()\n\n\nshow_as_image(samples_images[99, :])\n\n\n\n\n\n# Create an NMF model: model\nmodel_i = NMF(n_components=7)\n\n# Apply fit_transform to samples: features\nfeatures_i = model_i.fit_transform(samples_images)\n\n# Call show_as_image on each component\nfor component in model_i.components_:\n    show_as_image(component)\n\n# Assign the 0th row of features: digit_features\ndigit_features_i = features_i[0,:]\n\n# Print digit_features\ndigit_features_i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narray([4.76823559e-01, 0.00000000e+00, 0.00000000e+00, 5.90605054e-01,\n       4.81559442e-01, 0.00000000e+00, 7.37535093e-16])\n\n\n\n\nPCA doesn’t learn parts\nUnlike NMF, PCA doesn’t learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. We will verify by inspecting the components of a PCA model fit to the dataset of LED digit images\n\n# Create a PCA instance: model\nmodel_i = PCA(n_components=7)\n\n# Apply fit_transform to samples: features\nfeatures_i = model_i.fit_transform(samples_images)\n\n# Call show_as_image on each component\nfor component in model_i.components_:\n    show_as_image(component)\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe components of PCA do not represent meaningful parts of images of LED digits!"
  },
  {
    "objectID": "posts/2020-07-14-unsupervised learning in python.html#building-recommender-systems-using-nmf",
    "href": "posts/2020-07-14-unsupervised learning in python.html#building-recommender-systems-using-nmf",
    "title": "Unsupervised Learning in Python",
    "section": "Building recommender systems using NMF",
    "text": "Building recommender systems using NMF\n\nFinding similar articles\n\nEngineer at a large online newspaper\nTask: recommend articles similar to article being read by customer\nSimilar articles should have similar topics\n\n\n\nStrategy\n\nApply NMF to the word-frequency array\nNMF feature values describe the topics\n… so similar documents have similar NMF feature values\nCompare NMF feature values?\n\n\n\nVersions of articles\n\nDifferent versions of the same document have same topic proportions\n… exact feature values may be different!\nE.g. because one version uses many meaningless words\nBut all versions lie on the same line through the origin\n\n\n\nCosine similarity\n\nUses the angle between the lines\nHigher values means more similar\nMaximum value is 1, when angle is 0\n\n\n\nWhich articles are similar to ‘Cristiano Ronaldo’?\nfinding the articles most similar to the article about the footballer Cristiano Ronaldo.\n\n# Normalize the NMF features: norm_features\nnorm_features_wp = normalize(nmf_features_wp)\n\n# Create a DataFrame: df\nwp_df = pd.DataFrame(norm_features_wp, index=articles_titles)\n\n# Select the row corresponding to 'Cristiano Ronaldo': article\narticle = wp_df.loc['Cristiano Ronaldo']\n\n# Compute the dot products: similarities\nsimilarities = wp_df.dot(article)\n\n# Display those with the largest cosine similarity\nsimilarities.nlargest()\n\nCristiano Ronaldo                1.000000\nFranck Ribéry                    0.999972\nRadamel Falcao                   0.999942\nZlatan Ibrahimović               0.999942\nFrance national football team    0.999923\ndtype: float64\n\n\n\n\nRecommend musical artists part I\nrecommend popular music artists!\n\nartists_df = pd.read_csv(\"datasets/Musical_artists/scrobbler-small-sample.csv\")\nartists = csr_matrix(artists_df)\n\n\n# Create a MaxAbsScaler: scaler\nscaler = MaxAbsScaler()\n\n# Create an NMF model: nmf\nnmf = NMF(n_components=20)\n\n# Create a Normalizer: normalizer\nnormalizer = Normalizer()\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(scaler, nmf, normalizer)\n\n# Apply fit_transform to artists: norm_features\nnorm_features = pipeline.fit_transform(artists)\n\n\n\nRecommend musical artists part II\nSuppose you were a big fan of Bruce Springsteen - which other musicial artists might you like? We will use the NMF features and the cosine similarity to find similar musical artists.\n\nartist_names = pd.read_csv(\"datasets/Musical_artists/artists.csv\")[\"0\"].to_list()\n\n\n# Create a DataFrame: df\ndf = pd.DataFrame(norm_features, index=artist_names)\n\n# Select row of 'Bruce Springsteen': artist\nartist = df.loc['Bruce Springsteen']\n\n# Compute cosine similarities: similarities\nsimilarities = df.dot(artist)\n\n# Display those with highest cosine similarity\nsimilarities.nlargest()\n\nValueError: Shape of passed values is (2894, 20), indices imply (111, 20)\n\n\n{{ ‘Source: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html.’ | fndetail: 1 }} {{ ‘Source: https://archive.ics.uci.edu/ml/datasets/Wine.’ | fndetail: 2 }} {{ ‘These fish measurement data were sourced from the Journal of Statistics Education..’ | fndetail: 3 }} {{ ‘Source: http://www.eurovision.tv/page/results’ | fndetail: 4 }} {{ ‘The Wikipedia dataset you will be working with was obtained from here.’ | fndetail: 5 }}"
  },
  {
    "objectID": "posts/2021-04-30-python_variables_and_data_types.html#storing-information-using-variables",
    "href": "posts/2021-04-30-python_variables_and_data_types.html#storing-information-using-variables",
    "title": "A Quick Tour of Variables and Data Types in Python",
    "section": "Storing information using variables",
    "text": "Storing information using variables\nComputers are useful for two purposes: storing information and performing operations on stored information. While working with a programming language such as Python, informations is stored in variables. You can think of variables are containers for storing data. The data stored within a variable is called it’s value. It’s really easy to create variables in Python.\n\nmy_favorite_color = \"black\"\n\n\nmy_favorite_color\n\n'black'\n\n\nA variable is created using an assignment statement, which begins with the variable’s name, followed by the assignment operator = (different from the equality comparision operator ==), followed by the value to be stored within the variable.\nYou can also values to multiple variables in a single statement by separating the variable names and values with commas.\n\ncolor1, color2, color3 = \"red\", \"green\", \"blue\"\n\n\ncolor1\n\n'red'\n\n\n\ncolor2\n\n'green'\n\n\n\ncolor3\n\n'blue'\n\n\nYou can assign the same value to multiple variables by chaining multiple assignment operations within a single statement.\n\ncolor4 = color5 = color6 = \"magenta\"\n\n\ncolor4\n\n'magenta'\n\n\n\ncolor5\n\n'magenta'\n\n\n\ncolor6\n\n'magenta'\n\n\nYou can change the value stored within a variable simply by assigning a new value to it using another assignment statement. Be careful while reassgining variables: when you assign a new value to the variable, the old value is lost and no longer accessible.\n\nmy_favorite_color = \"red\"\n\n\nmy_favorite_color\n\n'red'\n\n\nWhile assigning a new value to a variable, you can also use the previous value of the variable to determine the new value.\n\ncounter = 10\n\n\ncounter = counter + 1\n\n\ncounter\n\n11\n\n\nThe pattern var = var op something (where op is an arithmetic operator like +, -, *, /) is very commonly used, so Python provides a shorthand syntax for it.\n\ncounter = 10\n\n\n# Same as `counter = counter + 4`\ncounter += 4\n\n\ncounter\n\n14\n\n\nVariable names can be short (a, x, y etc.) or descriptive ( my_favorite_color, profit_margin, the_3_musketeers etc.). Howerver, you must follow these rules while naming Python variables:\n\nA variable’s name must start with a letter or the underscore character _. It cannot start with a number.\nA variable name can only contain lowercase or uppercase letters, digits or underscores (a-z, A-Z, 0-9 and _).\nVariable names are case-sensitive i.e. a_variable, A_Variable and A_VARIABLE are all different variables.\n\nHere are some valid variable names:\n\na_variable = 23\nis_today_Saturday = False\nmy_favorite_car = \"Delorean\"\nthe_3_musketeers = [\"Athos\", \"Porthos\", \"Aramis\"] \n\nLet’s also try creating some variables with invalid names. Python prints a syntax error if your variable’s name is invalid.\n\nSyntax: The syntax of a programming language refers to the rules which govern what a valid instruction or statement in the language should look like. If a statement does not follow these rules, Python stops execution and informs you that there is a syntax error. You can think of syntax as the rules of grammar for a programming language.\n\n\na variable = 23\n\nSyntaxError: ignored\n\n\n\nis_today_$aturday = False\n\nSyntaxError: ignored\n\n\n\nmy-favorite-car = \"Delorean\"\n\nSyntaxError: ignored\n\n\n\n3_musketeers = [\"Athos\", \"Porthos\", \"Aramis\"]\n\nSyntaxError: ignored"
  },
  {
    "objectID": "posts/2021-04-30-python_variables_and_data_types.html#built-in-data-types-in-python",
    "href": "posts/2021-04-30-python_variables_and_data_types.html#built-in-data-types-in-python",
    "title": "A Quick Tour of Variables and Data Types in Python",
    "section": "Built-in data types in Python",
    "text": "Built-in data types in Python\nAny data or information stored within a Python variable has a type. The type of data stored within a variable can be checked using the type function.\n\na_variable\n\n23\n\n\n\ntype(a_variable)\n\nint\n\n\n\nis_today_Saturday\n\nFalse\n\n\n\ntype(is_today_Saturday)\n\nbool\n\n\n\nmy_favorite_car\n\n'Delorean'\n\n\n\ntype(my_favorite_car)\n\nstr\n\n\n\nthe_3_musketeers\n\n['Athos', 'Porthos', 'Aramis']\n\n\n\ntype(the_3_musketeers)\n\nlist\n\n\nPython has several built-in data types for storing different types of information in variables. Following are at some commonly used data types:\n\nInteger\nFloat\nBoolean\nNone\nString\nList\nTuple\nDictionary\n\nInteger, float, boolean, None and string are primitive data types because they represent a single value. Other data types like list, tuple and dictionary are often called data structures or containers because they hold multiple pieces of data together.\n\nInteger\nIntegers represent positive or negative whole numbers, from negative infinity to infinity. Note that integers should not include decimal points. Integers have the type int.\n\ncurrent_year = 2021\n\n\ncurrent_year\n\n2021\n\n\n\ntype(current_year)\n\nint\n\n\nUnlike some other programming languages, integers in Python can be arbirarily large (or small). There’s no lowest or highest value for integers, and there’s just one int type (as opposed to short, int, long, long long, unsigned int etc. in C/C++/Java).\n\na_large_negative_number = -23374038374832934334234317348343\n\n\na_large_negative_number\n\n-23374038374832934334234317348343\n\n\n\ntype(a_large_negative_number)\n\nint\n\n\n\n\nFloat\nFloats (or floating point numbers) are numbers with a decimal point. There are no limits on the value of a float or the number of digits before or after the decimal point. Floating point numbers have the type float.\n\npi = 3.141592653589793238\n\n\npi\n\n3.141592653589793\n\n\n\ntype(pi)\n\nfloat\n\n\nNote that a whole number is treated as a float if it is written with a decimal point, even though the decimal portion of the number is zero.\n\na_number = 3.0\n\n\na_number\n\n3.0\n\n\n\ntype(a_number)\n\nfloat\n\n\n\nanother_number = 4.\n\n\nanother_number\n\n4.0\n\n\n\ntype(another_number)\n\nfloat\n\n\nFloating point numbers can also be written using the scientific notation with an “e” to indicate the power of 10.\n\none_hundredth = 1e-2\n\n\none_hundredth\n\n0.01\n\n\n\ntype(one_hundredth)\n\nfloat\n\n\n\navogadro_number = 6.02214076e23\n\n\navogadro_number\n\n6.02214076e+23\n\n\n\ntype(avogadro_number)\n\nfloat\n\n\nFloats can be converted into integers and vice versa using the float and int functions. The operation of coverting one type of value into another is called casting.\n\nfloat(current_year)\n\n2021.0\n\n\n\nfloat(a_large_negative_number)\n\n-2.3374038374832935e+31\n\n\n\nint(pi)\n\n3\n\n\n\nint(avogadro_number)\n\n602214075999999987023872\n\n\nWhile performing arithmetic operations, integers are automatically converted to floats if any of the operands is a float. Also, the division operator / always returns a float, even if both operands are integers. Use the // operator if you want the result of division to be an int.\n\ntype(45 * 3.0)\n\nfloat\n\n\n\ntype(45 * 3)\n\nint\n\n\n\ntype(10/3)\n\nfloat\n\n\n\ntype(10/2)\n\nfloat\n\n\n\ntype(10//2)\n\nint\n\n\n\n\nBoolean\nBooleans represent one of 2 values: True and False. Booleans have the type bool.\n\nis_today_Sunday = True\n\n\nis_today_Sunday\n\nTrue\n\n\n\ntype(is_today_Saturday)\n\nbool\n\n\nBooleans are generally returned as the result of a comparision operation (e.g. ==, >= etc.).\n\ncost_of_ice_bag = 1.25\nis_ice_bag_expensive = cost_of_ice_bag >= 10\n\n\nis_ice_bag_expensive\n\nFalse\n\n\n\ntype(is_ice_bag_expensive)\n\nbool\n\n\nBooleans are automatically converted to ints when used in arithmetic operations. True is converted to 1 and False is converted to 0.\n\n5 + False\n\n5\n\n\n\n3. + True\n\n4.0\n\n\nAny value in Python can be converted to a Boolean using the bool function.\nOnly the following values evaluate to False (they are often called falsy values):\n\nThe value False itself\nThe integer 0\nThe float 0.0\nThe empty value None\nThe empty text \"\"\nThe empty list []\nThe empty tuple ()\nThe empty dictionary {}\nThe emtpy set set()\nThe empty range range(0)\n\nEverything else evaluates to True (a value that evalutes to True is often called a truthy value).\n\nbool(False)\n\nFalse\n\n\n\nbool(0)\n\nFalse\n\n\n\nbool(0.0)\n\nFalse\n\n\n\nbool(None)\n\nFalse\n\n\n\nbool(\"\")\n\nFalse\n\n\n\nbool([])\n\nFalse\n\n\n\nbool(())\n\nFalse\n\n\n\nbool({})\n\nFalse\n\n\n\nbool(set())\n\nFalse\n\n\n\nbool(range(0))\n\nFalse\n\n\n\nbool(True), bool(1), bool(2.0), bool(\"hello\"), bool([1,2]), bool((2,3)), bool(range(10))\n\n(True, True, True, True, True, True, True)\n\n\n\n\nNone\nThe None type includes a single value None, used to indicate the absence of a value. None has the type NoneType. It is often used to declare a variable whose value may be assigned later.\n\nnothing = None\n\n\ntype(nothing)\n\nNoneType\n\n\n\n\nString\nA string is used to represent text (a string of characters) in Python. Strings must be surrounded using quotations (either the single quote ' or the double quote \"). Strings have the type string.\n\ntoday = \"Friday\"\n\n\ntoday\n\n'Friday'\n\n\n\ntype(today)\n\nstr\n\n\nYou can use single quotes inside a string written with double quotes, and vice versa.\n\nmy_favorite_movie = \"One Flew over the Cuckoo's Nest\" \n\n\nmy_favorite_movie\n\n\"One Flew over the Cuckoo's Nest\"\n\n\n\nmy_favorite_pun = 'Thanks for explaining the word \"many\" to me, it means a lot.'\n\n\nmy_favorite_pun\n\n'Thanks for explaining the word \"many\" to me, it means a lot.'\n\n\nTo use the a double quote within a string written with double quotes, escape the inner quotes by prefixing them with the \\ character.\n\nanother_pun = \"The first time I got a universal remote control, I thought to myself \\\"This changes everything\\\".\"\n\n\nanother_pun\n\n'The first time I got a universal remote control, I thought to myself \"This changes everything\".'\n\n\nStrings created using single or double quotes must begin and end on the same line. To create multiline strings, use three single quotes ''' or three double quotes \"\"\" to begin and end the string. Line breaks are represented using the newline character \\n.\n\nyet_another_pun = '''Son: \"Dad, can you tell me what a solar eclipse is?\" \nDad: \"No sun.\"'''\n\n\nyet_another_pun\n\n'Son: \"Dad, can you tell me what a solar eclipse is?\" \\nDad: \"No sun.\"'\n\n\nMultiline strings are best displayed using the print function.\n\nprint(yet_another_pun)\n\nSon: \"Dad, can you tell me what a solar eclipse is?\" \nDad: \"No sun.\"\n\n\n\na_music_pun = \"\"\"\nTwo windmills are standing in a field and one asks the other, \n\"What kind of music do you like?\"  \n\nThe other says, \n\"I'm a big metal fan.\"\n\"\"\"\n\n\nprint(a_music_pun)\n\n\nTwo windmills are standing in a field and one asks the other, \n\"What kind of music do you like?\"  \n\nThe other says, \n\"I'm a big metal fan.\"\n\n\n\nYou can check the length of a string using the len function.\n\nlen(my_favorite_movie)\n\n31\n\n\nNote that special characters like \\n and escaped characters like \\\" count as a single character, even though they are written and sometimes printed as 2 characters.\n\nmultiline_string = \"\"\"a\nb\"\"\"\nmultiline_string\n\n'a\\nb'\n\n\n\nlen(multiline_string)\n\n3\n\n\nA string can be converted into a list of characters using list function.\n\nlist(multiline_string)\n\n['a', '\\n', 'b']\n\n\nStrings also support several list operations, which are discussed in the next section. We’ll look at a couple of examples here.\nYou can access individual characters within a string using the [] indexing notation. Note the character indices go from 0 to n-1, where n is the length of the string.\n\ntoday = \"Saturday\"\n\n\ntoday[0]\n\n'S'\n\n\n\ntoday[3]\n\n'u'\n\n\n\ntoday[7]\n\n'y'\n\n\nYou can access a part of a string using by providing a start:end range instead of a single index in [].\n\ntoday[5:8]\n\n'day'\n\n\nYou can also check whether a string contains a some text using the in operator.\n\n'day' in today\n\nTrue\n\n\n\n'Sun' in today\n\nFalse\n\n\nTwo or more strings can be joined or concatenated using the + operator. Be careful while concatenating strings, sometimes you may need to add a space character \" \" between words.\n\nfull_name = \"Derek O'Brien\"\n\n\ngreeting = \"Hello\"\n\n\ngreeting + full_name\n\n\"HelloDerek O'Brien\"\n\n\n\ngreeting + \" \" + full_name + \"!\" # additional space\n\n\"Hello Derek O'Brien!\"\n\n\nString in Python have many built-in methods that can be used to manipulate them. Let’s try out some common string methods.\n\nMethods: Methods are functions associated with data types, and are accessed using the . notatation e.g. variable_name.method() or \"a string\".method(). Methods are a powerful technique for associating common operations with values of specific data types.\n\nThe .lower(), .upper() and .capitalize() methods are used to change the case of the characters.\n\ntoday.lower()\n\n'saturday'\n\n\n\n\"saturday\".upper()\n\n'SATURDAY'\n\n\n\n\"monday\".capitalize() # changes first character to uppercase\n\n'Monday'\n\n\nThe .replace method is used to replace a part of the string with another string. It takes the portion to be replaced and the replacement text as inputs or arguments.\n\nanother_day = today.replace(\"Satur\", \"Wednes\")\n\n\nanother_day\n\n'Wednesday'\n\n\nNote that a new string is returned, and the original string is not modified.\n\ntoday\n\n'Saturday'\n\n\nThe .split method can be used to split a string into a list of strings based using the character(s) provided.\n\n\"Sun,Mon,Tue,Wed,Thu,Fri,Sat\".split(\",\")\n\n['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n\n\nThe .strip method is used to remove whitespace characters from the beginning and end of a string.\n\na_long_line = \"       This is a long line with some space before, after,     and some space in the middle..    \"\n\n\na_long_line_stripped = a_long_line.strip()\n\n\na_long_line_stripped\n\n'This is a long line with some space before, after,     and some space in the middle..'\n\n\nThe .format method is used to combine values of other data types e.g. integers, floats, booleans, lists etc. with strings. It is often used to create output messages for display.\n\n# Input variables\ncost_of_ice_bag = 1.25\nprofit_margin = .2\nnumber_of_bags = 500\n\n# Template for output message\noutput_template = \"\"\"If a grocery store sells ice bags at $ {} per bag, with a profit margin of {} %, \nthen the total profit it makes by selling {} ice bags is $ {}.\"\"\"\n\nprint(output_template)\n\nIf a grocery store sells ice bags at $ {} per bag, with a profit margin of {} %, \nthen the total profit it makes by selling {} ice bags is $ {}.\n\n\n\n# Inserting values into the string\ntotal_profit = cost_of_ice_bag * profit_margin * number_of_bags\noutput_message = output_template.format(cost_of_ice_bag, profit_margin*100, number_of_bags, total_profit)\n\nprint(output_message)\n\nIf a grocery store sells ice bags at $ 1.25 per bag, with a profit margin of 20.0 %, \nthen the total profit it makes by selling 500 ice bags is $ 125.0.\n\n\nNotice how the placeholders {} in the output_template string are replaced with the arguments provided to the .format method.\nIt is also possible use the string concatenation operator + to combine strings with other values, however, those values must first be converted to strings using the str function.\n\n\"If a grocery store sells ice bags at $ \" + cost_of_ice_bag + \", with a profit margin of \" + profit_margin\n\nTypeError: ignored\n\n\n\n\"If a grocery store sells ice bags at $ \" + str(cost_of_ice_bag) + \", with a profit margin of \" + str(profit_margin)\n\n'If a grocery store sells ice bags at $ 1.25, with a profit margin of 0.2'\n\n\nIn fact, the str can be used to convert a value of any data type into a string.\n\nstr(23)\n\n'23'\n\n\n\nstr(23.432)\n\n'23.432'\n\n\n\nstr(True)\n\n'True'\n\n\n\nthe_3_musketeers = [\"Athos\", \"Porthos\", \"Aramis\"]\nstr(the_3_musketeers)\n\n\"['Athos', 'Porthos', 'Aramis']\"\n\n\nNote that all string methods returns new values, and DO NOT change the existing string. You can find a full list of string methods here: https://www.w3schools.com/python/python_ref_string.asp.\nStrings also support the comparision operators == and != for checking whether two strings are equal\n\nfirst_name = \"John\"\n\n\nfirst_name == \"Doe\"\n\nFalse\n\n\n\nfirst_name == \"John\"\n\nTrue\n\n\n\nfirst_name != \"Jane\"\n\nTrue\n\n\nWe’ve looked at the primitive data types in Python, and we’re now ready to explore non-primitive data structures or containers.\n\n\nList\nA list in Python is an ordered collection of values. Lists can hold values of different data types, and support operations to add, remove and change values. Lists have the type list.\nTo create a list, enclose a list of values within square brackets [ and ], separated by commas.\n\nfruits = ['apple', 'banana', 'cherry']\n\n\nfruits\n\n['apple', 'banana', 'cherry']\n\n\n\ntype(fruits)\n\nlist\n\n\nLet’s try creating a list containing values of different data types, including another list.\n\na_list = [23, 'hello', None, 3.14, fruits, 3 <= 5]\n\n\na_list\n\n[23, 'hello', None, 3.14, ['apple', 'banana', 'cherry'], True]\n\n\n\nempty_list = []\n\n\nempty_list\n\n[]\n\n\nTo determine the number of values in a list, use the len function. In general, the len function can be used to determine of values in several other data types.\n\nlen(fruits)\n\n3\n\n\n\nprint(\"Number of fruits:\", len(fruits))\n\nNumber of fruits: 3\n\n\n\nlen(a_list)\n\n6\n\n\n\nlen(empty_list)\n\n0\n\n\nYou can access the elements of a list using the the index of the element, starting from the index 0.\n\nfruits[0]\n\n'apple'\n\n\n\nfruits[1]\n\n'banana'\n\n\n\nfruits[2]\n\n'cherry'\n\n\nIf you try to access an index equal to or higher than the length of the list, Python returns an IndexError.\n\nfruits[3]\n\nIndexError: ignored\n\n\n\nfruits[4]\n\nIndexError: ignored\n\n\n\nfruits[-1]\n\n'cherry'\n\n\n\nfruits[-2]\n\n'banana'\n\n\n\nfruits[-3]\n\n'apple'\n\n\n\nfruits[-4]\n\nIndexError: ignored\n\n\nYou can also access a range of values from the list. The result is itself a list. Let us look at some examples.\n\na_list = [23, 'hello', None, 3.14, fruits, 3 <= 5]\n\n\na_list\n\n[23, 'hello', None, 3.14, ['apple', 'banana', 'cherry'], True]\n\n\n\nlen(a_list)\n\n6\n\n\n\na_list[2:5]\n\n[None, 3.14, ['apple', 'banana', 'cherry']]\n\n\nNote that the start index (2 in the above example) of the range is included in the list, but the end index (5 in the above example) is not included. So, the result has 3 values (indices 2, 3 and 4).\nHere are some experiments you should try out (use the empty cells below):\n\nTry setting one or both indices of the range are larger than the size of the list e.g. a_list[2:10]\nTry setting the start index of the range to be larger than the end index of the range e.g. list_a[2:10]\nTry leaving out the start or end index of a range e.g. a_list[2:] or a_list[:5]\nTry using negative indices for the range e.g. a_list[-2:-5] or a_list[-5:-2] (can you explain the results?)\n\n\nThe flexible and interactive nature of Jupyter notebooks makes them a great tool for learning and experimentation. Most questions that arise while you are learning Python for the first time can be resolved by simply typing the code into a cell and executing it. Let your curiosity run wild, and discover what Python is capable of, and what it isn’t!\n\nYou can also change the value at a specific index within a list using the assignment operation.\n\nfruits\n\n['apple', 'banana', 'cherry']\n\n\n\nfruits[1] = 'blueberry'\n\n\nfruits\n\n['apple', 'blueberry', 'cherry']\n\n\nA new value can be added to the end of a list using the append method.\n\nfruits.append('dates')\n\n\nfruits\n\n['apple', 'blueberry', 'cherry', 'dates']\n\n\nA new value can also be inserted a specific index using the insert method.\n\nfruits.insert(1, 'banana')\n\n\nfruits\n\n['apple', 'banana', 'blueberry', 'cherry', 'dates']\n\n\nYou can remove a value from the list using the remove method.\n\nfruits.remove('blueberry')\n\n\nfruits\n\n['apple', 'banana', 'cherry', 'dates']\n\n\nWhat happens if a list has multiple instances of the value passed to .remove? Try it out.\nTo remove an element from a specific index, use the pop method. The method also returns the removed element.\n\nfruits\n\n['apple', 'banana', 'cherry', 'dates']\n\n\n\nfruits.pop(1)\n\n'banana'\n\n\n\nfruits\n\n['apple', 'cherry', 'dates']\n\n\nIf no index is provided, the pop method removes the last element of the list.\n\nfruits.pop()\n\n'dates'\n\n\n\nfruits\n\n['apple', 'cherry']\n\n\nYou can test whether a list contains a value using the in operator.\n\n'pineapple' in fruits\n\nFalse\n\n\n\n'cherry' in fruits\n\nTrue\n\n\nTo combine two or more lists, use the + operator. This operation is also called concatenation.\n\nfruits\n\n['apple', 'cherry']\n\n\n\nmore_fruits = fruits + ['pineapple', 'tomato', 'guava'] + ['dates', 'banana']\n\n\nmore_fruits\n\n['apple', 'cherry', 'pineapple', 'tomato', 'guava', 'dates', 'banana']\n\n\nTo create a copy of a list, use the copy method. Modifying the copied list does not affect the original list.\n\nmore_fruits_copy = more_fruits.copy()\n\n\nmore_fruits_copy\n\n['apple', 'cherry', 'pineapple', 'tomato', 'guava', 'dates', 'banana']\n\n\n\n# Modify the copy\nmore_fruits_copy.remove('pineapple')\nmore_fruits_copy.pop()\nmore_fruits_copy\n\n['apple', 'cherry', 'tomato', 'guava', 'dates']\n\n\n\n# Original list remains unchanged\nmore_fruits\n\n['apple', 'cherry', 'pineapple', 'tomato', 'guava', 'dates', 'banana']\n\n\nNote that you cannot create a copy of a list by simply creating a new variable using the assignment operator =. The new variable will point to the same list, and any modifications performed using one variable will affect the other.\n\nmore_fruits\n\n['apple', 'cherry', 'pineapple', 'tomato', 'guava', 'dates', 'banana']\n\n\n\nmore_fruits_not_a_copy = more_fruits\n\n\nmore_fruits_not_a_copy.remove('pineapple')\nmore_fruits_not_a_copy.pop()\n\n'banana'\n\n\n\nmore_fruits_not_a_copy\n\n['apple', 'cherry', 'tomato', 'guava', 'dates']\n\n\n\nmore_fruits\n\n['apple', 'cherry', 'tomato', 'guava', 'dates']\n\n\nJust like strings, there are several in-built methods to manipulate a list. Unlike strings, however, most list methods modify the original list, rather than returning a new one. Check out some common list operations here: https://www.w3schools.com/python/python_ref_list.asp\nFollowing are some exercises you can try out with list methods (use the blank code cells below):\n\nReverse the order of elements in a list\nAdd the elements of one list to the end of another list\nSort a list of strings in alphabetical order\nSort a list of numbers in decreasing order\n\n\n\nTuple\nA tuple is an ordered collection of values, similar to a list, however it is not possible to add, remove or modify values in a tuple. A tuple is created by enclosing values within parantheses ( and ), separated by commas.\n\nAny data structure that cannot be modified after creation is called immutable. You can think of tuples as immutable lists.\n\nLet’s try some experiments with tuples.\n\nfruits = ('apple', 'cherry', 'dates')\n\n\n# check no. of elements\nlen(fruits)\n\n3\n\n\n\n# get an element (positive index)\nfruits[0]\n\n'apple'\n\n\n\n# get an element (negative index)\nfruits[-2]\n\n'cherry'\n\n\n\n# check if it contains an element\n'dates' in fruits\n\nTrue\n\n\n\n# try to change an element\nfruits[0] = 'avocado'\n\nTypeError: ignored\n\n\n\n# try to append an element\nfruits.append('blueberry')\n\nAttributeError: ignored\n\n\n\n# try to remove an element\nfruits.remove('apple')\n\nAttributeError: ignored\n\n\nYou can also skip the parantheses ( and ) while creating a tuple. Python automatically converts comma-separated values into a tuple.\n\nthe_3_musketeers = 'Athos', 'Porthos', 'Aramis'\n\n\nthe_3_musketeers\n\n('Athos', 'Porthos', 'Aramis')\n\n\nYou can also create a tuple with just one element, if you include a comma after the element. Just wrapping it with parantheses ( and ) won’t create a tuple.\n\nsingle_element_tuple = 4,\n\n\nsingle_element_tuple\n\n(4,)\n\n\n\nanother_single_element_tuple = (4,)\n\n\nanother_single_element_tuple\n\n(4,)\n\n\n\nnot_a_tuple = (4)\n\n\nnot_a_tuple\n\n4\n\n\nTuples are often used to create multiple variables with a single statement.\n\npoint = (3, 4)\n\n\npoint_x, point_y = point\n\n\npoint_x\n\n3\n\n\n\npoint_y\n\n4\n\n\nYou can convert a list into a tuple using the tuple function, and vice versa using the list function\n\ntuple(['one', 'two', 'three'])\n\n('one', 'two', 'three')\n\n\n\nlist(('Athos', 'Porthos', 'Aramis'))\n\n['Athos', 'Porthos', 'Aramis']\n\n\nTuples have just 2 built-in methods: count and index. Can you figure out what they do? While look could look for documentation and examples online, there’s an easier way to check the documentation of a method, using the help function.\n\na_tuple = 23, \"hello\", False, None, 23, 37, \"hello\"\n\n\nhelp(a_tuple.count)\n\nHelp on built-in function count:\n\ncount(value, /) method of builtins.tuple instance\n    Return number of occurrences of value.\n\n\n\nWithin a Jupyter notebook, you can also start a code cell with ? and type the name of a function or method. When you execute this cell, you will see the documentation for the function/method in a pop-up window.\n\n?a_tuple.index\n\nTry using count and index with a_tuple in the code cells below.\n\n\nDictionary\nA dictionary is an unordered collection of items. Each item stored in a dictionary has a key and value. Keys are used to retrieve values from the dictionary. Dictionaries have the type dict.\nDictionaries are often used to store many pieces of information e.g. details about a person, in a single variable. Dictionaries are created by enclosing key-value pairs within curly brackets { and }.\n\nperson1 = {\n    'name': 'John Doe',\n    'sex': 'Male',\n    'age': 32,\n    'married': True\n}\n\n\nperson1\n\n{'age': 32, 'married': True, 'name': 'John Doe', 'sex': 'Male'}\n\n\nDictionaries can also be created using the dict function.\n\nperson2 = dict(name='Jane Judy', sex='Female', age=28, married=False)\n\n\nperson2\n\n{'age': 28, 'married': False, 'name': 'Jane Judy', 'sex': 'Female'}\n\n\n\ntype(person1)\n\ndict\n\n\nKeys can be used to access values using square brackets [ and ].\n\nperson1['name']\n\n'John Doe'\n\n\n\nperson1['married']\n\nTrue\n\n\n\nperson2['name']\n\n'Jane Judy'\n\n\nIf a key isn’t present in the dictionary, then a KeyError is returned.\n\nperson1['address']\n\nKeyError: ignored\n\n\nThe get method can also be used to access the value associated with a key.\n\nperson2.get(\"name\")\n\n'Jane Judy'\n\n\nThe get method also accepts a default value which is returned if the key is not present in the dictionary.\n\nperson2.get(\"address\", \"Unknown\")\n\n'Unknown'\n\n\nYou can check whether a key is present in a dictionary using the in operator.\n\n'name' in person1\n\nTrue\n\n\n\n'address' in person1\n\nFalse\n\n\nYou can change the value associated with a key using the assignment operator.\n\nperson2['married']\n\nFalse\n\n\n\nperson2['married'] = True\n\n\nperson2['married']\n\nTrue\n\n\nThe assignment operator can also be used to add new key-value pairs to the dictonary.\n\nperson1\n\n{'age': 32, 'married': True, 'name': 'John Doe', 'sex': 'Male'}\n\n\n\nperson1['address'] = '1, Penny Lane'\n\n\nperson1\n\n{'address': '1, Penny Lane',\n 'age': 32,\n 'married': True,\n 'name': 'John Doe',\n 'sex': 'Male'}\n\n\nTo remove a key and the associated value from a dictionary, use the pop method.\n\nperson1.pop('address')\n\n'1, Penny Lane'\n\n\n\nperson1\n\n{'age': 32, 'married': True, 'name': 'John Doe', 'sex': 'Male'}\n\n\nDictonaries also provide methods to view the list of keys, values or key-value pairs inside it.\n\nperson1.keys()\n\ndict_keys(['name', 'sex', 'age', 'married'])\n\n\n\nperson1.values()\n\ndict_values(['John Doe', 'Male', 32, True])\n\n\n\nperson1.items()\n\ndict_items([('name', 'John Doe'), ('sex', 'Male'), ('age', 32), ('married', True)])\n\n\n\nperson1.items()[1]\n\nTypeError: ignored\n\n\nThe result of the keys, values or items look like lists but don’t seem to support the indexing operator [] for retrieving elements.\nCan you figure out how to access an element at a specific index from these results? Try it below. Hint: Use the list function\nDictionaries provide many other methods. You can learn more about them here: https://www.w3schools.com/python/python_ref_dictionary.asp\nHere are some experiments you can try out with dictionaries (use the empty cells below): * What happens if you use the same key multiple times while creating a dictionary? * How can you create a copy of a dictionary (modifying the copy should not change the original)? * Can the value associated with a key itself be a dictionary? * How can you add the key value pairs from one dictionary into another dictionary? Hint: See the update method. * Can the keys of a dictionary be something other than a string e.g. a number, boolean, list etc.?"
  },
  {
    "objectID": "posts/2021-04-30-python_variables_and_data_types.html#further-reading",
    "href": "posts/2021-04-30-python_variables_and_data_types.html#further-reading",
    "title": "A Quick Tour of Variables and Data Types in Python",
    "section": "Further Reading",
    "text": "Further Reading\nWe’ve now completed our exploration of variables and common data types in Python. Following are some resources to learn more about data types in Python:\n\nPython official documentation: https://docs.python.org/3/tutorial/index.html\nPython Tutorial at W3Schools: https://www.w3schools.com/python/\nPractical Python Programming: https://dabeaz-course.github.io/practical-python/Notes/Contents.html\n\nYou are now ready to move on to the next tutorial: Branching using conditional statements and loops in Python"
  },
  {
    "objectID": "posts/2020-10-19-avocados.html",
    "href": "posts/2020-10-19-avocados.html",
    "title": "Avocado",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "posts/2020-10-19-avocados.html#total-number-sold-for-each-avocado-size-group",
    "href": "posts/2020-10-19-avocados.html#total-number-sold-for-each-avocado-size-group",
    "title": "Avocado",
    "section": "Total number sold for each avocado size group",
    "text": "Total number sold for each avocado size group\n\nnb_sold_by_size = avocados.groupby('size')['nb_sold'].sum()\nnb_sold_by_size.plot(kind='bar', x='size', y='nb_sold')\nplt.show()\n\n\n\n\nBedazzling bar plot! It looks like small avocados were the most-purchased size, but large avocados were a close second."
  },
  {
    "objectID": "posts/2020-10-19-avocados.html#changes-in-sales-over-time",
    "href": "posts/2020-10-19-avocados.html#changes-in-sales-over-time",
    "title": "Avocado",
    "section": "Changes in sales over time",
    "text": "Changes in sales over time\n\nvisualizing the change in avocado sales over three years.\n\nLine plots are designed to visualize the relationship between two numeric variables, where each data values is connected to the next one. They are especially useful for visualizing change in a number over time, since each time point is naturally connected to the next time point. ### the total number of avocados sold on each date\n\n\nnb_sold_by_date = avocados.groupby('date')['nb_sold'].sum()\nnb_sold_by_date.head()\n\ndate\n2015-01-04    27279606.03\n2015-01-11    25081927.33\n2015-01-18    24961540.48\n2015-01-25    24094678.66\n2015-02-01    39838734.08\nName: nb_sold, dtype: float64\n\n\n\nnb_sold_by_date.plot(kind='line', x='date', y='nb_sold')\nplt.show()\n\n\n\n\nHere, it looks like the number of avocados spikes around the same time each year."
  },
  {
    "objectID": "posts/2020-10-19-avocados.html#avocado-supply-and-demand",
    "href": "posts/2020-10-19-avocados.html#avocado-supply-and-demand",
    "title": "Avocado",
    "section": "Avocado supply and demand",
    "text": "Avocado supply and demand\ncomparing the number of avocados sold to average price and see if they’re at all related. If they’re related, one number may be used to predict the other.\n\navocados.plot(kind='scatter', x='nb_sold', y='avg_price', title='Number of Avocados sold Vs. Average price')\nplt.show()\n\n\n\n\nIt looks like when more avocados are sold, prices go down. However, this doesn’t mean that fewer sales causes higher prices - we can only tell that they’re correlated with each other."
  },
  {
    "objectID": "posts/2020-10-19-avocados.html#price-of-conventional-vs.-organic-avocados",
    "href": "posts/2020-10-19-avocados.html#price-of-conventional-vs.-organic-avocados",
    "title": "Avocado",
    "section": "Price of conventional vs. organic avocados",
    "text": "Price of conventional vs. organic avocados\n\navocados[avocados['type']=='conventional']['avg_price'].hist(bins=20, alpha=0.5)\navocados[avocados['type']=='organic']['avg_price'].hist(bins=20, alpha=0.5)\nplt.legend(['conventional', 'organic'])\nplt.title='Price of Conventional vs. organic avocados'\nplt.show()\n\n\n\n\nWe can see that on average, organic avocados are more expensive than conventional ones, but their price distributions have some overlap."
  },
  {
    "objectID": "posts/2020-10-19-avocados.html#removing-missing-values",
    "href": "posts/2020-10-19-avocados.html#removing-missing-values",
    "title": "Avocado",
    "section": "Removing missing values",
    "text": "Removing missing values\nevery row containing missing values will be deleted\n\navocados_complete = avocados_2016.dropna()\navocados_complete.isna().any()\n\nUnnamed: 0         False\ndate               False\navg_price          False\ntotal_sold         False\nsmall_sold         False\nlarge_sold         False\nxl_sold            False\ntotal_bags_sold    False\nsmall_bags_sold    False\nlarge_bags_sold    False\nxl_bags_sold       False\ndtype: bool\n\n\n\navocados_2016[[\"small_sold\", \"large_sold\", \"xl_sold\"]].hist()\nplt.show()"
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html",
    "title": "Analyzing Police Activity with Pandas",
    "section": "",
    "text": "Before beginning our analysis, it is critical that we first examine and clean the dataset, to make working with it a more efficient process. We will fixing data types, handle missing values, and dropping columns and rows while exploring the Stanford Open Policing Project dataset.\n\n\n\n\nWe’ll be analyzing a dataset of traffic stops in Rhode Island that was collected by the Stanford Open Policing Project.\nBefore beginning our analysis, it’s important that we familiarize yourself with the dataset. We read the dataset into pandas, examine the first few rows, and then count the number of missing values.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\n\n\n# Read 'police.csv' into a DataFrame named ri\nri = pd.read_csv(\"../datasets/police.csv\")\n\n# Examine the head of the DataFrame\ndisplay(ri.head())\n\n# Count the number of missing values in each column\nri.isnull().sum()\n\n\n\n\n\n  \n    \n      \n      state\n      stop_date\n      stop_time\n      county_name\n      driver_gender\n      driver_race\n      violation_raw\n      violation\n      search_conducted\n      search_type\n      stop_outcome\n      is_arrested\n      stop_duration\n      drugs_related_stop\n      district\n    \n  \n  \n    \n      0\n      RI\n      2005-01-04\n      12:55\n      NaN\n      M\n      White\n      Equipment/Inspection Violation\n      Equipment\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n    \n    \n      1\n      RI\n      2005-01-23\n      23:15\n      NaN\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone K3\n    \n    \n      2\n      RI\n      2005-02-17\n      04:15\n      NaN\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n    \n    \n      3\n      RI\n      2005-02-20\n      17:15\n      NaN\n      M\n      White\n      Call for Service\n      Other\n      False\n      NaN\n      Arrest Driver\n      True\n      16-30 Min\n      False\n      Zone X1\n    \n    \n      4\n      RI\n      2005-02-24\n      01:20\n      NaN\n      F\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X3\n    \n  \n\n\n\n\nstate                     0\nstop_date                 0\nstop_time                 0\ncounty_name           91741\ndriver_gender          5205\ndriver_race            5202\nviolation_raw          5202\nviolation              5202\nsearch_conducted          0\nsearch_type           88434\nstop_outcome           5202\nis_arrested            5202\nstop_duration          5202\ndrugs_related_stop        0\ndistrict                  0\ndtype: int64\n\n\nIt looks like most of the columns have at least some missing values. We’ll figure out how to handle these values in the next.\n\n\n\n\nWe’ll drop the county_name column because it only contains missing values, and we’ll drop the state column because all of the traffic stops took place in one state (Rhode Island).\n\n# Examine the shape of the DataFrame\nprint(ri.shape)\n\n# Drop the 'county_name' and 'state' columns\nri.drop([\"county_name\", \"state\"], axis='columns', inplace=True)\n\n# Examine the shape of the DataFrame (again)\nprint(ri.shape)\n\n(91741, 15)\n(91741, 13)\n\n\nWe’ll continue to remove unnecessary data from the DataFrame\n### Dropping rows\nthe driver_gender column will be critical to many of our analyses. Because only a small fraction of rows are missing driver_gender, we’ll drop those rows from the dataset.\n\n# Count the number of missing values in each column\ndisplay(ri.isnull().sum())\n\n# Drop all rows that are missing 'driver_gender'\nri.dropna(subset=[\"driver_gender\"], inplace=True)\n\n# Count the number of missing values in each column (again)\ndisplay(ri.isnull().sum())\n\n# Examine the shape of the DataFrame\nri.shape\n\nstop_date                 0\nstop_time                 0\ndriver_gender          5205\ndriver_race            5202\nviolation_raw          5202\nviolation              5202\nsearch_conducted          0\nsearch_type           88434\nstop_outcome           5202\nis_arrested            5202\nstop_duration          5202\ndrugs_related_stop        0\ndistrict                  0\ndtype: int64\n\n\nstop_date                 0\nstop_time                 0\ndriver_gender             0\ndriver_race               0\nviolation_raw             0\nviolation                 0\nsearch_conducted          0\nsearch_type           83229\nstop_outcome              0\nis_arrested               0\nstop_duration             0\ndrugs_related_stop        0\ndistrict                  0\ndtype: int64\n\n\n(86536, 13)\n\n\nWe dropped around 5,000 rows, which is a small fraction of the dataset, and now only one column remains with any missing values.\n\n\n\n\n\n\n\nri.dtypes\n\nstop_date             object\nstop_time             object\ndriver_gender         object\ndriver_race           object\nviolation_raw         object\nviolation             object\nsearch_conducted        bool\nsearch_type           object\nstop_outcome          object\nis_arrested           object\nstop_duration         object\ndrugs_related_stop      bool\ndistrict              object\ndtype: object\n\n\n\nstop_date: should be datetime\nstop_time: should be datetime\ndriver_gender: should be category\ndriver_race: should be category\nviolation_raw: should be category\nviolation: should be category\ndistrict: should be category\nis_arrested: should be bool\n\nWe’ll fix the data type of the is_arrested column\n\n# Examine the head of the 'is_arrested' column\ndisplay(ri.is_arrested.head())\n\n# Change the data type of 'is_arrested' to 'bool'\nri['is_arrested'] = ri.is_arrested.astype('bool')\n\n# Check the data type of 'is_arrested' \nri.is_arrested.dtype\n\n0    False\n1    False\n2    False\n3     True\n4    False\nName: is_arrested, dtype: object\n\n\ndtype('bool')\n\n\n\n\n\n\n\n\nCurrently, the date and time of each traffic stop are stored in separate object columns: stop_date and stop_time. We’ll combine these two columns into a single column, and then convert it to datetime format.\n\nri['stop_date_time'] = pd.to_datetime(ri.stop_date.str.replace(\"/\", \"-\").str.cat(ri.stop_time, sep=\" \"))\nri.dtypes\n\nstop_date                     object\nstop_time                     object\ndriver_gender                 object\ndriver_race                   object\nviolation_raw                 object\nviolation                     object\nsearch_conducted                bool\nsearch_type                   object\nstop_outcome                  object\nis_arrested                     bool\nstop_duration                 object\ndrugs_related_stop              bool\ndistrict                      object\nstop_date_time        datetime64[ns]\ndtype: object\n\n\n\n\n\n\n# Set 'stop_datetime' as the index\nri.set_index(\"stop_date_time\", inplace=True)\n\n# Examine the index\ndisplay(ri.index)\n\n# Examine the columns\nri.columns\n\nDatetimeIndex(['2005-01-04 12:55:00', '2005-01-23 23:15:00',\n               '2005-02-17 04:15:00', '2005-02-20 17:15:00',\n               '2005-02-24 01:20:00', '2005-03-14 10:00:00',\n               '2005-03-29 21:55:00', '2005-04-04 21:25:00',\n               '2005-07-14 11:20:00', '2005-07-14 19:55:00',\n               ...\n               '2015-12-31 13:23:00', '2015-12-31 18:59:00',\n               '2015-12-31 19:13:00', '2015-12-31 20:20:00',\n               '2015-12-31 20:50:00', '2015-12-31 21:21:00',\n               '2015-12-31 21:59:00', '2015-12-31 22:04:00',\n               '2015-12-31 22:09:00', '2015-12-31 22:47:00'],\n              dtype='datetime64[ns]', name='stop_date_time', length=86536, freq=None)\n\n\nIndex(['stop_date', 'stop_time', 'driver_gender', 'driver_race',\n       'violation_raw', 'violation', 'search_conducted', 'search_type',\n       'stop_outcome', 'is_arrested', 'stop_duration', 'drugs_related_stop',\n       'district'],\n      dtype='object')"
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#do-the-genders-commit-different-violations",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#do-the-genders-commit-different-violations",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Do the genders commit different violations?",
    "text": "Do the genders commit different violations?\n\nExamining traffic violations\nBefore comparing the violations being committed by each gender, we should examine the violations committed by all drivers to get a baseline understanding of the data.\nWe’ll count the unique values in the violation column, and then separately express those counts as proportions.\n\n# Count the unique values in 'violation'\ndisplay(ri.violation.value_counts())\n\n# Express the counts as proportions\nri.violation.value_counts(normalize=True)\n\nSpeeding               48423\nMoving violation       16224\nEquipment              10921\nOther                   4409\nRegistration/plates     3703\nSeat belt               2856\nName: violation, dtype: int64\n\n\nSpeeding               0.559571\nMoving violation       0.187483\nEquipment              0.126202\nOther                  0.050950\nRegistration/plates    0.042791\nSeat belt              0.033004\nName: violation, dtype: float64\n\n\nMore than half of all violations are for speeding, followed by other moving violations and equipment violations.\n\n\nComparing violations by gender\nThe question we’re trying to answer is whether male and female drivers tend to commit different types of traffic violations.\nWe’ll first create a DataFrame for each gender, and then analyze the violations in each DataFrame separately.\n\n# Create a DataFrame of female drivers\nfemale = ri[ri.driver_gender==\"F\"]\n\n# Create a DataFrame of male drivers\nmale = ri[ri.driver_gender==\"M\"]\n\n# Compute the violations by female drivers (as proportions)\ndisplay(female.violation.value_counts(normalize=True))\n\n# Compute the violations by male drivers (as proportions)\nmale.violation.value_counts(normalize=True)\n\nSpeeding               0.658114\nMoving violation       0.138218\nEquipment              0.105199\nRegistration/plates    0.044418\nOther                  0.029738\nSeat belt              0.024312\nName: violation, dtype: float64\n\n\nSpeeding               0.522243\nMoving violation       0.206144\nEquipment              0.134158\nOther                  0.058985\nRegistration/plates    0.042175\nSeat belt              0.036296\nName: violation, dtype: float64\n\n\nAbout two-thirds of female traffic stops are for speeding, whereas stops of males are more balanced among the six categories. This doesn’t mean that females speed more often than males, however, since we didn’t take into account the number of stops or drivers."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#does-gender-affect-who-gets-a-ticket-for-speeding",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#does-gender-affect-who-gets-a-ticket-for-speeding",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does gender affect who gets a ticket for speeding?",
    "text": "Does gender affect who gets a ticket for speeding?\n\nComparing speeding outcomes by gender\nWhen a driver is pulled over for speeding, many people believe that gender has an impact on whether the driver will receive a ticket or a warning. Can we find evidence of this in the dataset?\nFirst, we’ll create two DataFrames of drivers who were stopped for speeding: one containing females and the other containing males.\nThen, for each gender, we’ll use the stop_outcome column to calculate what percentage of stops resulted in a “Citation” (meaning a ticket) versus a “Warning”.\n\n# Create a DataFrame of female drivers stopped for speeding\nfemale_and_speeding = ri[(ri.driver_gender==\"F\") & (ri.violation ==\"Speeding\")]\n\n# Create a DataFrame of male drivers stopped for speeding\nmale_and_speeding = ri[(ri.driver_gender==\"M\") & (ri.violation ==\"Speeding\")]\n\n# Compute the stop outcomes for female drivers (as proportions)\ndisplay(female_and_speeding.stop_outcome.value_counts(normalize=True))\n\n# Compute the stop outcomes for male drivers (as proportions)\nmale_and_speeding.stop_outcome.value_counts(normalize=True)\n\nCitation            0.952192\nWarning             0.040074\nArrest Driver       0.005752\nN/D                 0.000959\nArrest Passenger    0.000639\nNo Action           0.000383\nName: stop_outcome, dtype: float64\n\n\nCitation            0.944595\nWarning             0.036184\nArrest Driver       0.015895\nArrest Passenger    0.001281\nNo Action           0.001068\nN/D                 0.000976\nName: stop_outcome, dtype: float64\n\n\nThe numbers are similar for males and females: about 95% of stops for speeding result in a ticket. Thus, the data fails to show that gender has an impact on who gets a ticket for speeding.\n## Does gender affect whose vehicle is searched? ### Calculating the search rate\nDuring a traffic stop, the police officer sometimes conducts a search of the vehicle. We’ll calculate the percentage of all stops in the ri DataFrame that result in a vehicle search, also known as the search rate.\n\n# Check the data type of 'search_conducted'\nprint(ri.search_conducted.dtype)\n\n# Calculate the search rate by counting the values\ndisplay(ri.search_conducted.value_counts(normalize=True))\n\n# Calculate the search rate by taking the mean\nri.search_conducted.mean()\n\nbool\n\n\nFalse    0.961785\nTrue     0.038215\nName: search_conducted, dtype: float64\n\n\n0.0382153092354627\n\n\nIt looks like the search rate is about 3.8%. Next, we’ll examine whether the search rate varies by driver gender.\n### Comparing search rates by gender\nWe’ll compare the rates at which female and male drivers are searched during a traffic stop. Remember that the vehicle search rate across all stops is about 3.8%.\nFirst, we’ll filter the DataFrame by gender and calculate the search rate for each group separately. Then, we’ll perform the same calculation for both genders at once using a .groupby().\n\nri[ri.driver_gender==\"F\"].search_conducted.mean()\n\n0.019180617481282074\n\n\n\nri[ri.driver_gender==\"M\"].search_conducted.mean()\n\n0.04542557598546892\n\n\n\nri.groupby(\"driver_gender\").search_conducted.mean()\n\ndriver_gender\nF    0.019181\nM    0.045426\nName: search_conducted, dtype: float64\n\n\nMale drivers are searched more than twice as often as female drivers. Why might this be?\n\n\nAdding a second factor to the analysis\nEven though the search rate for males is much higher than for females, it’s possible that the difference is mostly due to a second factor.\nFor example, we might hypothesize that the search rate varies by violation type, and the difference in search rate between males and females is because they tend to commit different violations.\nwe can test this hypothesis by examining the search rate for each combination of gender and violation. If the hypothesis was true, out would find that males and females are searched at about the same rate for each violation. Let’s find out below if that’s the case!\n\n# Calculate the search rate for each combination of gender and violation\nri.groupby([\"driver_gender\", \"violation\"]).search_conducted.mean()\n\ndriver_gender  violation          \nF              Equipment              0.039984\n               Moving violation       0.039257\n               Other                  0.041018\n               Registration/plates    0.054924\n               Seat belt              0.017301\n               Speeding               0.008309\nM              Equipment              0.071496\n               Moving violation       0.061524\n               Other                  0.046191\n               Registration/plates    0.108802\n               Seat belt              0.035119\n               Speeding               0.027885\nName: search_conducted, dtype: float64\n\n\n\nri.groupby([\"violation\", \"driver_gender\"]).search_conducted.mean()\n\nviolation            driver_gender\nEquipment            F                0.039984\n                     M                0.071496\nMoving violation     F                0.039257\n                     M                0.061524\nOther                F                0.041018\n                     M                0.046191\nRegistration/plates  F                0.054924\n                     M                0.108802\nSeat belt            F                0.017301\n                     M                0.035119\nSpeeding             F                0.008309\n                     M                0.027885\nName: search_conducted, dtype: float64\n\n\nFor all types of violations, the search rate is higher for males than for females, disproving our hypothesis."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#does-gender-affect-who-is-frisked-during-a-search",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#does-gender-affect-who-is-frisked-during-a-search",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does gender affect who is frisked during a search?",
    "text": "Does gender affect who is frisked during a search?\n\nCounting protective frisks\nDuring a vehicle search, the police officer may pat down the driver to check if they have a weapon. This is known as a “protective frisk.”\nWe’ll first check to see how many times “Protective Frisk” was the only search type. Then, we’ll use a string method to locate all instances in which the driver was frisked.\n\n# Count the 'search_type' values\ndisplay(ri.search_type.value_counts())\n\n# Check if 'search_type' contains the string 'Protective Frisk'\nri['frisk'] = ri.search_type.str.contains('Protective Frisk', na=False)\n\n# Check the data type of 'frisk'\nprint(ri.frisk.dtype)\n\n# Take the sum of 'frisk'\nprint(ri.frisk.sum())\n\nIncident to Arrest                                          1290\nProbable Cause                                               924\nInventory                                                    219\nReasonable Suspicion                                         214\nProtective Frisk                                             164\nIncident to Arrest,Inventory                                 123\nIncident to Arrest,Probable Cause                            100\nProbable Cause,Reasonable Suspicion                           54\nProbable Cause,Protective Frisk                               35\nIncident to Arrest,Inventory,Probable Cause                   35\nIncident to Arrest,Protective Frisk                           33\nInventory,Probable Cause                                      25\nProtective Frisk,Reasonable Suspicion                         19\nIncident to Arrest,Inventory,Protective Frisk                 18\nIncident to Arrest,Probable Cause,Protective Frisk            13\nInventory,Protective Frisk                                    12\nIncident to Arrest,Reasonable Suspicion                        8\nProbable Cause,Protective Frisk,Reasonable Suspicion           5\nIncident to Arrest,Probable Cause,Reasonable Suspicion         5\nIncident to Arrest,Inventory,Reasonable Suspicion              4\nIncident to Arrest,Protective Frisk,Reasonable Suspicion       2\nInventory,Reasonable Suspicion                                 2\nInventory,Probable Cause,Protective Frisk                      1\nInventory,Probable Cause,Reasonable Suspicion                  1\nInventory,Protective Frisk,Reasonable Suspicion                1\nName: search_type, dtype: int64\n\n\nbool\n303\n\n\nIt looks like there were 303 drivers who were frisked. Next, we’ll examine whether gender affects who is frisked.\n\n\nComparing frisk rates by gender\nWe’ll compare the rates at which female and male drivers are frisked during a search. Are males frisked more often than females, perhaps because police officers consider them to be higher risk?\nBefore doing any calculations, it’s important to filter the DataFrame to only include the relevant subset of data, namely stops in which a search was conducted.\n\n# Create a DataFrame of stops in which a search was conducted\nsearched = ri[ri.search_conducted == True]\n\n# Calculate the overall frisk rate by taking the mean of 'frisk'\nprint(searched.frisk.mean())\n\n# Calculate the frisk rate for each gender\nsearched.groupby(\"driver_gender\").frisk.mean()\n\n0.09162382824312065\n\n\ndriver_gender\nF    0.074561\nM    0.094353\nName: frisk, dtype: float64\n\n\nThe frisk rate is higher for males than for females, though we can’t conclude that this difference is caused by the driver’s gender."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#does-time-of-the-day-affect-arrest-rate",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#does-time-of-the-day-affect-arrest-rate",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does time of the day affect arrest rate?",
    "text": "Does time of the day affect arrest rate?\n\nCalculating the hourly arrest rate\nWhen a police officer stops a driver, a small percentage of those stops ends in an arrest. This is known as the arrest rate. We’ll find out whether the arrest rate varies by time of day.\nFirst, we’ll calculate the arrest rate across all stops in the ri DataFrame. Then, we’ll calculate the hourly arrest rate by using the hour attribute of the index. The hour ranges from 0 to 23, in which:\n\n0 = midnight\n12 = noon\n23 = 11 PM\n\n\n# Calculate the overall arrest rate\nprint(ri.is_arrested.mean())\n\n# Calculate the hourly arrest rate\n\n# Save the hourly arrest rate\nhourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()\nhourly_arrest_rate\n\n0.0355690117407784\n\n\nstop_date_time\n0     0.051431\n1     0.064932\n2     0.060798\n3     0.060549\n4     0.048000\n5     0.042781\n6     0.013813\n7     0.013032\n8     0.021854\n9     0.025206\n10    0.028213\n11    0.028897\n12    0.037399\n13    0.030776\n14    0.030605\n15    0.030679\n16    0.035281\n17    0.040619\n18    0.038204\n19    0.032245\n20    0.038107\n21    0.064541\n22    0.048666\n23    0.047592\nName: is_arrested, dtype: float64\n\n\nNext we’ll plot the data so that you can visually examine the arrest rate trends.\n### Plotting the hourly arrest rate\nWe’ll create a line plot from the hourly_arrest_rate object.\n\n\n\n\n\n\nImportant\n\n\n\nA line plot is appropriate in this case because you’re showing how a quantity changes over time.\n\n\nThis plot should help us to spot some trends that may not have been obvious when examining the raw numbers!\n\n# Create a line plot of 'hourly_arrest_rate'\nhourly_arrest_rate.plot()\n\n# Add the xlabel, ylabel, and title\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Arrest Rate\")\nplt.title(\"Arrest Rate by Time of Day\")\n\n# Display the plot\nplt.show()\n\n\n\n\nThe arrest rate has a significant spike overnight, and then dips in the early morning hours.\n## Are drug-related stops on the rise?\n\n\nPlotting drug-related stops\nIn a small portion of traffic stops, drugs are found in the vehicle during a search. In this exercise, you’ll assess whether these drug-related stops are becoming more common over time.\nThe Boolean column drugs_related_stop indicates whether drugs were found during a given stop. We’ll calculate the annual drug rate by resampling this column, and then we’ll use a line plot to visualize how the rate has changed over time.\n\n# Calculate the annual rate of drug-related stops\n# Save the annual rate of drug-related stops\nannual_drug_rate = ri.drugs_related_stop.resample(\"A\").mean()\ndisplay(annual_drug_rate)\n\n# Create a line plot of 'annual_drug_rate'\nannual_drug_rate.plot()\n\n# Display the plot\nplt.show()\n\nstop_date_time\n2005-12-31    0.006501\n2006-12-31    0.007258\n2007-12-31    0.007970\n2008-12-31    0.007505\n2009-12-31    0.009889\n2010-12-31    0.010081\n2011-12-31    0.009731\n2012-12-31    0.009921\n2013-12-31    0.013094\n2014-12-31    0.013826\n2015-12-31    0.012266\nFreq: A-DEC, Name: drugs_related_stop, dtype: float64\n\n\n\n\n\nThe rate of drug-related stops nearly doubled over the course of 10 years. Why might that be the case?\n\n\nComparing drug and search rates\nThe rate of drug-related stops increased significantly between 2005 and 2015. We might hypothesize that the rate of vehicle searches was also increasing, which would have led to an increase in drug-related stops even if more drivers were not carrying drugs.\nWe can test this hypothesis by calculating the annual search rate, and then plotting it against the annual drug rate. If the hypothesis is true, then we’ll see both rates increasing over time.\n\n# Calculate and save the annual search rate\nannual_search_rate = ri.search_conducted.resample(\"A\").mean()\n\n# Concatenate 'annual_drug_rate' and 'annual_search_rate'\nannual = pd.concat([annual_drug_rate, annual_search_rate], axis=\"columns\")\n\n# Create subplots from 'annual'\nannual.plot(subplots=True)\n\n# Display the subplots\nplt.show()\n\n\n\n\nThe rate of drug-related stops increased even though the search rate decreased, disproving our hypothesis."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#what-violations-are-caught-in-each-district",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#what-violations-are-caught-in-each-district",
    "title": "Analyzing Police Activity with Pandas",
    "section": "What violations are caught in each district?",
    "text": "What violations are caught in each district?\n\nTallying violations by district\nThe state of Rhode Island is broken into six police districts, also known as zones. How do the zones compare in terms of what violations are caught by police?\nWe’ll create a frequency table to determine how many violations of each type took place in each of the six zones. Then, we’ll filter the table to focus on the “K” zones, which we’ll examine further.\n\n# Create a frequency table of districts and violations\n# Save the frequency table as 'all_zones'\nall_zones = pd.crosstab(ri.district, ri.violation)\ndisplay(all_zones)\n\n# Select rows 'Zone K1' through 'Zone K3'\n# Save the smaller table as 'k_zones'\nk_zones = all_zones.loc[\"Zone K1\":\"Zone K3\"]\nk_zones\n\n\n\n\n\n  \n    \n      violation\n      Equipment\n      Moving violation\n      Other\n      Registration/plates\n      Seat belt\n      Speeding\n    \n    \n      district\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Zone K1\n      672\n      1254\n      290\n      120\n      0\n      5960\n    \n    \n      Zone K2\n      2061\n      2962\n      942\n      768\n      481\n      10448\n    \n    \n      Zone K3\n      2302\n      2898\n      705\n      695\n      638\n      12322\n    \n    \n      Zone X1\n      296\n      671\n      143\n      38\n      74\n      1119\n    \n    \n      Zone X3\n      2049\n      3086\n      769\n      671\n      820\n      8779\n    \n    \n      Zone X4\n      3541\n      5353\n      1560\n      1411\n      843\n      9795\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      violation\n      Equipment\n      Moving violation\n      Other\n      Registration/plates\n      Seat belt\n      Speeding\n    \n    \n      district\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Zone K1\n      672\n      1254\n      290\n      120\n      0\n      5960\n    \n    \n      Zone K2\n      2061\n      2962\n      942\n      768\n      481\n      10448\n    \n    \n      Zone K3\n      2302\n      2898\n      705\n      695\n      638\n      12322\n    \n  \n\n\n\n\nWe’ll plot the violations so that you can compare these districts.\n\n\nPlotting violations by district\nNow that we’ve created a frequency table focused on the “K” zones, we’ll visualize the data to help us compare what violations are being caught in each zone.\nFirst we’ll create a bar plot, which is an appropriate plot type since we’re comparing categorical data. Then we’ll create a stacked bar plot in order to get a slightly different look at the data.\n\n# Create a bar plot of 'k_zones'\nk_zones.plot(kind=\"bar\")\n\n# Display the plot\nplt.show()\n\n\n\n\n\n# Create a stacked bar plot of 'k_zones'\nk_zones.plot(kind=\"bar\", stacked=True)\n\n# Display the plot\nplt.show()\n\n\n\n\nThe vast majority of traffic stops in Zone K1 are for speeding, and Zones K2 and K3 are remarkably similar to one another in terms of violations."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#how-long-might-you-be-stopped-for-a-violation",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#how-long-might-you-be-stopped-for-a-violation",
    "title": "Analyzing Police Activity with Pandas",
    "section": "How long might you be stopped for a violation?",
    "text": "How long might you be stopped for a violation?\n\nConverting stop durations to numbers\nIn the traffic stops dataset, the stop_duration column tells us approximately how long the driver was detained by the officer. Unfortunately, the durations are stored as strings, such as '0-15 Min'. How can we make this data easier to analyze?\nWe’ll convert the stop durations to integers. Because the precise durations are not available, we’ll have to estimate the numbers using reasonable values:\n\nConvert '0-15 Min' to 8\nConvert '16-30 Min' to 23\nConvert '30+ Min' to 45\n\n\n# Create a dictionary that maps strings to integers\nmapping = {\"0-15 Min\":8, '16-30 Min':23, '30+ Min':45}\n\n# Convert the 'stop_duration' strings to integers using the 'mapping'\nri['stop_minutes'] = ri.stop_duration.map(mapping)\n\n# Print the unique values in 'stop_minutes'\nri.stop_minutes.unique()\n\narray([ 8, 23, 45], dtype=int64)\n\n\nNext we’ll analyze the stop length for each type of violation.\n\n\nPlotting stop length\nIf you were stopped for a particular violation, how long might you expect to be detained?\nWe’ll visualize the average length of time drivers are stopped for each type of violation. Rather than using the violation column we’ll use violation_raw since it contains more detailed descriptions of the violations.\n\n# Calculate the mean 'stop_minutes' for each value in 'violation_raw'\n# Save the resulting Series as 'stop_length'\nstop_length = ri.groupby(\"violation_raw\").stop_minutes.mean()\ndisplay(stop_length)\n\n# Sort 'stop_length' by its values and create a horizontal bar plot\nstop_length.sort_values().plot(kind=\"barh\")\n\n# Display the plot\nplt.show()\n\nviolation_raw\nAPB                                 17.967033\nCall for Service                    22.124371\nEquipment/Inspection Violation      11.445655\nMotorist Assist/Courtesy            17.741463\nOther Traffic Violation             13.844490\nRegistration Violation              13.736970\nSeatbelt Violation                   9.662815\nSpecial Detail/Directed Patrol      15.123632\nSpeeding                            10.581562\nSuspicious Person                   14.910714\nViolation of City/Town Ordinance    13.254144\nWarrant                             24.055556\nName: stop_minutes, dtype: float64"
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#exploring-the-weather-dataset",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#exploring-the-weather-dataset",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Exploring the weather dataset",
    "text": "Exploring the weather dataset\n\nPlotting the temperature\nWe’ll examine the temperature columns from the weather dataset to assess whether the data seems trustworthy. First we’ll print the summary statistics, and then you’ll visualize the data using a box plot.\n\n# Read 'weather.csv' into a DataFrame named 'weather'\nweather = pd.read_csv(\"../datasets/weather.csv\")\ndisplay(weather.head())\n\n# Describe the temperature columns\ndisplay(weather[[\"TMIN\", \"TAVG\", \"TMAX\"]].describe().T)\n\n# Create a box plot of the temperature columns\nweather[[\"TMIN\", \"TAVG\", \"TMAX\"]].plot(kind='box')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n  \n    \n      \n      STATION\n      DATE\n      TAVG\n      TMIN\n      TMAX\n      AWND\n      WSF2\n      WT01\n      WT02\n      WT03\n      ...\n      WT11\n      WT13\n      WT14\n      WT15\n      WT16\n      WT17\n      WT18\n      WT19\n      WT21\n      WT22\n    \n  \n  \n    \n      0\n      USW00014765\n      2005-01-01\n      44.0\n      35\n      53\n      8.95\n      25.1\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      USW00014765\n      2005-01-02\n      36.0\n      28\n      44\n      9.40\n      14.1\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      USW00014765\n      2005-01-03\n      49.0\n      44\n      53\n      6.93\n      17.0\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      NaN\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      USW00014765\n      2005-01-04\n      42.0\n      39\n      45\n      6.93\n      16.1\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      1.0\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      USW00014765\n      2005-01-05\n      36.0\n      28\n      43\n      7.83\n      17.0\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      NaN\n      NaN\n      1.0\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 27 columns\n\n\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      TMIN\n      4017.0\n      43.484441\n      17.020298\n      -5.0\n      30.0\n      44.0\n      58.0\n      77.0\n    \n    \n      TAVG\n      1217.0\n      52.493016\n      17.830714\n      6.0\n      39.0\n      54.0\n      68.0\n      86.0\n    \n    \n      TMAX\n      4017.0\n      61.268608\n      18.199517\n      15.0\n      47.0\n      62.0\n      77.0\n      102.0\n    \n  \n\n\n\n\n\n\n\nThe temperature data looks good so far: the TAVG values are in between TMIN and TMAX, and the measurements and ranges seem reasonable.\n### Plotting the temperature difference\nWe’ll continue to assess whether the dataset seems trustworthy by plotting the difference between the maximum and minimum temperatures.\n\n# Create a 'TDIFF' column that represents temperature difference\nweather[\"TDIFF\"] = weather.TMAX - weather.TMIN\n\n# Describe the 'TDIFF' column\ndisplay(weather.TDIFF.describe())\n\n# Create a histogram with 20 bins to visualize 'TDIFF'\nweather.TDIFF.plot(kind=\"hist\", bins=20)\n\n# Display the plot\nplt.show()\n\ncount    4017.000000\nmean       17.784167\nstd         6.350720\nmin         2.000000\n25%        14.000000\n50%        18.000000\n75%        22.000000\nmax        43.000000\nName: TDIFF, dtype: float64\n\n\n\n\n\nThe TDIFF column has no negative values and its distribution is approximately normal, both of which are signs that the data is trustworthy."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#categorizing-the-weather",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#categorizing-the-weather",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Categorizing the weather",
    "text": "Categorizing the weather\n\nCounting bad weather conditions\nThe weather DataFrame contains 20 columns that start with 'WT', each of which represents a bad weather condition. For example:\n\nWT05 indicates “Hail”\nWT11 indicates “High or damaging winds”\nWT17 indicates “Freezing rain”\n\nFor every row in the dataset, each WT column contains either a 1 (meaning the condition was present that day) or NaN (meaning the condition was not present).\nWe’ll quantify “how bad” the weather was each day by counting the number of 1 values in each row.\n\n# Copy 'WT01' through 'WT22' to a new DataFrame\nWT = weather.loc[:, \"WT01\":\"WT22\"]\n\n# Calculate the sum of each row in 'WT'\nweather['bad_conditions'] = WT.sum(axis=\"columns\")\n\n# Replace missing values in 'bad_conditions' with '0'\nweather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')\n\n# Create a histogram to visualize 'bad_conditions'\nweather.bad_conditions.plot(kind=\"hist\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIt looks like many days didn’t have any bad weather conditions, and only a small portion of days had more than four bad weather conditions.\n\n\nRating the weather conditions\nWe counted the number of bad weather conditions each day. We’ll use the counts to create a rating system for the weather.\nThe counts range from 0 to 9, and should be converted to ratings as follows:\n\nConvert 0 to ‘good’\nConvert 1 through 4 to ‘bad’\nConvert 5 through 9 to ‘worse’\n\n\n# Count the unique values in 'bad_conditions' and sort the index\ndisplay(weather.bad_conditions.value_counts().sort_index())\n\n# Create a dictionary that maps integers to strings\nmapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad', 5:'worse', 6:'worse', 7:'worse', 8:'worse', 9:'worse'}\n\n# Convert the 'bad_conditions' integers to strings using the 'mapping'\nweather['rating'] = weather.bad_conditions.map(mapping)\n\n# Count the unique values in 'rating'\nweather.rating.value_counts()\n\n0    1749\n1     613\n2     367\n3     380\n4     476\n5     282\n6     101\n7      41\n8       4\n9       4\nName: bad_conditions, dtype: int64\n\n\nbad      1836\ngood     1749\nworse     432\nName: rating, dtype: int64\n\n\n\n\nChanging the data type to category\nSince the rating column only has a few possible values, we’ll change its data type to category in order to store the data more efficiently. we’ll also specify a logical order for the categories, which will be useful for future work.\n\n# Create a list of weather ratings in logical order\ncats = ['good', 'bad', 'worse']\n# Change the data type of 'rating' to category\nweather['rating'] = weather.rating.astype(CategoricalDtype(ordered=True, categories=cats))\n\n# Examine the head of 'rating'\nweather.rating.head()\n\n0    bad\n1    bad\n2    bad\n3    bad\n4    bad\nName: rating, dtype: category\nCategories (3, object): [good < bad < worse]\n\n\nWe’ll use the rating column in future exercises to analyze the effects of weather on police behavior."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#merging-datasets",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#merging-datasets",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Merging datasets",
    "text": "Merging datasets\n\nPreparing the DataFrames\nWe’ll prepare the traffic stop and weather rating DataFrames so that they’re ready to be merged:\n\nWith the ri DataFrame, we’ll move the stop_datetime index to a column since the index will be lost during the merge.\nWith the weather DataFrame, we’ll select the DATE and rating columns and put them in a new DataFrame.\n\n\n# Reset the index of 'ri'\nri.reset_index(inplace=True)\n\n# Examine the head of 'ri'\ndisplay(ri.head())\n\n# Create a DataFrame from the 'DATE' and 'rating' columns\nweather_rating = weather[[\"DATE\", \"rating\"]]\n\n# Examine the head of 'weather_rating'\nweather_rating.head()\n\n\n\n\n\n  \n    \n      \n      stop_date_time\n      stop_date\n      stop_time\n      driver_gender\n      driver_race\n      violation_raw\n      violation\n      search_conducted\n      search_type\n      stop_outcome\n      is_arrested\n      stop_duration\n      drugs_related_stop\n      district\n      frisk\n      stop_minutes\n    \n  \n  \n    \n      0\n      2005-01-04 12:55:00\n      2005-01-04\n      12:55\n      M\n      White\n      Equipment/Inspection Violation\n      Equipment\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n    \n    \n      1\n      2005-01-23 23:15:00\n      2005-01-23\n      23:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone K3\n      False\n      8\n    \n    \n      2\n      2005-02-17 04:15:00\n      2005-02-17\n      04:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n    \n    \n      3\n      2005-02-20 17:15:00\n      2005-02-20\n      17:15\n      M\n      White\n      Call for Service\n      Other\n      False\n      NaN\n      Arrest Driver\n      True\n      16-30 Min\n      False\n      Zone X1\n      False\n      23\n    \n    \n      4\n      2005-02-24 01:20:00\n      2005-02-24\n      01:20\n      F\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X3\n      False\n      8\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      DATE\n      rating\n    \n  \n  \n    \n      0\n      2005-01-01\n      bad\n    \n    \n      1\n      2005-01-02\n      bad\n    \n    \n      2\n      2005-01-03\n      bad\n    \n    \n      3\n      2005-01-04\n      bad\n    \n    \n      4\n      2005-01-05\n      bad\n    \n  \n\n\n\n\nThe ri and weather_rating DataFrames are now ready to be merged.\n\n\nMerging the DataFrames\nWe’ll merge the ri and weather_rating DataFrames into a new DataFrame, ri_weather.\nThe DataFrames will be joined using the stop_date column from ri and the DATE column from weather_rating. Thankfully the date formatting matches exactly, which is not always the case!\nOnce the merge is complete, we’ll set stop_datetime as the index\n\n# Examine the shape of 'ri'\nprint(ri.shape)\n\n# Merge 'ri' and 'weather_rating' using a left join\nri_weather = pd.merge(left=ri, right=weather_rating, left_on='stop_date', right_on='DATE', how='left')\n\n# Examine the shape of 'ri_weather'\nprint(ri_weather.shape)\n\n# Set 'stop_datetime' as the index of 'ri_weather'\nri_weather.set_index('stop_date_time', inplace=True)\nri_weather.head()\n\n(86536, 16)\n(86536, 18)\n\n\n\n\n\n\n  \n    \n      \n      stop_date\n      stop_time\n      driver_gender\n      driver_race\n      violation_raw\n      violation\n      search_conducted\n      search_type\n      stop_outcome\n      is_arrested\n      stop_duration\n      drugs_related_stop\n      district\n      frisk\n      stop_minutes\n      DATE\n      rating\n    \n    \n      stop_date_time\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2005-01-04 12:55:00\n      2005-01-04\n      12:55\n      M\n      White\n      Equipment/Inspection Violation\n      Equipment\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n      2005-01-04\n      bad\n    \n    \n      2005-01-23 23:15:00\n      2005-01-23\n      23:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone K3\n      False\n      8\n      2005-01-23\n      worse\n    \n    \n      2005-02-17 04:15:00\n      2005-02-17\n      04:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n      2005-02-17\n      good\n    \n    \n      2005-02-20 17:15:00\n      2005-02-20\n      17:15\n      M\n      White\n      Call for Service\n      Other\n      False\n      NaN\n      Arrest Driver\n      True\n      16-30 Min\n      False\n      Zone X1\n      False\n      23\n      2005-02-20\n      bad\n    \n    \n      2005-02-24 01:20:00\n      2005-02-24\n      01:20\n      F\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X3\n      False\n      8\n      2005-02-24\n      bad\n    \n  \n\n\n\n\nWe’ll use ri_weather to analyze the relationship between weather conditions and police behavior."
  },
  {
    "objectID": "posts/2020-09-28-analyzing police activity with pandas.html#does-weather-affect-the-arrest-rate",
    "href": "posts/2020-09-28-analyzing police activity with pandas.html#does-weather-affect-the-arrest-rate",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does weather affect the arrest rate?",
    "text": "Does weather affect the arrest rate?\n\nComparing arrest rates by weather rating\nDo police officers arrest drivers more often when the weather is bad? Let’s find out below!\n\nFirst, we’ll calculate the overall arrest rate.\nThen, we’ll calculate the arrest rate for each of the weather ratings we previously assigned.\nFinally, we’ll add violation type as a second factor in the analysis, to see if that accounts for any differences in the arrest rate.\n\nSince we previously defined a logical order for the weather categories, good < bad < worse, they will be sorted that way in the results.\n\n# Calculate the overall arrest rate\nprint(ri_weather.is_arrested.mean())\n\n0.0355690117407784\n\n\n\n# Calculate the arrest rate for each 'rating'\nri_weather.groupby(\"rating\").is_arrested.mean()\n\nrating\ngood     0.033715\nbad      0.036261\nworse    0.041667\nName: is_arrested, dtype: float64\n\n\n\n# Calculate the arrest rate for each 'violation' and 'rating'\nri_weather.groupby([\"violation\", 'rating']).is_arrested.mean()\n\nviolation            rating\nEquipment            good      0.059007\n                     bad       0.066311\n                     worse     0.097357\nMoving violation     good      0.056227\n                     bad       0.058050\n                     worse     0.065860\nOther                good      0.076966\n                     bad       0.087443\n                     worse     0.062893\nRegistration/plates  good      0.081574\n                     bad       0.098160\n                     worse     0.115625\nSeat belt            good      0.028587\n                     bad       0.022493\n                     worse     0.000000\nSpeeding             good      0.013405\n                     bad       0.013314\n                     worse     0.016886\nName: is_arrested, dtype: float64\n\n\nThe arrest rate increases as the weather gets worse, and that trend persists across many of the violation types. This doesn’t prove a causal link, but it’s quite an interesting result!\n\n\nSelecting from a multi-indexed Series\nThe output of a single .groupby() operation on multiple columns is a Series with a MultiIndex. Working with this type of object is similar to working with a DataFrame:\n\nThe outer index level is like the DataFrame rows.\nThe inner index level is like the DataFrame columns.\n\n\n# Save the output of the groupby operation from the last exercise\narrest_rate = ri_weather.groupby(['violation', 'rating']).is_arrested.mean()\n\n\n# Print the arrest rate for moving violations in bad weather\ndisplay(arrest_rate.loc[\"Moving violation\", \"bad\"])\n\n# Print the arrest rates for speeding violations in all three weather conditions\narrest_rate.loc[\"Speeding\"]\n\n0.05804964058049641\n\n\nrating\ngood     0.013405\nbad      0.013314\nworse    0.016886\nName: is_arrested, dtype: float64\n\n\n\n\nReshaping the arrest rate data\nWe’ll start by reshaping the arrest_rate Series into a DataFrame. This is a useful step when working with any multi-indexed Series, since it enables you to access the full range of DataFrame methods.\nThen, we’ll create the exact same DataFrame using a pivot table. This is a great example of how pandas often gives you more than one way to reach the same result!\n\n# Unstack the 'arrest_rate' Series into a DataFrame\ndisplay(arrest_rate.unstack())\n\n# Create the same DataFrame using a pivot table\nri_weather.pivot_table(index='violation', columns='rating', values='is_arrested')\n\n\n\n\n\n  \n    \n      rating\n      good\n      bad\n      worse\n    \n    \n      violation\n      \n      \n      \n    \n  \n  \n    \n      Equipment\n      0.059007\n      0.066311\n      0.097357\n    \n    \n      Moving violation\n      0.056227\n      0.058050\n      0.065860\n    \n    \n      Other\n      0.076966\n      0.087443\n      0.062893\n    \n    \n      Registration/plates\n      0.081574\n      0.098160\n      0.115625\n    \n    \n      Seat belt\n      0.028587\n      0.022493\n      0.000000\n    \n    \n      Speeding\n      0.013405\n      0.013314\n      0.016886\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      rating\n      good\n      bad\n      worse\n    \n    \n      violation\n      \n      \n      \n    \n  \n  \n    \n      Equipment\n      0.059007\n      0.066311\n      0.097357\n    \n    \n      Moving violation\n      0.056227\n      0.058050\n      0.065860\n    \n    \n      Other\n      0.076966\n      0.087443\n      0.062893\n    \n    \n      Registration/plates\n      0.081574\n      0.098160\n      0.115625\n    \n    \n      Seat belt\n      0.028587\n      0.022493\n      0.000000\n    \n    \n      Speeding\n      0.013405\n      0.013314\n      0.016886"
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html",
    "href": "posts/2020-06-15-exploring boston weather data.html",
    "title": "Exploring Boston Weather Data",
    "section": "",
    "text": "weather = readRDS(gzcon(url('https://assets.datacamp.com/production/repositories/34/datasets/b3c1036d9a60a9dfe0f99051d2474a54f76055ea/weather.rds')))"
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#column-names-are-values",
    "href": "posts/2020-06-15-exploring boston weather data.html#column-names-are-values",
    "title": "Exploring Boston Weather Data",
    "section": "Column names are values",
    "text": "Column names are values\nThe weather dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day.\nThe tidyr package provides the gather() function for exactly this scenario.\ngather(df, time, val, t1:t3)\n\ngather() allows us to select multiple columns to be gathered by using the : operator.\n\n\n# Gather the columns\nweather2 <- gather(weather, day, value, X1:X31, na.rm = TRUE)\n\n# View the head\nhead(weather2)\n\n\n\nXyearmonthmeasuredayvalue\n\n    1                2014             12               Max.TemperatureF X1               64               \n    2                2014             12               Mean.TemperatureFX1               52               \n    3                2014             12               Min.TemperatureF X1               39               \n    4                2014             12               Max.Dew.PointF   X1               46               \n    5                2014             12               MeanDew.PointF   X1               40               \n    6                2014             12               Min.DewpointF    X1               26"
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#values-are-variable-names",
    "href": "posts/2020-06-15-exploring boston weather data.html#values-are-variable-names",
    "title": "Exploring Boston Weather Data",
    "section": "Values are variable names",
    "text": "Values are variable names\nOur data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset.\nThe spread() function from tidyr is designed to help with this.\n\nspread(df2, time, val)\n\n# First remove column of row names\nwithout_x <- weather2[, -1]\n\n# Spread the data\nweather3 <- spread(without_x, measure, value)\n\n# View the head\nhead(weather3)\n\n\n\nyearmonthdayCloudCoverEventsMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureF...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees\n\n    2014     12       X1       6        Rain     46       29       74       30.45    64       ...      10       13       40       26       52       30.01    39       10       0.01     268      \n    2014     12       X10      8        Rain     45       29       100      29.58    48       ...      3        13       39       37       89       29.43    38       1        0.28     357      \n    2014     12       X11      8        Rain-Snow37       28       92       29.81    39       ...      7        13       31       27       82       29.44    32       1        0.02     230      \n    2014     12       X12      7        Snow     28       21       85       29.88    39       ...      10       11       27       25       64       29.81    31       7        T        286      \n    2014     12       X13      5                 28       23       75       29.86    42       ...      10       12       26       24       55       29.78    32       10       T        298      \n    2014     12       X14      4                 29       20       82       29.91    45       ...      10       10       27       25       53       29.78    33       10       0.00     306      \n\n\n\n\nThis dataset is looking much better already!"
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#clean-up-dates",
    "href": "posts/2020-06-15-exploring boston weather data.html#clean-up-dates",
    "title": "Exploring Boston Weather Data",
    "section": "Clean up dates",
    "text": "Clean up dates\nNow that the weather dataset adheres to tidy data principles, the next step is to prepare it for analysis. We’ll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task.\n\n# Remove X's from day column\nweather3$day <- str_replace(weather3$day, 'X', '')\n\n# Unite the year, month, and day columns\nweather4 <- unite(weather3, date, year, month, day, sep = \"-\")\n\n# Convert date column to proper date format using lubridates's ymd()\nweather4$date <- ymd(weather4$date)\n\n# Rearrange columns using dplyr's select()\nweather5 <- select(weather4, date, Events, CloudCover:WindDirDegrees)\n\n# View the head of weather5\nhead(weather5)\n\n\n\ndateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees\n\n    2014-12-01Rain      6         46        29        74        30.45     64        10        22        ...       10        13        40        26        52        30.01     39        10        0.01      268       \n    2014-12-10Rain      8         45        29        100       29.58     48        10        23        ...       3         13        39        37        89        29.43     38        1         0.28      357       \n    2014-12-11Rain-Snow 8         37        28        92        29.81     39        10        21        ...       7         13        31        27        82        29.44     32        1         0.02      230       \n    2014-12-12Snow      7         28        21        85        29.88     39        10        16        ...       10        11        27        25        64        29.81     31        7         T         286       \n    2014-12-13          5         28        23        75        29.86     42        10        17        ...       10        12        26        24        55        29.78     32        10        T         298       \n    2014-12-14          4         29        20        82        29.91     45        10        15        ...       10        10        27        25        53        29.78     33        10        0.00      306"
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#a-closer-look-at-column-types",
    "href": "posts/2020-06-15-exploring boston weather data.html#a-closer-look-at-column-types",
    "title": "Exploring Boston Weather Data",
    "section": "A closer look at column types",
    "text": "A closer look at column types\nIt’s important for analysis that variables are coded appropriately. This is not yet the case with our weather data.\n\n# View the structure of weather5\nstr(weather5)\n\n# Examine the first 20 rows of weather5. Are most of the characters numeric?\nhead(weather5, 20)\n\n# See what happens if we try to convert PrecipitationIn to numeric\nas.numeric(weather5$PrecipitationIn)\n\n'data.frame':   366 obs. of  23 variables:\n $ date                     : Date, format: \"2014-12-01\" \"2014-12-10\" ...\n $ Events                   : chr  \"Rain\" \"Rain\" \"Rain-Snow\" \"Snow\" ...\n $ CloudCover               : chr  \"6\" \"8\" \"8\" \"7\" ...\n $ Max.Dew.PointF           : chr  \"46\" \"45\" \"37\" \"28\" ...\n $ Max.Gust.SpeedMPH        : chr  \"29\" \"29\" \"28\" \"21\" ...\n $ Max.Humidity             : chr  \"74\" \"100\" \"92\" \"85\" ...\n $ Max.Sea.Level.PressureIn : chr  \"30.45\" \"29.58\" \"29.81\" \"29.88\" ...\n $ Max.TemperatureF         : chr  \"64\" \"48\" \"39\" \"39\" ...\n $ Max.VisibilityMiles      : chr  \"10\" \"10\" \"10\" \"10\" ...\n $ Max.Wind.SpeedMPH        : chr  \"22\" \"23\" \"21\" \"16\" ...\n $ Mean.Humidity            : chr  \"63\" \"95\" \"87\" \"75\" ...\n $ Mean.Sea.Level.PressureIn: chr  \"30.13\" \"29.5\" \"29.61\" \"29.85\" ...\n $ Mean.TemperatureF        : chr  \"52\" \"43\" \"36\" \"35\" ...\n $ Mean.VisibilityMiles     : chr  \"10\" \"3\" \"7\" \"10\" ...\n $ Mean.Wind.SpeedMPH       : chr  \"13\" \"13\" \"13\" \"11\" ...\n $ MeanDew.PointF           : chr  \"40\" \"39\" \"31\" \"27\" ...\n $ Min.DewpointF            : chr  \"26\" \"37\" \"27\" \"25\" ...\n $ Min.Humidity             : chr  \"52\" \"89\" \"82\" \"64\" ...\n $ Min.Sea.Level.PressureIn : chr  \"30.01\" \"29.43\" \"29.44\" \"29.81\" ...\n $ Min.TemperatureF         : chr  \"39\" \"38\" \"32\" \"31\" ...\n $ Min.VisibilityMiles      : chr  \"10\" \"1\" \"1\" \"7\" ...\n $ PrecipitationIn          : chr  \"0.01\" \"0.28\" \"0.02\" \"T\" ...\n $ WindDirDegrees           : chr  \"268\" \"357\" \"230\" \"286\" ...\n\n\n\n\ndateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees\n\n    2014-12-01Rain      6         46        29        74        30.45     64        10        22        ...       10        13        40        26        52        30.01     39        10        0.01      268       \n    2014-12-10Rain      8         45        29        100       29.58     48        10        23        ...       3         13        39        37        89        29.43     38        1         0.28      357       \n    2014-12-11Rain-Snow 8         37        28        92        29.81     39        10        21        ...       7         13        31        27        82        29.44     32        1         0.02      230       \n    2014-12-12Snow      7         28        21        85        29.88     39        10        16        ...       10        11        27        25        64        29.81     31        7         T         286       \n    2014-12-13          5         28        23        75        29.86     42        10        17        ...       10        12        26        24        55        29.78     32        10        T         298       \n    2014-12-14          4         29        20        82        29.91     45        10        15        ...       10        10        27        25        53        29.78     33        10        0.00      306       \n    2014-12-15          2         33        21        89        30.15     42        10        15        ...       10        6         29        27        60        29.91     32        10        0.00      324       \n    2014-12-16Rain      8         42        10        96        30.17     44        10        8         ...       9         4         36        30        73        29.92     35        5         T         79        \n    2014-12-17Rain      8         46        26        100       29.91     49        10        20        ...       6         11        41        32        70        29.69     41        1         0.43      311       \n    2014-12-18Rain      7         34        30        89        29.87     44        10        23        ...       10        14        30        26        57        29.71     36        10        0.01      281       \n    2014-12-19          4         25        23        69        30.15     37        10        17        ...       10        11        22        20        56        29.86     29        10        0.00      305       \n    2014-12-02Rain-Snow 7         40        29        92        30.71     42        10        24        ...       8         15        27        17        51        30.4      33        2         0.10      62        \n    2014-12-20Snow      6         30        26        89        30.31     36        10        21        ...       10        10        24        20        69        30.17     27        7         T         350       \n    2014-12-21Snow      8         30        20        85        30.37     36        10        16        ...       9         9         27        25        69        30.28     30        6         T         2         \n    2014-12-22Rain      7         39        22        89        30.4      44        10        18        ...       10        8         34        25        69        30.3      33        4         0.05      24        \n    2014-12-23Rain      8         45        25        100       30.31     47        10        20        ...       5         13        42        37        82        30.16     42        1         0.25      63        \n    2014-12-24Fog-Rain  8         46        15        100       30.13     46        2         13        ...       1         6         44        41        96        29.55     41        0         0.56      12        \n    2014-12-25Rain      6         58        40        100       29.96     59        10        28        ...       8         14        43        29        49        29.47     44        1         0.14      250       \n    2014-12-26          1         31        25        70        30.16     50        10        18        ...       10        11        29        28        49        29.99     37        10        0.00      255       \n    2014-12-27          3         34        21        70        30.22     52        10        17        ...       10        9         31        29        50        30.03     38        10        0.00      251       \n\n\n\n\nWarning message in eval(expr, envir, enclos):\n\"NAs introduced by coercion\"\n\n\n\n    0.01\n    0.28\n    0.02\n    <NA>\n    <NA>\n    0\n    0\n    <NA>\n    0.43\n    0.01\n    0\n    0.1\n    <NA>\n    <NA>\n    0.05\n    0.25\n    0.56\n    0.14\n    0\n    0\n    0.01\n    0\n    0.44\n    0\n    0\n    0\n    0.11\n    1.09\n    0.13\n    0.03\n    2.9\n    0\n    0\n    0\n    0.2\n    0\n    <NA>\n    0.12\n    0\n    0\n    0.15\n    0\n    0\n    0\n    0\n    <NA>\n    0\n    0.71\n    0\n    0.1\n    0.95\n    0.01\n    <NA>\n    0.62\n    0.06\n    0.05\n    0.57\n    0\n    0.02\n    <NA>\n    0\n    0.01\n    0\n    0.05\n    0.01\n    0.03\n    0\n    0.23\n    0.39\n    0\n    0.02\n    0.01\n    0.06\n    0.78\n    0\n    0.17\n    0.11\n    0\n    <NA>\n    0.07\n    0.02\n    0\n    0\n    0\n    0\n    0.09\n    <NA>\n    0.07\n    0.37\n    0.88\n    0.17\n    0.06\n    0.01\n    0\n    0\n    0.8\n    0.27\n    0\n    0.14\n    0\n    0\n    0.01\n    0.05\n    0.09\n    0\n    0\n    0\n    0.04\n    0.8\n    0.21\n    0.12\n    0\n    0.26\n    <NA>\n    0\n    0.02\n    <NA>\n    0\n    0\n    <NA>\n    0\n    0\n    0.09\n    0\n    0\n    0\n    0.01\n    0\n    0\n    0.06\n    0\n    0\n    0\n    0.61\n    0.54\n    <NA>\n    0\n    <NA>\n    0\n    0\n    0.1\n    0.07\n    0\n    0.03\n    0\n    0.39\n    0\n    0\n    0.03\n    0.26\n    0.09\n    0\n    0\n    0\n    0.02\n    0\n    0\n    0\n    <NA>\n    0\n    0\n    0.27\n    0\n    0\n    0\n    <NA>\n    0\n    0\n    <NA>\n    0\n    0\n    <NA>\n    0\n    0\n    0\n    0.91\n    0\n    0.02\n    0\n    0\n    0\n    0\n    0.38\n    0\n    0\n    0\n    <NA>\n    0\n    0.4\n    <NA>\n    0\n    0\n    0\n    0.74\n    0.04\n    1.72\n    0\n    0.01\n    0\n    0\n    <NA>\n    0.2\n    1.43\n    <NA>\n    0\n    0\n    0\n    <NA>\n    0.09\n    0\n    <NA>\n    <NA>\n    0.5\n    1.12\n    0\n    0\n    0\n    0.03\n    <NA>\n    0\n    <NA>\n    0.14\n    <NA>\n    0\n    <NA>\n    <NA>\n    0\n    0\n    0.01\n    0\n    <NA>\n    0.06\n    0\n    0\n    0\n    0.02\n    0\n    <NA>\n    0\n    0\n    0.02\n    <NA>\n    0.15\n    <NA>\n    0\n    0.83\n    0\n    0\n    0\n    0.08\n    0\n    0\n    0.14\n    0\n    0\n    0\n    0.63\n    <NA>\n    0.02\n    <NA>\n    0\n    <NA>\n    0\n    0\n    0\n    0\n    0\n    0\n    0.49\n    0\n    0\n    0\n    0\n    0\n    0\n    0.17\n    0.66\n    0.01\n    0.38\n    0\n    0\n    0\n    0\n    0\n    0\n    0\n    <NA>\n    0\n    0\n    0\n    0\n    0\n    0\n    0\n    0\n    0.04\n    0.01\n    2.46\n    <NA>\n    0\n    0\n    0\n    0.2\n    0\n    <NA>\n    0\n    0\n    0\n    0.12\n    0\n    0\n    <NA>\n    <NA>\n    <NA>\n    0\n    0.08\n    <NA>\n    0.07\n    <NA>\n    0\n    0\n    0.03\n    0\n    0\n    0.36\n    0.73\n    0.01\n    0\n    0\n    0\n    0\n    0\n    0\n    0\n    0.34\n    <NA>\n    0.07\n    0.54\n    0.04\n    0.01\n    0\n    0\n    0\n    0\n    0\n    <NA>\n    0\n    0.86\n    0\n    0.3\n    0.04\n    0\n    0\n    0\n    0\n    0.21\n    0\n    0\n    0\n    0\n    0\n    0\n    0\n    0\n    0\n    0.14"
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#column-type-conversions",
    "href": "posts/2020-06-15-exploring boston weather data.html#column-type-conversions",
    "title": "Exploring Boston Weather Data",
    "section": "Column type conversions",
    "text": "Column type conversions\n\"T\" was used to denote a trace amount (i.e. too small to be accurately measured) of precipitation in the PrecipitationIn column. In order to coerce this column to numeric, wwe’ll need to deal with this somehow. To keep things simple, we will just replace \"T\" with zero, as a string (\"0\").\n\n# Replace \"T\" with \"0\" (T = trace)\nweather5$PrecipitationIn <- str_replace(weather5$PrecipitationIn, \"T\", \"0\")\n\n# Convert characters to numerics\nweather6 <- mutate_at(weather5, vars(CloudCover:WindDirDegrees), funs(as.numeric))\n\n# Look at result\nstr(weather6)\n\nWarning message:\n\"`funs()` is deprecated as of dplyr 0.8.0.\nPlease use a list of either functions or lambdas: \n\n  # Simple named list: \n  list(mean = mean, median = median)\n\n  # Auto named with `tibble::lst()`: \n  tibble::lst(mean, median)\n\n  # Using lambdas\n  list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_warnings()` to see where this warning was generated.\"\n\n\n'data.frame':   366 obs. of  23 variables:\n $ date                     : Date, format: \"2014-12-01\" \"2014-12-10\" ...\n $ Events                   : chr  \"Rain\" \"Rain\" \"Rain-Snow\" \"Snow\" ...\n $ CloudCover               : num  6 8 8 7 5 4 2 8 8 7 ...\n $ Max.Dew.PointF           : num  46 45 37 28 28 29 33 42 46 34 ...\n $ Max.Gust.SpeedMPH        : num  29 29 28 21 23 20 21 10 26 30 ...\n $ Max.Humidity             : num  74 100 92 85 75 82 89 96 100 89 ...\n $ Max.Sea.Level.PressureIn : num  30.4 29.6 29.8 29.9 29.9 ...\n $ Max.TemperatureF         : num  64 48 39 39 42 45 42 44 49 44 ...\n $ Max.VisibilityMiles      : num  10 10 10 10 10 10 10 10 10 10 ...\n $ Max.Wind.SpeedMPH        : num  22 23 21 16 17 15 15 8 20 23 ...\n $ Mean.Humidity            : num  63 95 87 75 65 68 75 85 85 73 ...\n $ Mean.Sea.Level.PressureIn: num  30.1 29.5 29.6 29.9 29.8 ...\n $ Mean.TemperatureF        : num  52 43 36 35 37 39 37 40 45 40 ...\n $ Mean.VisibilityMiles     : num  10 3 7 10 10 10 10 9 6 10 ...\n $ Mean.Wind.SpeedMPH       : num  13 13 13 11 12 10 6 4 11 14 ...\n $ MeanDew.PointF           : num  40 39 31 27 26 27 29 36 41 30 ...\n $ Min.DewpointF            : num  26 37 27 25 24 25 27 30 32 26 ...\n $ Min.Humidity             : num  52 89 82 64 55 53 60 73 70 57 ...\n $ Min.Sea.Level.PressureIn : num  30 29.4 29.4 29.8 29.8 ...\n $ Min.TemperatureF         : num  39 38 32 31 32 33 32 35 41 36 ...\n $ Min.VisibilityMiles      : num  10 1 1 7 10 10 10 5 1 10 ...\n $ PrecipitationIn          : num  0.01 0.28 0.02 0 0 0 0 0 0.43 0.01 ...\n $ WindDirDegrees           : num  268 357 230 286 298 306 324 79 311 281 ...\n\n\nIt looks like our data are finally in the correct formats and organized in a logical manner! Now that our data are in the right form, we can begin the analysis."
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#find-missing-values",
    "href": "posts/2020-06-15-exploring boston weather data.html#find-missing-values",
    "title": "Exploring Boston Weather Data",
    "section": "Find missing values",
    "text": "Find missing values\nBefore dealing with missing values in the data, it’s important to find them and figure out why they exist in the first place.\n\nIf the dataset is too big to look at all at once, like it is here, we will use sum() and is.na() to quickly size up the situation by counting the number of NA values.\n\nThe summary() function also come in handy for identifying which variables contain the missing values. Finally, the which() function is useful for locating the missing values within a particular column.\n\n# Count missing values\nsum(is.na(weather6))\n\n# Find missing values\nsummary(weather6)\n\n# Find indices of NAs in Max.Gust.SpeedMPH\nind <- which(is.na(weather6$Max.Gust.SpeedMPH))\n\n# Look at the full rows for records missing Max.Gust.SpeedMPH\nweather6[ind, ]\n\n6\n\n\n      date               Events            CloudCover    Max.Dew.PointF \n Min.   :2014-12-01   Length:366         Min.   :0.000   Min.   :-6.00  \n 1st Qu.:2015-03-02   Class :character   1st Qu.:3.000   1st Qu.:32.00  \n Median :2015-06-01   Mode  :character   Median :5.000   Median :47.50  \n Mean   :2015-06-01                      Mean   :4.708   Mean   :45.48  \n 3rd Qu.:2015-08-31                      3rd Qu.:7.000   3rd Qu.:61.00  \n Max.   :2015-12-01                      Max.   :8.000   Max.   :75.00  \n                                                                        \n Max.Gust.SpeedMPH  Max.Humidity     Max.Sea.Level.PressureIn Max.TemperatureF\n Min.   : 0.00     Min.   :  39.00   Min.   :29.58            Min.   :18.00   \n 1st Qu.:21.00     1st Qu.:  73.25   1st Qu.:30.00            1st Qu.:42.00   \n Median :25.50     Median :  86.00   Median :30.14            Median :60.00   \n Mean   :26.99     Mean   :  85.69   Mean   :30.16            Mean   :58.93   \n 3rd Qu.:31.25     3rd Qu.:  93.00   3rd Qu.:30.31            3rd Qu.:76.00   \n Max.   :94.00     Max.   :1000.00   Max.   :30.88            Max.   :96.00   \n NA's   :6                                                                    \n Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity  \n Min.   : 2.000      Min.   : 8.00     Min.   :28.00  \n 1st Qu.:10.000      1st Qu.:16.00     1st Qu.:56.00  \n Median :10.000      Median :20.00     Median :66.00  \n Mean   : 9.907      Mean   :20.62     Mean   :66.02  \n 3rd Qu.:10.000      3rd Qu.:24.00     3rd Qu.:76.75  \n Max.   :10.000      Max.   :38.00     Max.   :98.00  \n                                                      \n Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles\n Min.   :29.49             Min.   : 8.00     Min.   :-1.000      \n 1st Qu.:29.87             1st Qu.:36.25     1st Qu.: 8.000      \n Median :30.03             Median :53.50     Median :10.000      \n Mean   :30.04             Mean   :51.40     Mean   : 8.861      \n 3rd Qu.:30.19             3rd Qu.:68.00     3rd Qu.:10.000      \n Max.   :30.77             Max.   :84.00     Max.   :10.000      \n                                                                 \n Mean.Wind.SpeedMPH MeanDew.PointF   Min.DewpointF     Min.Humidity  \n Min.   : 4.00      Min.   :-11.00   Min.   :-18.00   Min.   :16.00  \n 1st Qu.: 8.00      1st Qu.: 24.00   1st Qu.: 16.25   1st Qu.:35.00  \n Median :10.00      Median : 41.00   Median : 35.00   Median :46.00  \n Mean   :10.68      Mean   : 38.96   Mean   : 32.25   Mean   :48.31  \n 3rd Qu.:13.00      3rd Qu.: 56.00   3rd Qu.: 51.00   3rd Qu.:60.00  \n Max.   :22.00      Max.   : 71.00   Max.   : 68.00   Max.   :96.00  \n                                                                     \n Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn \n Min.   :29.16            Min.   :-3.00    Min.   : 0.000      Min.   :0.0000  \n 1st Qu.:29.76            1st Qu.:30.00    1st Qu.: 2.000      1st Qu.:0.0000  \n Median :29.94            Median :46.00    Median :10.000      Median :0.0000  \n Mean   :29.93            Mean   :43.33    Mean   : 6.716      Mean   :0.1016  \n 3rd Qu.:30.09            3rd Qu.:60.00    3rd Qu.:10.000      3rd Qu.:0.0400  \n Max.   :30.64            Max.   :74.00    Max.   :10.000      Max.   :2.9000  \n                                                                               \n WindDirDegrees \n Min.   :  1.0  \n 1st Qu.:113.0  \n Median :222.0  \n Mean   :200.1  \n 3rd Qu.:275.0  \n Max.   :360.0  \n                \n\n\n\n\ndateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees\n\n    1612015-05-18Fog       6         52        NA        100       30.30     58        10        16        ...        8        10        48        43        57        30.12     49         0        0          72       \n    2052015-06-03          7         48        NA         93       30.31     56        10        14        ...       10         7        45        43        71        30.19     47        10        0          90       \n    2732015-08-08          4         61        NA         87       30.02     76        10        14        ...       10         6        57        54        49        29.95     61        10        0          45       \n    2752015-09-01          1         63        NA         78       30.06     79        10        15        ...       10         9        62        59        52        29.96     69        10        0          54       \n    3082015-10-12          0         56        NA         89       29.86     76        10        15        ...       10         8        51        48        41        29.74     51        10        0         199       \n    3582015-11-03          1         44        NA         82       30.25     73        10        16        ...       10         8        42        40        31        30.06     47        10        0         281       \n\n\n\n\nIn this situation it’s unclear why these values are missing and there doesn’t appear to be any obvious pattern to their missingness, so we’ll leave them alone for now."
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#an-obvious-error",
    "href": "posts/2020-06-15-exploring boston weather data.html#an-obvious-error",
    "title": "Exploring Boston Weather Data",
    "section": "An obvious error",
    "text": "An obvious error\nBesides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with summary().\nOnce implausible values are identified, they must be dealt with in an intelligent and informed way.\n\nSometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data.\n\n\n# Review distributions for all variables\nsummary(weather6)\n\n# Find row with Max.Humidity of 1000\nind <- which(weather6$Max.Humidity==1000)\n\n# Look at the data for that day\nweather6[ind, ]\n\n# Change 1000 to 100\nweather6$Max.Humidity[ind] <- 100\n\n      date               Events            CloudCover    Max.Dew.PointF \n Min.   :2014-12-01   Length:366         Min.   :0.000   Min.   :-6.00  \n 1st Qu.:2015-03-02   Class :character   1st Qu.:3.000   1st Qu.:32.00  \n Median :2015-06-01   Mode  :character   Median :5.000   Median :47.50  \n Mean   :2015-06-01                      Mean   :4.708   Mean   :45.48  \n 3rd Qu.:2015-08-31                      3rd Qu.:7.000   3rd Qu.:61.00  \n Max.   :2015-12-01                      Max.   :8.000   Max.   :75.00  \n                                                                        \n Max.Gust.SpeedMPH  Max.Humidity     Max.Sea.Level.PressureIn Max.TemperatureF\n Min.   : 0.00     Min.   :  39.00   Min.   :29.58            Min.   :18.00   \n 1st Qu.:21.00     1st Qu.:  73.25   1st Qu.:30.00            1st Qu.:42.00   \n Median :25.50     Median :  86.00   Median :30.14            Median :60.00   \n Mean   :26.99     Mean   :  85.69   Mean   :30.16            Mean   :58.93   \n 3rd Qu.:31.25     3rd Qu.:  93.00   3rd Qu.:30.31            3rd Qu.:76.00   \n Max.   :94.00     Max.   :1000.00   Max.   :30.88            Max.   :96.00   \n NA's   :6                                                                    \n Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity  \n Min.   : 2.000      Min.   : 8.00     Min.   :28.00  \n 1st Qu.:10.000      1st Qu.:16.00     1st Qu.:56.00  \n Median :10.000      Median :20.00     Median :66.00  \n Mean   : 9.907      Mean   :20.62     Mean   :66.02  \n 3rd Qu.:10.000      3rd Qu.:24.00     3rd Qu.:76.75  \n Max.   :10.000      Max.   :38.00     Max.   :98.00  \n                                                      \n Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles\n Min.   :29.49             Min.   : 8.00     Min.   :-1.000      \n 1st Qu.:29.87             1st Qu.:36.25     1st Qu.: 8.000      \n Median :30.03             Median :53.50     Median :10.000      \n Mean   :30.04             Mean   :51.40     Mean   : 8.861      \n 3rd Qu.:30.19             3rd Qu.:68.00     3rd Qu.:10.000      \n Max.   :30.77             Max.   :84.00     Max.   :10.000      \n                                                                 \n Mean.Wind.SpeedMPH MeanDew.PointF   Min.DewpointF     Min.Humidity  \n Min.   : 4.00      Min.   :-11.00   Min.   :-18.00   Min.   :16.00  \n 1st Qu.: 8.00      1st Qu.: 24.00   1st Qu.: 16.25   1st Qu.:35.00  \n Median :10.00      Median : 41.00   Median : 35.00   Median :46.00  \n Mean   :10.68      Mean   : 38.96   Mean   : 32.25   Mean   :48.31  \n 3rd Qu.:13.00      3rd Qu.: 56.00   3rd Qu.: 51.00   3rd Qu.:60.00  \n Max.   :22.00      Max.   : 71.00   Max.   : 68.00   Max.   :96.00  \n                                                                     \n Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn \n Min.   :29.16            Min.   :-3.00    Min.   : 0.000      Min.   :0.0000  \n 1st Qu.:29.76            1st Qu.:30.00    1st Qu.: 2.000      1st Qu.:0.0000  \n Median :29.94            Median :46.00    Median :10.000      Median :0.0000  \n Mean   :29.93            Mean   :43.33    Mean   : 6.716      Mean   :0.1016  \n 3rd Qu.:30.09            3rd Qu.:60.00    3rd Qu.:10.000      3rd Qu.:0.0400  \n Max.   :30.64            Max.   :74.00    Max.   :10.000      Max.   :2.9000  \n                                                                               \n WindDirDegrees \n Min.   :  1.0  \n 1st Qu.:113.0  \n Median :222.0  \n Mean   :200.1  \n 3rd Qu.:275.0  \n Max.   :360.0  \n                \n\n\n\n\ndateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees\n\n    1352015-04-21           Fog-Rain-Thunderstorm6                    57                   94                   1000                 29.75                65                   10                   20                   ...                  5                    10                   49                   36                   42                   29.53                46                   0                    0.54                 184                  \n\n\n\n\nOnce you find obvious errors, it’s not too hard to fix them if you know which values they should take."
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#another-obvious-error",
    "href": "posts/2020-06-15-exploring boston weather data.html#another-obvious-error",
    "title": "Exploring Boston Weather Data",
    "section": "Another obvious error",
    "text": "Another obvious error\nWe’ve discovered and repaired one obvious error in the data, but it appears that there’s another. Sometimes we get lucky and can infer the correct or intended value from the other data. For example, if you know the minimum and maximum values of a particular metric on a given day…\n\n# Look at summary of Mean.VisibilityMiles\nsummary(weather6$Mean.VisibilityMiles)\n\n# Get index of row with -1 value\nind <- which(weather6$Mean.VisibilityMiles == -1)\n\n# Look at full row\nweather6[ind,]\n\n# Set Mean.VisibilityMiles to the appropriate value\nweather6$Mean.VisibilityMiles[ind] <- 10\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.000   8.000  10.000   8.861  10.000  10.000 \n\n\n\n\ndateEventsCloudCoverMax.Dew.PointFMax.Gust.SpeedMPHMax.HumidityMax.Sea.Level.PressureInMax.TemperatureFMax.VisibilityMilesMax.Wind.SpeedMPH...Mean.VisibilityMilesMean.Wind.SpeedMPHMeanDew.PointFMin.DewpointFMin.HumidityMin.Sea.Level.PressureInMin.TemperatureFMin.VisibilityMilesPrecipitationInWindDirDegrees\n\n    1922015-06-18          5         54        23        72        30.14     76        10        17        ...       -1        10        49        45        46        29.93     57        10        0         189       \n\n\n\n\nOur data are looking tidy. Just a quick sanity check left!"
  },
  {
    "objectID": "posts/2020-06-15-exploring boston weather data.html#check-other-extreme-values",
    "href": "posts/2020-06-15-exploring boston weather data.html#check-other-extreme-values",
    "title": "Exploring Boston Weather Data",
    "section": "Check other extreme values",
    "text": "Check other extreme values\nIn addition to dealing with obvious errors in the data, we want to see if there are other extreme values. In addition to the trusty summary() function, hist() is useful for quickly getting a feel for how different variables are distributed.\n\n# Review summary of full data once more\nsummary(weather6)\n\n# Look at histogram for MeanDew.PointF\nhist(weather6$MeanDew.PointF)\n\n# Look at histogram for Min.TemperatureF\nhist(weather6$Min.TemperatureF)\n\n# Compare to histogram for Mean.TemperatureF\nhist(weather6$Mean.TemperatureF)\n\n      date               Events            CloudCover    Max.Dew.PointF \n Min.   :2014-12-01   Length:366         Min.   :0.000   Min.   :-6.00  \n 1st Qu.:2015-03-02   Class :character   1st Qu.:3.000   1st Qu.:32.00  \n Median :2015-06-01   Mode  :character   Median :5.000   Median :47.50  \n Mean   :2015-06-01                      Mean   :4.708   Mean   :45.48  \n 3rd Qu.:2015-08-31                      3rd Qu.:7.000   3rd Qu.:61.00  \n Max.   :2015-12-01                      Max.   :8.000   Max.   :75.00  \n                                                                        \n Max.Gust.SpeedMPH  Max.Humidity    Max.Sea.Level.PressureIn Max.TemperatureF\n Min.   : 0.00     Min.   : 39.00   Min.   :29.58            Min.   :18.00   \n 1st Qu.:21.00     1st Qu.: 73.25   1st Qu.:30.00            1st Qu.:42.00   \n Median :25.50     Median : 86.00   Median :30.14            Median :60.00   \n Mean   :26.99     Mean   : 83.23   Mean   :30.16            Mean   :58.93   \n 3rd Qu.:31.25     3rd Qu.: 93.00   3rd Qu.:30.31            3rd Qu.:76.00   \n Max.   :94.00     Max.   :100.00   Max.   :30.88            Max.   :96.00   \n NA's   :6                                                                   \n Max.VisibilityMiles Max.Wind.SpeedMPH Mean.Humidity  \n Min.   : 2.000      Min.   : 8.00     Min.   :28.00  \n 1st Qu.:10.000      1st Qu.:16.00     1st Qu.:56.00  \n Median :10.000      Median :20.00     Median :66.00  \n Mean   : 9.907      Mean   :20.62     Mean   :66.02  \n 3rd Qu.:10.000      3rd Qu.:24.00     3rd Qu.:76.75  \n Max.   :10.000      Max.   :38.00     Max.   :98.00  \n                                                      \n Mean.Sea.Level.PressureIn Mean.TemperatureF Mean.VisibilityMiles\n Min.   :29.49             Min.   : 8.00     Min.   : 1.000      \n 1st Qu.:29.87             1st Qu.:36.25     1st Qu.: 8.000      \n Median :30.03             Median :53.50     Median :10.000      \n Mean   :30.04             Mean   :51.40     Mean   : 8.891      \n 3rd Qu.:30.19             3rd Qu.:68.00     3rd Qu.:10.000      \n Max.   :30.77             Max.   :84.00     Max.   :10.000      \n                                                                 \n Mean.Wind.SpeedMPH MeanDew.PointF   Min.DewpointF     Min.Humidity  \n Min.   : 4.00      Min.   :-11.00   Min.   :-18.00   Min.   :16.00  \n 1st Qu.: 8.00      1st Qu.: 24.00   1st Qu.: 16.25   1st Qu.:35.00  \n Median :10.00      Median : 41.00   Median : 35.00   Median :46.00  \n Mean   :10.68      Mean   : 38.96   Mean   : 32.25   Mean   :48.31  \n 3rd Qu.:13.00      3rd Qu.: 56.00   3rd Qu.: 51.00   3rd Qu.:60.00  \n Max.   :22.00      Max.   : 71.00   Max.   : 68.00   Max.   :96.00  \n                                                                     \n Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles PrecipitationIn \n Min.   :29.16            Min.   :-3.00    Min.   : 0.000      Min.   :0.0000  \n 1st Qu.:29.76            1st Qu.:30.00    1st Qu.: 2.000      1st Qu.:0.0000  \n Median :29.94            Median :46.00    Median :10.000      Median :0.0000  \n Mean   :29.93            Mean   :43.33    Mean   : 6.716      Mean   :0.1016  \n 3rd Qu.:30.09            3rd Qu.:60.00    3rd Qu.:10.000      3rd Qu.:0.0400  \n Max.   :30.64            Max.   :74.00    Max.   :10.000      Max.   :2.9000  \n                                                                               \n WindDirDegrees \n Min.   :  1.0  \n 1st Qu.:113.0  \n Median :222.0  \n Mean   :200.1  \n 3rd Qu.:275.0  \n Max.   :360.0  \n                \n\n\n\n\n\n\n\n\n\n\n\nIt looks like you have sufficiently tidied your data!"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "",
    "text": "This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it’s probably because he’s thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It’s the contaminated hands of the doctors delivering the babies. And they won’t listen to him and wash their hands!\n\n\nIn this notebook, we’re going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let’s start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital.\n\n\n# importing modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read datasets/yearly_deaths_by_clinic.csv into yearly\nyearly = pd.read_csv(\"datasets/yearly_deaths_by_clinic.csv\")\n\n# Print out yearly\nyearly\n\n\n\n\n\n  \n    \n      \n      year\n      births\n      deaths\n      clinic\n    \n  \n  \n    \n      0\n      1841\n      3036\n      237\n      clinic 1\n    \n    \n      1\n      1842\n      3287\n      518\n      clinic 1\n    \n    \n      2\n      1843\n      3060\n      274\n      clinic 1\n    \n    \n      3\n      1844\n      3157\n      260\n      clinic 1\n    \n    \n      4\n      1845\n      3492\n      241\n      clinic 1\n    \n    \n      5\n      1846\n      4010\n      459\n      clinic 1\n    \n    \n      6\n      1841\n      2442\n      86\n      clinic 2\n    \n    \n      7\n      1842\n      2659\n      202\n      clinic 2\n    \n    \n      8\n      1843\n      2739\n      164\n      clinic 2\n    \n    \n      9\n      1844\n      2956\n      68\n      clinic 2\n    \n    \n      10\n      1845\n      3241\n      66\n      clinic 2\n    \n    \n      11\n      1846\n      3754\n      105\n      clinic 2"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-alarming-number-of-deaths",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-alarming-number-of-deaths",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "2. The alarming number of deaths",
    "text": "2. The alarming number of deaths\n\nThe table above shows the number of women giving birth at the two clinics at the Vienna General Hospital for the years 1841 to 1846. You’ll notice that giving birth was very dangerous; an alarming number of women died as the result of childbirth, most of them from childbed fever.\n\n\nWe see this more clearly if we look at the proportion of deaths out of the number of women giving birth. Let’s zoom in on the proportion of deaths at Clinic 1.\n\n\n# Calculate proportion of deaths per no. births\nyearly[\"proportion_deaths\"] = yearly.deaths/yearly.births\n# Extract clinic 1 data into yearly1 and clinic 2 data into yearly2\nyearly1 = yearly[yearly.clinic==\"clinic 1\"]\nyearly2 = yearly[yearly.clinic==\"clinic 2\"]\n\n# Print out yearly1\nyearly1\n\n\n\n\n\n  \n    \n      \n      year\n      births\n      deaths\n      clinic\n      proportion_deaths\n    \n  \n  \n    \n      0\n      1841\n      3036\n      237\n      clinic 1\n      0.078063\n    \n    \n      1\n      1842\n      3287\n      518\n      clinic 1\n      0.157591\n    \n    \n      2\n      1843\n      3060\n      274\n      clinic 1\n      0.089542\n    \n    \n      3\n      1844\n      3157\n      260\n      clinic 1\n      0.082357\n    \n    \n      4\n      1845\n      3492\n      241\n      clinic 1\n      0.069015\n    \n    \n      5\n      1846\n      4010\n      459\n      clinic 1\n      0.114464"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#death-at-the-clinics",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#death-at-the-clinics",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "3. Death at the clinics",
    "text": "3. Death at the clinics\n\nIf we now plot the proportion of deaths at both clinic 1 and clinic 2 we’ll see a curious pattern…\n\n\n# This makes plots appear in the notebook\n%matplotlib inline\n\n# Plot yearly proportion of deaths at the two clinics\nax = yearly1.plot(x=\"year\", y=\"proportion_deaths\", label=\"Clinic 1\")\nyearly2.plot(x=\"year\", y=\"proportion_deaths\", label=\"Clinic 2\", ax=ax)\nax.set_ylabel(\"Proportion deaths\")\n\n<matplotlib.text.Text at 0x7fab04d06d30>"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-handwashing-begins",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-handwashing-begins",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "4. The handwashing begins",
    "text": "4. The handwashing begins\n\nWhy is the proportion of deaths constantly so much higher in Clinic 1? Semmelweis saw the same pattern and was puzzled and distressed. The only difference between the clinics was that many medical students served at Clinic 1, while mostly midwife students served at Clinic 2. While the midwives only tended to the women giving birth, the medical students also spent time in the autopsy rooms examining corpses.\n\n\nSemmelweis started to suspect that something on the corpses, spread from the hands of the medical students, caused childbed fever. So in a desperate attempt to stop the high mortality rates, he decreed: Wash your hands! This was an unorthodox and controversial request, nobody in Vienna knew about bacteria at this point in time.\n\n\nLet’s load in monthly data from Clinic 1 to see if the handwashing had any effect.\n\n\n# Read datasets/monthly_deaths.csv into monthly\nmonthly = pd.read_csv(\"datasets/monthly_deaths.csv\", parse_dates=[\"date\"])\n\n# Calculate proportion of deaths per no. births\nmonthly[\"proportion_deaths\"] = monthly.deaths/monthly.births\n\n# Print out the first rows in monthly\nmonthly.head()\n\n\n\n\n\n  \n    \n      \n      date\n      births\n      deaths\n      proportion_deaths\n    \n  \n  \n    \n      0\n      1841-01-01\n      254\n      37\n      0.145669\n    \n    \n      1\n      1841-02-01\n      239\n      18\n      0.075314\n    \n    \n      2\n      1841-03-01\n      277\n      12\n      0.043321\n    \n    \n      3\n      1841-04-01\n      255\n      4\n      0.015686\n    \n    \n      4\n      1841-05-01\n      255\n      2\n      0.007843"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-effect-of-handwashing",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-effect-of-handwashing",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "5. The effect of handwashing",
    "text": "5. The effect of handwashing\n\nWith the data loaded we can now look at the proportion of deaths over time. In the plot below we haven’t marked where obligatory handwashing started, but it reduced the proportion of deaths to such a degree that you should be able to spot it!\n\n\n# Plot monthly proportion of deaths\nax = monthly.plot(x=\"date\", y=\"proportion_deaths\")\nax.set_ylabel(\"Proportion deaths\")\n\n<matplotlib.text.Text at 0x7faae5d53b38>"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-effect-of-handwashing-highlighted",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-effect-of-handwashing-highlighted",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "6. The effect of handwashing highlighted",
    "text": "6. The effect of handwashing highlighted\n\nStarting from the summer of 1847 the proportion of deaths is drastically reduced and, yes, this was when Semmelweis made handwashing obligatory.\n\n\nThe effect of handwashing is made even more clear if we highlight this in the graph.\n\n\n# Date when handwashing was made mandatory\nimport pandas as pd\nhandwashing_start = pd.to_datetime('1847-06-01')\n\n# Split monthly into before and after handwashing_start\nbefore_washing = monthly[monthly.date<handwashing_start]\nafter_washing = monthly[monthly.date>=handwashing_start]\n\n# Plot monthly proportion of deaths before and after handwashing\nax = before_washing.plot(x=\"date\", y=\"proportion_deaths\", label=\"Before Washing\")\nafter_washing.plot(ax=ax, x=\"date\", y=\"proportion_deaths\", label=\"After Washing\")\nax.set_ylabel(\"Proportion deaths\")\n\n<matplotlib.text.Text at 0x7faae5c44940>"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#more-handwashing-fewer-deaths",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#more-handwashing-fewer-deaths",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "7. More handwashing, fewer deaths?",
    "text": "7. More handwashing, fewer deaths?\n\nAgain, the graph shows that handwashing had a huge effect. How much did it reduce the monthly proportion of deaths on average?\n\n\n# Difference in mean monthly proportion of deaths due to handwashing\nbefore_proportion = before_washing.proportion_deaths\nafter_proportion = after_washing.proportion_deaths\nmean_diff = after_proportion.mean() - before_proportion.mean()\nmean_diff\n\n-0.08395660751183336"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#a-bootstrap-analysis-of-semmelweis-handwashing-data",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#a-bootstrap-analysis-of-semmelweis-handwashing-data",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "8. A Bootstrap analysis of Semmelweis handwashing data",
    "text": "8. A Bootstrap analysis of Semmelweis handwashing data\n\nIt reduced the proportion of deaths by around 8 percentage points! From 10% on average to just 2% (which is still a high number by modern standards).\n\n\nTo get a feeling for the uncertainty around how much handwashing reduces mortalities we could look at a confidence interval (here calculated using the bootstrap method).\n\n\n# A bootstrap analysis of the reduction of deaths due to handwashing\nboot_mean_diff = []\nfor i in range(3000):\n    boot_before = before_proportion.sample(frac=1, replace=True)\n    boot_after = after_proportion.sample(frac=1, replace=True)\n    boot_mean_diff.append( boot_after.mean() - boot_before.mean() )\n\n# Calculating a 95% confidence interval from boot_mean_diff \nconfidence_interval = pd.Series(boot_mean_diff).quantile([.025, .975])\nconfidence_interval\n\n0.025   -0.102262\n0.975   -0.067096\ndtype: float64"
  },
  {
    "objectID": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-fate-of-dr.-semmelweis",
    "href": "posts/2020-07-01-dr. semmelweis and the discovery of handwashing.html#the-fate-of-dr.-semmelweis",
    "title": "Dr. Semmelweis and the Discovery of Handwashing",
    "section": "9. The fate of Dr. Semmelweis",
    "text": "9. The fate of Dr. Semmelweis\n\nSo handwashing reduced the proportion of deaths by between 6.7 and 10 percentage points, according to a 95% confidence interval. All in all, it would seem that Semmelweis had solid evidence that handwashing was a simple but highly effective procedure that could save many lives.\n\n\nThe tragedy is that, despite the evidence, Semmelweis’ theory — that childbed fever was caused by some “substance” (what we today know as bacteria) from autopsy room corpses — was ridiculed by contemporary scientists. The medical community largely rejected his discovery and in 1849 he was forced to leave the Vienna General Hospital for good.\n\n\nOne reason for this was that statistics and statistical arguments were uncommon in medical science in the 1800s. Semmelweis only published his data as long tables of raw data, but he didn’t show any graphs nor confidence intervals. If he would have had access to the analysis we’ve just put together he might have been more successful in getting the Viennese doctors to wash their hands.\n\n\n# The data Semmelweis collected points to that:\ndoctors_should_wash_their_hands = True"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html",
    "href": "posts/2020-10-19-cluster analysis in r.html",
    "title": "Cluster Analysis in R",
    "section": "",
    "text": "Cluster analysis is a powerful toolkit in the data science workbench. It is used to find groups of observations (clusters) that share similar characteristics. These similarities can inform all kinds of business decisions; for example, in marketing, it is used to identify distinct groups of customers for which advertisements can be tailored. We will explore two commonly used clustering methods - hierarchical clustering and k-means clustering. We’ll build a strong intuition for how they work and how to interpret their results. We’ll develop this intuition by exploring three different datasets: soccer player positions, wholesale customer spending data, and longitudinal occupational wage data."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#what-is-cluster-analysis",
    "href": "posts/2020-10-19-cluster analysis in r.html#what-is-cluster-analysis",
    "title": "Cluster Analysis in R",
    "section": "What is cluster analysis?",
    "text": "What is cluster analysis?\nA form of exploratory data analysis (EDA) where observations are divided into meaningful groups that share common characteristics(features).\n\nWhen to cluster?\n\nIdentifying distinct groups of stocks that follow similar trading patterns.\nUsing consumer behavior data to identify distinct segments within a market."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#distance-between-two-observations",
    "href": "posts/2020-10-19-cluster analysis in r.html#distance-between-two-observations",
    "title": "Cluster Analysis in R",
    "section": "Distance between two observations",
    "text": "Distance between two observations\n\nDistance vs Similarity\n\\(distance = 1 - similarity\\)\n\n\ndist() function\n\ntwo_players = data.frame(X=c(0, 9), Y=c(0,12))\ntwo_players %>%\n    dist(method=\"euclidean\")\n\n   1\n2 15\n\n\n\nthree_players = data.frame(X=c(0, 9, -2), Y=c(0,12, 19))\nthree_players %>%\n    dist()\n\n         1        2\n2 15.00000         \n3 19.10497 13.03840\n\n\n\nplayers <- readRDS(gzcon(url(\"https://assets.datacamp.com/production/repositories/1219/datasets/94af7037c5834527cc8799a9723ebf3b5af73015/lineup.rds\")))\nhead(players)\n\n\n\nxy\n\n     -1 1 \n     -2-3 \n      8 6 \n      7-8 \n    -12 8 \n    -15 0 \n\n\n\n\n\n# Plot the positions of the players\nggplot(two_players, aes(x = X, y = Y)) + \n  geom_point() +\n  # Assuming a 40x60 field\n  lims(x = c(-30,30), y = c(-20, 20))\n\n\n\n\n\n# Split the players data frame into two observations\nplayer1 <- two_players[1, ]\nplayer2 <- two_players[2, ]\n\n# Calculate and print their distance using the Euclidean Distance formula\nplayer_distance <- sqrt( (player1$X - player2$X)^2 + (player1$Y - player2$Y)^2 )\nplayer_distance\n\n15\n\n\nThe dist() function makes life easier when working with many dimensions and observations.\n\ndist(three_players)\n\n         1        2\n2 15.00000         \n3 19.10497 13.03840\n\n\n\nthree_players\n\n\n\nXY\n\n     0 0\n     912\n    -219"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#the-importance-of-scale",
    "href": "posts/2020-10-19-cluster analysis in r.html#the-importance-of-scale",
    "title": "Cluster Analysis in R",
    "section": "The importance of scale",
    "text": "The importance of scale\nwhen a variable is on a larger scale than other variables in data it may disproportionately influence the resulting distance calculated between the observations.\nscale() function by default centers & scales column features."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#measuring-distance-for-categorical-data",
    "href": "posts/2020-10-19-cluster analysis in r.html#measuring-distance-for-categorical-data",
    "title": "Cluster Analysis in R",
    "section": "Measuring distance for categorical data",
    "text": "Measuring distance for categorical data\n\nDummication in R\ndummy.data.frame()\n\njob_survey = read.csv(\"datasets/job_survey.csv\")\njob_survey\n\n\n\njob_satisfactionis_happy\n\n    LowNo \n    LowNo \n    Hi Yes\n    LowNo \n    MidNo \n\n\n\n\n\n# Dummify the Survey Data\ndummy_survey <- dummy.data.frame(job_survey)\n\n# Calculate the Distance\ndist_survey <- dist(dummy_survey, method=\"binary\")\n# Print the Distance Matrix\ndist_survey\n\nWarning message in model.matrix.default(~x - 1, model.frame(~x - 1), contrasts = FALSE):\n\"non-list contrasts argument ignored\"Warning message in model.matrix.default(~x - 1, model.frame(~x - 1), contrasts = FALSE):\n\"non-list contrasts argument ignored\"\n\n\n          1         2         3         4\n2 0.0000000                              \n3 1.0000000 1.0000000                    \n4 0.0000000 0.0000000 1.0000000          \n5 0.6666667 0.6666667 1.0000000 0.6666667\n\n\nthis distance metric successfully captured that observations 1 and 2 are identical (distance of 0)"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#comparing-more-than-two-observations",
    "href": "posts/2020-10-19-cluster analysis in r.html#comparing-more-than-two-observations",
    "title": "Cluster Analysis in R",
    "section": "Comparing more than two observations",
    "text": "Comparing more than two observations\n\nHierarchical clustering\n\nComplete Linkage: maximum distance between two sets\nSingle Linkage: minimum distance between two sets\nAverage Linkage: average distance between two sets"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#capturing-k-clusters",
    "href": "posts/2020-10-19-cluster analysis in r.html#capturing-k-clusters",
    "title": "Cluster Analysis in R",
    "section": "Capturing K clusters",
    "text": "Capturing K clusters\n\nHierarchical clustering in R\n\nhclust() function to calculate the iterative linkage steps\ncutree() function to extract the cluster assignments for the desired number (k) of clusters.\n\npositions of 12 players at the start of a 6v6 soccer match.\n\nhead(players)\n\n\n\nxy\n\n     -1 1 \n     -2-3 \n      8 6 \n      7-8 \n    -12 8 \n    -15 0 \n\n\n\n\n\ndist_players = dist(players, method = \"euclidean\")\nhc_players = hclust(dist_players, method = \"complete\")\n\n\n\nExtracting K clusters\n\n(cluster_assignments <- cutree(hc_players, k=2))\n\n\n    1\n    1\n    2\n    2\n    1\n    1\n    1\n    2\n    2\n    2\n    1\n    2\n\n\n\n\nhead(\n    players_clustered <- players %>%\n        mutate(cluster = cluster_assignments), \n    10)\n\n\n\nxycluster\n\n     -1  11  \n     -2 -31  \n      8  62  \n      7 -82  \n    -12  81  \n    -15  01  \n    -13-101  \n     15 162  \n     21  22  \n     12-152  \n\n\n\n\nplayers_clustered data frame contains the x & y positions of 12 players at the start of a 6v6 soccer game to which we have added clustering assignments based on the following parameters:\n\nDistance: Euclidean\nNumber of Clusters (k): 2\nLinkage Method: Complete\n\n\n\nExploring the clusters\n\n# Count the cluster assignments\ncount(players_clustered, cluster)\n\n\n\nclustern\n\n    16\n    26\n\n\n\n\n\n\nVisualizing K Clusters\nBecause clustering analysis is always in part qualitative, it is incredibly important to have the necessary tools to explore the results of the clustering.\n\nplayers_clustered %>%\n    ggplot(aes(x=x, y=y, color=factor(cluster)))+\n    geom_point()"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#visualizing-the-dendrogram",
    "href": "posts/2020-10-19-cluster analysis in r.html#visualizing-the-dendrogram",
    "title": "Cluster Analysis in R",
    "section": "Visualizing the dendrogram",
    "text": "Visualizing the dendrogram\n\nPlotting the dendrogram\n\nplot(hc_players)\n\n\n\n\n\n# Prepare the Distance Matrix\ndist_players <- dist(players)\n\n# Generate hclust for complete, single & average linkage methods\nhc_complete <- hclust(dist_players, method=\"complete\")\nhc_single <- hclust(dist_players, method=\"single\")\nhc_average <- hclust(dist_players, method=\"average\")\n\n# Plot & Label the 3 Dendrograms Side-by-Side\n# Hint: To see these Side-by-Side run the 4 lines together as one command\npar(mfrow = c(1,3))\nplot(hc_complete, main = 'Complete Linkage')\nplot(hc_single, main = 'Single Linkage')\nplot(hc_average, main = 'Average Linkage')\n\n\n\n\n\n\nHeight of the tree\nAn advantage of working with a clustering method like hierarchical clustering is that you can describe the relationships between your observations based on both the distance metric and the linkage metric selected (the combination of which defines the height of the tree)."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#cutting-the-tree",
    "href": "posts/2020-10-19-cluster analysis in r.html#cutting-the-tree",
    "title": "Cluster Analysis in R",
    "section": "Cutting the tree",
    "text": "Cutting the tree\n\nColoring the dendrogram - height\n\ndend_players = as.dendrogram(hc_players)\ndend_colored = color_branches(dend_players, h=15)\nplot(dend_colored)\n\n\n\n\n\ndend_players = as.dendrogram(hc_players)\ndend_colored = color_branches(dend_players, h=10)\nplot(dend_colored)\n\n\n\n\n\n\nColoring the dendrogram - K\n\ndend_players = as.dendrogram(hc_players)\ndend_colored = color_branches(dend_players, k=2)\nplot(dend_colored)\n\n\n\n\n\n\ncutree() using height\n\ncluster_assignments <- cutree(hc_players, h=15)\ncluster_assignments\n\n\n    1\n    1\n    2\n    3\n    4\n    4\n    5\n    2\n    6\n    3\n    4\n    6\n\n\n\n\nhead(\n    players_clustered <- \n    players %>%\n        mutate(cluster = cluster_assignments),\n    10)\n\n\n\nxycluster\n\n     -1  11  \n     -2 -31  \n      8  62  \n      7 -83  \n    -12  84  \n    -15  04  \n    -13-105  \n     15 162  \n     21  26  \n     12-153  \n\n\n\n\n\ndist_players <- dist(players, method = 'euclidean')\nhc_players <- hclust(dist_players, method = \"complete\")\n\n# Create a dendrogram object from the hclust variable\ndend_players <- as.dendrogram(hc_players)\n\n# Plot the dendrogram\nplot(dend_players)\n\n\n\n\n\n# Color branches by cluster formed from the cut at a height of 20 & plot\ndend_20 <- color_branches(dend_players, h = 20)\n\n# Plot the dendrogram with clusters colored below height 20\nplot(dend_20)\n\n\n\n\n\n# Color branches by cluster formed from the cut at a height of 40 & plot\ndend_40 <- color_branches(dend_players, h=40)\n\n# Plot the dendrogram with clusters colored below height 40\nplot(dend_40)\n\n\n\n\nThe height of any branch is determined by the linkage and distance decisions (in this case complete linkage and Euclidean distance). While the members of the clusters that form below a desired height have a maximum linkage+distance amongst themselves that is less than the desired height."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#making-sense-of-the-clusters",
    "href": "posts/2020-10-19-cluster analysis in r.html#making-sense-of-the-clusters",
    "title": "Cluster Analysis in R",
    "section": "Making sense of the clusters",
    "text": "Making sense of the clusters\n\nWholesale dataset\n\n45 observations\n3 features:\n\nMilk Spending\nGrocery Spending\nFrozen Food Spending\n\n\n\nhead(\n    ws_customers <- readRDS(gzcon(url(\"https://assets.datacamp.com/production/repositories/1219/datasets/3558d2b5564714d85120cb77a904a2859bb3d03e/ws_customers.rds\"))) , \n    10\n)\n\n\n\nMilkGroceryFrozen\n\n    1110312469 902 \n     2013 6550 909 \n     1897 5234 417 \n     1304 36433045 \n     3199 69861455 \n     4560 9965 934 \n      879 2060 264 \n     6243 6360 824 \n    13316203991809 \n     5302 9785 364 \n\n\n\n\n\n\nExploring more than 2 dimensions\n\nPlot 2 dimensions at a time\nVisualize using PCA\nSummary statistics by feature\n\n\n\nSegment wholesale customers\n\n# Calculate Euclidean distance between customers\ndist_customers <- dist(ws_customers, method=\"euclidean\")\n\n# Generate a complete linkage analysis \nhc_customers <- hclust(dist_customers, method=\"complete\")\n\n# Plot the dendrogram\nplot(hc_customers)\n\n\n\n\n\n# Create a cluster assignment vector at h = 15000\nclust_customers <- cutree(hc_customers, h=15000)\n\n# Generate the segmented customers data frame\nhead(\n    segment_customers <- mutate(ws_customers, cluster = clust_customers), \n    10\n)\n\n\n\nMilkGroceryFrozencluster\n\n    1110312469 902 1    \n     2013 6550 909 2    \n     1897 5234 417 2    \n     1304 36433045 2    \n     3199 69861455 2    \n     4560 9965 934 2    \n      879 2060 264 2    \n     6243 6360 824 2    \n    13316203991809 3    \n     5302 9785 364 2    \n\n\n\n\n\n\nExplore wholesale customer clusters\nSince we are working with more than 2 dimensions it would be challenging to visualize a scatter plot of the clusters, instead we will rely on summary statistics to explore these clusters. We will analyze the mean amount spent in each cluster for all three categories.\n\n# Count the number of customers that fall into each cluster\ncount(segment_customers, cluster)\n\n\n\nclustern\n\n    1  5\n    2 29\n    3  5\n    4  6\n\n\n\n\n\n# Color the dendrogram based on the height cutoff\ndend_customers <- as.dendrogram(hc_customers)\ndend_colored <- color_branches(dend_customers, h=15000)\n\n# Plot the colored dendrogram\nplot(dend_colored)\n\n\n\n\n\n# Calculate the mean for each category\nsegment_customers %>% \n  group_by(cluster) %>% \n  summarise_all(list(mean))\n\n\n\nclusterMilkGroceryFrozen\n\n    1        16950.00012891.400  991.200\n    2         2512.828 5228.931 1795.517\n    3        10452.20022550.600 1354.800\n    4         1249.500 3916.83310888.667\n\n\n\n\n\nCustomers in cluster 1 spent more money on Milk than any other cluster.\nCustomers in cluster 3 spent more money on Grocery than any other cluster.\nCustomers in cluster 4 spent more money on Frozen goods than any other cluster.\nThe majority of customers fell into cluster 2 and did not show any excessive spending in any category.\n\nwhether they are meaningful depends heavily on the business context of the clustering."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#evaluating-different-values-of-k-by-eye",
    "href": "posts/2020-10-19-cluster analysis in r.html#evaluating-different-values-of-k-by-eye",
    "title": "Cluster Analysis in R",
    "section": "Evaluating different values of K by eye",
    "text": "Evaluating different values of K by eye\n\nGenerating the elbow plot\n\ntot_withinss <- map_dbl(1:10, function(k){\n    model_l <- kmeans(lineup, centers = k)\n    model_l$tot.withinss\n})\nhead(\n    elbow_df_l <- data.frame(\n    k=1:10,\n    tot_withinss = tot_withinss\n), 10)\n\n\n\nktot_withinss\n\n     1       3489.9167\n     2       1434.5000\n     3        881.2500\n     4        622.5000\n     5        481.6667\n     6        265.1667\n     7        350.5000\n     8         96.5000\n     9         82.0000\n    10         73.5000\n\n\n\n\n\n\nGenerating the elbow plot\n\nelbow_df_l %>%\n    ggplot(aes(x=k, y=tot_withinss)) +\n    geom_line() +\n    scale_x_continuous(breaks = 1:10)"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#silhouette-analysis-observation-level-performance",
    "href": "posts/2020-10-19-cluster analysis in r.html#silhouette-analysis-observation-level-performance",
    "title": "Cluster Analysis in R",
    "section": "Silhouette analysis: observation level performance",
    "text": "Silhouette analysis: observation level performance\n\nSilhouette analysis\nSilhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:\n\nValues close to 1 suggest that the observation is well matched to the assigned cluster\nValues close to 0 suggest that the observation is borderline matched between two clusters\nValues close to -1 suggest that the observations may be assigned to the wrong cluster\n\n\n\nCalculating S(i)\n\npam_k3 <- pam(lineup, k=3)\nhead(pam_k3$silinfo$widths, 10)\n\n\n\nclusterneighborsil_width\n\n    41           2            0.465320054\n    21           3            0.321729341\n    101           2            0.311385893\n    11           3            0.271890169\n    92           1            0.443606497\n    82           1            0.398547473\n    122           1            0.393982685\n    32           1           -0.009151755\n    113           1            0.546797052\n    63           1            0.529967901\n\n\n\n\n\n\nSilhouette plot\n\nsil_plot <- silhouette(pam_k3)\nplot(sil_plot)\n\n\n\n\n\n# Generate a k-means model using the pam() function with a k = 2\npam_k2 <- pam(lineup, k = 2)\n\n# Plot the silhouette visual for the pam_k2 model\nplot(silhouette(pam_k2))\n\n\n\n\nfor k = 2, no observation has a silhouette width close to 0? What about the fact that for k = 3, observation 3 is close to 0 and is negative? This suggests that k = 3 is not the right number of clusters.\n\n\nAverage silhouette width\n\npam_k3$silinfo$avg.width\n\n0.353414012920685\n\n\n\n1: Well matched to each cluster\n0: Onborder between clusters\n-1: Poorly matched to each cluster\n\n\n\nHighest average silhouette width\n\nsil_width <- map_dbl(2:10, function(k){\n    model_sil <- pam(lineup, k=k)\n    model_sil$silinfo$avg.width\n})\nhead(sil_df <- data.frame(\n    k = 2:10,\n    sil_width = sil_width\n), 10)\n\n\n\nksil_width\n\n     2       0.4164141\n     3       0.3534140\n     4       0.3535534\n     5       0.3724115\n     6       0.3436130\n     7       0.3236397\n     8       0.3275222\n     9       0.2547311\n    10       0.2099424\n\n\n\n\n\n\nChoosing K using average silhouette width\n\nsil_df %>%\n    ggplot(aes(x=k, y=sil_width)) +\n    geom_line() +\n    scale_x_continuous(breaks = 2:10)"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#making-sense-of-the-k-means-clusters",
    "href": "posts/2020-10-19-cluster analysis in r.html#making-sense-of-the-k-means-clusters",
    "title": "Cluster Analysis in R",
    "section": "Making sense of the K-means clusters",
    "text": "Making sense of the K-means clusters\n\nSegmenting with K-means\n\nEstimate the “best” k using average silhouette width\nRun k-means with the suggested k\nCharacterize the spending habits of these clusters of customers\n\n\n\nRevisiting wholesale data: “Best” k\n\nhead(ws_customers, 10)\n\n\n\nMilkGroceryFrozen\n\n    1110312469 902 \n     2013 6550 909 \n     1897 5234 417 \n     1304 36433045 \n     3199 69861455 \n     4560 9965 934 \n      879 2060 264 \n     6243 6360 824 \n    13316203991809 \n     5302 9785 364 \n\n\n\n\n\n# Use map_dbl to run many models with varying value of k\nsil_width <- map_dbl(2:10,  function(k){\n  model <- pam(x = ws_customers, k = k)\n  model$silinfo$avg.width\n})\n\n# Generate a data frame containing both k and sil_width\nsil_df <- data.frame(\n  k = 2:10,\n  sil_width = sil_width\n)\n\n# Plot the relationship between k and sil_width\nggplot(sil_df, aes(x = k, y = sil_width)) +\n  geom_line() +\n  scale_x_continuous(breaks = 2:10)\n\n\n\n\nk = 2 has the highest average sillhouette width and is the “best” value of k we will move forward with.\n\n\nRevisiting wholesale data: Exploration\nFrom the previous analysis we have found that k = 2 has the highest average silhouette width. We will continue to analyze the wholesale customer data by building and exploring a kmeans model with 2 clusters.\n\nset.seed(42)\n\n# Build a k-means model for the customers_spend with a k of 2\nmodel_customers <- kmeans(ws_customers, centers=2)\n\n# Extract the vector of cluster assignments from the model\nclust_customers <- model_customers$cluster\n\n# Build the segment_customers data frame\nsegment_customers <- mutate(ws_customers, cluster = clust_customers)\n\n# Calculate the size of each cluster\ncount(segment_customers, cluster)\n\n\n\nclustern\n\n    1 35\n    2 10\n\n\n\n\n\n# Calculate the mean for each category\nsegment_customers %>% \n  group_by(cluster) %>% \n  summarise_all(list(mean))\n\n\n\nclusterMilkGroceryFrozen\n\n    1         2296.257 5004    3354.343 \n    2        13701.10017721    1173.000 \n\n\n\n\nIt seems that in this case cluster 1 consists of individuals who proportionally spend more on Frozen food while cluster 2 customers spent more on Milk and Grocery. When we explored this data using hierarchical clustering, the method resulted in 4 clusters while using k-means got us 2. Both of these results are valid, but which one is appropriate for this would require more subject matter expertise. Generating clusters is a science, but interpreting them is an art."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#occupational-wage-data",
    "href": "posts/2020-10-19-cluster analysis in r.html#occupational-wage-data",
    "title": "Cluster Analysis in R",
    "section": "Occupational wage data",
    "text": "Occupational wage data\ndata from the Occupational Employment Statistics (OES) program which produces employment and wage estimates annually. This data contains the yearly average income from 2001 to 2016 for 22 occupation groups. We would like to use this data to identify clusters of occupations that maintained similar income trends.\n\noes <- readRDS(gzcon(url(\"https://assets.datacamp.com/production/repositories/1219/datasets/1e1ec9f146a25d7c71a6f6f0f46c3de7bcefd36c/oes.rds\")))\nhead(oes)\n\n\n\n200120022003200420052006200720082010201120122013201420152016\n\n    Management70800 78870 83400 87090 88450 91930 96150 100310105440107410108570110550112490115020118020\n    Business Operations50580 53350 56000 57120 57930 60000 62410  64720 67690 68740 69550 71020 72410 73800 75070\n    Computer Science60350 61630 64150 66370 67100 69240 72190  74500 77230 78730 80180 82010 83970 86170 87880\n    Architecture/Engineering56330 58020 60390 63060 63910 66190 68880  71430 75550 77120 79000 80100 81520 82980 84300\n    Life/Physical/Social Sci.49710 52380 54930 57550 58030 59660 62020  64280 66390 67470 68360 69400 70070 71220 72930\n    Community Services34190 34630 35800 37050 37530 39000 40540  41790 43180 43830 44240 44710 45310 46160 47200\n\n\n\n\n\n22 Occupation Observations\n15 Measurements of Average Income from 2001-2016\n\n\nglimpse(oes)\n\n num [1:22, 1:15] 70800 50580 60350 56330 49710 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:22] \"Management\" \"Business Operations\" \"Computer Science\" \"Architecture/Engineering\" ...\n  ..$ : chr [1:15] \"2001\" \"2002\" \"2003\" \"2004\" ...\n\n\n\nsummary(oes)\n\n      2001            2002            2003            2004      \n Min.   :16720   Min.   :17180   Min.   :17400   Min.   :17620  \n 1st Qu.:26728   1st Qu.:27393   1st Qu.:27858   1st Qu.:28535  \n Median :34575   Median :35205   Median :36180   Median :37335  \n Mean   :37850   Mean   :39701   Mean   :41018   Mean   :42275  \n 3rd Qu.:49875   3rd Qu.:53108   3rd Qu.:55733   3rd Qu.:57443  \n Max.   :70800   Max.   :78870   Max.   :83400   Max.   :87090  \n      2005            2006            2007            2008       \n Min.   :17840   Min.   :18430   Min.   :19440   Min.   : 20220  \n 1st Qu.:29043   1st Qu.:29688   1st Qu.:30810   1st Qu.: 31643  \n Median :37790   Median :39030   Median :40235   Median : 41510  \n Mean   :42775   Mean   :44329   Mean   :46074   Mean   : 47763  \n 3rd Qu.:58005   3rd Qu.:59915   3rd Qu.:62313   3rd Qu.: 64610  \n Max.   :88450   Max.   :91930   Max.   :96150   Max.   :100310  \n      2010             2011             2012             2013       \n Min.   : 21240   Min.   : 21430   Min.   : 21380   Min.   : 21580  \n 1st Qu.: 32863   1st Qu.: 33430   1st Qu.: 33795   1st Qu.: 34120  \n Median : 42995   Median : 43610   Median : 44055   Median : 44565  \n Mean   : 49758   Mean   : 50555   Mean   : 51077   Mean   : 51800  \n 3rd Qu.: 67365   3rd Qu.: 68423   3rd Qu.: 69253   3rd Qu.: 70615  \n Max.   :105440   Max.   :107410   Max.   :108570   Max.   :110550  \n      2014             2015             2016       \n Min.   : 21980   Min.   : 22850   Min.   : 23850  \n 1st Qu.: 34718   1st Qu.: 35425   1st Qu.: 36350  \n Median : 45265   Median : 46075   Median : 46945  \n Mean   : 52643   Mean   : 53785   Mean   : 55117  \n 3rd Qu.: 71825   3rd Qu.: 73155   3rd Qu.: 74535  \n Max.   :112490   Max.   :115020   Max.   :118020  \n\n\n\ndim(oes)\n\n\n    22\n    15\n\n\n\nthere are no missing values, no categorical and the features are on the same scale.\n\nNext steps: hierarchical clustering\n\nEvaluate whether pre-processing is necessary\nCreate a distance matrix\nBuild a dendrogram\nExtract clusters from dendrogram\nExplore resulting clusters\n\n\n\nHierarchical clustering: Occupation trees\nThe oes data is ready for hierarchical clustering without any preprocessing steps necessary. We will take the necessary steps to build a dendrogram of occupations based on their yearly average salaries and propose clusters using a height of 100,000.\n\n# Calculate Euclidean distance between the occupations\ndist_oes <- dist(oes, method = \"euclidean\")\n\n# Generate an average linkage analysis \nhc_oes <- hclust(dist_oes, method = \"average\")\n\n# Create a dendrogram object from the hclust variable\ndend_oes <- as.dendrogram(hc_oes)\n\n# Plot the dendrogram\nplot(dend_oes)\n\n\n\n\n\n# Color branches by cluster formed from the cut at a height of 100000\ndend_colored <- color_branches(dend_oes, h = 100000)\n\n# Plot the colored dendrogram\nplot(dend_colored)\n\n\n\n\nBased on the dendrogram it may be reasonable to start with the three clusters formed at a height of 100,000. The members of these clusters appear to be tightly grouped but different from one another. Let’s continue this exploration.\n\n\nHierarchical clustering: Preparing for exploration\nWe have now created a potential clustering for the oes data, before we can explore these clusters with ggplot2 we will need to process the oes data matrix into a tidy data frame with each occupation assigned its cluster.\n\n# Use rownames_to_column to move the rownames into a column of the data frame\ndf_oes <- rownames_to_column(as.data.frame(oes), var = 'occupation')\n\n# Create a cluster assignment vector at h = 100,000\ncut_oes <- cutree(hc_oes, h = 100000)\n\n# Generate the segmented the oes data frame\nclust_oes <- mutate(df_oes, cluster = cut_oes)\n\n# Create a tidy data frame by gathering the year and values into two columns\nhead(gathered_oes <- gather(data = clust_oes, \n                       key = year, \n                       value = mean_salary, \n                       -occupation, -cluster), 10)\n\n\n\noccupationclusteryearmean_salary\n\n    Management                1                         2001                      70800                     \n    Business Operations       2                         2001                      50580                     \n    Computer Science          2                         2001                      60350                     \n    Architecture/Engineering  2                         2001                      56330                     \n    Life/Physical/Social Sci. 2                         2001                      49710                     \n    Community Services        3                         2001                      34190                     \n    Legal                     1                         2001                      69030                     \n    Education/Training/Library3                         2001                      39130                     \n    Arts/Design/Entertainment 3                         2001                      39770                     \n    Healthcare Practitioners  2                         2001                      49930                     \n\n\n\n\n\n\nHierarchical clustering: Plotting occupational clusters\nWe have succesfully created all the parts necessary to explore the results of this hierarchical clustering work. We will leverage the named assignment vector cut_oes and the tidy data frame gathered_oes to analyze the resulting clusters.\n\n# View the clustering assignments by sorting the cluster assignment vector\nsort(cut_oes)\n\n\n    Management\n        1\n    Legal\n        1\n    Business Operations\n        2\n    Computer Science\n        2\n    Architecture/Engineering\n        2\n    Life/Physical/Social Sci.\n        2\n    Healthcare Practitioners\n        2\n    Community Services\n        3\n    Education/Training/Library\n        3\n    Arts/Design/Entertainment\n        3\n    Healthcare Support\n        3\n    Protective Service\n        3\n    Food Preparation\n        3\n    Grounds Cleaning & Maint.\n        3\n    Personal Care\n        3\n    Sales\n        3\n    Office Administrative\n        3\n    Farming/Fishing/Forestry\n        3\n    Construction\n        3\n    Installation/Repair/Maint.\n        3\n    Production\n        3\n    Transportation/Moving\n        3\n\n\n\n\n# Plot the relationship between mean_salary and year and color the lines by the assigned cluster\nggplot(gathered_oes, aes(x = year, y = mean_salary, color = factor(cluster))) + \n    geom_line(aes(group = occupation))\n\n\n\n\nFrom this work it looks like both Management & Legal professions (cluster 1) experienced the most rapid growth in these 15 years. Let’s see what we can get by exploring this data using k-means."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in r.html#reviewing-the-hc-results",
    "href": "posts/2020-10-19-cluster analysis in r.html#reviewing-the-hc-results",
    "title": "Cluster Analysis in R",
    "section": "Reviewing the HC results",
    "text": "Reviewing the HC results\n\nNext steps: k-means clustering\n\nEvaluate whether pre-processing is necessary\nEstimate the “best” k using the elbow plot\nEstimate the “best” k using the maximum average silhouette width\nExplore resulting clusters\n\n\n\nK-means: Elbow analysis\nleverage the k-means elbow plot to propose the “best” number of clusters.\n\n# Use map_dbl to run many models with varying value of k (centers)\ntot_withinss <- map_dbl(1:10,  function(k){\n  model <- kmeans(x = oes, centers = k)\n  model$tot.withinss\n})\n\n# Generate a data frame containing both k and tot_withinss\nelbow_df <- data.frame(\n  k = 1:10,\n  tot_withinss = tot_withinss\n)\n\n# Plot the elbow plot\nggplot(elbow_df, aes(x = k, y = tot_withinss)) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\nK-means: Average Silhouette Widths\nSo hierarchical clustering resulting in 3 clusters and the elbow method suggests 2. We will use average silhouette widths to explore what the “best” value of k should be.\n\n# Use map_dbl to run many models with varying value of k\nsil_width <- map_dbl(2:10,  function(k){\n  model <- pam(oes, k = k)\n  model$silinfo$avg.width\n})\n\n# Generate a data frame containing both k and sil_width\nsil_df <- data.frame(\n  k = 2:10,\n  sil_width = sil_width\n)\n\n# Plot the relationship between k and sil_width\nggplot(sil_df, aes(x = k, y = sil_width)) +\n  geom_line() +\n  scale_x_continuous(breaks = 2:10)\n\n\n\n\nIt seems that this analysis results in another value of k, this time 7 is the top contender (although 2 comes very close).\n\n\nComparing the two clustering methods\n\n\n\n\n\n\n\n\n\nHierarchical Clustering\nk-means\n\n\n\n\nDistance Used:\nvirtually any\neuclidean only\n\n\nResults Stable:\nYes\nNo\n\n\nEvaluating # of Clusters:\ndendrogram, silhouette,elbow\nsilhouette,elbow\n\n\nComputation Complexity:\nRelatively Higher\nRelativelyLower"
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html",
    "title": "Analyzing Police Activity with Pandas",
    "section": "",
    "text": "Before beginning our analysis, it is critical that we first examine and clean the dataset, to make working with it a more efficient process. We will fixing data types, handle missing values, and dropping columns and rows while exploring the Stanford Open Policing Project dataset.\n\n\n\n\nWe’ll be analyzing a dataset of traffic stops in Rhode Island that was collected by the Stanford Open Policing Project.\nBefore beginning our analysis, it’s important that we familiarize yourself with the dataset. We read the dataset into pandas, examine the first few rows, and then count the number of missing values.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import CategoricalDtype\n\n\n# Read 'police.csv' into a DataFrame named ri\nri = pd.read_csv(\"../datasets/police.csv\")\n\n# Examine the head of the DataFrame\ndisplay(ri.head())\n\n# Count the number of missing values in each column\nri.isnull().sum()\n\n\n\n\n\n  \n    \n      \n      state\n      stop_date\n      stop_time\n      county_name\n      driver_gender\n      driver_race\n      violation_raw\n      violation\n      search_conducted\n      search_type\n      stop_outcome\n      is_arrested\n      stop_duration\n      drugs_related_stop\n      district\n    \n  \n  \n    \n      0\n      RI\n      2005-01-04\n      12:55\n      NaN\n      M\n      White\n      Equipment/Inspection Violation\n      Equipment\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n    \n    \n      1\n      RI\n      2005-01-23\n      23:15\n      NaN\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone K3\n    \n    \n      2\n      RI\n      2005-02-17\n      04:15\n      NaN\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n    \n    \n      3\n      RI\n      2005-02-20\n      17:15\n      NaN\n      M\n      White\n      Call for Service\n      Other\n      False\n      NaN\n      Arrest Driver\n      True\n      16-30 Min\n      False\n      Zone X1\n    \n    \n      4\n      RI\n      2005-02-24\n      01:20\n      NaN\n      F\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X3\n    \n  \n\n\n\n\nstate                     0\nstop_date                 0\nstop_time                 0\ncounty_name           91741\ndriver_gender          5205\ndriver_race            5202\nviolation_raw          5202\nviolation              5202\nsearch_conducted          0\nsearch_type           88434\nstop_outcome           5202\nis_arrested            5202\nstop_duration          5202\ndrugs_related_stop        0\ndistrict                  0\ndtype: int64\n\n\nIt looks like most of the columns have at least some missing values. We’ll figure out how to handle these values in the next.\n\n\n\n\nWe’ll drop the county_name column because it only contains missing values, and we’ll drop the state column because all of the traffic stops took place in one state (Rhode Island).\n\n# Examine the shape of the DataFrame\nprint(ri.shape)\n\n# Drop the 'county_name' and 'state' columns\nri.drop([\"county_name\", \"state\"], axis='columns', inplace=True)\n\n# Examine the shape of the DataFrame (again)\nprint(ri.shape)\n\n(91741, 15)\n(91741, 13)\n\n\nWe’ll continue to remove unnecessary data from the DataFrame\n### Dropping rows\nthe driver_gender column will be critical to many of our analyses. Because only a small fraction of rows are missing driver_gender, we’ll drop those rows from the dataset.\n\n# Count the number of missing values in each column\ndisplay(ri.isnull().sum())\n\n# Drop all rows that are missing 'driver_gender'\nri.dropna(subset=[\"driver_gender\"], inplace=True)\n\n# Count the number of missing values in each column (again)\ndisplay(ri.isnull().sum())\n\n# Examine the shape of the DataFrame\nri.shape\n\nstop_date                 0\nstop_time                 0\ndriver_gender          5205\ndriver_race            5202\nviolation_raw          5202\nviolation              5202\nsearch_conducted          0\nsearch_type           88434\nstop_outcome           5202\nis_arrested            5202\nstop_duration          5202\ndrugs_related_stop        0\ndistrict                  0\ndtype: int64\n\n\nstop_date                 0\nstop_time                 0\ndriver_gender             0\ndriver_race               0\nviolation_raw             0\nviolation                 0\nsearch_conducted          0\nsearch_type           83229\nstop_outcome              0\nis_arrested               0\nstop_duration             0\ndrugs_related_stop        0\ndistrict                  0\ndtype: int64\n\n\n(86536, 13)\n\n\nWe dropped around 5,000 rows, which is a small fraction of the dataset, and now only one column remains with any missing values.\n\n\n\n\n\n\n\nri.dtypes\n\nstop_date             object\nstop_time             object\ndriver_gender         object\ndriver_race           object\nviolation_raw         object\nviolation             object\nsearch_conducted        bool\nsearch_type           object\nstop_outcome          object\nis_arrested           object\nstop_duration         object\ndrugs_related_stop      bool\ndistrict              object\ndtype: object\n\n\n\nstop_date: should be datetime\nstop_time: should be datetime\ndriver_gender: should be category\ndriver_race: should be category\nviolation_raw: should be category\nviolation: should be category\ndistrict: should be category\nis_arrested: should be bool\n\nWe’ll fix the data type of the is_arrested column\n\n# Examine the head of the 'is_arrested' column\ndisplay(ri.is_arrested.head())\n\n# Change the data type of 'is_arrested' to 'bool'\nri['is_arrested'] = ri.is_arrested.astype('bool')\n\n# Check the data type of 'is_arrested' \nri.is_arrested.dtype\n\n0    False\n1    False\n2    False\n3     True\n4    False\nName: is_arrested, dtype: object\n\n\ndtype('bool')\n\n\n\n\n\n\n\n\nCurrently, the date and time of each traffic stop are stored in separate object columns: stop_date and stop_time. We’ll combine these two columns into a single column, and then convert it to datetime format.\n\nri['stop_date_time'] = pd.to_datetime(ri.stop_date.str.replace(\"/\", \"-\").str.cat(ri.stop_time, sep=\" \"))\nri.dtypes\n\nstop_date                     object\nstop_time                     object\ndriver_gender                 object\ndriver_race                   object\nviolation_raw                 object\nviolation                     object\nsearch_conducted                bool\nsearch_type                   object\nstop_outcome                  object\nis_arrested                     bool\nstop_duration                 object\ndrugs_related_stop              bool\ndistrict                      object\nstop_date_time        datetime64[ns]\ndtype: object\n\n\n\n\n\n\n# Set 'stop_datetime' as the index\nri.set_index(\"stop_date_time\", inplace=True)\n\n# Examine the index\ndisplay(ri.index)\n\n# Examine the columns\nri.columns\n\nDatetimeIndex(['2005-01-04 12:55:00', '2005-01-23 23:15:00',\n               '2005-02-17 04:15:00', '2005-02-20 17:15:00',\n               '2005-02-24 01:20:00', '2005-03-14 10:00:00',\n               '2005-03-29 21:55:00', '2005-04-04 21:25:00',\n               '2005-07-14 11:20:00', '2005-07-14 19:55:00',\n               ...\n               '2015-12-31 13:23:00', '2015-12-31 18:59:00',\n               '2015-12-31 19:13:00', '2015-12-31 20:20:00',\n               '2015-12-31 20:50:00', '2015-12-31 21:21:00',\n               '2015-12-31 21:59:00', '2015-12-31 22:04:00',\n               '2015-12-31 22:09:00', '2015-12-31 22:47:00'],\n              dtype='datetime64[ns]', name='stop_date_time', length=86536, freq=None)\n\n\nIndex(['stop_date', 'stop_time', 'driver_gender', 'driver_race',\n       'violation_raw', 'violation', 'search_conducted', 'search_type',\n       'stop_outcome', 'is_arrested', 'stop_duration', 'drugs_related_stop',\n       'district'],\n      dtype='object')"
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#do-the-genders-commit-different-violations",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#do-the-genders-commit-different-violations",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Do the genders commit different violations?",
    "text": "Do the genders commit different violations?\n\nExamining traffic violations\nBefore comparing the violations being committed by each gender, we should examine the violations committed by all drivers to get a baseline understanding of the data.\nWe’ll count the unique values in the violation column, and then separately express those counts as proportions.\n\n# Count the unique values in 'violation'\ndisplay(ri.violation.value_counts())\n\n# Express the counts as proportions\nri.violation.value_counts(normalize=True)\n\nSpeeding               48423\nMoving violation       16224\nEquipment              10921\nOther                   4409\nRegistration/plates     3703\nSeat belt               2856\nName: violation, dtype: int64\n\n\nSpeeding               0.559571\nMoving violation       0.187483\nEquipment              0.126202\nOther                  0.050950\nRegistration/plates    0.042791\nSeat belt              0.033004\nName: violation, dtype: float64\n\n\nMore than half of all violations are for speeding, followed by other moving violations and equipment violations.\n\n\nComparing violations by gender\nThe question we’re trying to answer is whether male and female drivers tend to commit different types of traffic violations.\nWe’ll first create a DataFrame for each gender, and then analyze the violations in each DataFrame separately.\n\n# Create a DataFrame of female drivers\nfemale = ri[ri.driver_gender==\"F\"]\n\n# Create a DataFrame of male drivers\nmale = ri[ri.driver_gender==\"M\"]\n\n# Compute the violations by female drivers (as proportions)\ndisplay(female.violation.value_counts(normalize=True))\n\n# Compute the violations by male drivers (as proportions)\nmale.violation.value_counts(normalize=True)\n\nSpeeding               0.658114\nMoving violation       0.138218\nEquipment              0.105199\nRegistration/plates    0.044418\nOther                  0.029738\nSeat belt              0.024312\nName: violation, dtype: float64\n\n\nSpeeding               0.522243\nMoving violation       0.206144\nEquipment              0.134158\nOther                  0.058985\nRegistration/plates    0.042175\nSeat belt              0.036296\nName: violation, dtype: float64\n\n\nAbout two-thirds of female traffic stops are for speeding, whereas stops of males are more balanced among the six categories. This doesn’t mean that females speed more often than males, however, since we didn’t take into account the number of stops or drivers."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#does-gender-affect-who-gets-a-ticket-for-speeding",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#does-gender-affect-who-gets-a-ticket-for-speeding",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does gender affect who gets a ticket for speeding?",
    "text": "Does gender affect who gets a ticket for speeding?\n\nComparing speeding outcomes by gender\nWhen a driver is pulled over for speeding, many people believe that gender has an impact on whether the driver will receive a ticket or a warning. Can we find evidence of this in the dataset?\nFirst, we’ll create two DataFrames of drivers who were stopped for speeding: one containing females and the other containing males.\nThen, for each gender, we’ll use the stop_outcome column to calculate what percentage of stops resulted in a “Citation” (meaning a ticket) versus a “Warning”.\n\n# Create a DataFrame of female drivers stopped for speeding\nfemale_and_speeding = ri[(ri.driver_gender==\"F\") & (ri.violation ==\"Speeding\")]\n\n# Create a DataFrame of male drivers stopped for speeding\nmale_and_speeding = ri[(ri.driver_gender==\"M\") & (ri.violation ==\"Speeding\")]\n\n# Compute the stop outcomes for female drivers (as proportions)\ndisplay(female_and_speeding.stop_outcome.value_counts(normalize=True))\n\n# Compute the stop outcomes for male drivers (as proportions)\nmale_and_speeding.stop_outcome.value_counts(normalize=True)\n\nCitation            0.952192\nWarning             0.040074\nArrest Driver       0.005752\nN/D                 0.000959\nArrest Passenger    0.000639\nNo Action           0.000383\nName: stop_outcome, dtype: float64\n\n\nCitation            0.944595\nWarning             0.036184\nArrest Driver       0.015895\nArrest Passenger    0.001281\nNo Action           0.001068\nN/D                 0.000976\nName: stop_outcome, dtype: float64\n\n\nThe numbers are similar for males and females: about 95% of stops for speeding result in a ticket. Thus, the data fails to show that gender has an impact on who gets a ticket for speeding.\n## Does gender affect whose vehicle is searched? ### Calculating the search rate\nDuring a traffic stop, the police officer sometimes conducts a search of the vehicle. We’ll calculate the percentage of all stops in the ri DataFrame that result in a vehicle search, also known as the search rate.\n\n# Check the data type of 'search_conducted'\nprint(ri.search_conducted.dtype)\n\n# Calculate the search rate by counting the values\ndisplay(ri.search_conducted.value_counts(normalize=True))\n\n# Calculate the search rate by taking the mean\nri.search_conducted.mean()\n\nbool\n\n\nFalse    0.961785\nTrue     0.038215\nName: search_conducted, dtype: float64\n\n\n0.0382153092354627\n\n\nIt looks like the search rate is about 3.8%. Next, we’ll examine whether the search rate varies by driver gender.\n### Comparing search rates by gender\nWe’ll compare the rates at which female and male drivers are searched during a traffic stop. Remember that the vehicle search rate across all stops is about 3.8%.\nFirst, we’ll filter the DataFrame by gender and calculate the search rate for each group separately. Then, we’ll perform the same calculation for both genders at once using a .groupby().\n\nri[ri.driver_gender==\"F\"].search_conducted.mean()\n\n0.019180617481282074\n\n\n\nri[ri.driver_gender==\"M\"].search_conducted.mean()\n\n0.04542557598546892\n\n\n\nri.groupby(\"driver_gender\").search_conducted.mean()\n\ndriver_gender\nF    0.019181\nM    0.045426\nName: search_conducted, dtype: float64\n\n\nMale drivers are searched more than twice as often as female drivers. Why might this be?\n\n\nAdding a second factor to the analysis\nEven though the search rate for males is much higher than for females, it’s possible that the difference is mostly due to a second factor.\nFor example, we might hypothesize that the search rate varies by violation type, and the difference in search rate between males and females is because they tend to commit different violations.\nwe can test this hypothesis by examining the search rate for each combination of gender and violation. If the hypothesis was true, out would find that males and females are searched at about the same rate for each violation. Let’s find out below if that’s the case!\n\n# Calculate the search rate for each combination of gender and violation\nri.groupby([\"driver_gender\", \"violation\"]).search_conducted.mean()\n\ndriver_gender  violation          \nF              Equipment              0.039984\n               Moving violation       0.039257\n               Other                  0.041018\n               Registration/plates    0.054924\n               Seat belt              0.017301\n               Speeding               0.008309\nM              Equipment              0.071496\n               Moving violation       0.061524\n               Other                  0.046191\n               Registration/plates    0.108802\n               Seat belt              0.035119\n               Speeding               0.027885\nName: search_conducted, dtype: float64\n\n\n\nri.groupby([\"violation\", \"driver_gender\"]).search_conducted.mean()\n\nviolation            driver_gender\nEquipment            F                0.039984\n                     M                0.071496\nMoving violation     F                0.039257\n                     M                0.061524\nOther                F                0.041018\n                     M                0.046191\nRegistration/plates  F                0.054924\n                     M                0.108802\nSeat belt            F                0.017301\n                     M                0.035119\nSpeeding             F                0.008309\n                     M                0.027885\nName: search_conducted, dtype: float64\n\n\nFor all types of violations, the search rate is higher for males than for females, disproving our hypothesis."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#does-gender-affect-who-is-frisked-during-a-search",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#does-gender-affect-who-is-frisked-during-a-search",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does gender affect who is frisked during a search?",
    "text": "Does gender affect who is frisked during a search?\n\nCounting protective frisks\nDuring a vehicle search, the police officer may pat down the driver to check if they have a weapon. This is known as a “protective frisk.”\nWe’ll first check to see how many times “Protective Frisk” was the only search type. Then, we’ll use a string method to locate all instances in which the driver was frisked.\n\n# Count the 'search_type' values\ndisplay(ri.search_type.value_counts())\n\n# Check if 'search_type' contains the string 'Protective Frisk'\nri['frisk'] = ri.search_type.str.contains('Protective Frisk', na=False)\n\n# Check the data type of 'frisk'\nprint(ri.frisk.dtype)\n\n# Take the sum of 'frisk'\nprint(ri.frisk.sum())\n\nIncident to Arrest                                          1290\nProbable Cause                                               924\nInventory                                                    219\nReasonable Suspicion                                         214\nProtective Frisk                                             164\nIncident to Arrest,Inventory                                 123\nIncident to Arrest,Probable Cause                            100\nProbable Cause,Reasonable Suspicion                           54\nProbable Cause,Protective Frisk                               35\nIncident to Arrest,Inventory,Probable Cause                   35\nIncident to Arrest,Protective Frisk                           33\nInventory,Probable Cause                                      25\nProtective Frisk,Reasonable Suspicion                         19\nIncident to Arrest,Inventory,Protective Frisk                 18\nIncident to Arrest,Probable Cause,Protective Frisk            13\nInventory,Protective Frisk                                    12\nIncident to Arrest,Reasonable Suspicion                        8\nProbable Cause,Protective Frisk,Reasonable Suspicion           5\nIncident to Arrest,Probable Cause,Reasonable Suspicion         5\nIncident to Arrest,Inventory,Reasonable Suspicion              4\nIncident to Arrest,Protective Frisk,Reasonable Suspicion       2\nInventory,Reasonable Suspicion                                 2\nInventory,Probable Cause,Protective Frisk                      1\nInventory,Probable Cause,Reasonable Suspicion                  1\nInventory,Protective Frisk,Reasonable Suspicion                1\nName: search_type, dtype: int64\n\n\nbool\n303\n\n\nIt looks like there were 303 drivers who were frisked. Next, we’ll examine whether gender affects who is frisked.\n\n\nComparing frisk rates by gender\nWe’ll compare the rates at which female and male drivers are frisked during a search. Are males frisked more often than females, perhaps because police officers consider them to be higher risk?\nBefore doing any calculations, it’s important to filter the DataFrame to only include the relevant subset of data, namely stops in which a search was conducted.\n\n# Create a DataFrame of stops in which a search was conducted\nsearched = ri[ri.search_conducted == True]\n\n# Calculate the overall frisk rate by taking the mean of 'frisk'\nprint(searched.frisk.mean())\n\n# Calculate the frisk rate for each gender\nsearched.groupby(\"driver_gender\").frisk.mean()\n\n0.09162382824312065\n\n\ndriver_gender\nF    0.074561\nM    0.094353\nName: frisk, dtype: float64\n\n\nThe frisk rate is higher for males than for females, though we can’t conclude that this difference is caused by the driver’s gender."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#does-time-of-the-day-affect-arrest-rate",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#does-time-of-the-day-affect-arrest-rate",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does time of the day affect arrest rate?",
    "text": "Does time of the day affect arrest rate?\n\nCalculating the hourly arrest rate\nWhen a police officer stops a driver, a small percentage of those stops ends in an arrest. This is known as the arrest rate. We’ll find out whether the arrest rate varies by time of day.\nFirst, we’ll calculate the arrest rate across all stops in the ri DataFrame. Then, we’ll calculate the hourly arrest rate by using the hour attribute of the index. The hour ranges from 0 to 23, in which:\n\n0 = midnight\n12 = noon\n23 = 11 PM\n\n\n# Calculate the overall arrest rate\nprint(ri.is_arrested.mean())\n\n# Calculate the hourly arrest rate\n\n# Save the hourly arrest rate\nhourly_arrest_rate = ri.groupby(ri.index.hour).is_arrested.mean()\nhourly_arrest_rate\n\n0.0355690117407784\n\n\nstop_date_time\n0     0.051431\n1     0.064932\n2     0.060798\n3     0.060549\n4     0.048000\n5     0.042781\n6     0.013813\n7     0.013032\n8     0.021854\n9     0.025206\n10    0.028213\n11    0.028897\n12    0.037399\n13    0.030776\n14    0.030605\n15    0.030679\n16    0.035281\n17    0.040619\n18    0.038204\n19    0.032245\n20    0.038107\n21    0.064541\n22    0.048666\n23    0.047592\nName: is_arrested, dtype: float64\n\n\nNext we’ll plot the data so that you can visually examine the arrest rate trends.\n### Plotting the hourly arrest rate\nWe’ll create a line plot from the hourly_arrest_rate object.\n\n\n\n\n\n\nImportant\n\n\n\nA line plot is appropriate in this case because you’re showing how a quantity changes over time.\n\n\nThis plot should help us to spot some trends that may not have been obvious when examining the raw numbers!\n\n# Create a line plot of 'hourly_arrest_rate'\nhourly_arrest_rate.plot()\n\n# Add the xlabel, ylabel, and title\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Arrest Rate\")\nplt.title(\"Arrest Rate by Time of Day\")\n\n# Display the plot\nplt.show()\n\n\n\n\nThe arrest rate has a significant spike overnight, and then dips in the early morning hours.\n## Are drug-related stops on the rise?\n\n\nPlotting drug-related stops\nIn a small portion of traffic stops, drugs are found in the vehicle during a search. In this exercise, you’ll assess whether these drug-related stops are becoming more common over time.\nThe Boolean column drugs_related_stop indicates whether drugs were found during a given stop. We’ll calculate the annual drug rate by resampling this column, and then we’ll use a line plot to visualize how the rate has changed over time.\n\n# Calculate the annual rate of drug-related stops\n# Save the annual rate of drug-related stops\nannual_drug_rate = ri.drugs_related_stop.resample(\"A\").mean()\ndisplay(annual_drug_rate)\n\n# Create a line plot of 'annual_drug_rate'\nannual_drug_rate.plot()\n\n# Display the plot\nplt.show()\n\nstop_date_time\n2005-12-31    0.006501\n2006-12-31    0.007258\n2007-12-31    0.007970\n2008-12-31    0.007505\n2009-12-31    0.009889\n2010-12-31    0.010081\n2011-12-31    0.009731\n2012-12-31    0.009921\n2013-12-31    0.013094\n2014-12-31    0.013826\n2015-12-31    0.012266\nFreq: A-DEC, Name: drugs_related_stop, dtype: float64\n\n\n\n\n\nThe rate of drug-related stops nearly doubled over the course of 10 years. Why might that be the case?\n\n\nComparing drug and search rates\nThe rate of drug-related stops increased significantly between 2005 and 2015. We might hypothesize that the rate of vehicle searches was also increasing, which would have led to an increase in drug-related stops even if more drivers were not carrying drugs.\nWe can test this hypothesis by calculating the annual search rate, and then plotting it against the annual drug rate. If the hypothesis is true, then we’ll see both rates increasing over time.\n\n# Calculate and save the annual search rate\nannual_search_rate = ri.search_conducted.resample(\"A\").mean()\n\n# Concatenate 'annual_drug_rate' and 'annual_search_rate'\nannual = pd.concat([annual_drug_rate, annual_search_rate], axis=\"columns\")\n\n# Create subplots from 'annual'\nannual.plot(subplots=True)\n\n# Display the subplots\nplt.show()\n\n\n\n\nThe rate of drug-related stops increased even though the search rate decreased, disproving our hypothesis."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#what-violations-are-caught-in-each-district",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#what-violations-are-caught-in-each-district",
    "title": "Analyzing Police Activity with Pandas",
    "section": "What violations are caught in each district?",
    "text": "What violations are caught in each district?\n\nTallying violations by district\nThe state of Rhode Island is broken into six police districts, also known as zones. How do the zones compare in terms of what violations are caught by police?\nWe’ll create a frequency table to determine how many violations of each type took place in each of the six zones. Then, we’ll filter the table to focus on the “K” zones, which we’ll examine further.\n\n# Create a frequency table of districts and violations\n# Save the frequency table as 'all_zones'\nall_zones = pd.crosstab(ri.district, ri.violation)\ndisplay(all_zones)\n\n# Select rows 'Zone K1' through 'Zone K3'\n# Save the smaller table as 'k_zones'\nk_zones = all_zones.loc[\"Zone K1\":\"Zone K3\"]\nk_zones\n\n\n\n\n\n  \n    \n      violation\n      Equipment\n      Moving violation\n      Other\n      Registration/plates\n      Seat belt\n      Speeding\n    \n    \n      district\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Zone K1\n      672\n      1254\n      290\n      120\n      0\n      5960\n    \n    \n      Zone K2\n      2061\n      2962\n      942\n      768\n      481\n      10448\n    \n    \n      Zone K3\n      2302\n      2898\n      705\n      695\n      638\n      12322\n    \n    \n      Zone X1\n      296\n      671\n      143\n      38\n      74\n      1119\n    \n    \n      Zone X3\n      2049\n      3086\n      769\n      671\n      820\n      8779\n    \n    \n      Zone X4\n      3541\n      5353\n      1560\n      1411\n      843\n      9795\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      violation\n      Equipment\n      Moving violation\n      Other\n      Registration/plates\n      Seat belt\n      Speeding\n    \n    \n      district\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Zone K1\n      672\n      1254\n      290\n      120\n      0\n      5960\n    \n    \n      Zone K2\n      2061\n      2962\n      942\n      768\n      481\n      10448\n    \n    \n      Zone K3\n      2302\n      2898\n      705\n      695\n      638\n      12322\n    \n  \n\n\n\n\nWe’ll plot the violations so that you can compare these districts.\n\n\nPlotting violations by district\nNow that we’ve created a frequency table focused on the “K” zones, we’ll visualize the data to help us compare what violations are being caught in each zone.\nFirst we’ll create a bar plot, which is an appropriate plot type since we’re comparing categorical data. Then we’ll create a stacked bar plot in order to get a slightly different look at the data.\n\n# Create a bar plot of 'k_zones'\nk_zones.plot(kind=\"bar\")\n\n# Display the plot\nplt.show()\n\n\n\n\n\n# Create a stacked bar plot of 'k_zones'\nk_zones.plot(kind=\"bar\", stacked=True)\n\n# Display the plot\nplt.show()\n\n\n\n\nThe vast majority of traffic stops in Zone K1 are for speeding, and Zones K2 and K3 are remarkably similar to one another in terms of violations."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#how-long-might-you-be-stopped-for-a-violation",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#how-long-might-you-be-stopped-for-a-violation",
    "title": "Analyzing Police Activity with Pandas",
    "section": "How long might you be stopped for a violation?",
    "text": "How long might you be stopped for a violation?\n\nConverting stop durations to numbers\nIn the traffic stops dataset, the stop_duration column tells us approximately how long the driver was detained by the officer. Unfortunately, the durations are stored as strings, such as '0-15 Min'. How can we make this data easier to analyze?\nWe’ll convert the stop durations to integers. Because the precise durations are not available, we’ll have to estimate the numbers using reasonable values:\n\nConvert '0-15 Min' to 8\nConvert '16-30 Min' to 23\nConvert '30+ Min' to 45\n\n\n# Create a dictionary that maps strings to integers\nmapping = {\"0-15 Min\":8, '16-30 Min':23, '30+ Min':45}\n\n# Convert the 'stop_duration' strings to integers using the 'mapping'\nri['stop_minutes'] = ri.stop_duration.map(mapping)\n\n# Print the unique values in 'stop_minutes'\nri.stop_minutes.unique()\n\narray([ 8, 23, 45], dtype=int64)\n\n\nNext we’ll analyze the stop length for each type of violation.\n\n\nPlotting stop length\nIf you were stopped for a particular violation, how long might you expect to be detained?\nWe’ll visualize the average length of time drivers are stopped for each type of violation. Rather than using the violation column we’ll use violation_raw since it contains more detailed descriptions of the violations.\n\n# Calculate the mean 'stop_minutes' for each value in 'violation_raw'\n# Save the resulting Series as 'stop_length'\nstop_length = ri.groupby(\"violation_raw\").stop_minutes.mean()\ndisplay(stop_length)\n\n# Sort 'stop_length' by its values and create a horizontal bar plot\nstop_length.sort_values().plot(kind=\"barh\")\n\n# Display the plot\nplt.show()\n\nviolation_raw\nAPB                                 17.967033\nCall for Service                    22.124371\nEquipment/Inspection Violation      11.445655\nMotorist Assist/Courtesy            17.741463\nOther Traffic Violation             13.844490\nRegistration Violation              13.736970\nSeatbelt Violation                   9.662815\nSpecial Detail/Directed Patrol      15.123632\nSpeeding                            10.581562\nSuspicious Person                   14.910714\nViolation of City/Town Ordinance    13.254144\nWarrant                             24.055556\nName: stop_minutes, dtype: float64"
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#exploring-the-weather-dataset",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#exploring-the-weather-dataset",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Exploring the weather dataset",
    "text": "Exploring the weather dataset\n\nPlotting the temperature\nWe’ll examine the temperature columns from the weather dataset to assess whether the data seems trustworthy. First we’ll print the summary statistics, and then you’ll visualize the data using a box plot.\n\n# Read 'weather.csv' into a DataFrame named 'weather'\nweather = pd.read_csv(\"../datasets/weather.csv\")\ndisplay(weather.head())\n\n# Describe the temperature columns\ndisplay(weather[[\"TMIN\", \"TAVG\", \"TMAX\"]].describe().T)\n\n# Create a box plot of the temperature columns\nweather[[\"TMIN\", \"TAVG\", \"TMAX\"]].plot(kind='box')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n  \n    \n      \n      STATION\n      DATE\n      TAVG\n      TMIN\n      TMAX\n      AWND\n      WSF2\n      WT01\n      WT02\n      WT03\n      ...\n      WT11\n      WT13\n      WT14\n      WT15\n      WT16\n      WT17\n      WT18\n      WT19\n      WT21\n      WT22\n    \n  \n  \n    \n      0\n      USW00014765\n      2005-01-01\n      44.0\n      35\n      53\n      8.95\n      25.1\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      USW00014765\n      2005-01-02\n      36.0\n      28\n      44\n      9.40\n      14.1\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      USW00014765\n      2005-01-03\n      49.0\n      44\n      53\n      6.93\n      17.0\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      NaN\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      USW00014765\n      2005-01-04\n      42.0\n      39\n      45\n      6.93\n      16.1\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      1.0\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      USW00014765\n      2005-01-05\n      36.0\n      28\n      43\n      7.83\n      17.0\n      1.0\n      NaN\n      NaN\n      ...\n      NaN\n      1.0\n      NaN\n      NaN\n      1.0\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 27 columns\n\n\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      TMIN\n      4017.0\n      43.484441\n      17.020298\n      -5.0\n      30.0\n      44.0\n      58.0\n      77.0\n    \n    \n      TAVG\n      1217.0\n      52.493016\n      17.830714\n      6.0\n      39.0\n      54.0\n      68.0\n      86.0\n    \n    \n      TMAX\n      4017.0\n      61.268608\n      18.199517\n      15.0\n      47.0\n      62.0\n      77.0\n      102.0\n    \n  \n\n\n\n\n\n\n\nThe temperature data looks good so far: the TAVG values are in between TMIN and TMAX, and the measurements and ranges seem reasonable.\n### Plotting the temperature difference\nWe’ll continue to assess whether the dataset seems trustworthy by plotting the difference between the maximum and minimum temperatures.\n\n# Create a 'TDIFF' column that represents temperature difference\nweather[\"TDIFF\"] = weather.TMAX - weather.TMIN\n\n# Describe the 'TDIFF' column\ndisplay(weather.TDIFF.describe())\n\n# Create a histogram with 20 bins to visualize 'TDIFF'\nweather.TDIFF.plot(kind=\"hist\", bins=20)\n\n# Display the plot\nplt.show()\n\ncount    4017.000000\nmean       17.784167\nstd         6.350720\nmin         2.000000\n25%        14.000000\n50%        18.000000\n75%        22.000000\nmax        43.000000\nName: TDIFF, dtype: float64\n\n\n\n\n\nThe TDIFF column has no negative values and its distribution is approximately normal, both of which are signs that the data is trustworthy."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#categorizing-the-weather",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#categorizing-the-weather",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Categorizing the weather",
    "text": "Categorizing the weather\n\nCounting bad weather conditions\nThe weather DataFrame contains 20 columns that start with 'WT', each of which represents a bad weather condition. For example:\n\nWT05 indicates “Hail”\nWT11 indicates “High or damaging winds”\nWT17 indicates “Freezing rain”\n\nFor every row in the dataset, each WT column contains either a 1 (meaning the condition was present that day) or NaN (meaning the condition was not present).\nWe’ll quantify “how bad” the weather was each day by counting the number of 1 values in each row.\n\n# Copy 'WT01' through 'WT22' to a new DataFrame\nWT = weather.loc[:, \"WT01\":\"WT22\"]\n\n# Calculate the sum of each row in 'WT'\nweather['bad_conditions'] = WT.sum(axis=\"columns\")\n\n# Replace missing values in 'bad_conditions' with '0'\nweather['bad_conditions'] = weather.bad_conditions.fillna(0).astype('int')\n\n# Create a histogram to visualize 'bad_conditions'\nweather.bad_conditions.plot(kind=\"hist\")\n\n# Display the plot\nplt.show()\n\n\n\n\nIt looks like many days didn’t have any bad weather conditions, and only a small portion of days had more than four bad weather conditions.\n\n\nRating the weather conditions\nWe counted the number of bad weather conditions each day. We’ll use the counts to create a rating system for the weather.\nThe counts range from 0 to 9, and should be converted to ratings as follows:\n\nConvert 0 to ‘good’\nConvert 1 through 4 to ‘bad’\nConvert 5 through 9 to ‘worse’\n\n\n# Count the unique values in 'bad_conditions' and sort the index\ndisplay(weather.bad_conditions.value_counts().sort_index())\n\n# Create a dictionary that maps integers to strings\nmapping = {0:'good', 1:'bad', 2:'bad', 3:'bad', 4:'bad', 5:'worse', 6:'worse', 7:'worse', 8:'worse', 9:'worse'}\n\n# Convert the 'bad_conditions' integers to strings using the 'mapping'\nweather['rating'] = weather.bad_conditions.map(mapping)\n\n# Count the unique values in 'rating'\nweather.rating.value_counts()\n\n0    1749\n1     613\n2     367\n3     380\n4     476\n5     282\n6     101\n7      41\n8       4\n9       4\nName: bad_conditions, dtype: int64\n\n\nbad      1836\ngood     1749\nworse     432\nName: rating, dtype: int64\n\n\n\n\nChanging the data type to category\nSince the rating column only has a few possible values, we’ll change its data type to category in order to store the data more efficiently. we’ll also specify a logical order for the categories, which will be useful for future work.\n\n# Create a list of weather ratings in logical order\ncats = ['good', 'bad', 'worse']\n# Change the data type of 'rating' to category\nweather['rating'] = weather.rating.astype(CategoricalDtype(ordered=True, categories=cats))\n\n# Examine the head of 'rating'\nweather.rating.head()\n\n0    bad\n1    bad\n2    bad\n3    bad\n4    bad\nName: rating, dtype: category\nCategories (3, object): [good < bad < worse]\n\n\nWe’ll use the rating column in future exercises to analyze the effects of weather on police behavior."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#merging-datasets",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#merging-datasets",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Merging datasets",
    "text": "Merging datasets\n\nPreparing the DataFrames\nWe’ll prepare the traffic stop and weather rating DataFrames so that they’re ready to be merged:\n\nWith the ri DataFrame, we’ll move the stop_datetime index to a column since the index will be lost during the merge.\nWith the weather DataFrame, we’ll select the DATE and rating columns and put them in a new DataFrame.\n\n\n# Reset the index of 'ri'\nri.reset_index(inplace=True)\n\n# Examine the head of 'ri'\ndisplay(ri.head())\n\n# Create a DataFrame from the 'DATE' and 'rating' columns\nweather_rating = weather[[\"DATE\", \"rating\"]]\n\n# Examine the head of 'weather_rating'\nweather_rating.head()\n\n\n\n\n\n  \n    \n      \n      stop_date_time\n      stop_date\n      stop_time\n      driver_gender\n      driver_race\n      violation_raw\n      violation\n      search_conducted\n      search_type\n      stop_outcome\n      is_arrested\n      stop_duration\n      drugs_related_stop\n      district\n      frisk\n      stop_minutes\n    \n  \n  \n    \n      0\n      2005-01-04 12:55:00\n      2005-01-04\n      12:55\n      M\n      White\n      Equipment/Inspection Violation\n      Equipment\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n    \n    \n      1\n      2005-01-23 23:15:00\n      2005-01-23\n      23:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone K3\n      False\n      8\n    \n    \n      2\n      2005-02-17 04:15:00\n      2005-02-17\n      04:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n    \n    \n      3\n      2005-02-20 17:15:00\n      2005-02-20\n      17:15\n      M\n      White\n      Call for Service\n      Other\n      False\n      NaN\n      Arrest Driver\n      True\n      16-30 Min\n      False\n      Zone X1\n      False\n      23\n    \n    \n      4\n      2005-02-24 01:20:00\n      2005-02-24\n      01:20\n      F\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X3\n      False\n      8\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      DATE\n      rating\n    \n  \n  \n    \n      0\n      2005-01-01\n      bad\n    \n    \n      1\n      2005-01-02\n      bad\n    \n    \n      2\n      2005-01-03\n      bad\n    \n    \n      3\n      2005-01-04\n      bad\n    \n    \n      4\n      2005-01-05\n      bad\n    \n  \n\n\n\n\nThe ri and weather_rating DataFrames are now ready to be merged.\n\n\nMerging the DataFrames\nWe’ll merge the ri and weather_rating DataFrames into a new DataFrame, ri_weather.\nThe DataFrames will be joined using the stop_date column from ri and the DATE column from weather_rating. Thankfully the date formatting matches exactly, which is not always the case!\nOnce the merge is complete, we’ll set stop_datetime as the index\n\n# Examine the shape of 'ri'\nprint(ri.shape)\n\n# Merge 'ri' and 'weather_rating' using a left join\nri_weather = pd.merge(left=ri, right=weather_rating, left_on='stop_date', right_on='DATE', how='left')\n\n# Examine the shape of 'ri_weather'\nprint(ri_weather.shape)\n\n# Set 'stop_datetime' as the index of 'ri_weather'\nri_weather.set_index('stop_date_time', inplace=True)\nri_weather.head()\n\n(86536, 16)\n(86536, 18)\n\n\n\n\n\n\n  \n    \n      \n      stop_date\n      stop_time\n      driver_gender\n      driver_race\n      violation_raw\n      violation\n      search_conducted\n      search_type\n      stop_outcome\n      is_arrested\n      stop_duration\n      drugs_related_stop\n      district\n      frisk\n      stop_minutes\n      DATE\n      rating\n    \n    \n      stop_date_time\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2005-01-04 12:55:00\n      2005-01-04\n      12:55\n      M\n      White\n      Equipment/Inspection Violation\n      Equipment\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n      2005-01-04\n      bad\n    \n    \n      2005-01-23 23:15:00\n      2005-01-23\n      23:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone K3\n      False\n      8\n      2005-01-23\n      worse\n    \n    \n      2005-02-17 04:15:00\n      2005-02-17\n      04:15\n      M\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X4\n      False\n      8\n      2005-02-17\n      good\n    \n    \n      2005-02-20 17:15:00\n      2005-02-20\n      17:15\n      M\n      White\n      Call for Service\n      Other\n      False\n      NaN\n      Arrest Driver\n      True\n      16-30 Min\n      False\n      Zone X1\n      False\n      23\n      2005-02-20\n      bad\n    \n    \n      2005-02-24 01:20:00\n      2005-02-24\n      01:20\n      F\n      White\n      Speeding\n      Speeding\n      False\n      NaN\n      Citation\n      False\n      0-15 Min\n      False\n      Zone X3\n      False\n      8\n      2005-02-24\n      bad\n    \n  \n\n\n\n\nWe’ll use ri_weather to analyze the relationship between weather conditions and police behavior."
  },
  {
    "objectID": "posts/2020-06-28-analyzing police activity with pandas.html#does-weather-affect-the-arrest-rate",
    "href": "posts/2020-06-28-analyzing police activity with pandas.html#does-weather-affect-the-arrest-rate",
    "title": "Analyzing Police Activity with Pandas",
    "section": "Does weather affect the arrest rate?",
    "text": "Does weather affect the arrest rate?\n\nComparing arrest rates by weather rating\nDo police officers arrest drivers more often when the weather is bad? Let’s find out below!\n\nFirst, we’ll calculate the overall arrest rate.\nThen, we’ll calculate the arrest rate for each of the weather ratings we previously assigned.\nFinally, we’ll add violation type as a second factor in the analysis, to see if that accounts for any differences in the arrest rate.\n\nSince we previously defined a logical order for the weather categories, good < bad < worse, they will be sorted that way in the results.\n\n# Calculate the overall arrest rate\nprint(ri_weather.is_arrested.mean())\n\n0.0355690117407784\n\n\n\n# Calculate the arrest rate for each 'rating'\nri_weather.groupby(\"rating\").is_arrested.mean()\n\nrating\ngood     0.033715\nbad      0.036261\nworse    0.041667\nName: is_arrested, dtype: float64\n\n\n\n# Calculate the arrest rate for each 'violation' and 'rating'\nri_weather.groupby([\"violation\", 'rating']).is_arrested.mean()\n\nviolation            rating\nEquipment            good      0.059007\n                     bad       0.066311\n                     worse     0.097357\nMoving violation     good      0.056227\n                     bad       0.058050\n                     worse     0.065860\nOther                good      0.076966\n                     bad       0.087443\n                     worse     0.062893\nRegistration/plates  good      0.081574\n                     bad       0.098160\n                     worse     0.115625\nSeat belt            good      0.028587\n                     bad       0.022493\n                     worse     0.000000\nSpeeding             good      0.013405\n                     bad       0.013314\n                     worse     0.016886\nName: is_arrested, dtype: float64\n\n\nThe arrest rate increases as the weather gets worse, and that trend persists across many of the violation types. This doesn’t prove a causal link, but it’s quite an interesting result!\n\n\nSelecting from a multi-indexed Series\nThe output of a single .groupby() operation on multiple columns is a Series with a MultiIndex. Working with this type of object is similar to working with a DataFrame:\n\nThe outer index level is like the DataFrame rows.\nThe inner index level is like the DataFrame columns.\n\n\n# Save the output of the groupby operation from the last exercise\narrest_rate = ri_weather.groupby(['violation', 'rating']).is_arrested.mean()\n\n\n# Print the arrest rate for moving violations in bad weather\ndisplay(arrest_rate.loc[\"Moving violation\", \"bad\"])\n\n# Print the arrest rates for speeding violations in all three weather conditions\narrest_rate.loc[\"Speeding\"]\n\n0.05804964058049641\n\n\nrating\ngood     0.013405\nbad      0.013314\nworse    0.016886\nName: is_arrested, dtype: float64\n\n\n\n\nReshaping the arrest rate data\nWe’ll start by reshaping the arrest_rate Series into a DataFrame. This is a useful step when working with any multi-indexed Series, since it enables you to access the full range of DataFrame methods.\nThen, we’ll create the exact same DataFrame using a pivot table. This is a great example of how pandas often gives you more than one way to reach the same result!\n\n# Unstack the 'arrest_rate' Series into a DataFrame\ndisplay(arrest_rate.unstack())\n\n# Create the same DataFrame using a pivot table\nri_weather.pivot_table(index='violation', columns='rating', values='is_arrested')\n\n\n\n\n\n  \n    \n      rating\n      good\n      bad\n      worse\n    \n    \n      violation\n      \n      \n      \n    \n  \n  \n    \n      Equipment\n      0.059007\n      0.066311\n      0.097357\n    \n    \n      Moving violation\n      0.056227\n      0.058050\n      0.065860\n    \n    \n      Other\n      0.076966\n      0.087443\n      0.062893\n    \n    \n      Registration/plates\n      0.081574\n      0.098160\n      0.115625\n    \n    \n      Seat belt\n      0.028587\n      0.022493\n      0.000000\n    \n    \n      Speeding\n      0.013405\n      0.013314\n      0.016886\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      rating\n      good\n      bad\n      worse\n    \n    \n      violation\n      \n      \n      \n    \n  \n  \n    \n      Equipment\n      0.059007\n      0.066311\n      0.097357\n    \n    \n      Moving violation\n      0.056227\n      0.058050\n      0.065860\n    \n    \n      Other\n      0.076966\n      0.087443\n      0.062893\n    \n    \n      Registration/plates\n      0.081574\n      0.098160\n      0.115625\n    \n    \n      Seat belt\n      0.028587\n      0.022493\n      0.000000\n    \n    \n      Speeding\n      0.013405\n      0.013314\n      0.016886"
  },
  {
    "objectID": "posts/2020-08-25-working with missing data.html",
    "href": "posts/2020-08-25-working with missing data.html",
    "title": "Working with Missing Data",
    "section": "",
    "text": "import pandas as pd\n# to display all columns\npd.set_option('display.max.columns', None)\n# to display the entire contents of a cell\npd.set_option('display.max_colwidth', None)\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use(\"ggplot\")\n\n\nmvc = pd.read_csv(\"../datasets/nypd_mvc_2018.csv\")\nmvc.head()\n\n\n\n\n\n  \n    \n      \n      unique_key\n      date\n      time\n      borough\n      location\n      on_street\n      cross_street\n      off_street\n      pedestrians_injured\n      cyclist_injured\n      motorist_injured\n      total_injured\n      pedestrians_killed\n      cyclist_killed\n      motorist_killed\n      total_killed\n      vehicle_1\n      vehicle_2\n      vehicle_3\n      vehicle_4\n      vehicle_5\n      cause_vehicle_1\n      cause_vehicle_2\n      cause_vehicle_3\n      cause_vehicle_4\n      cause_vehicle_5\n    \n  \n  \n    \n      0\n      3869058\n      2018-03-23\n      21:40\n      MANHATTAN\n      (40.742832, -74.00771)\n      WEST 15 STREET\n      10 AVENUE\n      NaN\n      0\n      0\n      0\n      0.0\n      0\n      0\n      0\n      0.0\n      PASSENGER VEHICLE\n      NaN\n      NaN\n      NaN\n      NaN\n      Following Too Closely\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      3847947\n      2018-02-13\n      14:45\n      BROOKLYN\n      (40.623714, -73.99314)\n      16 AVENUE\n      62 STREET\n      NaN\n      0\n      0\n      0\n      0.0\n      0\n      0\n      0\n      0.0\n      SPORT UTILITY / STATION WAGON\n      DS\n      NaN\n      NaN\n      NaN\n      Backing Unsafely\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      3914294\n      2018-06-04\n      0:00\n      NaN\n      (40.591755, -73.9083)\n      BELT PARKWAY\n      NaN\n      NaN\n      0\n      0\n      1\n      1.0\n      0\n      0\n      0\n      0.0\n      Station Wagon/Sport Utility Vehicle\n      Sedan\n      NaN\n      NaN\n      NaN\n      Following Too Closely\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      3915069\n      2018-06-05\n      6:36\n      QUEENS\n      (40.73602, -73.87954)\n      GRAND AVENUE\n      VANLOON STREET\n      NaN\n      0\n      0\n      0\n      0.0\n      0\n      0\n      0\n      0.0\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n      Glare\n      Passing Too Closely\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      3923123\n      2018-06-16\n      15:45\n      BRONX\n      (40.884727, -73.89945)\n      NaN\n      NaN\n      208       WEST 238 STREET\n      0\n      0\n      0\n      0.0\n      0\n      0\n      0\n      0.0\n      Station Wagon/Sport Utility Vehicle\n      Sedan\n      NaN\n      NaN\n      NaN\n      Turning Improperly\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nA summary of the columns and their data is below:\n\nunique_key: A unique identifier for each collision.\ndate, time: Date and time of the collision.\nborough: The borough, or area of New York City, where the collision occurred.\nlocation: Latitude and longitude coordinates for the collision.\non_street, cross_street, off_street: Details of the street or intersection where the collision occurred.\npedestrians_injured: Number of pedestrians who were injured.\ncyclist_injured: Number of people traveling on a bicycle who were injured.\nmotorist_injured: Number of people traveling in a vehicle who were injured.\ntotal_injured: Total number of people injured.\npedestrians_killed: Number of pedestrians who were killed.\ncyclist_killed: Number of people traveling on a bicycle who were killed.\nmotorist_killed: Number of people traveling in a vehicle who were killed.\ntotal_killed: Total number of people killed.\nvehicle_1 through vehicle_5: Type of each vehicle involved in the accident.\ncause_vehicle_1 through cause_vehicle_5: Contributing factor for each vehicle in the accident.\n\n\n\n\nmvc.isna().sum()\n\nunique_key                 0\ndate                       0\ntime                       0\nborough                20646\nlocation                3885\non_street              13961\ncross_street           29249\noff_street             44093\npedestrians_injured        0\ncyclist_injured            0\nmotorist_injured           0\ntotal_injured              1\npedestrians_killed         0\ncyclist_killed             0\nmotorist_killed            0\ntotal_killed               5\nvehicle_1                355\nvehicle_2              12262\nvehicle_3              54352\nvehicle_4              57158\nvehicle_5              57681\ncause_vehicle_1          175\ncause_vehicle_2         8692\ncause_vehicle_3        54134\ncause_vehicle_4        57111\ncause_vehicle_5        57671\ndtype: int64\n\n\nTo give us a better picture of the null values in the data, let’s calculate the percentage of null values in each column.\n\nnull_df = pd.DataFrame({'null_counts':mvc.isna().sum(), 'null_pct':mvc.isna().sum()/mvc.shape[0] * 100}).T.astype(int).T\nnull_df\n\n\n\n\n\n  \n    \n      \n      null_counts\n      null_pct\n    \n  \n  \n    \n      unique_key\n      0\n      0\n    \n    \n      date\n      0\n      0\n    \n    \n      time\n      0\n      0\n    \n    \n      borough\n      20646\n      35\n    \n    \n      location\n      3885\n      6\n    \n    \n      on_street\n      13961\n      24\n    \n    \n      cross_street\n      29249\n      50\n    \n    \n      off_street\n      44093\n      76\n    \n    \n      pedestrians_injured\n      0\n      0\n    \n    \n      cyclist_injured\n      0\n      0\n    \n    \n      motorist_injured\n      0\n      0\n    \n    \n      total_injured\n      1\n      0\n    \n    \n      pedestrians_killed\n      0\n      0\n    \n    \n      cyclist_killed\n      0\n      0\n    \n    \n      motorist_killed\n      0\n      0\n    \n    \n      total_killed\n      5\n      0\n    \n    \n      vehicle_1\n      355\n      0\n    \n    \n      vehicle_2\n      12262\n      21\n    \n    \n      vehicle_3\n      54352\n      93\n    \n    \n      vehicle_4\n      57158\n      98\n    \n    \n      vehicle_5\n      57681\n      99\n    \n    \n      cause_vehicle_1\n      175\n      0\n    \n    \n      cause_vehicle_2\n      8692\n      15\n    \n    \n      cause_vehicle_3\n      54134\n      93\n    \n    \n      cause_vehicle_4\n      57111\n      98\n    \n    \n      cause_vehicle_5\n      57671\n      99\n    \n  \n\n\n\n\nAbout a third of the columns have no null values, with the rest ranging from less than 1% to 99%! To make things easier, let’s start by looking at the group of columns that relate to people killed in collisions.\n\nnull_df.loc[[column for column in mvc.columns if \"killed\" in column]]\n\n\n\n\n\n  \n    \n      \n      null_counts\n      null_pct\n    \n  \n  \n    \n      pedestrians_killed\n      0\n      0\n    \n    \n      cyclist_killed\n      0\n      0\n    \n    \n      motorist_killed\n      0\n      0\n    \n    \n      total_killed\n      5\n      0\n    \n  \n\n\n\n\nWe can see that each of the individual categories have no missing values, but the total_killed column has five missing values.\nIf you think about it, the total number of people killed should be the sum of each of the individual categories. We might be able to “fill in” the missing values with the sums of the individual columns for that row.\n\n\n\n\n\n\nImportant\n\n\n\nThe technical name for filling in a missing value with a replacement value is called imputation.\n\n\n\n\n\n\nkilled = mvc[[col for col in mvc.columns if 'killed' in col]].copy()\nkilled_manual_sum = killed.iloc[:, :3].sum(axis=\"columns\")\nkilled_mask = killed_manual_sum != killed.total_killed\nkilled_non_eq = killed[killed_mask]\nkilled_non_eq\n\n\n\n\n\n  \n    \n      \n      pedestrians_killed\n      cyclist_killed\n      motorist_killed\n      total_killed\n    \n  \n  \n    \n      3508\n      0\n      0\n      0\n      NaN\n    \n    \n      20163\n      0\n      0\n      0\n      NaN\n    \n    \n      22046\n      0\n      0\n      1\n      0.0\n    \n    \n      48719\n      0\n      0\n      0\n      NaN\n    \n    \n      55148\n      0\n      0\n      0\n      NaN\n    \n    \n      55699\n      0\n      0\n      0\n      NaN\n    \n  \n\n\n\n\n\n\n\nThe killed_non_eq dataframe has six rows. We can categorize these into two categories:\n\nFive rows where the total_killed is not equal to the sum of the other columns because the total value is missing.\nOne row where the total_killed is less than the sum of the other columns.\n\nFrom this, we can conclude that filling null values with the sum of the columns is a fairly good choice for our imputation, given that only six rows out of around 58,000 don’t match this pattern.\nWe’ve also identified a row that has suspicious data - one that doesn’t sum correctly. Once we have imputed values for all rows with missing values for total_killed, we’ll mark this suspect row by setting its value to NaN.\n\n# fix the killed values\nkilled['total_killed'] = killed['total_killed'].mask(killed['total_killed'].isnull(), killed_manual_sum)\nkilled['total_killed'] = killed['total_killed'].mask(killed['total_killed'] != killed_manual_sum, np.nan)\n\n# Create an injured dataframe and manually sum values\ninjured = mvc[[col for col in mvc.columns if 'injured' in col]].copy()\ninjured_manual_sum = injured.iloc[:,:3].sum(axis=1)\n\ninjured['total_injured'] = injured.total_injured.mask(injured.total_injured.isnull(), injured_manual_sum)\n\ninjured['total_injured'] = injured.total_injured.mask(injured.total_injured != injured_manual_sum, np.nan)\n\nLet’s summarize the count of null values before and after our changes:\n\nsummary = {\n    'injured': [\n        mvc['total_injured'].isnull().sum(),\n        injured['total_injured'].isnull().sum()\n    ],\n    'killed': [\n        mvc['total_killed'].isnull().sum(),\n        killed['total_killed'].isnull().sum()\n    ]\n}\npd.DataFrame(summary, index=['before','after'])\n\n\n\n\n\n  \n    \n      \n      injured\n      killed\n    \n  \n  \n    \n      before\n      1\n      5\n    \n    \n      after\n      21\n      1\n    \n  \n\n\n\n\nFor the total_killed column, the number of values has gone down from 5 to 1. For the total_injured column, the number of values has actually gone up — from 1 to 21. This might sound like we’ve done the opposite of what we set out to do, but what we’ve actually done is fill all the null values and identify values that have suspect data. This will make any analysis we do on this data more accurate in the long run.\nLet’s assign the values from the killed and injured dataframe back to the main mvc dataframe:\n\nmvc['total_injured']=injured.total_injured\nmvc['total_killed']=killed.total_killed\n\n\n\n\nEarlier, we used a table of numbers to understand the number of missing values in our dataframe. A different approach we can take is to use a plot to visualize the missing values. The function below uses seaborn.heatmap() to represent null values as dark squares and non-null values as light squares:\n\ndef plot_null_matrix(df, figsize=(20,15)):\n    \"\"\"Plot null values as light squares and non-null values as dark squares\"\"\"\n    plt.figure(figsize=figsize)\n    df_null = df.isnull()\n    sns.heatmap(~df_null, cbar=False, yticklabels=False)\n    plt.xticks(rotation=90, size=\"x-large\")\n    plt.show()\n\nLet’s look at how the function works by using it to plot just the first row of our mvc dataframe. We’ll display the first row as a table immediately below so it’s easy to compare:\n\nplot_null_matrix(mvc.head(1), figsize=(20,2))\n\n\n\n\n\nmvc.head(1)\n\n\n\n\n\n  \n    \n      \n      unique_key\n      date\n      time\n      borough\n      location\n      on_street\n      cross_street\n      off_street\n      pedestrians_injured\n      cyclist_injured\n      motorist_injured\n      total_injured\n      pedestrians_killed\n      cyclist_killed\n      motorist_killed\n      total_killed\n      vehicle_1\n      vehicle_2\n      vehicle_3\n      vehicle_4\n      vehicle_5\n      cause_vehicle_1\n      cause_vehicle_2\n      cause_vehicle_3\n      cause_vehicle_4\n      cause_vehicle_5\n    \n  \n  \n    \n      0\n      3869058\n      2018-03-23\n      21:40\n      MANHATTAN\n      (40.742832, -74.00771)\n      WEST 15 STREET\n      10 AVENUE\n      NaN\n      0\n      0\n      0\n      0.0\n      0\n      0\n      0\n      0.0\n      PASSENGER VEHICLE\n      NaN\n      NaN\n      NaN\n      NaN\n      Following Too Closely\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nEach value is represented by a light square, and each missing value is represented by a dark square.\nLet’s look at what a plot matrix looks like for the whole dataframe:\n\nplot_null_matrix(mvc)\n\n\n\n\nWe can make some immediate interpretations about our dataframe:\n\nThe first three columns have few to no missing values.\nThe next five columns have missing values scattered throughout, with each column seeming to have its own density of missing values.\nThe next eight columns are the injury and killed columns we just cleaned, and only have a few missing values.\nThe last 10 columns seem to break into two groups of five, with each group of five having similar patterns of null/non-null values.\n\nLet’s examine the pattern in the last 10 columns a little more closely. We can calculate the relationship between two sets of columns, known as correlation.\n\ncols_with_missing_vals = mvc.columns[mvc.isnull().sum()>0]\nmissing_corr = mvc[cols_with_missing_vals].isnull().corr()\nmissing_corr\n\n\n\n\n\n  \n    \n      \n      borough\n      location\n      on_street\n      cross_street\n      off_street\n      total_injured\n      total_killed\n      vehicle_1\n      vehicle_2\n      vehicle_3\n      vehicle_4\n      vehicle_5\n      cause_vehicle_1\n      cause_vehicle_2\n      cause_vehicle_3\n      cause_vehicle_4\n      cause_vehicle_5\n    \n  \n  \n    \n      borough\n      1.000000\n      0.190105\n      -0.350190\n      0.409107\n      0.362189\n      -0.002827\n      0.005582\n      -0.018325\n      -0.077516\n      -0.061932\n      -0.020406\n      -0.010733\n      -0.012115\n      -0.058596\n      -0.060542\n      -0.020158\n      -0.011348\n    \n    \n      location\n      0.190105\n      1.000000\n      -0.073975\n      -0.069719\n      0.084579\n      -0.001486\n      0.015496\n      -0.010466\n      -0.033842\n      -0.000927\n      0.004655\n      -0.005797\n      -0.003458\n      -0.021373\n      0.000684\n      0.004604\n      -0.004841\n    \n    \n      on_street\n      -0.350190\n      -0.073975\n      1.000000\n      0.557767\n      -0.991030\n      0.006220\n      -0.002344\n      -0.001889\n      0.119647\n      0.020867\n      0.004172\n      -0.002768\n      0.001307\n      0.087374\n      0.017426\n      0.002737\n      -0.003107\n    \n    \n      cross_street\n      0.409107\n      -0.069719\n      0.557767\n      1.000000\n      -0.552763\n      0.002513\n      0.004112\n      -0.017018\n      0.043799\n      -0.049910\n      -0.021137\n      -0.012003\n      -0.009102\n      0.031189\n      -0.052159\n      -0.022074\n      -0.013455\n    \n    \n      off_street\n      0.362189\n      0.084579\n      -0.991030\n      -0.552763\n      1.000000\n      -0.004266\n      0.002323\n      0.001812\n      -0.121129\n      -0.022404\n      -0.004074\n      0.002492\n      -0.001738\n      -0.088187\n      -0.019120\n      -0.002580\n      0.002863\n    \n    \n      total_injured\n      -0.002827\n      -0.001486\n      0.006220\n      0.002513\n      -0.004266\n      1.000000\n      -0.000079\n      0.079840\n      0.025644\n      -0.002757\n      0.002118\n      0.001073\n      0.131140\n      0.030082\n      -0.002388\n      0.002188\n      0.001102\n    \n    \n      total_killed\n      0.005582\n      0.015496\n      -0.002344\n      0.004112\n      0.002323\n      -0.000079\n      1.000000\n      -0.000327\n      0.008017\n      0.001057\n      0.000462\n      0.000234\n      -0.000229\n      0.009888\n      0.001091\n      0.000477\n      0.000240\n    \n    \n      vehicle_1\n      -0.018325\n      -0.010466\n      -0.001889\n      -0.017018\n      0.001812\n      0.079840\n      -0.000327\n      1.000000\n      0.151516\n      0.019972\n      0.008732\n      0.004425\n      0.604281\n      0.180678\n      0.020624\n      0.009022\n      0.004545\n    \n    \n      vehicle_2\n      -0.077516\n      -0.033842\n      0.119647\n      0.043799\n      -0.121129\n      0.025644\n      0.008017\n      0.151516\n      1.000000\n      0.131813\n      0.057631\n      0.029208\n      0.106214\n      0.784402\n      0.132499\n      0.058050\n      0.029264\n    \n    \n      vehicle_3\n      -0.061932\n      -0.000927\n      0.020867\n      -0.049910\n      -0.022404\n      -0.002757\n      0.001057\n      0.019972\n      0.131813\n      1.000000\n      0.437214\n      0.221585\n      0.014000\n      0.106874\n      0.961316\n      0.448525\n      0.225067\n    \n    \n      vehicle_4\n      -0.020406\n      0.004655\n      0.004172\n      -0.021137\n      -0.004074\n      0.002118\n      0.000462\n      0.008732\n      0.057631\n      0.437214\n      1.000000\n      0.506810\n      0.006121\n      0.046727\n      0.423394\n      0.963723\n      0.515058\n    \n    \n      vehicle_5\n      -0.010733\n      -0.005797\n      -0.002768\n      -0.012003\n      0.002492\n      0.001073\n      0.000234\n      0.004425\n      0.029208\n      0.221585\n      0.506810\n      1.000000\n      0.003102\n      0.023682\n      0.214580\n      0.490537\n      0.973664\n    \n    \n      cause_vehicle_1\n      -0.012115\n      -0.003458\n      0.001307\n      -0.009102\n      -0.001738\n      0.131140\n      -0.000229\n      0.604281\n      0.106214\n      0.014000\n      0.006121\n      0.003102\n      1.000000\n      0.131000\n      0.014457\n      0.006324\n      0.003186\n    \n    \n      cause_vehicle_2\n      -0.058596\n      -0.021373\n      0.087374\n      0.031189\n      -0.088187\n      0.030082\n      0.009888\n      0.180678\n      0.784402\n      0.106874\n      0.046727\n      0.023682\n      0.131000\n      1.000000\n      0.110362\n      0.048277\n      0.024322\n    \n    \n      cause_vehicle_3\n      -0.060542\n      0.000684\n      0.017426\n      -0.052159\n      -0.019120\n      -0.002388\n      0.001091\n      0.020624\n      0.132499\n      0.961316\n      0.423394\n      0.214580\n      0.014457\n      0.110362\n      1.000000\n      0.437440\n      0.220384\n    \n    \n      cause_vehicle_4\n      -0.020158\n      0.004604\n      0.002737\n      -0.022074\n      -0.002580\n      0.002188\n      0.000477\n      0.009022\n      0.058050\n      0.448525\n      0.963723\n      0.490537\n      0.006324\n      0.048277\n      0.437440\n      1.000000\n      0.503805\n    \n    \n      cause_vehicle_5\n      -0.011348\n      -0.004841\n      -0.003107\n      -0.013455\n      0.002863\n      0.001102\n      0.000240\n      0.004545\n      0.029264\n      0.225067\n      0.515058\n      0.973664\n      0.003186\n      0.024322\n      0.220384\n      0.503805\n      1.000000\n    \n  \n\n\n\n\nEach value is between -1 and 1, and represents the relationship between two columns. A number close to 1 or -1 represents a strong relationship, where a number in the middle (close to 0) represents a weak relationship.\nIf you look closely, you can see a diagonal line of 1s going from top left to bottom right. These values represent each columns relationship with itself, which of course is a perfect relationship. The values on the top/right of this “line of 1s” mirror the values on the bottom/left of this line: The table actually repeats every value twice!\nLet’s create a correlation plot of just those last 10 columns to see if we can more closely identify the pattern we saw earlier in the matrix plot.\n\ndef plot_null_correlations(df):\n    \"\"\"create a correlation matrix only for columns with at least one missing value\"\"\"\n    cols_with_missing_vals = df.columns[df.isnull().sum() > 0]\n    missing_corr = df[cols_with_missing_vals].isnull().corr()\n    \n    # create a mask to avoid repeated values and make\n    # the plot easier to read\n    missing_corr = missing_corr.iloc[1:, :-1]\n   \n    mask = np.triu(np.ones_like(missing_corr), k=1)\n    \n    # plot a heatmap of the values\n    plt.figure(figsize=(20,14))\n    ax = sns.heatmap(missing_corr, vmin=-1, vmax=1, cbar=False,\n                     cmap='RdBu', mask=mask, annot=True)\n    \n    # format the text in the plot to make it easier to read\n    for text in ax.texts:\n        t = float(text.get_text())\n        if -0.05 < t < 0.01:\n            text.set_text('')\n        else:\n            text.set_text(round(t, 2))\n        text.set_fontsize('x-large')\n    plt.xticks(rotation=90, size='x-large')\n    plt.yticks(rotation=0, size='x-large')\n\n    plt.show()\n\n\nplot_null_correlations(mvc[[column for column in mvc.columns if 'vehicle' in column]])\n\n\n\n\nIn our correlation plot:\n\nThe “line of 1s” and the repeated values are removed so that it’s not visually overwhelming.\nValues very close to 0, where there is little to no relationship, aren’t labeled.\nValues close to 1 are dark blue and values close to -1 are light blue — the depth of color represents the strength of the relationship.\n\n\n\n\nWhen a vehicle is in an accident, there is likely to be a cause, and vice-versa.\nLet’s explore the variations in missing values from these five pairs of columns. We’ll create a dataframe that counts, for each pair:\n\nThe number of values where the vehicle is missing when the cause is not missing.\nThe number of values where the cause is missing when the vehicle is not missing.\n\n\ncol_labels = ['v_number', 'vehicle_missing', 'cause_missing']\n\nvc_null_data = []\n\nfor v in range(1,6):\n    v_col = 'vehicle_{}'.format(v)\n    c_col = 'cause_vehicle_{}'.format(v)\n    v_null = mvc[mvc[v_col].isnull() & mvc[c_col].notnull()].shape[0]\n    c_null = mvc[mvc[v_col].notnull() & mvc[c_col].isnull()].shape[0]\n    vc_null_data.append([v, v_null, c_null])\n    \nvc_null_df = pd.DataFrame(vc_null_data, columns=col_labels)\nvc_null_df\n\n\n\n\n\n  \n    \n      \n      v_number\n      vehicle_missing\n      cause_missing\n    \n  \n  \n    \n      0\n      1\n      204\n      24\n    \n    \n      1\n      2\n      3793\n      223\n    \n    \n      2\n      3\n      242\n      24\n    \n    \n      3\n      4\n      50\n      3\n    \n    \n      4\n      5\n      10\n      0\n    \n  \n\n\n\n\n\n\n\nThe analysis we indicates that there are roughly 4,500 missing values across the 10 columns. The easiest option for handling these would be to drop the rows with missing values. This would mean losing almost 10% of the total data, which is something we ideally want to avoid.\nA better option is to impute the data, like we did earlier. Because the data in these columns is text data, we can’t perform a numeric calculation to impute missing data.\nOne common option when imputing is to use the most common value to fill in data. Let’s look at the common values across these columns and see if we can use that to make a decision.\nLet’s count the most common values for the cause set of columns. We’ll start by selecting only the columns containing the substring cause.\n\ncause = mvc[[c for c in mvc.columns if \"cause_\" in c]]\ncause.head()\n\n\n\n\n\n  \n    \n      \n      cause_vehicle_1\n      cause_vehicle_2\n      cause_vehicle_3\n      cause_vehicle_4\n      cause_vehicle_5\n    \n  \n  \n    \n      0\n      Following Too Closely\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      Backing Unsafely\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      Following Too Closely\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      Glare\n      Passing Too Closely\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Turning Improperly\n      Unspecified\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nNext, we’ll stack the values into a single series object:\n\ncause_1d = cause.stack()\ncause_1d.head()\n\n0  cause_vehicle_1    Following Too Closely\n   cause_vehicle_2              Unspecified\n1  cause_vehicle_1         Backing Unsafely\n   cause_vehicle_2              Unspecified\n2  cause_vehicle_1    Following Too Closely\ndtype: object\n\n\nYou may notice that the stacked version omits null values - this is fine, as we’re just interested in the most common non-null values.\nFinally, we count the values in the series:\n\ncause_counts = cause_1d.value_counts()\ntop10_causes = cause_counts.head(10)\ntop10_causes\n\nUnspecified                       57481\nDriver Inattention/Distraction    17650\nFollowing Too Closely              6567\nFailure to Yield Right-of-Way      4566\nPassing or Lane Usage Improper     3260\nPassing Too Closely                3045\nBacking Unsafely                   3001\nOther Vehicular                    2523\nUnsafe Lane Changing               2372\nTurning Improperly                 1590\ndtype: int64\n\n\nThe most common non-null value for the cause columns is Unspecified, which presumably indicates that the officer reporting the collision was unable to determine the cause for that vehicle.\nLet’s identify the most common non-null value for the vehicle columns.\n\nv_cols = [c for c in mvc.columns if c.startswith(\"vehicle\")]\ntop10_vehicles = mvc[v_cols].stack().value_counts().head(10)\ntop10_vehicles\n\nSedan                                  33133\nStation Wagon/Sport Utility Vehicle    26124\nPASSENGER VEHICLE                      16026\nSPORT UTILITY / STATION WAGON          12356\nTaxi                                    3482\nPick-up Truck                           2373\nTAXI                                    1892\nBox Truck                               1659\nBike                                    1190\nBus                                     1162\ndtype: int64\n\n\n\n\n\nThe top “cause” is an \"Unspecified\" placeholder. This is useful instead of a null value as it makes the distinction between a value that is missing because there were only a certain number of vehicles in the collision versus one that is because the contributing cause for a particular vehicle is unknown.\nThe vehicles columns don’t have an equivalent, but we can still use the same technique. Here’s the logic we’ll need to do for each pair of vehicle/cause columns:\n\nFor values where the vehicle is null and the cause is non-null, set the vehicle to Unspecified.\nFor values where the cause is null and the vehicle is not-null, set the cause to Unspecified.\n\n\ndef summarize_missing():\n    v_missing_data = []\n\n    for v in range(1,6):\n        v_col = 'vehicle_{}'.format(v)\n        c_col = 'cause_vehicle_{}'.format(v)\n\n        v_missing = (mvc[v_col].isnull() & mvc[c_col].notnull()).sum()\n        c_missing = (mvc[c_col].isnull() & mvc[v_col].notnull()).sum()\n\n        v_missing_data.append([v, v_missing, c_missing])\n\n    col_labels = columns=[\"vehicle_number\", \"vehicle_missing\", \"cause_missing\"]\n    return pd.DataFrame(v_missing_data, columns=col_labels)\n\nsummarize_missing()\n\n\n\n\n\n  \n    \n      \n      vehicle_number\n      vehicle_missing\n      cause_missing\n    \n  \n  \n    \n      0\n      1\n      204\n      24\n    \n    \n      1\n      2\n      3793\n      223\n    \n    \n      2\n      3\n      242\n      24\n    \n    \n      3\n      4\n      50\n      3\n    \n    \n      4\n      5\n      10\n      0\n    \n  \n\n\n\n\n\nfor v in range(1,6):\n    v_col = 'vehicle_{}'.format(v)\n    c_col = 'cause_vehicle_{}'.format(v)\n    mvc[v_col] = mvc[v_col].mask( mvc[v_col].isnull() & mvc[c_col].notnull(), 'Unspecified')\n    mvc[c_col] = mvc[c_col].mask(mvc[v_col].notnull() & mvc[c_col].isnull(), \"Unspecified\")\n\nsummarize_missing()\n\n\n\n\n\n  \n    \n      \n      vehicle_number\n      vehicle_missing\n      cause_missing\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n    \n    \n      1\n      2\n      0\n      0\n    \n    \n      2\n      3\n      0\n      0\n    \n    \n      3\n      4\n      0\n      0\n    \n    \n      4\n      5\n      0\n      0\n    \n  \n\n\n\n\n\n\n\nLet’s view the work we’ve done across the past few screens by looking at the null correlation plot for the last 10 columns:\n\nveh_cols = [c for c in mvc.columns if 'vehicle' in c]\nplot_null_correlations(mvc[veh_cols])\n\n\n\n\nYou can see the perfect correlation between each pair of vehicle/cause columns represented by 1.0 in each square, which means that there is a perfect relationship between the five pairs of vehicle/cause columns\nLet’s now turn our focus to the final set of columns that contain missing values — the columns that relate to the location of the accident. We’ll start by looking at the first few rows to refamiliarize ourselves with the data:\n\nloc_cols = ['borough', 'location', 'on_street', 'off_street', 'cross_street']\nlocation_data = mvc[loc_cols]\nlocation_data.head()\n\n\n\n\n\n  \n    \n      \n      borough\n      location\n      on_street\n      off_street\n      cross_street\n    \n  \n  \n    \n      0\n      MANHATTAN\n      (40.742832, -74.00771)\n      WEST 15 STREET\n      NaN\n      10 AVENUE\n    \n    \n      1\n      BROOKLYN\n      (40.623714, -73.99314)\n      16 AVENUE\n      NaN\n      62 STREET\n    \n    \n      2\n      NaN\n      (40.591755, -73.9083)\n      BELT PARKWAY\n      NaN\n      NaN\n    \n    \n      3\n      QUEENS\n      (40.73602, -73.87954)\n      GRAND AVENUE\n      NaN\n      VANLOON STREET\n    \n    \n      4\n      BRONX\n      (40.884727, -73.89945)\n      NaN\n      208       WEST 238 STREET\n      NaN\n    \n  \n\n\n\n\nNext, let’s look at counts of the null values in each column:\n\nlocation_data.isnull().sum()\n\nborough         20646\nlocation         3885\non_street       13961\noff_street      44093\ncross_street    29249\ndtype: int64\n\n\nThese columns have a lot of missing values! Keep in mind that all of these five columns represent the same thing — the location of the collision. We can potentially use the non-null values to impute some of the null values.\nTo see where we might be able to do this, let’s look for correlations between the missing values:\n\nplot_null_correlations(location_data)\n\n\n\n\nNone of these columns have strong correlations except for off_street and on_street which have a near perfect negative correlation. That means for almost every row that has a null value in one column, the other has a non-null value and vice-versa.\nThe final way we’ll look at the null values in these columns is to plot a null matrix, but we’ll sort the data first. This will gather some of the null and non-null values together and make patterns more obvious:\n\nsorted_location_data = location_data.sort_values(loc_cols)\nplot_null_matrix(sorted_location_data)\n\n\n\n\nLet’s make some observations about the missing values across these columns:\n\nAbout two-thirds of rows have non-null values for borough, but of those values that are missing, most have non-null values for location and one or more of the street name columns.\nLess than one-tenth of rows have missing values in the location column, but most of these have non-null values in one or more of the street name columns.\nMost rows have a non-null value for either on_street or off_street, and some also have a value for cross_street.\n\nCombined, this means that we will be able to impute a lot of the missing values by using the other columns in each row. To do this, we can use geolocation APIs that take either an address or location coordinates, and return information about that location.\n\n\n\n\nsup_data = pd.read_csv('../datasets/supplemental_data.csv')\nsup_data.head()\n\n\n\n\n\n  \n    \n      \n      unique_key\n      location\n      on_street\n      off_street\n      borough\n    \n  \n  \n    \n      0\n      3869058\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      3847947\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      3914294\n      NaN\n      BELT PARKWAY\n      NaN\n      BROOKLYN\n    \n    \n      3\n      3915069\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      3923123\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nThe supplemental data has five columns from our original data set — the unique_key that identifies each collision, and four of the five location columns. The cross_street column is not included because the geocoding APIs we used don’t include data on the nearest cross street to any single location.\nLet’s take a look at a null matrix for the supplemental data:\n\nplot_null_matrix(sup_data)\n\n\n\n\nApart from the unique_key column, you’ll notice that there are a lot more missing values than our main data set. This makes sense, as we didn’t prepare supplemental data where the original data set had non-null values.\n\nmvc.unique_key.equals(sup_data.unique_key)\n\nTrue\n\n\nboth the original and supplemental data has the same values in the same order, we’ll be able to use Series.mask() to add our supplemental data to our original data.\n\nlocation_cols = ['location', 'on_street', 'off_street', 'borough']\nmvc[location_cols].isnull().sum()\n\nlocation       3885\non_street     13961\noff_street    44093\nborough       20646\ndtype: int64\n\n\n\nfor c in location_cols:\n    mvc[c] = mvc[c].mask(mvc[c].isnull(), sup_data[c])\n    \nmvc[location_cols].isnull().sum()\n\nlocation         77\non_street     13734\noff_street    36131\nborough         232\ndtype: int64\n\n\nwe’ve imputed thousands of values to reduce the number of missing values across our data set. Let’s look at a summary of the null values\n\nmvc.isnull().sum()\n\nunique_key                 0\ndate                       0\ntime                       0\nborough                  232\nlocation                  77\non_street              13734\ncross_street           29249\noff_street             36131\npedestrians_injured        0\ncyclist_injured            0\nmotorist_injured           0\ntotal_injured             21\npedestrians_killed         0\ncyclist_killed             0\nmotorist_killed            0\ntotal_killed               1\nvehicle_1                151\nvehicle_2               8469\nvehicle_3              54110\nvehicle_4              57108\nvehicle_5              57671\ncause_vehicle_1          151\ncause_vehicle_2         8469\ncause_vehicle_3        54110\ncause_vehicle_4        57108\ncause_vehicle_5        57671\ndtype: int64\n\n\nIf we’d like to continue working with this data, we can:\n\nDrop the rows that had suspect values for injured and killed totals.\nClean the values in the vehicle_1 through vehicle_5 columns by analyzing the different values and merging duplicates and near-duplicates.\nAnalyze whether collisions are more likely in certain locations, at certain times, or for certain vehicle types."
  },
  {
    "objectID": "posts/2021-04-30-python_functions_and_scope.html",
    "href": "posts/2021-04-30-python_functions_and_scope.html",
    "title": "Writing Reusable Code using Functions in Python",
    "section": "",
    "text": "This tutorial covers the following topics:"
  },
  {
    "objectID": "posts/2021-04-30-python_functions_and_scope.html#creating-and-using-functions",
    "href": "posts/2021-04-30-python_functions_and_scope.html#creating-and-using-functions",
    "title": "Writing Reusable Code using Functions in Python",
    "section": "Creating and using functions",
    "text": "Creating and using functions\nA function is a reusable set of instructions that takes one or more inputs, performs some operations, and often returns an output. Python contains many in-built functions like print, len, etc., and provides the ability to define new ones.\n\ntoday = \"Friday\"\nprint(\"Today is\", today)\n\nToday is Friday\n\n\nYou can define a new function using the def keyword.\n\ndef say_hello():\n    print('Hello there!')\n    print('How are you?')\n\nNote the round brackets or parentheses () and colon : after the function’s name. Both are essential parts of the syntax. The function’s body contains an indented block of statements. The statements inside a function’s body are not executed when the function is defined. To execute the statements, we need to call or invoke the function.\n\nsay_hello()\n\nHello there!\nHow are you?\n\n\n\nFunction arguments\nFunctions can accept zero or more values as inputs (also knows as arguments or parameters). Arguments help us write flexible functions that can perform the same operations on different values. Further, functions can return a result that can be stored in a variable or used in other expressions.\nHere’s a function that filters out the even numbers from a list and returns a new list using the return keyword.\n\ndef filter_even(number_list):\n    result_list = []\n    for number in number_list:\n        if number % 2 == 0:\n            result_list.append(number)\n    return result_list\n\nCan you understand what the function does by looking at the code? If not, try executing each line of the function’s body separately within a code cell with an actual list of numbers in place of number_list.\n\neven_list = filter_even([1, 2, 3, 4, 5, 6, 7])\n\n\neven_list\n\n[2, 4, 6]"
  },
  {
    "objectID": "posts/2021-04-30-python_functions_and_scope.html#writing-great-functions-in-python",
    "href": "posts/2021-04-30-python_functions_and_scope.html#writing-great-functions-in-python",
    "title": "Writing Reusable Code using Functions in Python",
    "section": "Writing great functions in Python",
    "text": "Writing great functions in Python\nAs a programmer, you will spend most of your time writing and using functions. Python offers many features to make your functions powerful and flexible. Let’s explore some of these by solving a problem:\n\nRadha is planning to buy a house that costs $1,260,000. She considering two options to finance her purchase:\n\nOption 1: Make an immediate down payment of $300,000, and take loan 8-year loan with an interest rate of 10% (compounded monthly) for the remaining amount.\nOption 2: Take a 10-year loan with an interest rate of 8% (compounded monthly) for the entire amount.\n\nBoth these loans have to be paid back in equal monthly installments (EMIs). Which loan has a lower EMI among the two?\n\nSince we need to compare the EMIs for two loan options, defining a function to calculate the EMI for a loan would be a great idea. The inputs to the function would be cost of the house, the down payment, duration of the loan, rate of interest etc. We’ll build this function step by step.\nFirst, let’s write a simple function that calculates the EMI on the entire cost of the house, assuming that the loan must be paid back in one year, and there is no interest or down payment.\n\ndef loan_emi(amount):\n    emi = amount / 12\n    print('The EMI is ${}'.format(emi))\n\n\nloan_emi(1260000)\n\nThe EMI is $105000.0\n\n\n\nLocal variables and scope\nLet’s add a second argument to account for the duration of the loan in months.\n\ndef loan_emi(amount, duration):\n    emi = amount / duration\n    print('The EMI is ${}'.format(emi))\n\nNote that the variable emi defined inside the function is not accessible outside. The same is true for the parameters amount and duration. These are all local variables that lie within the scope of the function.\n\nScope: Scope refers to the region within the code where a particular variable is visible. Every function (or class definition) defines a scope within Python. Variables defined in this scope are called local variables. Variables that are available everywhere are called global variables. Scope rules allow you to use the same variable names in different functions without sharing values from one to the other.\n\n\nemi\n\nNameError: ignored\n\n\n\namount\n\nNameError: ignored\n\n\n\nduration\n\nNameError: ignored\n\n\nWe can now compare a 6-year loan vs. a 10-year loan (assuming no down payment or interest).\n\nloan_emi(1260000, 8*12)\n\nThe EMI is $13125.0\n\n\n\nloan_emi(1260000, 10*12)\n\nThe EMI is $10500.0\n\n\n\n\nReturn values\nAs you might expect, the EMI for the 6-year loan is higher compared to the 10-year loan. Right now, we’re printing out the result. It would be better to return it and store the results in variables for easier comparison. We can do this using the return statement\n\ndef loan_emi(amount, duration):\n    emi = amount / duration\n    return emi\n\n\nemi1 = loan_emi(1260000, 8*12)\n\n\nemi2 = loan_emi(1260000, 10*12)\n\n\nemi1\n\n13125.0\n\n\n\nemi2\n\n10500.0\n\n\n\n\nOptional arguments\nNext, let’s add another argument to account for the immediate down payment. We’ll make this an optional argument with a default value of 0.\n\ndef loan_emi(amount, duration, down_payment=0):\n    loan_amount = amount - down_payment\n    emi = loan_amount / duration\n    return emi\n\n\nemi1 = loan_emi(1260000, 8*12, 3e5)\n\n\nemi1\n\n10000.0\n\n\n\nemi2 = loan_emi(1260000, 10*12)\n\n\nemi2\n\n10500.0\n\n\nNext, let’s add the interest calculation into the function. Here’s the formula used to calculate the EMI for a loan:\n\n\n\nImg\n\n\nwhere:\n\nP is the loan amount (principal)\nn is the no. of months\nr is the rate of interest per month\n\nThe derivation of this formula is beyond the scope of this tutorial. See this video for an explanation: https://youtu.be/Coxza9ugW4E .\n\ndef loan_emi(amount, duration, rate, down_payment=0):\n    loan_amount = amount - down_payment\n    emi = loan_amount * rate * ((1+rate)**duration) / (((1+rate)**duration)-1)\n    return emi\n\nNote that while defining the function, required arguments like cost, duration and rate must appear before optional arguments like down_payment.\nLet’s calculate the EMI for Option 1\n\nloan_emi(1260000, 8*12, 0.1/12, 3e5)\n\n14567.19753389219\n\n\nWhile calculating the EMI for Option 2, we need not include the down_payment argument.\n\nloan_emi(1260000, 10*12, 0.08/12)\n\n15287.276888775077\n\n\n\n\nNamed arguments\nInvoking a function with many arguments can often get confusing and is prone to human errors. Python provides the option of invoking functions with named arguments for better clarity. You can also split function invocation into multiple lines.\n\nemi1 = loan_emi(\n    amount=1260000, \n    duration=8*12, \n    rate=0.1/12, \n    down_payment=3e5\n)\n\n\nemi1\n\n14567.19753389219\n\n\n\nemi2 = loan_emi(amount=1260000, duration=10*12, rate=0.08/12)\n\n\nemi2\n\n15287.276888775077\n\n\n\n\nModules and library functions\nWe can already see that the EMI for Option 1 is lower than the EMI for Option 2. However, it would be nice to round up the amount to full dollars, rather than showing digits after the decimal. To achieve this, we might want to write a function that can take a number and round it up to the next integer (e.g., 1.2 is rounded up to 2). That would be a great exercise to try out!\nHowever, since rounding numbers is a fairly common operation, Python provides a function for it (along with thousands of other functions) as part of the Python Standard Library. Functions are organized into modules that need to be imported to use the functions they contain.\n\nModules: Modules are files containing Python code (variables, functions, classes, etc.). They provide a way of organizing the code for large Python projects into files and folders. The key benefit of using modules is namespaces: you must import the module to use its functions within a Python script or notebook. Namespaces provide encapsulation and avoid naming conflicts between your code and a module or across modules.\n\nWe can use the ceil function (short for ceiling) from the math module to round up numbers. Let’s import the module and use it to round up the number 1.2.\n\nimport math\n\n\nhelp(math.ceil)\n\nHelp on built-in function ceil in module math:\n\nceil(x, /)\n    Return the ceiling of x as an Integral.\n    \n    This is the smallest integer >= x.\n\n\n\n\nmath.ceil(1.2)\n\n2\n\n\nLet’s now use the math.ceil function within the home_loan_emi function to round up the EMI amount.\n\nUsing functions to build other functions is a great way to reuse code and implement complex business logic while still keeping the code small, understandable, and manageable. Ideally, a function should do one thing and one thing only. If you find yourself writing a function that does too many things, consider splitting it into multiple smaller, independent functions. As a rule of thumb, try to limit your functions to 10 lines of code or less. Good programmers always write short, simple, and readable functions.\n\n\ndef loan_emi(amount, duration, rate, down_payment=0):\n    loan_amount = amount - down_payment\n    emi = loan_amount * rate * ((1+rate)**duration) / (((1+rate)**duration)-1)\n    emi = math.ceil(emi)\n    return emi\n\n\nemi1 = loan_emi(\n    amount=1260000, \n    duration=8*12, \n    rate=0.1/12, \n    down_payment=3e5\n)\n\n\nemi1\n\n14568\n\n\n\nemi2 = loan_emi(amount=1260000, duration=10*12, rate=0.08/12)\n\n\nemi2\n\n15288\n\n\nLet’s compare the EMIs and display a message for the option with the lower EMI.\n\nif emi1 < emi2:\n    print(\"Option 1 has the lower EMI: ${}\".format(emi1))\nelse:\n    print(\"Option 2 has the lower EMI: ${}\".format(emi2))\n\nOption 1 has the lower EMI: $14568\n\n\n\n\nReusing and improving functions\nNow we know for sure that “Option 1” has the lower EMI among the two options. But what’s even better is that we now have a handy function loan_emi that we can use to solve many other similar problems with just a few lines of code. Let’s try it with a couple more questions.\n\nQ: Shaun is currently paying back a home loan for a house he bought a few years ago. The cost of the house was $800,000. Shaun made a down payment of 25% of the price. He financed the remaining amount using a 6-year loan with an interest rate of 7% per annum (compounded monthly). Shaun is now buying a car worth $60,000, which he is planning to finance using a 1-year loan with an interest rate of 12% per annum. Both loans are paid back in EMIs. What is the total monthly payment Shaun makes towards loan repayment?\n\nThis question is now straightforward to solve, using the loan_emi function we’ve already defined.\n\ncost_of_house = 800000\nhome_loan_duration = 6*12 # months\nhome_loan_rate = 0.07/12 # monthly\nhome_down_payment = .25 * 800000\n\nemi_house = loan_emi(amount=cost_of_house,\n                     duration=home_loan_duration,\n                     rate=home_loan_rate, \n                     down_payment=home_down_payment)\n\nemi_house\n\n10230\n\n\n\ncost_of_car = 60000\ncar_loan_duration = 1*12 # months\ncar_loan_rate = .12/12 # monthly\n\nemi_car = loan_emi(amount=cost_of_car, \n                   duration=car_loan_duration, \n                   rate=car_loan_rate)\n\nemi_car\n\n5331\n\n\n\nprint(\"Shaun makes a total monthly payment of ${} towards loan repayments.\".format(emi_house+emi_car))\n\nShaun makes a total monthly payment of $15561 towards loan repayments.\n\n\n\n\nExceptions and try-except\n\nQ: If you borrow $100,000 using a 10-year loan with an interest rate of 9% per annum, what is the total amount you end up paying as interest?\n\nOne way to solve this problem is to compare the EMIs for two loans: one with the given rate of interest and another with a 0% rate of interest. The total interest paid is then simply the sum of monthly differences over the duration of the loan.\n\nemi_with_interest = loan_emi(amount=100000, duration=10*12, rate=0.09/12)\nemi_with_interest\n\n1267\n\n\n\nemi_without_interest = loan_emi(amount=100000, duration=10*12, rate=0./12)\nemi_without_interest\n\nZeroDivisionError: ignored\n\n\nSomething seems to have gone wrong! If you look at the error message above carefully, Python tells us precisely what is wrong. Python throws a ZeroDivisionError with a message indicating that we’re trying to divide a number by zero. ZeroDivisonError is an exception that stops further execution of the program.\n\nException: Even if a statement or expression is syntactically correct, it may cause an error when the Python interpreter tries to execute it. Errors detected during execution are called exceptions. Exceptions typically stop further execution of the program unless handled within the program using try-except statements.\n\nPython provides many built-in exceptions thrown when built-in operators, functions, or methods are used incorrectly: https://docs.python.org/3/library/exceptions.html#built-in-exceptions. You can also define your custom exception by extending the Exception class (more on that later).\nYou can use the try and except statements to handle an exception. Here’s an example:\n\ntry:\n    print(\"Now computing the result..\")\n    result = 5 / 0\n    print(\"Computation was completed successfully\")\nexcept ZeroDivisionError:\n    print(\"Failed to compute result because you were trying to divide by zero\")\n    result = None\n\nprint(result)\n\nNow computing the result..\nFailed to compute result because you were trying to divide by zero\nNone\n\n\nWhen an exception occurs inside a try block, the block’s remaining statements are skipped. The except block is executed if the type of exception thrown matches that of the exception being handled. After executing the except block, the program execution returns to the normal flow.\nYou can also handle more than one type of exception using multiple except statements. Learn more about exceptions here: https://www.w3schools.com/python/python_try_except.asp .\nLet’s enhance the loan_emi function to use try-except to handle the scenario where the interest rate is 0%. It’s common practice to make changes/enhancements to functions over time as new scenarios and use cases come up. It makes functions more robust & versatile.\n\ndef loan_emi(amount, duration, rate, down_payment=0):\n    loan_amount = amount - down_payment\n    try:\n        emi = loan_amount * rate * ((1+rate)**duration) / (((1+rate)**duration)-1)\n    except ZeroDivisionError:\n        emi = loan_amount / duration\n    emi = math.ceil(emi)\n    return emi\n\nWe can use the updated loan_emi function to solve our problem.\n\nQ: If you borrow $100,000 using a 10-year loan with an interest rate of 9% per annum, what is the total amount you end up paying as interest?\n\n\nemi_with_interest = loan_emi(amount=100000, duration=10*12, rate=0.09/12)\nemi_with_interest\n\n1267\n\n\n\nemi_without_interest = loan_emi(amount=100000, duration=10*12, rate=0)\nemi_without_interest\n\n834\n\n\n\ntotal_interest = (emi_with_interest - emi_without_interest) * 10*12\n\n\nprint(\"The total interest paid is ${}.\".format(total_interest))\n\nThe total interest paid is $51960.\n\n\n\n\nDocumenting functions using Docstrings\nWe can add some documentation within our function using a docstring. A docstring is simply a string that appears as the first statement within the function body, and is used by the help function. A good docstring describes what the function does, and provides some explanation about the arguments.\n\ndef loan_emi(amount, duration, rate, down_payment=0):\n    \"\"\"Calculates the equal montly installment (EMI) for a loan.\n    \n    Arguments:\n        amount - Total amount to be spent (loan + down payment)\n        duration - Duration of the loan (in months)\n        rate - Rate of interest (monthly)\n        down_payment (optional) - Optional intial payment (deducted from amount)\n    \"\"\"\n    loan_amount = amount - down_payment\n    try:\n        emi = loan_amount * rate * ((1+rate)**duration) / (((1+rate)**duration)-1)\n    except ZeroDivisionError:\n        emi = loan_amount / duration\n    emi = math.ceil(emi)\n    return emi\n\nIn the docstring above, we’ve provided some additional information that the duration and rate are measured in months. You might even consider naming the arguments duration_months and rate_monthly, to avoid any confusion whatsoever. Can you think of some other ways to improve the function?\n\nhelp(loan_emi)\n\nHelp on function loan_emi in module __main__:\n\nloan_emi(amount, duration, rate, down_payment=0)\n    Calculates the equal montly installment (EMI) for a loan.\n    \n    Arguments:\n        amount - Total amount to be spent (loan + down payment)\n        duration - Duration of the loan (in months)\n        rate - Rate of interest (monthly)\n        down_payment (optional) - Optional intial payment (deducted from amount)"
  },
  {
    "objectID": "posts/2021-04-30-python_functions_and_scope.html#exercise---data-analysis-for-vacation-planning",
    "href": "posts/2021-04-30-python_functions_and_scope.html#exercise---data-analysis-for-vacation-planning",
    "title": "Writing Reusable Code using Functions in Python",
    "section": "Exercise - Data Analysis for Vacation Planning",
    "text": "Exercise - Data Analysis for Vacation Planning\nYou’re planning a vacation, and you need to decide which city you want to visit. You have shortlisted four cities and identified the return flight cost, daily hotel cost, and weekly car rental cost. While renting a car, you need to pay for entire weeks, even if you return the car sooner.\n\n\n\n\n\n\n\n\n\nCity\nReturn Flight ($)\nHotel per day ($)\nWeekly Car Rental ($)\n\n\n\n\nParis\n200\n20\n200\n\n\nLondon\n250\n30\n120\n\n\nDubai\n370\n15\n80\n\n\nMumbai\n450\n10\n70\n\n\n\nAnswer the following questions using the data above:\n\nIf you’re planning a 1-week long trip, which city should you visit to spend the least amount of money?\nHow does the answer to the previous question change if you change the trip’s duration to four days, ten days or two weeks?\nIf your total budget for the trip is $1000, which city should you visit to maximize the duration of your trip? Which city should you visit if you want to minimize the duration?\nHow does the answer to the previous question change if your budget is $600, $2000, or $1500?\n\nHint: To answer these questions, it will help to define a function cost_of_trip with relevant inputs like flight cost, hotel rate, car rental rate, and duration of the trip. You may find the math.ceil function useful for calculating the total cost of car rental.\n\n# Use these cells to answer the question - build the function step-by-step"
  },
  {
    "objectID": "posts/2021-04-30-python_functions_and_scope.html#summary-and-further-reading",
    "href": "posts/2021-04-30-python_functions_and_scope.html#summary-and-further-reading",
    "title": "Writing Reusable Code using Functions in Python",
    "section": "Summary and Further Reading",
    "text": "Summary and Further Reading\nWith this, we complete our discussion of functions in Python. We’ve covered the following topics in this tutorial:\n\nCreating and using functions\nFunctions with one or more arguments\nLocal variables and scope\nReturning values using return\nUsing default arguments to make a function flexible\nUsing named arguments while invoking a function\nImporting modules and using library functions\nReusing and improving functions to handle new use cases\nHandling exceptions with try-except\nDocumenting functions using docstrings\n\nThis tutorial on functions in Python is by no means exhaustive. Here are a few more topics to learn about:\n\nFunctions with an arbitrary number of arguments using (*args and **kwargs)\nDefining functions inside functions (and closures)\nA function that invokes itself (recursion)\nFunctions that accept other functions as arguments or return other functions\nFunctions that enhance other functions (decorators)\n\nFollowing are some resources to learn about more functions in Python:\n\nPython Tutorial at W3Schools: https://www.w3schools.com/python/\nPractical Python Programming: https://dabeaz-course.github.io/practical-python/Notes/Contents.html\nPython official documentation: https://docs.python.org/3/tutorial/index.html"
  },
  {
    "objectID": "posts/2021-04-30-python_functions_and_scope.html#questions-for-revision",
    "href": "posts/2021-04-30-python_functions_and_scope.html#questions-for-revision",
    "title": "Writing Reusable Code using Functions in Python",
    "section": "Questions for Revision",
    "text": "Questions for Revision\nTry answering the following questions to test your understanding of the topics covered in this notebook:\n\nWhat is a function?\nWhat are the benefits of using functions?\nWhat are some built-in functions in Python?\nHow do you define a function in Python? Give an example.\nWhat is the body of a function?\nWhen are the statements in the body of a function executed?\nWhat is meant by calling or invoking a function? Give an example.\nWhat are function arguments? How are they useful?\nHow do you store the result of a function in a variable?\nWhat is the purpose of the return keyword in Python?\nCan you return multiple values from a function?\nCan a return statement be used inside an if block or a for loop?\nCan the return keyword be used outside a function?\nWhat is scope in a programming region?\nHow do you define a variable inside a function?\nWhat are local & global variables?\nCan you access the variables defined inside a function outside its body? Why or why not?\nWhat do you mean by the statement “a function defines a scope within Python”?\nDo for and while loops define a scope, like functions?\nDo if-else blocks define a scope, like functions?\nWhat are optional function arguments & default values? Give an example.\nWhy should the required arguments appear before the optional arguments in a function definition?\nHow do you invoke a function with named arguments? Illustrate with an example.\nCan you split a function invocation into multiple lines?\nWrite a function that takes a number and rounds it up to the nearest integer.\nWhat are modules in Python?\nWhat is a Python library?\nWhat is the Python Standard Library?\nWhere can you learn about the modules and functions available in the Python standard library?\nHow do you install a third-party library?\nWhat is a module namespace? How is it useful?\nWhat problems would you run into if Python modules did not provide namespaces?\nHow do you import a module?\nHow do you use a function from an imported module? Illustrate with an example.\nCan you invoke a function inside the body of another function? Give an example.\nWhat is the single responsibility principle, and how does it apply while writing functions?\nWhat some characteristics of well-written functions?\nCan you use if statements or while loops within a function? Illustrate with an example.\nWhat are exceptions in Python? When do they occur?\nHow are exceptions different from syntax errors?\nWhat are the different types of in-built exceptions in Python? Where can you learn about them?\nHow do you prevent the termination of a program due to an exception?\nWhat is the purpose of the try-except statements in Python?\nWhat is the syntax of the try-except statements? Give an example.\nWhat happens if an exception occurs inside a try block?\nHow do you handle two different types of exceptions using except? Can you have multiple except blocks under a single try block?\nHow do you create an except block to handle any type of exception?\nIllustrate the usage of try-except inside a function with an example.\nWhat is a docstring? Why is it useful?\nHow do you display the docstring for a function?\nWhat are *args and **kwargs? How are they useful? Give an example.\nCan you define functions inside functions?\nWhat is function closure in Python? How is it useful? Give an example.\nWhat is recursion? Illustrate with an example.\nCan functions accept other functions as arguments? Illustrate with an example.\nCan functions return other functions as results? Illustrate with an example.\nWhat are decorators? How are they useful?\nImplement a function decorator which prints the arguments and result of wrapped functions.\nWhat are some in-built decorators in Python?\nWhat are some popular Python libraries?"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html",
    "title": "Supervised Learning with scikit-learn",
    "section": "",
    "text": "Machine learning is the field that teaches machines and computers to learn from existing data to make predictions on new data: Will a tumor be benign or malignant? Which of your customers will take their business elsewhere? Is a particular email spam? We will use Python to perform supervised learning, an essential component of machine learning. We will build predictive models, tune their parameters, and determine how well they will perform with unseen data—all while using real world datasets. We be using scikit-learn, one of the most popular and user-friendly machine learning libraries for Python."
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#supervised-learning",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#supervised-learning",
    "title": "Supervised Learning with scikit-learn",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nWhat is machine learning?\n\nThe art and science of:\nGiving computers the ability to learn to make decisions from data\nwithout being explicitly programmed!\nExamples:\nLearning to predict whether an email is spam or not\nClustering wikipedia entries into different categories\nSupervised learning: Uses labeled data\nUnsupervised learning: Uses unlabeled data\n\n\n\nUnsupervised learning\n\nUncovering hidden patterns from unlabeled data\nExample:\nGrouping customers into distinct categories (Clustering)\n\n\n\nReinforcement learning\n\nSoftware agents interact with an environment\nLearn how to optimize their behavior\nGiven a system of rewards and punishments\nDraws inspiration from behavioral psychology\nApplications\nEconomics\nGenetics\nGame playing\nAlphaGo: First computer to defeat the world champion in Go\n\n\n\nSupervised learning\n\nPredictor variables/features and a target variable\nAim:\nPredict the target variable, given the predictor variables\nClassication: Target variable consists of categories\nRegression: Target variable is continuous\n\n\n\nNaming conventions\n\nFeatures = predictor variables = independent variables\nTarget variable = dependent variable = response variable\n\n\n\nSupervised learning\n\nAutomate time-consuming or expensive manual tasks\nExample: Doctor’s diagnosis\nMake predictions about the future\nExample: Will a customer click on an ad or not?\nNeed labeled data\nHistorical data with labels\nExperiments to get labeled data\nCrowd-sourcing labeled data\n\n\n\nSupervised learning in Python\n\nWe will use scikit-learn/sklearn\nIntegrates well with the SciPy stack\nOtherlibraries\nTensor Flow\nkeras"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#exploratory-data-analysis",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#exploratory-data-analysis",
    "title": "Supervised Learning with scikit-learn",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nThe Iris dataset\n\nFeatures:\nPetal length\nPetal width\nSepal length\nSepal width\nTarget variable:\nSpecies Versicolor\nVirginica\nSetosa\n\n\n\nThe Iris dataset in scikit-learn\n\niris = datasets.load_iris()\ntype(iris)\n\nsklearn.utils.Bunch\n\n\n\niris.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n\n\n\ntype(iris.data)\n\nnumpy.ndarray\n\n\n\ntype(iris.target)\n\nnumpy.ndarray\n\n\n\niris.data.shape\n\n(150, 4)\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n\n\n\n\nExploratory data analysis (EDA)\n\nX = iris.data\ny= iris.target\ndf = pd.DataFrame(X, columns=iris.feature_names)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sepal length (cm)\n      sepal width (cm)\n      petal length (cm)\n      petal width (cm)\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n    \n  \n\n\n\n\n\ndf2 = df.copy()\ndf2['target_names'] = iris.target\ndf2.head()\n\n\n\n\n\n  \n    \n      \n      sepal length (cm)\n      sepal width (cm)\n      petal length (cm)\n      petal width (cm)\n      target_names\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      0\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      0\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      0\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      0\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      0\n    \n  \n\n\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n\n\n\ndf2.target_names.value_counts()\n\n2    50\n1    50\n0    50\nName: target_names, dtype: int64\n\n\n\ndf2['target_names'] = df2.target_names.map({0:'setosa', 1:'versicolor', 2:'virginica'})\ndf2.head()\n\n\n\n\n\n  \n    \n      \n      sepal length (cm)\n      sepal width (cm)\n      petal length (cm)\n      petal width (cm)\n      target_names\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\n\nVisual EDA\n\n_ = pd.plotting.scatter_matrix(df, c=y, figsize=[8,8], s=150, marker=\"D\")\n\n\n\n\n\n\nNumerical EDA\nWe’ll be working with a dataset obtained from the UCI Machine Learning Repository consisting of votes made by US House of Representatives Congressmen. our goal will be to predict their party affiliation (‘Democrat’ or ‘Republican’) based on how they voted on certain key issues.\n\n\n\n\n\n\nNote\n\n\n\nHere, it’s worth noting that we have preprocessed this dataset to deal with missing values. This is so that our focus can be directed towards understanding how to train and evaluate supervised learning models.\n\n\nBefore thinking about what supervised learning models we can apply to this, however, we need to perform Exploratory data analysis (EDA) in order to understand the structure of the data.\n\nvotes = pd.read_csv(\"datasets/votes.csv\")\nvotes.head()\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      0\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      1\n    \n    \n      1\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n    \n    \n      2\n      democrat\n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n    \n    \n      3\n      democrat\n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n    \n    \n      4\n      democrat\n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      1\n    \n  \n\n\n\n\n\nvotes.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 435 entries, 0 to 434\nData columns (total 17 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   party              435 non-null    object\n 1   infants            435 non-null    int64 \n 2   water              435 non-null    int64 \n 3   budget             435 non-null    int64 \n 4   physician          435 non-null    int64 \n 5   salvador           435 non-null    int64 \n 6   religious          435 non-null    int64 \n 7   satellite          435 non-null    int64 \n 8   aid                435 non-null    int64 \n 9   missile            435 non-null    int64 \n 10  immigration        435 non-null    int64 \n 11  synfuels           435 non-null    int64 \n 12  education          435 non-null    int64 \n 13  superfund          435 non-null    int64 \n 14  crime              435 non-null    int64 \n 15  duty_free_exports  435 non-null    int64 \n 16  eaa_rsa            435 non-null    int64 \ndtypes: int64(16), object(1)\nmemory usage: 57.9+ KB\n\n\n\nvotes.describe()\n\n\n\n\n\n  \n    \n      \n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      count\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n      435.000000\n    \n    \n      mean\n      0.429885\n      0.558621\n      0.606897\n      0.406897\n      0.521839\n      0.650575\n      0.581609\n      0.590805\n      0.526437\n      0.512644\n      0.344828\n      0.393103\n      0.537931\n      0.609195\n      0.400000\n      0.857471\n    \n    \n      std\n      0.495630\n      0.497123\n      0.489002\n      0.491821\n      0.500098\n      0.477337\n      0.493863\n      0.492252\n      0.499876\n      0.500416\n      0.475859\n      0.489002\n      0.499133\n      0.488493\n      0.490462\n      0.349994\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      50%\n      0.000000\n      1.000000\n      1.000000\n      0.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n      1.000000\n      0.000000\n      1.000000\n    \n    \n      75%\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n    \n      max\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\n\n\nObservations\n\nThe DataFrame has a total of 435 rows and 17 columns.\nExcept for 'party', all of the columns are of type int64.\nThe first two rows of the DataFrame consist of votes made by Republicans and the next three rows consist of votes made by Democrats.\nThe target variable in this DataFrame is 'party'.\n\n\n\nVotes Visual EDA\nThe Numerical EDA we did gave us some very important information, such as the names and data types of the columns, and the dimensions of the DataFrame. Following this with some visual EDA will give us an even better understanding of the data. all the features in this dataset are binary; that is, they are either 0 or 1. So a different type of plot would be more useful here, such as Seaborn’s countplot.\n\ndef plot_countplot(column):\n    plt.figure()\n    sns.countplot(x=column, hue='party', data=votes, palette='RdBu')\n    plt.xticks([0,1], ['No', 'Yes'])\n    plt.show()\n    \nplot_countplot(\"education\")\n\n\n\n\nIt seems like Democrats voted resoundingly against this bill, compared to Republicans. This is the kind of information that our machine learning model will seek to learn when we try to predict party affiliation solely based on voting behavior. An expert in U.S politics may be able to predict this without machine learning, but probably not instantaneously - and certainly not if we are dealing with hundreds of samples!\n\nplot_countplot('infants')\n\n\n\n\n\nplot_countplot('water')\n\n\n\n\n\nplot_countplot(\"budget\")\n\n\n\n\n\nplot_countplot('physician')\n\n\n\n\n\nplot_countplot('salvador')\n\n\n\n\n\nplot_countplot('religious')\n\n\n\n\n\nplot_countplot('satellite')\n\n\n\n\n\nplot_countplot('aid')\n\n\n\n\n\nplot_countplot('missile')\n\n\n\n\n\nplot_countplot('immigration')\n\n\n\n\n\nplot_countplot('synfuels')\n\n\n\n\n\nplot_countplot('superfund')\n\n\n\n\n\nplot_countplot('crime')\n\n\n\n\n\nplot_countplot('duty_free_exports')\n\n\n\n\n\nplot_countplot('eaa_rsa')\n\n\n\n\n\n\nObservations\n\nDemocrats voted in favor of both 'satellite' and 'missile'"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#the-classification-challenge",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#the-classification-challenge",
    "title": "Supervised Learning with scikit-learn",
    "section": "The classification challenge",
    "text": "The classification challenge\n\nk-Nearest Neighbors\n\nBasic idea: Predict the label of a data point by\nLooking at the ‘k’ closest labeled data points\nTaking a majority vote\n\n\n\nScikit-learn fit and predict\n\nAll machine learning models implemented as Python classes\nThey implement the algorithms for learning and predicting\nStore the information learned from the data\nTraining a model on the data = ‘fitting’ a model to the data\n.fit() method\nTo predict the labels of new data: .predict() method\n\n\n\nIris k-NN: Intuition\n\n_ = sns.scatterplot(data=df2, x=\"petal width (cm)\", y=\"petal length (cm)\", hue='target_names')\nplt.show()\n\n\n\n\n\n\nIris dataset Using scikit-learn to fit a classier\n\nknn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(iris['data'], iris['target'])\n\nKNeighborsClassifier(n_neighbors=6)\n\n\n\niris['data'].shape\n\n(150, 4)\n\n\n\niris['target'].shape\n\n(150,)\n\n\n\n\nPredicting on unlabeled data\n\nX_new = np.array([[5.6, 2.8, 3.9, 1.1],\n                 [5.7, 2.6, 3.8, 1.3],\n                 [4.7, 3.2, 1.3, 0.2]])\nprediction = knn.predict(X_new)\nprediction\n\narray([1, 1, 0])\n\n\n\n\nk-Nearest Neighbors: Fit\nHaving explored the Congressional voting records dataset, it is time now to build our first classifier. We’ll will fit a k-Nearest Neighbors classifier to the voting dataset.\nThe features need to be in an array where each column is a feature and each row a different observation or data point - in this case, a Congressman’s voting record. The target needs to be a single column with the same number of observations as the feature data. We will name the feature array X and response variable y: This is in accordance with the common scikit-learn practice.\n\n# Create arrays for the features and the response variable\ny_votes = votes['party'].values\nX_votes = votes.drop('party', axis=1).values\n\n# Create a k-NN classifier with 6 neighbors\nknn_votes = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn_votes.fit(X_votes, y_votes)\n\nKNeighborsClassifier(n_neighbors=6)\n\n\nNow that the k-NN classifier with 6 neighbors has been fit to the data, it can be used to predict the labels of new data points.\n\n\nk-Nearest Neighbors: Predict\n\nX_new_votes = pd.read_csv(\"datasets/X_new_votes.csv\")\nX_new_votes.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n    \n  \n  \n    \n      0\n      0.696469\n      0.286139\n      0.226851\n      0.551315\n      0.719469\n      0.423106\n      0.980764\n      0.68483\n      0.480932\n      0.392118\n      0.343178\n      0.72905\n      0.438572\n      0.059678\n      0.398044\n      0.737995\n    \n  \n\n\n\n\nHaving fit a k-NN classifier, we can now use it to predict the label of a new data point.\n\n# Predict and print the label for the new data point X_new\nnew_prediction = knn_votes.predict(X_new_votes)\nprint(\"Prediction: {}\".format(new_prediction))\n\nPrediction: ['democrat']"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#measuring-model-performance",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#measuring-model-performance",
    "title": "Supervised Learning with scikit-learn",
    "section": "Measuring model performance",
    "text": "Measuring model performance\n\n\nIn classication, accuracy is a commonly used metric\nAccuracy = Fraction of correct predictions\nWhich data should be used to compute accuracy?\nHow well will the model perform on new data?\nCould compute accuracy on data used to fit classifier\nNOT indicative of ability to generalize\nSplit data into training and test set\nFit/train the classifier on the training set\nMake predictions on test set\nCompare predictions with the known labels\n\n\n\nModel complexity\n\nLarger k = smoother decision boundary = less complex model\nSmaller k = more complex model = can lead to overfitting\n\n\n\nX_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X, y, test_size=.3, random_state=21, stratify=y)\nknn_iris = KNeighborsClassifier(n_neighbors=8)\nknn_iris.fit(X_train_iris, y_train_iris)\ny_pred_iris = knn_iris.predict(X_test_iris)\n\nprint(f\"Test set predictions \\n{y_pred_iris}\")\n\nTest set predictions \n[2 1 2 2 1 0 1 0 0 1 0 2 0 2 2 0 0 0 1 0 2 2 2 0 1 1 1 0 0 1 2 2 0 0 1 2 2\n 1 1 2 1 1 0 2 1]\n\n\n\nknn_iris.score(X_test_iris, y_test_iris)\n\n0.9555555555555556\n\n\n\nThe digits recognition dataset\nWe’ll be working with the MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn’s included datasets.\nEach sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. Helpfully for the MNIST dataset, scikit-learn provides an 'images' key in addition to the 'data' and 'target' keys that we have seen with the Iris data. Because it is a 2D array of the images corresponding to each sample, this 'images' key is useful for visualizing the images. On the other hand, the 'data' key contains the feature array - that is, the images as a flattened array of 64 pixels.\n\n# Load the digits dataset: digits\ndigits = datasets.load_digits()\n\n# Print the keys and DESCR of the dataset\nprint(digits.keys())\nprint(digits.DESCR)\n\ndict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n.. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 5620\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n\n\n# Print the shape of the images and data keys\nprint(digits.images.shape)\ndigits.data.shape\n\n(1797, 8, 8)\n\n\n(1797, 64)\n\n\n\n# Display digit 1010\nplt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()\n\n\n\n\nIt looks like the image in question corresponds to the digit ‘5’. Now, can we build a classifier that can make this prediction not only for this image, but for all the other ones in the dataset?\n\n\nTrain/Test Split + Fit/Predict/Accuracy\nNow that we have learned about the importance of splitting your data into training and test sets, it’s time to practice doing this on the digits dataset! After creating arrays for the features and target variable, we will split them into training and test sets, fit a k-NN classifier to the training data, and then compute its accuracy using the .score() method.\n\n# Create feature and target arrays\nX_digits = digits.data\ny_digits = digits.target\n\n# Split into training and test set\nX_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(X_digits, y_digits, \n                                                                                test_size = 0.2, random_state= 42, \n                                                                                stratify=y_digits)\n\n# Create a k-NN classifier with 7 neighbors: knn_digits\nknn_digits = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn_digits.fit(X_train_digits, y_train_digits)\n\n# Print the accuracy\nknn_digits.score(X_test_digits, y_test_digits)\n\n0.9833333333333333\n\n\nIncredibly, this out of the box k-NN classifier with 7 neighbors has learned from the training data and predicted the labels of the images in the test set with 98% accuracy, and it did so in less than a second! This is one illustration of how incredibly useful machine learning techniques can be.\n\n\nOverfitting and underfitting\nWe will now construct such a model complexity curve for the digits dataset! We will compute and plot the training and testing accuracy scores for a variety of different neighbor values.\nBy observing how the accuracy scores differ for the training and testing sets with different values of k, we will develop your intuition for overfitting and underfitting.\n\n# Setup arrays to store train and test accuracies\nneighbors_digits = np.arange(1, 9)\ntrain_accuracy_digits = np.empty(len(neighbors_digits))\ntest_accuracy_digits = np.empty(len(neighbors_digits))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors_digits):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn_digits = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn_digits.fit(X_train_digits, y_train_digits)\n    \n    #Compute accuracy on the training set\n    train_accuracy_digits[i] = knn_digits.score(X_train_digits, y_train_digits)\n\n    #Compute accuracy on the testing set\n    test_accuracy_digits[i] = knn_digits.score(X_test_digits, y_test_digits)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors_digits, test_accuracy_digits, label = 'Testing Accuracy')\nplt.plot(neighbors_digits, train_accuracy_digits, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n\n\nIt looks like the test accuracy is highest when using 1 and35 neighbors. Using 8 neighbors or more seems to result in a simple model that underfits the data."
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#introduction-to-regression",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#introduction-to-regression",
    "title": "Supervised Learning with scikit-learn",
    "section": "Introduction to regression",
    "text": "Introduction to regression\nExample of an regression problem: A bike share company using time and weather data to predict the number of bikes being rented at any given hour. The target variable here - the number of bike rentals at any given hour - is quantitative, so this is best framed as a regression problem.\n\nBoston housing data\n\nboston = datasets.load_boston()\nboston.data.shape\n\n(506, 13)\n\n\n\nboston.target.shape\n\n(506,)\n\n\n\nboston.feature_names\n\narray(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')\n\n\n\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['MEDV'] = boston.target\nboston_df.head()\n\n\n\n\n\n  \n    \n      \n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n      MEDV\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0.0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1.0\n      296.0\n      15.3\n      396.90\n      4.98\n      24.0\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0.0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2.0\n      242.0\n      17.8\n      396.90\n      9.14\n      21.6\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0.0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2.0\n      242.0\n      17.8\n      392.83\n      4.03\n      34.7\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0.0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3.0\n      222.0\n      18.7\n      394.63\n      2.94\n      33.4\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0.0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3.0\n      222.0\n      18.7\n      396.90\n      5.33\n      36.2\n    \n  \n\n\n\n\n\n\nCreating feature and target arrays for the boston dataset\n\nX_boston = boston.data\ny_boston = boston.target\n\n\n\nPredicting house value from a single feature\n\nX_boston_rooms = X_boston[:,5]\ntype(X_boston_rooms), type(y_boston)\n\n(numpy.ndarray, numpy.ndarray)\n\n\n\ny_boston = y_boston.reshape(-1,1)\nX_boston_rooms = X_boston_rooms.reshape(-1,1)\n\n\n\nPlotting house value vs. number of rooms\n\nplt.scatter(X_boston_rooms, y_boston)\nplt.ylabel('Value of house /1000 ($)')\nplt.xlabel('Number of rooms')\nplt.show();\n\n\n\n\n\n\nFitting a regression model\n\nreg_boston = LinearRegression()\nreg_boston.fit(X_boston_rooms, y_boston)\nboston_prediction_space = np.linspace(min(X_boston_rooms), max(X_boston_rooms)).reshape(-1,1)\n\n\nplt.scatter(X_boston_rooms, y_boston, color=\"blue\")\nplt.plot(boston_prediction_space, reg_boston.predict(boston_prediction_space), color='black', linewidth=3)\nplt.show()\n\n\n\n\n\n\nImporting Gapminder data for supervised learning\nWe will work with Gapminder data that we have consolidated into one CSV file.\nSpecifically, our goal will be to use this data to predict the life expectancy in a given country based on features such as the country’s GDP, fertility rate, and population.\nSince the target variable here is quantitative, this is a regression problem. To begin, we will fit a linear regression with just one feature: 'fertility', which is the average number of children a woman in a given country gives birth to.\nBefore that, however, we need to import the data and get it into the form needed by scikit-learn. This involves creating feature and target variable arrays. Furthermore, since we are going to use only one feature to begin with, we need to do some reshaping using NumPy’s .reshape() method.\n\n# Read the CSV file into a DataFrame: gapminder_df\ngapminder = pd.read_csv(\"datasets/gapminder.csv\")\n\n# Create arrays for features and target variable\ny_gapminder = gapminder.life.values\nX_gapminder = gapminder.fertility.values\n\n# Print the dimensions of X and y before reshaping\nprint(\"Dimensions of y before reshaping: {}\".format(y_gapminder.shape))\nprint(\"Dimensions of X before reshaping: {}\".format(X_gapminder.shape))\n\n# Reshape X and y\ny_gapminder = y_gapminder.reshape(-1,1)\nX_gapminder = X_gapminder.reshape(-1,1)\n\n# Print the dimensions of X and y after reshaping\nprint(\"Dimensions of y after reshaping: {}\".format(y_gapminder.shape))\nprint(\"Dimensions of X after reshaping: {}\".format(X_gapminder.shape))\n\nDimensions of y before reshaping: (139,)\nDimensions of X before reshaping: (139,)\nDimensions of y after reshaping: (139, 1)\nDimensions of X after reshaping: (139, 1)\n\n\n\n\nExploring the Gapminder data\nAs always, it is important to explore the data before building models.\n\nsns.heatmap(gapminder.corr(), square=True, cmap=\"RdYlGn\")\nplt.show()\n\n\n\n\nCells that are in green show positive correlation, while cells that are in red show negative correlation. life and fertility are negatively correlated. GDP and life are positively correlated\n\ngapminder.head()\n\n\n\n\n\n  \n    \n      \n      population\n      fertility\n      HIV\n      CO2\n      BMI_male\n      GDP\n      BMI_female\n      life\n      child_mortality\n    \n  \n  \n    \n      0\n      34811059.0\n      2.73\n      0.1\n      3.328945\n      24.59620\n      12314.0\n      129.9049\n      75.3\n      29.5\n    \n    \n      1\n      19842251.0\n      6.43\n      2.0\n      1.474353\n      22.25083\n      7103.0\n      130.1247\n      58.3\n      192.0\n    \n    \n      2\n      40381860.0\n      2.24\n      0.5\n      4.785170\n      27.50170\n      14646.0\n      118.8915\n      75.5\n      15.4\n    \n    \n      3\n      2975029.0\n      1.40\n      0.1\n      1.804106\n      25.35542\n      7383.0\n      132.8108\n      72.5\n      20.0\n    \n    \n      4\n      21370348.0\n      1.96\n      0.1\n      18.016313\n      27.56373\n      41312.0\n      117.3755\n      81.5\n      5.2\n    \n  \n\n\n\n\n\ngapminder.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 139 entries, 0 to 138\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   population       139 non-null    float64\n 1   fertility        139 non-null    float64\n 2   HIV              139 non-null    float64\n 3   CO2              139 non-null    float64\n 4   BMI_male         139 non-null    float64\n 5   GDP              139 non-null    float64\n 6   BMI_female       139 non-null    float64\n 7   life             139 non-null    float64\n 8   child_mortality  139 non-null    float64\ndtypes: float64(9)\nmemory usage: 9.9 KB\n\n\n\nThe DataFrame has 139 samples (or rows) and 9 columns.\n\n\ngapminder.describe()\n\n\n\n\n\n  \n    \n      \n      population\n      fertility\n      HIV\n      CO2\n      BMI_male\n      GDP\n      BMI_female\n      life\n      child_mortality\n    \n  \n  \n    \n      count\n      1.390000e+02\n      139.000000\n      139.000000\n      139.000000\n      139.000000\n      139.000000\n      139.000000\n      139.000000\n      139.000000\n    \n    \n      mean\n      3.549977e+07\n      3.005108\n      1.915612\n      4.459874\n      24.623054\n      16638.784173\n      126.701914\n      69.602878\n      45.097122\n    \n    \n      std\n      1.095121e+08\n      1.615354\n      4.408974\n      6.268349\n      2.209368\n      19207.299083\n      4.471997\n      9.122189\n      45.724667\n    \n    \n      min\n      2.773150e+05\n      1.280000\n      0.060000\n      0.008618\n      20.397420\n      588.000000\n      117.375500\n      45.200000\n      2.700000\n    \n    \n      25%\n      3.752776e+06\n      1.810000\n      0.100000\n      0.496190\n      22.448135\n      2899.000000\n      123.232200\n      62.200000\n      8.100000\n    \n    \n      50%\n      9.705130e+06\n      2.410000\n      0.400000\n      2.223796\n      25.156990\n      9938.000000\n      126.519600\n      72.000000\n      24.000000\n    \n    \n      75%\n      2.791973e+07\n      4.095000\n      1.300000\n      6.589156\n      26.497575\n      23278.500000\n      130.275900\n      76.850000\n      74.200000\n    \n    \n      max\n      1.197070e+09\n      7.590000\n      25.900000\n      48.702062\n      28.456980\n      126076.000000\n      135.492000\n      82.600000\n      192.000000\n    \n  \n\n\n\n\nThe mean of life is 69.602878"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#the-basics-of-linear-regression",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#the-basics-of-linear-regression",
    "title": "Supervised Learning with scikit-learn",
    "section": "The basics of linear regression",
    "text": "The basics of linear regression\n\nRegression mechanics\n\n\\(y = ax + b\\)\n\\(y\\) = target\n\\(x\\) = single feature\n\\(a\\), \\(b\\) = parameters of model\nHow do we choose \\(a\\) and \\(b\\)?\nDefine an error functions for any given lineChoose the line that minimizes the error function\nOrdinary least squares(OLS): Minimize sum of squares of residuals\n\n\n\nLinear regression in higher dimensions\n\n\\(y=a_1x_1+a_2x_2+b\\)\nTo fit a linear regression model here:\nNeed to specify 3 variables\nIn higher dimensions:\nMust specify coefcient for each feature and the variable \\(b\\)\n\\(y=a_1x_1+a_2x_2+a_3x_3+...+a_nx_n+b\\)\nScikit-learn API works exactly the same way:\nPass two arrays: Features, and target\n\n\n\nLinear regression on all features in boston dataset\n\nX_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(X_boston, y_boston, \n                                                                                test_size=.3, random_state=42)\nreg_all_boston = LinearRegression()\nreg_all_boston.fit(X_train_boston, y_train_boston)\ny_pred_boston = reg_all_boston.predict(X_test_boston)\nreg_all_boston.score(X_test_boston, y_test_boston)\n\n0.7112260057484925\n\n\n\n\nFit & predict for regression in gapminder dataset\nWe will fit a linear regression and predict life expectancy using just one feature. We will use the 'fertility' feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is 'life'.\n\nsns.scatterplot(data=gapminder, x=\"fertility\", y=\"life\")\nplt.show()\n\n\n\n\nAs you can see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Our job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. We will also compute and print the \\(R^2\\) score using sckit-learn’s .score() method.\n\n# Create the regressor: reg\nreg_gapminder = LinearRegression()\n\n# Create the prediction space\nprediction_space = np.linspace(min(X_gapminder), max(X_gapminder)).reshape(-1,1)\n\n# Fit the model to the data\nreg_gapminder.fit(X_gapminder,y_gapminder)\n\n# Compute predictions over the prediction space: y_pred\ny_pred_gapminder = reg_gapminder.predict(prediction_space)\n\n# Print R^2 \nprint(reg_gapminder.score(X_gapminder, y_gapminder))\n\n0.6192442167740035\n\n\n\n# Plot regression line\nsns.scatterplot(data=gapminder, x=\"fertility\", y=\"life\")\nplt.plot(prediction_space, y_pred_gapminder, color='black', linewidth=3)\nplt.show()\n\n\n\n\nNotice how the line captures the underlying trend in the data. And the performance is quite decent for this basic regression model with only one feature!\n\n\nTrain/test split for regression\ntrain and test sets are vital to ensure that the supervised learning model is able to generalize well to new data. This was true for classification models, and is equally true for linear regression models.\nWe will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the \\(R^2\\) score, we will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models.\n\nX_gapminder = gapminder.drop(\"life\", axis=1).values\n\n\n# Create training and test sets\nX_train_gapminder, X_test_gapminder, y_train_gapminder, y_test_gapminder = train_test_split(X_gapminder, y_gapminder, test_size = .3, random_state=42)\n\n# Create the regressor: reg_all\nreg_all_gapminder = LinearRegression()\n\n# Fit the regressor to the training data\nreg_all_gapminder.fit(X_train_gapminder, y_train_gapminder)\n\n# Predict on the test data: y_pred\ny_pred_gapminder = reg_all_gapminder.predict(X_test_gapminder)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(reg_all_gapminder.score(X_test_gapminder, y_test_gapminder)))\nrmse_gapminder = np.sqrt(mean_squared_error(y_test_gapminder, y_pred_gapminder))\nprint(\"Root Mean Squared Error: {}\".format(rmse_gapminder))\n\nR^2: 0.8380468731430059\nRoot Mean Squared Error: 3.247601080037022\n\n\nUsing all features has improved the model score. This makes sense, as the model has more information to learn from. However, there is one potential pitfall to this process. Can you spot it?"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#cross-validation",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#cross-validation",
    "title": "Supervised Learning with scikit-learn",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nCross-validation motivation\n\nModel performance is dependent on way the data is split\nNot representative of the model’s ability to generalize\nSolution: Cross-validation!\n\n\n\nCross-validation and model performance\n\n5 folds = 5-fold CV\n10 folds = 10-fold CV\nk folds = k-fold CV\nMore folds = More computationally expensive\n\n\n\nCross-validation in scikit-learn: Boston\n\ncv_results_boston = cross_val_score(reg_all_boston, X_boston, y_boston, cv=5)\ncv_results_boston\n\narray([ 0.63919994,  0.71386698,  0.58702344,  0.07923081, -0.25294154])\n\n\n\nnp.mean(cv_results_boston)\n\n0.353275924395884\n\n\n\nnp.median(cv_results_boston)\n\n0.5870234363057776\n\n\n\n\n5-fold cross-validation\nCross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data.\nWe will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn’s cross_val_score() function uses R2 as the metric of choice for regression. Since We are performing 5-fold cross-validation, the function will return 5 scores. We will compute these 5 scores and then take their average.\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores_gapminder = cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores_gapminder)\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores_gapminder)))\n\n[0.81720569 0.82917058 0.90214134 0.80633989 0.94495637]\nAverage 5-Fold CV Score: 0.8599627722793267\n\n\nNow that we have cross-validated your model, we can more confidently evaluate its predictions.\n\n\nK-Fold CV comparison\n\n\n\n\n\n\nWarning\n\n\n\nCross validation is essential but do not forget that the more folds you use, the more computationally expensive cross-validation becomes.\n\n\n\n%timeit cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=3)\n\n8.03 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%timeit cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=10)\n\n31.8 ms ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n# Perform 3-fold CV\ncvscores_3_gapminder = cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=3)\nprint(np.mean(cvscores_3_gapminder))\n\n# Perform 10-fold CV\ncvscores_10_gapminder = cross_val_score(reg_gapminder, X_gapminder, y_gapminder, cv=10)\nprint(np.mean(cvscores_10_gapminder))\n\n0.8718712782621969\n0.8436128620131095"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#regularized-regression",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#regularized-regression",
    "title": "Supervised Learning with scikit-learn",
    "section": "Regularized regression",
    "text": "Regularized regression\n\nWhy regularize?\n\nRecall: Linear regression minimizes a loss function\nIt chooses a coefcient for each feature variable\nLarge coefcients can lead to overtting\nPenalizing large coefcients: Regularization\n\n\n\nRidge regression\n\nLoss function = OLS loss function + \\(\\alpha * \\sum_{i=1}^{n} a_i^2\\)\nAlpha: Parameter we need to choose\nPicking alpha here is similar to picking k in k-NN\nHyperparameter tuning\nAlpha controls model complexity\nAlpha = 0: We get back OLS (Can lead to overtting)\nVery high alpha: Can lead to undertting\n\n\n\nLasso regression\n\nLoss function = OLS loss function + \\(\\alpha * \\sum_{i=1}^{n} |a_i|\\)\n\n\n\nLasso regression for feature selection\n\nCan be used to select important features of a dataset\nShrinks the coefcients of less important features to exactly 0\n\n\n\nRidge regression in scikit-learn: Boston\n\nridge_boston = Ridge(alpha=.1, normalize=True)\nridge_boston.fit(X_train_boston, y_train_boston)\nridge_pred_boston = ridge_boston.predict(X_test_boston)\nridge_boston.score(X_test_boston, y_test_boston)\n\n0.6996938275127311\n\n\n\n\nLasso regression in scikit-learn: Boston\n\nlasso_boston = Lasso(alpha=.1, normalize=True)\nlasso_boston.fit(X_train_boston, y_train_boston)\nlasso_pred_boston = lasso_boston.predict(X_test_boston)\nlasso_boston.score(X_test_boston, y_test_boston)\n\n0.5950229535328551\n\n\n\n\nLasso for feature selection in scikit-learn: Boston\n\nnames_boston = boston.feature_names\nlasso_boston_2 = Lasso(alpha=.1)\nlasso_coef_boston = lasso_boston_2.fit(X_boston, y_boston).coef_\n_ = plt.plot(range(len(names_boston)), lasso_coef_boston)\n_ = plt.xticks(range(len(names_boston)), names_boston, rotation=60)\n_ = plt.ylabel(\"Coefficients\")\nplt.show()\n\n\n\n\n\n\nRegularization I: Lasso\nWe saw how Lasso selected out the ‘RM’ feature as being the most important for predicting Boston house prices, while shrinking the coefficients of certain other features to 0. Its ability to perform feature selection in this way becomes even more useful when you are dealing with data involving thousands of features.\nWe will fit a lasso regression to the Gapminder data we have been working with and plot the coefficients. Just as with the Boston data.\n\ndf_columns_gapminder = pd.Index(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n       'BMI_female', 'child_mortality'],\n      dtype='object')\n\n\n# Instantiate a lasso regressor: lasso\nlasso_gapminder = Lasso(alpha=.4, normalize=True)\n\n# Fit the regressor to the data\nlasso_gapminder.fit(X_gapminder,y_gapminder)\n\n# Compute and print the coefficients\nlasso_coef_gapminder = lasso_gapminder.fit(X_gapminder,y_gapminder).coef_\nprint(lasso_coef_gapminder)\n\n# Plot the coefficients\nplt.plot(range(len(df_columns_gapminder)), lasso_coef_gapminder)\nplt.xticks(range(len(df_columns_gapminder)), df_columns_gapminder.values, rotation=60)\nplt.margins(0.02)\nplt.show()\n\n[-0.         -0.         -0.          0.          0.          0.\n -0.         -0.07087587]\n\n\n\n\n\nAccording to the lasso algorithm, it seems like 'child_mortality' is the most important feature when predicting life expectancy.\n\n\nRegularization II: Ridge\nLasso is great for feature selection, but when building regression models, Ridge regression should be the first choice.\nlasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as \\(L1\\) regularization because the regularization term is the \\(L1\\) norm of the coefficients. This is not the only way to regularize, however.\n\ndef display_plot(cv_scores, cv_scores_std):\n    \"\"\"plots the R^2 score as well as standard error for each alpha\"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space_gapminder, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space_gapminder, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space_gapminder[0], alpha_space_gapminder[-1]])\n    ax.set_xscale('log')\n    plt.show()\n\nIf instead we took the sum of the squared values of the coefficients multiplied by some alpha - like in Ridge regression - we would be computing the \\(L2\\) norm. We will fit ridge regression models over a range of different alphas, and plot cross-validated \\(R^2\\) scores for each, using this function display_plot, which plots the \\(R^2\\) score as well as standard error for each alpha:\n\n# Setup the array of alphas and lists to store scores\nalpha_space_gapminder = np.logspace(-4, 0, 50)\nridge_scores_gapminder = []\nridge_scores_std_gapminder = []\n\n# Create a ridge regressor: ridge\nridge_gapminder = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space_gapminder:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge_gapminder.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores_gapminder = cross_val_score(ridge_gapminder, X_gapminder, y_gapminder, cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores_gapminder.append(np.mean(ridge_cv_scores_gapminder))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std_gapminder.append(np.std(ridge_cv_scores_gapminder))\n\n# Display the plot\ndisplay_plot(ridge_scores_gapminder, ridge_scores_std_gapminder)\n\n\n\n\nthe cross-validation scores change with different alphas."
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#logistic-regression-and-the-roc-curve",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#logistic-regression-and-the-roc-curve",
    "title": "Supervised Learning with scikit-learn",
    "section": "Logistic regression and the ROC curve",
    "text": "Logistic regression and the ROC curve\n\nLogistic regression for binary classication\n\nLogistic regression outputs probabilities\nIf the probability ‘p’ is greater than 0.5:\nThe data is labeled ‘1’\nIf the probability ‘p’ is less than 0.5:\nThe data is labeled ‘0’\n\n\n\nProbability thresholds\n\nBy default, logistic regression threshold = 0.5\nNot specific to logistic regression\nk-NN classifiers also have thresholds\nWhat happens if we vary the threshold?\n\n\n\nBuilding a logistic regression model\nTime to build our first logistic regression model! scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as ‘estimators’.\n\n# Create the classifier: logreg\nlogreg_pidd = LogisticRegression()\n\n# Fit the classifier to the training data\nlogreg_pidd.fit(X_train_pidd, y_train_pidd)\n\n# Predict the labels of the test set: y_pred\ny_pred_logreg_pidd = logreg_pidd.predict(X_test_pidd)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test_pidd, y_pred_logreg_pidd))\nprint(classification_report(y_test_pidd, y_pred_logreg_pidd))\n\n[[171  35]\n [ 35  67]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.83      0.83       206\n           1       0.66      0.66      0.66       102\n\n    accuracy                           0.77       308\n   macro avg       0.74      0.74      0.74       308\nweighted avg       0.77      0.77      0.77       308\n\n\n\n\n\nPrecision-recall Curve\nthe precision-recall curve is generated by plotting the precision and recall for different thresholds.\n\\[\nprecision = \\frac{TP}{TP+FP}\n\\] \\[\nrecall = \\frac{TP}{TP+FN}\n\\]\n\ndisp = plot_precision_recall_curve(logreg_pidd, X_test_pidd, y_test_pidd)\ndisp.ax_.set_title('Precision-Recall curve: ')\n\nText(0.5, 1.0, 'Precision-Recall curve: ')\n\n\n\n\n\n\nA recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes.\nPrecision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes.\nWhen the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions.\n\n\n\nPlotting an ROC curve\nClassification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models. most classifiers in scikit-learn have a .predict_proba() method which returns the probability of a given sample being in a particular class. Having built a logistic regression model, we’ll now evaluate its performance by plotting an ROC curve. In doing so, we’ll make use of the .predict_proba() method and become familiar with its functionality.\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob_pidd = logreg_pidd.predict_proba(X_test_pidd)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr_pidd, tpr_pidd, thresholds_pidd = roc_curve(y_test_pidd, y_pred_prob_pidd)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_pidd, tpr_pidd)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#area-under-the-roc-curve",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#area-under-the-roc-curve",
    "title": "Supervised Learning with scikit-learn",
    "section": "Area under the ROC curve",
    "text": "Area under the ROC curve\n\nArea under the ROC curve (AUC)\n\nLarger area under the ROC curve = better model\n\n\n\nAUC computation\nSay you have a binary classifier that in fact is just randomly making guesses. It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. The Area under this ROC curve would be 0.5. This is one way in which the AUC is an informative metric to evaluate a model. If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!\nWe’ll calculate AUC scores using the roc_auc_score() function from sklearn.metrics as well as by performing cross-validation on the diabetes dataset.\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test_pidd, y_pred_prob_pidd)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc_pidd = cross_val_score(logreg_pidd, X_pidd, y_pidd, cv=5, scoring=\"roc_auc\")\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc_pidd))\n\nAUC: 0.8329050066628594\nAUC scores computed using 5-fold cross-validation: [0.81962963 0.80537037 0.82555556 0.87377358 0.82509434]"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#hyperparameter-tuning",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#hyperparameter-tuning",
    "title": "Supervised Learning with scikit-learn",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\n\nHyperparameter tuning\n\nLinear regression: Choosing parameters\nRidge/lasso regression: Choosing alpha\nk-Nearest Neighbors: Choosing n_neighbors\nParameters like alpha and k: Hyperparameters\nHyperparameters cannot be learned by tting the model\n\n\n\nChoosing the correct hyperparameter\n\nTry a bunch of different hyperparameter values\nFit all of them separately\nSee how well each performs\nChoose the best performing one\nIt is essential to use cross-validation\n\n\n\nGridSearchCV in scikit-learn votes dataset\n\nparam_grid_votes = {\"n_neighbors\":np.arange(1,50)}\nknn_votes = KNeighborsClassifier()\nknn_cv_votes = GridSearchCV(knn_votes, param_grid=param_grid_votes, cv=5)\nknn_cv_votes.fit(X_votes, y_votes)\nknn_cv_votes.best_params_\n\n{'n_neighbors': 4}\n\n\n\nknn_cv_votes.best_score_\n\n0.9333333333333333\n\n\n\n\nHyperparameter tuning with GridSearchCV\nlogistic regression also has a regularization parameter: \\(C\\). \\(C\\) controls the inverse of the regularization strength, and this is what we will tune. A large \\(C\\) can lead to an overfit model, while a small \\(C\\) can lead to an underfit model.\n\n# Setup the hyperparameter grid\nc_space_pidd = np.logspace(-5, 8, 15)\nparam_grid_pidd = {'C': c_space_pidd}\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv_pidd = GridSearchCV(logreg_pidd, param_grid_pidd, cv=5)\n\n# Fit it to the data\nlogreg_cv_pidd.fit(X_pidd,y_pidd)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv_pidd.best_params_)) \nprint(\"Best score is {}\".format(logreg_cv_pidd.best_score_))\n\nTuned Logistic Regression Parameters: {'C': 1389495.494373136}\nBest score is 0.7787029963500551\n\n\n\n\nHyperparameter tuning with RandomizedSearchCV\nGridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.\nDecision trees have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for RandomizedSearchCV. Our goal is to use RandomizedSearchCV to find the optimal hyperparameters.\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist_pidd = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree_pidd = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv_pidd = RandomizedSearchCV(tree_pidd, param_dist_pidd, cv=5)\n\n# Fit it to the data\ntree_cv_pidd.fit(X,y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv_pidd.best_params_))\nprint(\"Best score is {}\".format(tree_cv_pidd.best_score_))\n\nTuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 2}\nBest score is 0.96\n\n\n\n\n\n\n\n\nNote\n\n\n\nRandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time."
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#hold-out-set-for-final-evaluation",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#hold-out-set-for-final-evaluation",
    "title": "Supervised Learning with scikit-learn",
    "section": "Hold-out set for final evaluation",
    "text": "Hold-out set for final evaluation\n\nHold-out set reasoning\n\nHow well can the model perform on never before seen data?\nUsing ALL data for cross-validation is not ideal\nSplit data into training and hold-out set at the beginning\nPerform grid search cross-validation on training set\nChoose best hyperparameters and evaluate on hold-out set\n\n\n\nHold-out set in practice I: Classification\nYou will now practice evaluating a model with tuned hyperparameters on a hold-out set. In addition to \\(C\\), logistic regression has a 'penalty' hyperparameter which specifies whether to use 'l1' or 'l2' regularization. Our job is to create a hold-out set, tune the 'C' and 'penalty' hyperparameters of a logistic regression classifier using GridSearchCV on the training set.\n\nparam_grid_pidd['penalty'] = ['l1', 'l2']\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv_pidd = GridSearchCV(logreg_pidd, param_grid_pidd, cv=5)\n\n# Fit it to the training data\nlogreg_cv_pidd.fit(X_train_pidd, y_train_pidd)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv_pidd.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv_pidd.best_score_))\n\nTuned Logistic Regression Parameter: {'C': 100000000.0, 'penalty': 'l2'}\nTuned Logistic Regression Accuracy: 0.7717391304347827\n\n\n\n\nHold-out set in practice II: Regression\nLasso used the \\(L1\\) penalty to regularize, while ridge used the \\(L2\\) penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the \\(L1\\) and \\(L2\\) penalties:\n\\[\na∗L1+b∗L2\n\\]\nIn scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an \\(L1\\) penalty, and anything lower is a combination of \\(L1\\) and \\(L2\\).\nWe will GridSearchCV to tune the 'l1_ratio' of an elastic net model trained on the Gapminder data.\n\n# Create the hyperparameter grid\nl1_space_gapminder = np.linspace(0, 1, 30)\nparam_grid_gapminder = {'l1_ratio': l1_space_gapminder}\n\n# Instantiate the ElasticNet regressor: elastic_net\nelastic_net_gapminder = ElasticNet()\n\n# Setup the GridSearchCV object: gm_cv\ngm_cv_gapminder = GridSearchCV(elastic_net_gapminder, param_grid_gapminder, cv=5)\n\n# Fit it to the training data\ngm_cv_gapminder.fit(X_train_gapminder, y_train_gapminder)\n\n# Predict on the test set and compute metrics\ny_pred_gapminder = gm_cv_gapminder.predict(X_test_gapminder)\nr2_gapminder = gm_cv_gapminder.score(X_test_gapminder, y_test_gapminder)\nmse_gapminder = mean_squared_error(y_test_gapminder, y_pred_gapminder)\nprint(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv_gapminder.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2_gapminder))\nprint(\"Tuned ElasticNet MSE: {}\".format(mse_gapminder))\n\nTuned ElasticNet l1 ratio: {'l1_ratio': 0.0}\nTuned ElasticNet R squared: 0.8442220994403307\nTuned ElasticNet MSE: 10.144762014599413"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#preprocessing-data",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#preprocessing-data",
    "title": "Supervised Learning with scikit-learn",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\nDealing with categorical features\n\nScikit-learn will not accept categorical features by default\nNeed to encode categorical features numerically\nConvert to ‘dummy variables’\n0: Observation was NOT that category\n1: Observation was that category\n\n\n\nDealing with categorical features in Python\n\nscikit-learn:\nOneHotEncoder()\npandas:\nget_dummies()\n\n\n\nAutomobile dataset\n\nmpg:Target Variable\nOrigin:Categorical Feature\n\n\nautos = pd.read_csv(\"datasets/autos.csv\")\nautos.head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      displ\n      hp\n      weight\n      accel\n      origin\n      size\n    \n  \n  \n    \n      0\n      18.0\n      250.0\n      88\n      3139\n      14.5\n      US\n      15.0\n    \n    \n      1\n      9.0\n      304.0\n      193\n      4732\n      18.5\n      US\n      20.0\n    \n    \n      2\n      36.1\n      91.0\n      60\n      1800\n      16.4\n      Asia\n      10.0\n    \n    \n      3\n      18.5\n      250.0\n      98\n      3525\n      19.0\n      US\n      15.0\n    \n    \n      4\n      34.3\n      97.0\n      78\n      2188\n      15.8\n      Europe\n      10.0\n    \n  \n\n\n\n\n\nautos.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 392 entries, 0 to 391\nData columns (total 7 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   mpg     392 non-null    float64\n 1   displ   392 non-null    float64\n 2   hp      392 non-null    int64  \n 3   weight  392 non-null    int64  \n 4   accel   392 non-null    float64\n 5   origin  392 non-null    object \n 6   size    392 non-null    float64\ndtypes: float64(4), int64(2), object(1)\nmemory usage: 21.6+ KB\n\n\n\nautos.describe()\n\n\n\n\n\n  \n    \n      \n      mpg\n      displ\n      hp\n      weight\n      accel\n      size\n    \n  \n  \n    \n      count\n      392.000000\n      392.000000\n      392.000000\n      392.000000\n      392.000000\n      392.000000\n    \n    \n      mean\n      23.445918\n      194.411990\n      104.469388\n      2977.584184\n      15.541327\n      13.679847\n    \n    \n      std\n      7.805007\n      104.644004\n      38.491160\n      849.402560\n      2.758864\n      4.264458\n    \n    \n      min\n      9.000000\n      68.000000\n      46.000000\n      1613.000000\n      8.000000\n      7.500000\n    \n    \n      25%\n      17.000000\n      105.000000\n      75.000000\n      2225.250000\n      13.775000\n      10.000000\n    \n    \n      50%\n      22.750000\n      151.000000\n      93.500000\n      2803.500000\n      15.500000\n      10.000000\n    \n    \n      75%\n      29.000000\n      275.750000\n      126.000000\n      3614.750000\n      17.025000\n      20.000000\n    \n    \n      max\n      46.600000\n      455.000000\n      230.000000\n      5140.000000\n      24.800000\n      20.000000\n    \n  \n\n\n\n\n\nautos.shape\n\n(392, 7)\n\n\n\n\nEDA w/ categorical feature\n\n_ = sns.boxplot(data=autos, x=\"origin\", y=\"mpg\", order=['Asia', 'US', 'Europe'])\nplt.show()\n\n\n\n\n\n\nEncoding dummy variables\n\nautos_origin = pd.get_dummies(autos)\nautos_origin.head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      displ\n      hp\n      weight\n      accel\n      size\n      origin_Asia\n      origin_Europe\n      origin_US\n    \n  \n  \n    \n      0\n      18.0\n      250.0\n      88\n      3139\n      14.5\n      15.0\n      0\n      0\n      1\n    \n    \n      1\n      9.0\n      304.0\n      193\n      4732\n      18.5\n      20.0\n      0\n      0\n      1\n    \n    \n      2\n      36.1\n      91.0\n      60\n      1800\n      16.4\n      10.0\n      1\n      0\n      0\n    \n    \n      3\n      18.5\n      250.0\n      98\n      3525\n      19.0\n      15.0\n      0\n      0\n      1\n    \n    \n      4\n      34.3\n      97.0\n      78\n      2188\n      15.8\n      10.0\n      0\n      1\n      0\n    \n  \n\n\n\n\n\nautos_origin = autos_origin.drop(\"origin_Asia\", axis=1)\nautos_origin.head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      displ\n      hp\n      weight\n      accel\n      size\n      origin_Europe\n      origin_US\n    \n  \n  \n    \n      0\n      18.0\n      250.0\n      88\n      3139\n      14.5\n      15.0\n      0\n      1\n    \n    \n      1\n      9.0\n      304.0\n      193\n      4732\n      18.5\n      20.0\n      0\n      1\n    \n    \n      2\n      36.1\n      91.0\n      60\n      1800\n      16.4\n      10.0\n      0\n      0\n    \n    \n      3\n      18.5\n      250.0\n      98\n      3525\n      19.0\n      15.0\n      0\n      1\n    \n    \n      4\n      34.3\n      97.0\n      78\n      2188\n      15.8\n      10.0\n      1\n      0\n    \n  \n\n\n\n\n\n\nLinear regression with dummy variables\n\nX_autos_origin = autos_origin[[\"origin_Europe\", \"origin_US\"]].values\ny_autos_origin = autos_origin['mpg'].values\n\n\nX_train_autos_origin, X_test_autos_origin, y_train_autos_origin, y_test_autos_origin, = train_test_split(X_autos_origin,\n                                                                                                         y_autos_origin, \n                                                                                                         test_size=.3, \n                                                                                                         random_state=42)\nridge_autos_origin = Ridge(alpha=.5, normalize=True).fit(X_train_autos_origin, y_train_autos_origin)\nridge_autos_origin.score(X_test_autos_origin, y_test_autos_origin)\n\n0.3241789154336545\n\n\n\n\nExploring categorical features\nThe Gapminder dataset that we worked with in previous section also contained a categorical 'Region' feature, which we dropped since we did not have the tools to deal with it. Now however, we do, so we have added it back in!\nWe will explore this feature. Boxplots are particularly useful for visualizing categorical features such as this.\n\ngapminder.head()\n\n\n\n\n\n  \n    \n      \n      population\n      fertility\n      HIV\n      CO2\n      BMI_male\n      GDP\n      BMI_female\n      life\n      child_mortality\n    \n  \n  \n    \n      0\n      34811059.0\n      2.73\n      0.1\n      3.328945\n      24.59620\n      12314.0\n      129.9049\n      75.3\n      29.5\n    \n    \n      1\n      19842251.0\n      6.43\n      2.0\n      1.474353\n      22.25083\n      7103.0\n      130.1247\n      58.3\n      192.0\n    \n    \n      2\n      40381860.0\n      2.24\n      0.5\n      4.785170\n      27.50170\n      14646.0\n      118.8915\n      75.5\n      15.4\n    \n    \n      3\n      2975029.0\n      1.40\n      0.1\n      1.804106\n      25.35542\n      7383.0\n      132.8108\n      72.5\n      20.0\n    \n    \n      4\n      21370348.0\n      1.96\n      0.1\n      18.016313\n      27.56373\n      41312.0\n      117.3755\n      81.5\n      5.2\n    \n  \n\n\n\n\n\ngapminder_2 = pd.read_csv(\"datasets/gapminder_2.csv\")\ngapminder_2.head()\n\n\n\n\n\n  \n    \n      \n      population\n      fertility\n      HIV\n      CO2\n      BMI_male\n      GDP\n      BMI_female\n      life\n      child_mortality\n      Region\n    \n  \n  \n    \n      0\n      34811059.0\n      2.73\n      0.1\n      3.328945\n      24.59620\n      12314.0\n      129.9049\n      75.3\n      29.5\n      Middle East & North Africa\n    \n    \n      1\n      19842251.0\n      6.43\n      2.0\n      1.474353\n      22.25083\n      7103.0\n      130.1247\n      58.3\n      192.0\n      Sub-Saharan Africa\n    \n    \n      2\n      40381860.0\n      2.24\n      0.5\n      4.785170\n      27.50170\n      14646.0\n      118.8915\n      75.5\n      15.4\n      America\n    \n    \n      3\n      2975029.0\n      1.40\n      0.1\n      1.804106\n      25.35542\n      7383.0\n      132.8108\n      72.5\n      20.0\n      Europe & Central Asia\n    \n    \n      4\n      21370348.0\n      1.96\n      0.1\n      18.016313\n      27.56373\n      41312.0\n      117.3755\n      81.5\n      5.2\n      East Asia & Pacific\n    \n  \n\n\n\n\n\n# Create a boxplot of life expectancy per region\ngapminder_2.boxplot(\"life\", \"Region\", rot=60)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nExploratory data analysis should always be the precursor to model building.\n\n\n\n\nCreating dummy variables\nscikit-learn does not accept non-numerical features. The 'Region' feature contains very useful information that can predict life expectancy. For example, Sub-Saharan Africa has a lower life expectancy compared to Europe and Central Asia. Therefore, if we are trying to predict life expectancy, it would be preferable to retain the 'Region' feature. To do this, we need to binarize it by creating dummy variables, which is what we will do.\n\n# Create dummy variables with drop_first=True: df_region\ngapminder_region = pd.get_dummies(gapminder_2, drop_first=True)\n\n# Print the new columns of df_region\nprint(gapminder_region.columns)\n\nIndex(['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP',\n       'BMI_female', 'life', 'child_mortality', 'Region_East Asia & Pacific',\n       'Region_Europe & Central Asia', 'Region_Middle East & North Africa',\n       'Region_South Asia', 'Region_Sub-Saharan Africa'],\n      dtype='object')\n\n\n\ngapminder_region.head()\n\n\n\n\n\n  \n    \n      \n      population\n      fertility\n      HIV\n      CO2\n      BMI_male\n      GDP\n      BMI_female\n      life\n      child_mortality\n      Region_East Asia & Pacific\n      Region_Europe & Central Asia\n      Region_Middle East & North Africa\n      Region_South Asia\n      Region_Sub-Saharan Africa\n    \n  \n  \n    \n      0\n      34811059.0\n      2.73\n      0.1\n      3.328945\n      24.59620\n      12314.0\n      129.9049\n      75.3\n      29.5\n      0\n      0\n      1\n      0\n      0\n    \n    \n      1\n      19842251.0\n      6.43\n      2.0\n      1.474353\n      22.25083\n      7103.0\n      130.1247\n      58.3\n      192.0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      2\n      40381860.0\n      2.24\n      0.5\n      4.785170\n      27.50170\n      14646.0\n      118.8915\n      75.5\n      15.4\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      2975029.0\n      1.40\n      0.1\n      1.804106\n      25.35542\n      7383.0\n      132.8108\n      72.5\n      20.0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      4\n      21370348.0\n      1.96\n      0.1\n      18.016313\n      27.56373\n      41312.0\n      117.3755\n      81.5\n      5.2\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\nNow that we have created the dummy variables, we can use the 'Region' feature to predict life expectancy!\n\n\nRegression with categorical features\nWe’ll use ridge regression to perform 5-fold cross-validation.\n\nX_gapminder_region = gapminder_region.drop(\"life\", axis=1).values\ny_gapminder_region = gapminder_region.life.values\n\n\n# Instantiate a ridge regressor: ridge\nridge_gapminder_region = Ridge(alpha=.5, normalize=True)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv_gapminder_region = cross_val_score(ridge_gapminder_region, X_gapminder_region, y_gapminder_region, cv=5)\n\n# Print the cross-validated scores\nprint(ridge_cv_gapminder_region)\n\n[0.86808336 0.80623545 0.84004203 0.7754344  0.87503712]\n\n\nWe now know how to build models using data that includes categorical features."
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#handling-missing-data",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#handling-missing-data",
    "title": "Supervised Learning with scikit-learn",
    "section": "Handling missing data",
    "text": "Handling missing data\n\nImputing missing data\n\nMaking an educated guess about the missing values\nExample: Using the mean of the non-missing entries\n\n\n\nPIMA Indians dataset\n\npidd.head()\n\n\n\n\n\n  \n    \n      \n      pregnancies\n      glucose\n      diastolic\n      triceps\n      insulin\n      bmi\n      dpf\n      age\n      diabetes\n    \n  \n  \n    \n      0\n      6\n      148\n      72\n      35.00000\n      155.548223\n      33.6\n      0.627\n      50\n      1\n    \n    \n      1\n      1\n      85\n      66\n      29.00000\n      155.548223\n      26.6\n      0.351\n      31\n      0\n    \n    \n      2\n      8\n      183\n      64\n      29.15342\n      155.548223\n      23.3\n      0.672\n      32\n      1\n    \n    \n      3\n      1\n      89\n      66\n      23.00000\n      94.000000\n      28.1\n      0.167\n      21\n      0\n    \n    \n      4\n      0\n      137\n      40\n      35.00000\n      168.000000\n      43.1\n      2.288\n      33\n      1\n    \n  \n\n\n\n\n\npidd.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   pregnancies  768 non-null    int64  \n 1   glucose      768 non-null    int64  \n 2   diastolic    768 non-null    int64  \n 3   triceps      768 non-null    float64\n 4   insulin      768 non-null    float64\n 5   bmi          768 non-null    float64\n 6   dpf          768 non-null    float64\n 7   age          768 non-null    int64  \n 8   diabetes     768 non-null    int64  \ndtypes: float64(4), int64(5)\nmemory usage: 54.1 KB\n\n\n\npidd.insulin.replace(0, np.nan, inplace=True)\npidd.bmi.replace(0, np.nan, inplace=True)\npidd.triceps.replace(0, np.nan, inplace=True)\npidd.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   pregnancies  768 non-null    int64  \n 1   glucose      768 non-null    int64  \n 2   diastolic    768 non-null    int64  \n 3   triceps      768 non-null    float64\n 4   insulin      768 non-null    float64\n 5   bmi          768 non-null    float64\n 6   dpf          768 non-null    float64\n 7   age          768 non-null    int64  \n 8   diabetes     768 non-null    int64  \ndtypes: float64(4), int64(5)\nmemory usage: 54.1 KB\n\n\n\npidd.head()\n\n\n\n\n\n  \n    \n      \n      pregnancies\n      glucose\n      diastolic\n      triceps\n      insulin\n      bmi\n      dpf\n      age\n      diabetes\n    \n  \n  \n    \n      0\n      6\n      148\n      72\n      35.00000\n      155.548223\n      33.6\n      0.627\n      50\n      1\n    \n    \n      1\n      1\n      85\n      66\n      29.00000\n      155.548223\n      26.6\n      0.351\n      31\n      0\n    \n    \n      2\n      8\n      183\n      64\n      29.15342\n      155.548223\n      23.3\n      0.672\n      32\n      1\n    \n    \n      3\n      1\n      89\n      66\n      23.00000\n      94.000000\n      28.1\n      0.167\n      21\n      0\n    \n    \n      4\n      0\n      137\n      40\n      35.00000\n      168.000000\n      43.1\n      2.288\n      33\n      1\n    \n  \n\n\n\n\n\n\nDropping missing data\nThe voting dataset1 contained a bunch of missing values that we dealt with for you behind the scenes.\n\nvotes2 = pd.read_csv(\"datasets/votes2.csv\")\nvotes2.head()\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      0\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      ?\n      1\n      1\n      1\n      0\n      1\n    \n    \n      1\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      ?\n    \n    \n      2\n      democrat\n      ?\n      1\n      1\n      ?\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n    \n    \n      3\n      democrat\n      0\n      1\n      1\n      0\n      ?\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n    \n    \n      4\n      democrat\n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      ?\n      1\n      1\n      1\n      1\n    \n  \n\n\n\n\nthere are certain data points labeled with a '?'. These denote missing values. We will convert the '?'s to NaNs, and then drop the rows that contain them from the DataFrame.\n\n# Convert '?' to NaN\nvotes2[votes2 == \"?\"] = np.nan\n\n# Print the number of NaNs\ndisplay(votes2.isnull().sum())\n\n# Print shape of original DataFrame\nprint(\"Shape of Original DataFrame: {}\".format(votes2.shape))\n\n# Print shape of new DataFrame\nprint(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(votes2.dropna().shape))\n\nparty                  0\ninfants               12\nwater                 48\nbudget                11\nphysician             11\nsalvador              15\nreligious             11\nsatellite             14\naid                   15\nmissile               22\nimmigration            7\nsynfuels              21\neducation             31\nsuperfund             25\ncrime                 17\nduty_free_exports     28\neaa_rsa              104\ndtype: int64\n\n\nShape of Original DataFrame: (435, 17)\nShape of DataFrame After Dropping All Rows with Missing Values: (232, 17)\n\n\nWhen many values in a dataset are missing, if you drop them, you may end up throwing away valuable information along with the missing data. It’s better instead to develop an imputation strategy. This is where domain knowledge is useful, but in the absence of it, you can impute missing values with the mean or the median of the row or column that the missing value is in.\n\n\nImputing missing data in a ML Pipeline I\nthere are many steps to building a model, from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating its performance on new data. Imputation can be seen as the first step of this machine learning process, the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor that allows you to piece together these steps into one process and thereby simplify your workflow.\nWe will be setting up a pipeline with two steps: the imputation step, followed by the instantiation of a classifier. We’ve seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree. Here we will be using the SVM (Support Vector Machine)\n\nvotes2.head()\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      0\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      NaN\n      1\n      1\n      1\n      0\n      1\n    \n    \n      1\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      NaN\n    \n    \n      2\n      democrat\n      NaN\n      1\n      1\n      NaN\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n    \n    \n      3\n      democrat\n      0\n      1\n      1\n      0\n      NaN\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n    \n    \n      4\n      democrat\n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      NaN\n      1\n      1\n      1\n      1\n    \n  \n\n\n\n\n\nvotes2.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 435 entries, 0 to 434\nData columns (total 17 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   party              435 non-null    object\n 1   infants            423 non-null    object\n 2   water              387 non-null    object\n 3   budget             424 non-null    object\n 4   physician          424 non-null    object\n 5   salvador           420 non-null    object\n 6   religious          424 non-null    object\n 7   satellite          421 non-null    object\n 8   aid                420 non-null    object\n 9   missile            413 non-null    object\n 10  immigration        428 non-null    object\n 11  synfuels           414 non-null    object\n 12  education          404 non-null    object\n 13  superfund          410 non-null    object\n 14  crime              418 non-null    object\n 15  duty_free_exports  407 non-null    object\n 16  eaa_rsa            331 non-null    object\ndtypes: object(17)\nmemory usage: 57.9+ KB\n\n\n\n# Setup the Imputation transformer: imp\nimp_votes = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n\n# Instantiate the SVC classifier: clf\nclf_votes = SVC()\n\n# Setup the pipeline with the required steps: steps\nsteps_votes = [('imputation', imp_votes),\n        ('SVM', clf_votes)]\n\nHaving set up the pipeline steps, we can now use it for classification.\n\n\nImputing missing data in a ML Pipeline II\nHaving setup the steps of the pipeline we will now use it on the voting dataset to classify a Congressman’s party affiliation. What makes pipelines so incredibly useful is the simple interface that they provide.\n\nX_votes[:5]\n\narray([[0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1],\n       [0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1],\n       [0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n       [0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n       [1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)\n\n\n\nvotes.head()\n\n\n\n\n\n  \n    \n      \n      party\n      infants\n      water\n      budget\n      physician\n      salvador\n      religious\n      satellite\n      aid\n      missile\n      immigration\n      synfuels\n      education\n      superfund\n      crime\n      duty_free_exports\n      eaa_rsa\n    \n  \n  \n    \n      0\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      1\n    \n    \n      1\n      republican\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n    \n    \n      2\n      democrat\n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n    \n    \n      3\n      democrat\n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n    \n    \n      4\n      democrat\n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      1\n    \n  \n\n\n\n\n\nX_votes = votes.drop(\"party\", axis=1)\ny_votes = votes.party\n\n\nX_train_votes, X_test_votes, y_train_votes, y_test_votes = train_test_split(X_votes, y_votes, test_size=.3, random_state=42)\n\n\n# Create the pipeline: pipeline\npipeline_votes = Pipeline(steps_votes)\n\n# Fit the pipeline to the train set\npipeline_votes.fit(X_train_votes, y_train_votes)\n\n# Predict the labels of the test set\ny_pred_votes = pipeline_votes.predict(X_test_votes)\n\n# Compute metrics\nprint(classification_report(y_test_votes, y_pred_votes))\n\n              precision    recall  f1-score   support\n\n    democrat       0.98      0.96      0.97        85\n  republican       0.94      0.96      0.95        46\n\n    accuracy                           0.96       131\n   macro avg       0.96      0.96      0.96       131\nweighted avg       0.96      0.96      0.96       131"
  },
  {
    "objectID": "posts/2020-07-10-supervised learning with scikit-learn.html#centering-and-scaling",
    "href": "posts/2020-07-10-supervised learning with scikit-learn.html#centering-and-scaling",
    "title": "Supervised Learning with scikit-learn",
    "section": "Centering and scaling",
    "text": "Centering and scaling\n\nWhy scale your data?\n\nMany models use some form of distance to inform them\nFeatures on larger scales can unduly influence the model\nExample: k-NN uses distance explicitly when making predictions\nWe want features to be on a similar scale\nNormalizing (or scaling and centering)\n\n\n\nWays to normalize your data\n\nStandardization: Subtract the mean and divide by variance\nAll features are centered around zero and have variance one\nCan also subtract the minimum and divide by the range\nMinimum zero and maximum one\nCan also normalize so the data ranges from -1 to +1\n\n\n\nCentering and scaling your data\nthe performance of a model can improve if the features are scaled. Note that this is not always the case: In the Congressional voting records dataset, for example, all of the features are binary. In such a situation, scaling will have minimal impact. We will explore scalling on White Wine Quality.\n\nwwq = pd.read_csv(\"datasets/white_wine_quality.csv\")\nwwq.head()\n\n\n\n\n\n  \n    \n      \n      fixed acidity\n      volatile acidity\n      citric acid\n      residual sugar\n      chlorides\n      free sulfur dioxide\n      total sulfur dioxide\n      density\n      pH\n      sulphates\n      alcohol\n      quality\n    \n  \n  \n    \n      0\n      7.0\n      0.27\n      0.36\n      20.7\n      0.045\n      45.0\n      170.0\n      1.0010\n      3.00\n      0.45\n      8.8\n      6\n    \n    \n      1\n      6.3\n      0.30\n      0.34\n      1.6\n      0.049\n      14.0\n      132.0\n      0.9940\n      3.30\n      0.49\n      9.5\n      6\n    \n    \n      2\n      8.1\n      0.28\n      0.40\n      6.9\n      0.050\n      30.0\n      97.0\n      0.9951\n      3.26\n      0.44\n      10.1\n      6\n    \n    \n      3\n      7.2\n      0.23\n      0.32\n      8.5\n      0.058\n      47.0\n      186.0\n      0.9956\n      3.19\n      0.40\n      9.9\n      6\n    \n    \n      4\n      7.2\n      0.23\n      0.32\n      8.5\n      0.058\n      47.0\n      186.0\n      0.9956\n      3.19\n      0.40\n      9.9\n      6\n    \n  \n\n\n\n\n\nX_wwq = pd.read_csv(\"datasets/X_wwq.csv\").values\nX_wwq[:5]\n\narray([[7.000e+00, 2.700e-01, 3.600e-01, 2.070e+01, 4.500e-02, 4.500e+01,\n        1.700e+02, 1.001e+00, 3.000e+00, 4.500e-01, 8.800e+00],\n       [6.300e+00, 3.000e-01, 3.400e-01, 1.600e+00, 4.900e-02, 1.400e+01,\n        1.320e+02, 9.940e-01, 3.300e+00, 4.900e-01, 9.500e+00],\n       [8.100e+00, 2.800e-01, 4.000e-01, 6.900e+00, 5.000e-02, 3.000e+01,\n        9.700e+01, 9.951e-01, 3.260e+00, 4.400e-01, 1.010e+01],\n       [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01,\n        1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00],\n       [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01,\n        1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00]])\n\n\n\ny_wwq = np.array([False, False, False, False, False, False, False, False, False,\n       False,  True,  True,  True, False,  True, False, False, False,\n       False,  True, False, False, False,  True, False, False, False,\n       False, False, False, False, False, False, False,  True,  True,\n        True, False,  True,  True, False, False, False, False, False,\n       False,  True,  True, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False,  True, False,  True, False,  True, False,  True,\n        True, False, False,  True, False, False,  True,  True, False,\n       False,  True, False,  True, False, False, False,  True, False,\n       False,  True, False, False, False, False, False, False,  True,\n       False,  True,  True,  True,  True,  True, False,  True, False,\n       False,  True, False,  True,  True,  True,  True,  True, False,\n       False,  True,  True,  True,  True,  True, False, False, False,\n        True, False, False, False,  True, False,  True,  True,  True,\n        True, False,  True, False, False,  True,  True, False, False,\n       False, False, False,  True, False, False, False, False, False,\n        True, False, False, False, False, False, False, False,  True,\n        True, False,  True,  True, False, False,  True,  True, False,\n       False,  True, False,  True, False,  True,  True,  True, False,\n       False,  True,  True, False,  True,  True, False,  True, False,\n        True, False,  True, False,  True,  True, False,  True,  True,\n        True,  True,  True,  True,  True, False,  True,  True,  True,\n        True,  True, False,  True, False,  True, False, False,  True,\n        True,  True,  True,  True,  True, False, False, False, False,\n        True, False, False, False,  True,  True, False, False, False,\n       False, False, False, False, False, False,  True,  True, False,\n       False,  True, False, False, False, False,  True,  True,  True,\n        True,  True, False, False, False, False, False,  True, False,\n        True,  True, False, False,  True, False,  True, False, False,\n       False,  True,  True,  True,  True, False, False,  True,  True,\n       False, False, False,  True,  True,  True,  True, False, False,\n       False, False, False, False,  True, False,  True, False,  True,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False, False, False, False, False,  True,\n       False, False,  True, False, False, False,  True, False, False,\n        True,  True, False, False, False,  True, False,  True, False,\n        True,  True, False, False, False,  True, False, False, False,\n       False,  True, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False, False,\n        True, False, False,  True, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False,  True, False, False,  True,  True,\n        True, False,  True, False, False,  True,  True,  True, False,\n        True, False,  True, False,  True, False, False,  True,  True,\n       False, False, False,  True, False, False, False, False, False,\n       False, False, False, False,  True,  True,  True,  True,  True,\n       False,  True, False, False,  True, False, False,  True, False,\n       False, False, False, False,  True,  True, False, False, False,\n        True,  True, False, False, False, False, False,  True, False,\n        True,  True,  True,  True, False,  True,  True, False, False,\n        True,  True, False,  True, False, False, False,  True, False,\n       False, False, False,  True, False,  True,  True,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False,  True,  True, False, False, False,  True,\n       False, False,  True, False, False, False, False, False, False,\n       False, False, False,  True, False, False,  True,  True,  True,\n       False, False,  True, False,  True, False, False, False, False,\n        True, False, False, False,  True,  True, False,  True, False,\n        True,  True, False, False, False, False, False, False, False,\n        True, False, False, False, False, False, False,  True, False,\n        True, False, False,  True, False, False,  True, False, False,\n        True, False, False,  True, False,  True, False, False, False,\n       False, False, False, False,  True,  True, False, False, False,\n       False, False, False, False, False,  True, False,  True,  True,\n        True, False,  True, False, False, False, False, False,  True,\n        True, False, False,  True,  True,  True, False, False, False,\n        True,  True,  True,  True, False, False, False, False,  True,\n        True, False,  True,  True, False,  True, False, False, False,\n        True,  True, False,  True, False, False, False,  True,  True,\n        True, False,  True, False,  True,  True,  True,  True, False,\n        True, False, False, False, False, False, False, False, False,\n        True,  True,  True,  True, False,  True,  True, False,  True,\n       False, False, False,  True, False, False, False, False,  True,\n       False, False, False, False, False,  True,  True, False,  True,\n        True,  True, False,  True, False, False,  True, False,  True,\n        True, False,  True, False,  True,  True,  True,  True, False,\n        True, False, False,  True,  True, False, False,  True,  True,\n       False, False,  True, False, False, False,  True, False, False,\n        True,  True, False, False, False,  True, False,  True,  True,\n        True, False, False, False, False,  True, False, False,  True,\n       False, False,  True, False, False,  True,  True, False, False,\n       False, False, False, False, False, False, False,  True, False,\n        True, False, False, False, False, False,  True, False, False,\n       False,  True, False, False, False, False, False, False,  True,\n       False, False, False,  True, False, False,  True, False, False,\n       False,  True, False, False, False, False, False, False, False,\n        True, False,  True,  True, False, False, False, False, False,\n        True,  True, False, False, False,  True, False, False, False,\n        True, False, False, False, False,  True,  True,  True,  True,\n        True, False, False,  True, False,  True, False, False, False,\n       False, False, False,  True, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True,  True, False, False, False,  True, False,  True, False,\n       False,  True,  True, False,  True, False, False, False, False,\n        True, False, False, False, False, False,  True,  True, False,\n        True,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False, False,\n       False, False,  True, False, False,  True,  True, False,  True,\n        True,  True,  True,  True,  True,  True,  True, False, False,\n       False, False, False, False, False, False, False,  True,  True,\n       False,  True,  True, False,  True, False,  True,  True,  True,\n        True,  True, False, False,  True, False, False, False, False,\n       False,  True,  True,  True,  True,  True, False, False,  True,\n       False,  True,  True, False, False, False, False, False, False,\n       False, False,  True, False, False, False, False, False, False,\n       False, False, False,  True, False,  True, False,  True, False,\n       False, False, False,  True, False, False, False,  True, False,\n       False,  True,  True,  True, False, False,  True, False, False,\n       False, False,  True, False, False, False, False, False,  True,\n       False, False, False, False, False,  True,  True,  True,  True,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False,  True, False, False, False, False,  True,\n       False,  True,  True, False,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True, False,  True, False,  True,\n        True,  True,  True, False,  True, False,  True, False,  True,\n        True,  True, False,  True, False, False, False, False, False,\n        True, False,  True, False, False, False,  True,  True, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False, False, False,  True,  True,  True, False, False,\n       False, False,  True, False, False, False,  True,  True, False,\n        True,  True,  True,  True, False, False, False,  True,  True,\n       False, False, False,  True, False, False, False,  True,  True,\n        True, False, False, False,  True,  True, False,  True,  True,\n        True,  True,  True,  True,  True, False,  True,  True,  True,\n        True, False,  True,  True, False, False, False, False, False,\n       False, False, False, False,  True,  True, False,  True, False,\n       False,  True, False,  True, False,  True, False, False,  True,\n        True,  True, False,  True, False, False,  True,  True, False,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True, False, False,  True,\n       False,  True, False, False, False,  True, False, False, False,\n        True, False,  True, False, False, False, False, False,  True,\n       False, False, False,  True, False,  True, False, False, False,\n       False, False,  True, False,  True, False,  True, False, False,\n       False, False,  True,  True, False, False, False,  True, False,\n        True, False, False,  True, False, False, False, False, False,\n       False, False, False, False,  True, False, False, False,  True,\n       False, False,  True,  True,  True, False, False, False,  True,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False,  True,  True, False,\n       False, False, False,  True, False,  True, False, False,  True,\n       False, False,  True, False, False, False, False,  True,  True,\n       False, False, False,  True, False,  True, False, False, False,\n        True, False, False, False, False, False,  True,  True, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True, False, False, False, False, False, False, False,  True,\n       False, False, False,  True,  True,  True, False, False, False,\n       False, False, False, False,  True,  True, False, False,  True,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True,  True, False, False, False, False, False, False, False,\n       False, False,  True, False,  True,  True, False,  True, False,\n        True,  True, False, False, False,  True, False,  True,  True,\n        True, False, False,  True,  True,  True, False,  True,  True,\n        True,  True,  True, False, False, False, False, False, False,\n       False, False,  True, False, False,  True,  True, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False,  True,  True, False, False,  True, False,  True, False,\n        True, False, False, False, False, False, False,  True,  True,\n       False, False, False, False,  True, False,  True,  True, False,\n       False, False,  True, False, False, False, False,  True,  True,\n       False, False, False, False, False, False,  True,  True, False,\n       False, False, False, False, False, False, False,  True, False,\n        True,  True, False, False,  True, False, False, False, False,\n        True, False, False, False, False, False,  True, False, False,\n       False, False,  True, False, False,  True, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True,  True, False,  True, False, False,  True, False,\n        True,  True, False,  True,  True, False, False,  True,  True,\n       False, False,  True,  True,  True,  True, False, False,  True,\n       False,  True,  True, False,  True, False, False, False, False,\n       False, False, False,  True, False, False, False, False, False,\n        True, False,  True, False, False, False,  True, False, False,\n       False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False, False, False, False,  True, False,\n       False,  True, False, False, False, False, False,  True, False,\n        True,  True,  True, False, False, False, False, False, False,\n       False,  True,  True, False, False,  True, False,  True,  True,\n        True, False,  True,  True, False, False, False, False,  True,\n       False,  True, False, False, False, False, False,  True,  True,\n       False,  True, False, False, False,  True,  True, False, False,\n       False, False, False,  True, False,  True, False,  True, False,\n       False,  True, False, False, False, False, False, False, False,\n        True,  True,  True,  True, False, False,  True,  True,  True,\n        True,  True, False, False,  True, False, False, False,  True,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False,  True, False, False, False,  True,\n       False,  True,  True, False, False, False, False,  True,  True,\n        True, False,  True, False, False, False, False, False, False,\n       False,  True,  True,  True,  True, False, False, False, False,\n        True,  True,  True,  True,  True, False, False, False, False,\n       False, False, False,  True, False, False, False,  True,  True,\n       False, False, False, False, False, False,  True, False,  True,\n        True, False,  True,  True, False,  True, False, False, False,\n       False,  True, False, False, False,  True,  True, False,  True,\n       False, False,  True, False, False, False, False, False,  True,\n       False, False, False,  True, False, False,  True,  True,  True,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False,  True, False, False, False, False,\n        True,  True,  True,  True, False, False, False, False,  True,\n       False, False,  True,  True,  True, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n        True,  True,  True,  True, False, False, False, False,  True,\n       False,  True,  True,  True, False,  True,  True,  True,  True,\n        True,  True,  True,  True,  True, False, False, False, False,\n       False, False,  True, False,  True, False, False,  True, False,\n       False,  True,  True, False, False, False, False, False,  True,\n       False, False, False, False,  True,  True,  True,  True, False,\n       False,  True, False,  True, False,  True, False,  True, False,\n        True, False, False,  True,  True,  True,  True,  True,  True,\n        True, False, False,  True, False,  True,  True,  True, False,\n       False,  True,  True,  True, False, False,  True,  True,  True,\n        True, False,  True, False, False, False, False, False, False,\n       False,  True,  True,  True, False,  True, False, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False,  True, False,  True, False, False, False, False, False,\n       False, False,  True,  True, False, False, False, False, False,\n       False,  True, False, False,  True,  True, False,  True, False,\n       False,  True, False, False, False, False, False, False,  True,\n        True,  True,  True, False, False, False,  True, False,  True,\n        True,  True, False,  True,  True,  True, False,  True, False,\n       False, False,  True,  True,  True,  True, False,  True,  True,\n        True,  True,  True, False,  True, False,  True,  True,  True,\n       False,  True,  True, False,  True,  True,  True, False, False,\n       False, False,  True,  True, False,  True,  True, False, False,\n        True,  True,  True, False,  True, False, False,  True, False,\n       False, False,  True,  True,  True,  True,  True,  True,  True,\n        True, False,  True,  True, False,  True,  True,  True,  True,\n        True, False, False, False,  True,  True,  True,  True, False,\n        True,  True,  True, False,  True,  True, False,  True,  True,\n       False,  True,  True,  True, False,  True,  True, False, False,\n       False, False,  True, False,  True,  True,  True,  True,  True,\n        True, False, False, False, False,  True,  True, False, False,\n        True, False, False,  True,  True,  True, False, False,  True,\n       False, False, False, False,  True,  True,  True, False, False,\n       False, False, False,  True,  True, False, False,  True, False,\n        True, False, False, False,  True,  True, False, False,  True,\n        True, False, False, False,  True, False, False, False,  True,\n       False, False,  True, False,  True, False,  True,  True,  True,\n        True, False, False,  True, False, False, False, False, False,\n        True, False,  True, False, False, False, False, False, False,\n       False, False,  True,  True,  True, False, False, False, False,\n       False,  True,  True, False,  True,  True, False, False,  True,\n        True,  True,  True,  True, False,  True, False, False,  True,\n       False, False,  True,  True, False, False,  True, False,  True,\n       False,  True, False, False,  True, False,  True,  True, False,\n       False, False, False, False, False,  True, False,  True, False,\n       False, False,  True, False, False,  True,  True,  True,  True,\n        True,  True,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False,  True,  True,  True,\n       False, False,  True,  True,  True, False,  True, False,  True,\n       False, False, False, False,  True,  True, False, False,  True,\n       False, False,  True, False,  True, False,  True,  True, False,\n        True, False, False,  True,  True,  True,  True,  True, False,\n       False,  True, False,  True, False, False,  True, False, False,\n        True,  True, False, False, False, False, False, False,  True,\n        True,  True, False,  True, False,  True,  True,  True,  True,\n       False, False, False, False,  True,  True,  True, False, False,\n       False, False,  True,  True,  True, False, False, False, False,\n        True,  True,  True,  True, False, False, False, False, False,\n        True,  True,  True, False, False, False,  True,  True, False,\n       False,  True,  True, False, False,  True,  True, False, False,\n        True,  True,  True,  True, False,  True,  True,  True, False,\n        True,  True, False,  True, False, False, False, False, False,\n        True,  True,  True, False, False,  True,  True, False,  True,\n       False,  True, False,  True,  True,  True,  True,  True, False,\n       False, False,  True,  True, False, False,  True, False, False,\n       False, False,  True,  True,  True,  True, False,  True,  True,\n        True, False, False,  True,  True, False, False, False, False,\n        True, False, False, False, False,  True, False, False, False,\n        True, False,  True, False, False, False, False,  True, False,\n        True,  True,  True,  True,  True,  True, False,  True,  True,\n        True,  True, False, False, False, False, False,  True, False,\n       False, False,  True, False, False, False, False, False,  True,\n        True, False, False,  True,  True, False, False, False, False,\n        True, False,  True,  True,  True, False, False, False, False,\n       False,  True,  True, False,  True, False, False, False,  True,\n        True, False,  True,  True, False,  True, False, False, False,\n        True, False, False,  True, False,  True, False,  True, False,\n        True, False,  True, False, False,  True, False,  True,  True,\n        True, False,  True,  True, False, False,  True, False, False,\n       False, False, False, False, False, False,  True, False, False,\n        True,  True, False,  True,  True, False,  True, False, False,\n        True, False, False, False, False, False, False, False, False,\n        True,  True, False,  True,  True, False, False, False, False,\n       False,  True,  True, False, False,  True,  True, False, False,\n       False,  True, False, False,  True,  True,  True, False,  True,\n        True, False, False, False,  True,  True,  True, False, False,\n       False,  True, False, False,  True,  True, False,  True,  True,\n        True,  True, False, False, False, False, False, False, False,\n       False,  True, False, False, False,  True, False, False, False,\n       False, False, False, False, False,  True, False, False, False,\n       False,  True, False, False, False,  True, False, False, False,\n       False, False, False, False, False, False,  True, False, False,\n       False,  True,  True,  True, False, False, False,  True, False,\n        True,  True,  True,  True, False, False, False,  True, False,\n       False, False, False,  True, False, False, False,  True, False,\n       False, False, False,  True,  True,  True, False, False, False,\n       False, False,  True, False, False,  True, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True,  True, False,  True, False, False, False,  True,  True,\n       False, False, False, False, False, False,  True, False, False,\n       False, False, False, False, False, False, False,  True,  True,\n       False, False, False, False,  True, False,  True,  True,  True,\n       False,  True,  True, False,  True, False, False,  True, False,\n       False,  True,  True, False, False, False, False,  True,  True,\n       False, False, False,  True, False, False, False,  True,  True,\n       False,  True, False,  True,  True,  True, False, False, False,\n       False, False, False,  True,  True,  True, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False,  True,  True,  True,  True,  True,  True,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False,  True, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True, False,  True, False, False, False,  True,  True,  True,\n       False, False, False,  True, False, False,  True,  True,  True,\n        True,  True, False, False,  True, False, False,  True,  True,\n       False, False, False,  True,  True, False,  True, False, False,\n       False, False, False,  True,  True, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n        True,  True,  True,  True, False, False, False,  True,  True,\n       False, False, False, False, False, False, False,  True,  True,\n        True,  True, False,  True, False, False,  True, False, False,\n       False,  True, False,  True, False, False, False,  True, False,\n        True,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False,  True,  True, False,\n       False, False, False,  True, False, False, False, False,  True,\n       False, False, False,  True, False, False, False,  True, False,\n       False, False, False, False, False,  True, False,  True,  True,\n        True, False,  True, False, False,  True,  True,  True, False,\n        True,  True, False,  True,  True, False,  True,  True, False,\n       False, False, False, False,  True,  True, False,  True,  True,\n        True,  True, False,  True, False, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False, False,  True,\n       False, False, False,  True,  True, False, False,  True, False,\n       False, False, False, False, False,  True, False,  True, False,\n        True, False,  True, False,  True, False, False, False,  True,\n       False, False, False, False,  True, False, False, False, False,\n       False, False,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False,  True, False, False,\n       False, False,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n        True, False,  True, False, False,  True, False, False, False,\n        True, False, False,  True, False, False, False,  True,  True,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False,  True,  True,  True, False,  True,\n       False, False, False,  True,  True,  True, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False,  True, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False, False, False, False,  True, False, False, False,\n       False, False, False,  True, False, False, False,  True, False,\n       False, False,  True,  True, False, False, False,  True,  True,\n       False, False,  True, False,  True, False, False, False, False,\n       False, False, False, False,  True,  True, False,  True, False,\n       False,  True,  True,  True, False, False,  True, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False,  True,  True,\n       False, False, False, False, False,  True, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False,  True, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False,  True, False, False, False, False, False,\n       False,  True,  True, False, False,  True, False, False,  True,\n       False,  True, False, False, False,  True, False,  True, False,\n       False, False, False, False, False, False,  True, False, False,\n        True, False, False,  True, False, False, False, False, False,\n        True, False, False,  True, False, False, False, False,  True,\n        True,  True,  True,  True, False, False,  True, False, False,\n       False, False, False, False, False,  True, False,  True, False,\n        True, False, False, False, False, False, False, False,  True,\n       False, False,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False,  True,\n       False,  True, False,  True,  True, False, False, False, False,\n        True, False,  True,  True, False, False,  True,  True, False,\n        True, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False,  True, False,  True, False,  True,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False,  True, False,  True, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n       False,  True, False, False, False,  True, False,  True,  True,\n       False, False, False, False, False, False, False,  True,  True,\n        True, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False,  True, False,  True, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False, False,  True, False, False, False,\n       False, False,  True, False, False, False,  True, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n       False, False, False, False,  True, False, False, False, False,\n        True,  True,  True, False, False, False, False, False, False,\n        True,  True,  True, False,  True,  True,  True, False,  True,\n       False,  True,  True, False,  True,  True, False, False, False,\n       False, False, False,  True, False, False,  True, False,  True,\n       False,  True,  True, False, False, False, False, False, False,\n       False, False,  True, False,  True, False, False,  True,  True,\n        True, False, False,  True,  True,  True, False, False, False,\n        True,  True, False,  True,  True,  True, False,  True,  True,\n       False, False,  True,  True, False,  True, False, False, False,\n        True, False, False,  True, False, False,  True,  True,  True,\n       False,  True,  True, False, False,  True, False, False,  True,\n        True,  True,  True, False,  True, False,  True, False,  True,\n        True, False, False,  True,  True,  True,  True,  True, False,\n        True, False, False, False,  True, False, False,  True, False,\n       False, False,  True, False, False, False,  True,  True, False,\n        True,  True, False, False,  True,  True, False, False, False,\n       False, False,  True, False,  True, False,  True, False, False,\n       False, False, False, False, False, False, False,  True,  True,\n       False, False, False, False, False, False,  True, False, False,\n       False, False,  True,  True,  True, False,  True,  True,  True,\n        True, False, False,  True, False, False, False, False, False,\n       False, False, False,  True,  True,  True, False, False, False,\n       False, False,  True,  True,  True, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False,  True, False, False, False,  True, False,  True,\n        True,  True, False,  True,  True,  True, False, False, False,\n        True, False,  True, False,  True, False, False,  True,  True,\n       False, False, False, False, False, False,  True,  True,  True,\n        True, False, False,  True, False,  True,  True, False, False,\n       False,  True, False, False,  True,  True, False,  True, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False,  True, False, False, False, False, False, False,\n       False, False, False, False,  True, False, False, False,  True,\n       False, False, False, False, False,  True, False, False, False,\n       False, False,  True, False, False, False, False, False, False,\n       False,  True,  True, False, False,  True,  True,  True,  True,\n       False, False, False, False,  True, False, False,  True,  True,\n        True, False,  True, False,  True,  True,  True,  True,  True,\n        True, False, False, False, False, False, False, False, False,\n       False, False,  True, False, False, False,  True, False, False,\n       False,  True,  True, False,  True,  True, False, False,  True,\n       False, False,  True, False, False,  True,  True, False,  True,\n       False, False, False, False, False, False,  True, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False,  True, False, False,  True,  True, False,\n       False, False,  True, False, False, False,  True, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False,  True,  True,  True,  True, False, False,\n        True,  True, False, False,  True, False,  True, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False, False,  True,  True,  True,  True, False,\n       False, False, False, False, False,  True,  True, False, False,\n       False, False, False, False,  True, False,  True, False, False,\n       False, False, False,  True, False, False, False, False, False,\n        True,  True, False, False,  True, False,  True, False, False,\n       False, False, False,  True,  True, False, False, False, False,\n        True, False, False, False, False, False, False, False, False,\n        True,  True,  True,  True,  True,  True, False,  True, False,\n       False,  True,  True, False,  True, False, False, False, False,\n       False, False, False, False, False,  True,  True,  True,  True,\n       False, False, False, False, False,  True,  True, False, False,\n       False, False, False, False, False,  True,  True,  True,  True,\n        True, False, False, False, False, False, False, False, False,\n       False, False, False,  True, False,  True, False, False, False,\n       False, False,  True,  True, False, False,  True,  True, False,\n       False, False, False,  True, False, False,  True,  True, False,\n        True,  True,  True, False, False, False, False,  True,  True,\n       False, False, False, False, False, False, False, False, False,\n        True, False, False, False, False,  True, False, False, False,\n        True, False, False, False, False,  True,  True,  True,  True,\n        True,  True,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False,  True, False, False,  True, False, False, False,\n        True, False,  True, False,  True, False, False,  True, False,\n        True, False,  True, False, False,  True,  True, False, False,\n        True, False,  True, False,  True,  True, False, False, False,\n       False,  True,  True, False, False, False,  True,  True,  True,\n        True, False, False,  True,  True,  True,  True,  True,  True,\n       False, False,  True,  True,  True, False, False,  True, False,\n        True, False, False, False, False,  True,  True, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False,  True, False, False,  True, False, False, False,\n       False,  True,  True,  True, False,  True,  True, False, False,\n       False, False,  True, False, False,  True,  True, False, False,\n        True,  True,  True, False,  True, False, False, False, False,\n        True,  True,  True,  True, False, False, False, False,  True,\n        True, False, False, False,  True,  True,  True,  True,  True,\n       False,  True,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False,  True,  True, False,\n       False, False, False, False,  True,  True, False,  True,  True,\n        True, False, False, False, False,  True, False, False, False,\n       False, False,  True,  True, False,  True, False, False, False,\n       False, False, False, False, False, False,  True, False, False,\n       False,  True, False,  True, False, False,  True,  True, False,\n       False, False, False,  True,  True, False, False, False, False,\n       False,  True,  True, False, False, False, False, False, False,\n       False, False,  True,  True,  True, False, False, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False,  True, False,  True,  True,  True,  True, False,  True,\n       False, False,  True, False, False, False, False, False,  True,\n       False, False, False, False, False, False, False, False, False,\n       False,  True, False,  True, False, False, False, False, False,\n       False, False, False, False,  True,  True,  True, False,  True,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False,  True, False, False,  True, False,\n        True,  True, False, False, False, False, False, False,  True,\n        True, False, False, False, False,  True,  True, False, False,\n       False, False, False,  True, False, False, False, False, False,\n       False,  True,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False, False,  True,  True, False, False, False,\n       False, False, False, False, False, False,  True, False, False,\n        True,  True,  True, False, False,  True, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False,  True, False,  True, False, False, False, False,\n        True,  True,  True, False, False,  True, False, False, False,\n        True, False, False, False, False, False,  True, False, False,\n       False,  True, False, False, False, False, False,  True, False,\n       False, False,  True, False,  True, False, False,  True, False,\n       False, False, False, False, False,  True, False, False, False,\n        True, False, False, False,  True, False, False, False,  True,\n       False, False, False,  True, False, False,  True, False, False,\n       False,  True, False,  True, False, False, False,  True, False,\n        True,  True,  True, False, False, False, False,  True, False,\n       False, False, False, False,  True, False, False, False, False,\n       False, False, False,  True, False, False, False, False,  True,\n        True, False, False, False,  True, False,  True, False, False,\n       False,  True, False, False, False,  True, False,  True, False,\n       False, False])\n\n\ny_wwq = pd.read_csv(\"datasets/y_wwq.csv\").values\ny_wwq[:5]\n\narray([[False],\n       [False],\n       [False],\n       [False],\n       [False]])\n\n\nsome features seem to have different units of measurement. 'density', for instance, takes values between 0.98 and 1.04, while 'total sulfur dioxide' ranges from 9 to 440. As a result, it may be worth scaling the features here. We will scale the features and compute the mean and standard deviation of the unscaled features compared to the scaled features.\n\n# Scale the features: X_scaled\nX_scaled_wwq = scale(X_wwq)\n\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X_wwq))) \nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X_wwq)))\n\nMean of Unscaled Features: 18.43268707245592\nStandard Deviation of Unscaled Features: 41.544947640946354\n\n\n\n# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled_wwq))) \nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled_wwq)))\n\nMean of Scaled Features: 2.660809650821445e-15\nStandard Deviation of Scaled Features: 1.0\n\n\n\n\nCentering and scaling in a pipeline\nWith regard to whether or not scaling is effective, the proof is in the pudding! We will See for ourselves whether or not scaling the features of the White Wine Quality dataset has any impact on its performance. We will use a k-NN classifier as part of a pipeline that includes scaling, and for the purposes of comparison\n\n# Setup the pipeline steps: steps\nsteps_wwq = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n        \n# Create the pipeline: pipeline\npipeline_wwq = Pipeline(steps_wwq)\n\n# Create train and test sets\nX_train_wwq, X_test_wwq, y_train_wwq, y_test_wwq = train_test_split(X_wwq,y_wwq, test_size=.3, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled_wwq = pipeline_wwq.fit(X_train_wwq, y_train_wwq)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled_wwq = KNeighborsClassifier().fit(X_train_wwq, y_train_wwq)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled_wwq.score(X_test_wwq, y_test_wwq)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled_wwq.score(X_test_wwq, y_test_wwq)))\n\nAccuracy with Scaling: 0.7700680272108843\nAccuracy without Scaling: 0.6979591836734694\n\n\nIt looks like scaling has significantly improved model performance!\n\n\nBringing it all together I: Pipeline for classification\nWe will build a pipeline that includes scaling and hyperparameter tuning to classify wine quality.\n\nX_wwq[:5]\n\narray([[7.000e+00, 2.700e-01, 3.600e-01, 2.070e+01, 4.500e-02, 4.500e+01,\n        1.700e+02, 1.001e+00, 3.000e+00, 4.500e-01, 8.800e+00],\n       [6.300e+00, 3.000e-01, 3.400e-01, 1.600e+00, 4.900e-02, 1.400e+01,\n        1.320e+02, 9.940e-01, 3.300e+00, 4.900e-01, 9.500e+00],\n       [8.100e+00, 2.800e-01, 4.000e-01, 6.900e+00, 5.000e-02, 3.000e+01,\n        9.700e+01, 9.951e-01, 3.260e+00, 4.400e-01, 1.010e+01],\n       [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01,\n        1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00],\n       [7.200e+00, 2.300e-01, 3.200e-01, 8.500e+00, 5.800e-02, 4.700e+01,\n        1.860e+02, 9.956e-01, 3.190e+00, 4.000e-01, 9.900e+00]])\n\n\n\n# Setup the pipeline\nsteps_wwq = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline_wwq = Pipeline(steps_wwq)\n\n# Specify the hyperparameter space\nparameters_wwq = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Create train and test sets\nX_train_wwq, X_test_wwq, y_train_wwq, y_test_wwq = train_test_split(X_wwq,y_wwq, test_size=.2, random_state=21)\n\n# Instantiate the GridSearchCV object: cv\ncv_wwq = GridSearchCV(pipeline_wwq, param_grid=parameters_wwq, cv=3)\n\n# Fit to the training set\ncv_wwq.fit(X_train_wwq, y_train_wwq)\n\n# Predict the labels of the test set: y_pred\ny_pred_wwq = cv_wwq.predict(X_test_wwq)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv_wwq.score(X_test_wwq, y_test_wwq)))\nprint(classification_report(y_test_wwq, y_pred_wwq))\nprint(\"Tuned Model Parameters: {}\".format(cv_wwq.best_params_))\n\nAccuracy: 0.7795918367346939\n              precision    recall  f1-score   support\n\n       False       0.83      0.85      0.84       662\n        True       0.67      0.63      0.65       318\n\n    accuracy                           0.78       980\n   macro avg       0.75      0.74      0.74       980\nweighted avg       0.78      0.78      0.78       980\n\nTuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.1}\n\n\n\n\nBringing it all together II: Pipeline for regression\nWe will return to the gapminder dataset, it had a lot of missing data. We will build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data. We will then tune the l1_ratio of the ElasticNet using GridSearchCV.\n\nX_gapminder = np.array([[3.48110590e+07, 2.73000000e+00, 1.00000000e-01, 3.32894466e+00,\n        2.45962000e+01, 1.23140000e+04, 1.29904900e+02, 2.95000000e+01],\n       [1.98422510e+07, 6.43000000e+00, 2.00000000e+00, 1.47435339e+00,\n        2.22508300e+01, 7.10300000e+03, 1.30124700e+02, 1.92000000e+02],\n       [4.03818600e+07, 2.24000000e+00, 5.00000000e-01, 4.78516998e+00,\n        2.75017000e+01, 1.46460000e+04, 1.18891500e+02, 1.54000000e+01],\n       [2.97502900e+06, 1.40000000e+00, 1.00000000e-01, 1.80410622e+00,\n        2.53554200e+01, 7.38300000e+03, 1.32810800e+02, 2.00000000e+01],\n       [2.13703480e+07, 1.96000000e+00, 1.00000000e-01, 1.80163133e+01,\n        2.75637300e+01, 4.13120000e+04, 1.17375500e+02, 5.20000000e+00],\n       [8.33146500e+06, 1.41000000e+00, 3.00000000e-01, 8.18316002e+00,\n        2.64674100e+01, 4.39520000e+04, 1.24139400e+02, 4.60000000e+00],\n       [8.86871300e+06, 1.99000000e+00, 1.00000000e-01, 5.10953829e+00,\n        2.56511700e+01, 1.43650000e+04, 1.28602400e+02, 4.33000000e+01],\n       [3.48587000e+05, 1.89000000e+00, 3.10000000e+00, 3.13192132e+00,\n        2.72459400e+01, 2.43730000e+04, 1.24386200e+02, 1.45000000e+01],\n       [1.48252473e+08, 2.38000000e+00, 6.00000000e-02, 3.19161002e-01,\n        2.03974200e+01, 2.26500000e+03, 1.25030700e+02, 5.59000000e+01],\n       [2.77315000e+05, 1.83000000e+00, 1.30000000e+00, 6.00827884e+00,\n        2.63843900e+01, 1.60750000e+04, 1.26394000e+02, 1.54000000e+01],\n       [9.52645300e+06, 1.42000000e+00, 2.00000000e-01, 6.48817388e+00,\n        2.61644300e+01, 1.44880000e+04, 1.29796800e+02, 7.20000000e+00],\n       [1.07791550e+07, 1.82000000e+00, 2.00000000e-01, 9.79733671e+00,\n        2.67591500e+01, 4.16410000e+04, 1.21822700e+02, 4.70000000e+00],\n       [3.06165000e+05, 2.91000000e+00, 2.40000000e+00, 1.36012592e+00,\n        2.70225500e+01, 8.29300000e+03, 1.20922400e+02, 2.01000000e+01],\n       [8.97352500e+06, 5.27000000e+00, 1.20000000e+00, 5.37539184e-01,\n        2.24183500e+01, 1.64600000e+03, 1.30272300e+02, 1.16300000e+02],\n       [6.94990000e+05, 2.51000000e+00, 2.00000000e-01, 6.01210310e-01,\n        2.28218000e+01, 5.66300000e+03, 1.25125800e+02, 4.81000000e+01],\n       [9.59991600e+06, 3.48000000e+00, 2.00000000e-01, 1.43182915e+00,\n        2.44333500e+01, 5.06600000e+03, 1.22415500e+02, 5.20000000e+01],\n       [1.96786600e+06, 2.86000000e+00, 2.49000000e+01, 2.54720549e+00,\n        2.21298400e+01, 1.38580000e+04, 1.33130700e+02, 6.38000000e+01],\n       [1.94769696e+08, 1.90000000e+00, 4.50000000e-01, 2.02377284e+00,\n        2.57862300e+01, 1.39060000e+04, 1.24874500e+02, 1.86000000e+01],\n       [7.51364600e+06, 1.43000000e+00, 1.00000000e-01, 6.69013908e+00,\n        2.65428600e+01, 1.53680000e+04, 1.28472100e+02, 1.37000000e+01],\n       [1.47090110e+07, 6.04000000e+00, 1.20000000e+00, 1.09419171e-01,\n        2.12715700e+01, 1.35800000e+03, 1.30665100e+02, 1.30400000e+02],\n       [8.82179500e+06, 6.48000000e+00, 3.50000000e+00, 3.13888013e-02,\n        2.15029100e+01, 7.23000000e+02, 1.34195500e+02, 1.08600000e+02],\n       [1.39336600e+07, 3.05000000e+00, 6.00000000e-01, 2.87547496e-01,\n        2.08049600e+01, 2.44200000e+03, 1.17552800e+02, 5.15000000e+01],\n       [1.95704180e+07, 5.17000000e+00, 5.30000000e+00, 2.95541639e-01,\n        2.36817300e+01, 2.57100000e+03, 1.27282300e+02, 1.13800000e+02],\n       [3.33632560e+07, 1.68000000e+00, 2.00000000e-01, 1.63503986e+01,\n        2.74521000e+01, 4.14680000e+04, 1.18057100e+02, 5.80000000e+00],\n       [1.11397400e+07, 6.81000000e+00, 3.40000000e+00, 4.78391264e-02,\n        2.14856900e+01, 1.75300000e+03, 1.27864000e+02, 1.68000000e+02],\n       [1.66459400e+07, 1.89000000e+00, 4.00000000e-01, 4.24025914e+00,\n        2.70154200e+01, 1.86980000e+04, 1.25541700e+02, 8.90000000e+00],\n       [4.49016600e+07, 2.43000000e+00, 5.00000000e-01, 1.47609182e+00,\n        2.49404100e+01, 1.04890000e+04, 1.24023500e+02, 1.97000000e+01],\n       [6.65414000e+05, 5.05000000e+00, 6.00000000e-02, 1.78853064e-01,\n        2.20613100e+01, 1.44000000e+03, 1.32135400e+02, 9.12000000e+01],\n       [3.83277100e+06, 5.10000000e+00, 3.50000000e+00, 3.84220477e-01,\n        2.18713400e+01, 5.02200000e+03, 1.31693500e+02, 7.26000000e+01],\n       [4.42950600e+06, 1.91000000e+00, 3.00000000e-01, 1.91193342e+00,\n        2.64789700e+01, 1.22190000e+04, 1.21350000e+02, 1.03000000e+01],\n       [1.92616470e+07, 4.91000000e+00, 3.70000000e+00, 3.61896603e-01,\n        2.25646900e+01, 2.85400000e+03, 1.31523700e+02, 1.16900000e+02],\n       [4.34415100e+06, 1.43000000e+00, 6.00000000e-02, 5.28790258e+00,\n        2.65962900e+01, 2.18730000e+04, 1.30392100e+02, 5.90000000e+00],\n       [1.12902390e+07, 1.50000000e+00, 1.00000000e-01, 2.70177717e+00,\n        2.50686700e+01, 1.77650000e+04, 1.26059400e+02, 6.30000000e+00],\n       [5.49530200e+06, 1.89000000e+00, 2.00000000e-01, 8.54150780e+00,\n        2.61328700e+01, 4.50170000e+04, 1.19581500e+02, 4.30000000e+00],\n       [8.09639000e+05, 3.76000000e+00, 2.60000000e+00, 6.12799524e-01,\n        2.33840300e+01, 2.50200000e+03, 1.29337600e+02, 8.10000000e+01],\n       [1.44476000e+07, 2.73000000e+00, 4.00000000e-01, 2.11051780e+00,\n        2.55884100e+01, 9.24400000e+03, 1.22986400e+02, 2.68000000e+01],\n       [7.89761220e+07, 2.95000000e+00, 6.00000000e-02, 2.51239420e+00,\n        2.67324300e+01, 9.97400000e+03, 1.25093100e+02, 3.14000000e+01],\n       [6.00419900e+06, 2.32000000e+00, 8.00000000e-01, 1.06776463e+00,\n        2.63675100e+01, 7.45000000e+03, 1.19932100e+02, 2.16000000e+01],\n       [6.86223000e+05, 5.31000000e+00, 4.70000000e+00, 6.79825323e+00,\n        2.37664000e+01, 4.01430000e+04, 1.32039200e+02, 1.18400000e+02],\n       [4.50063800e+06, 5.16000000e+00, 8.00000000e-01, 8.37456442e-02,\n        2.08850900e+01, 1.08800000e+03, 1.25794800e+02, 6.04000000e+01],\n       [1.33994100e+06, 1.62000000e+00, 1.20000000e+00, 1.30313789e+01,\n        2.62644600e+01, 2.47430000e+04, 1.29516100e+02, 5.50000000e+00],\n       [8.43206000e+05, 2.74000000e+00, 1.00000000e-01, 1.27777956e+00,\n        2.65307800e+01, 7.12900000e+03, 1.27476800e+02, 2.40000000e+01],\n       [5.31417000e+06, 1.85000000e+00, 1.00000000e-01, 1.06441143e+01,\n        2.67333900e+01, 4.21220000e+04, 1.26564500e+02, 3.30000000e+00],\n       [6.23095290e+07, 1.97000000e+00, 4.00000000e-01, 5.99902073e+00,\n        2.58532900e+01, 3.75050000e+04, 1.20014600e+02, 4.30000000e+00],\n       [1.47374100e+06, 4.28000000e+00, 5.30000000e+00, 1.07953932e+00,\n        2.40762000e+01, 1.58000000e+04, 1.30362500e+02, 6.80000000e+01],\n       [1.58674900e+06, 5.80000000e+00, 1.70000000e+00, 2.51002328e-01,\n        2.16502900e+01, 1.56600000e+03, 1.30208000e+02, 8.74000000e+01],\n       [4.34329000e+06, 1.79000000e+00, 1.00000000e-01, 1.41942978e+00,\n        2.55494200e+01, 5.90000000e+03, 1.30578900e+02, 1.93000000e+01],\n       [8.06659060e+07, 1.37000000e+00, 1.00000000e-01, 9.49724676e+00,\n        2.71650900e+01, 4.11990000e+04, 1.24904400e+02, 4.40000000e+00],\n       [2.31159190e+07, 4.19000000e+00, 1.80000000e+00, 3.66600849e-01,\n        2.28424700e+01, 2.90700000e+03, 1.28295300e+02, 7.99000000e+01],\n       [1.11617550e+07, 1.46000000e+00, 1.00000000e-01, 8.66123553e+00,\n        2.63378600e+01, 3.21970000e+04, 1.22934200e+02, 4.90000000e+00],\n       [1.41066870e+07, 4.12000000e+00, 8.00000000e-01, 8.35594820e-01,\n        2.52994700e+01, 6.96000000e+03, 1.20959600e+02, 3.69000000e+01],\n       [1.04273560e+07, 5.34000000e+00, 1.40000000e+00, 1.26964400e-01,\n        2.25244900e+01, 1.23000000e+03, 1.32276500e+02, 1.21000000e+02],\n       [1.56129300e+06, 5.25000000e+00, 2.50000000e+00, 1.56376432e-01,\n        2.16433800e+01, 1.32600000e+03, 1.30762700e+02, 1.27600000e+02],\n       [7.48096000e+05, 2.74000000e+00, 1.20000000e+00, 2.07341531e+00,\n        2.36846500e+01, 5.20800000e+03, 1.25151200e+02, 4.19000000e+01],\n       [9.70513000e+06, 3.50000000e+00, 2.00000000e+00, 2.49306755e-01,\n        2.36630200e+01, 1.60000000e+03, 1.25346100e+02, 8.33000000e+01],\n       [7.25947000e+06, 3.27000000e+00, 8.00000000e-01, 1.18745352e+00,\n        2.51087200e+01, 4.39100000e+03, 1.22962100e+02, 2.65000000e+01],\n       [1.00506990e+07, 1.33000000e+00, 6.00000000e-02, 5.45323172e+00,\n        2.71156800e+01, 2.33340000e+04, 1.28696800e+02, 7.20000000e+00],\n       [3.10033000e+05, 2.12000000e+00, 3.00000000e-01, 6.82190305e+00,\n        2.72068700e+01, 4.22940000e+04, 1.18738100e+02, 2.70000000e+00],\n       [1.19707011e+09, 2.64000000e+00, 3.20000000e-01, 1.52084942e+00,\n        2.09595600e+01, 3.90100000e+03, 1.23127400e+02, 6.56000000e+01],\n       [2.35360765e+08, 2.48000000e+00, 2.00000000e-01, 1.75504422e+00,\n        2.18557600e+01, 7.85600000e+03, 1.26421600e+02, 3.62000000e+01],\n       [7.25306930e+07, 1.88000000e+00, 2.00000000e-01, 7.89221094e+00,\n        2.53100300e+01, 1.59550000e+04, 1.25185900e+02, 2.14000000e+01],\n       [4.48014500e+06, 2.00000000e+00, 2.00000000e-01, 9.88253103e+00,\n        2.76532500e+01, 4.77130000e+04, 1.24780100e+02, 4.50000000e+00],\n       [7.09380800e+06, 2.92000000e+00, 2.00000000e-01, 1.00011881e+01,\n        2.71315100e+01, 2.85620000e+04, 1.21083800e+02, 4.90000000e+00],\n       [5.93192340e+07, 1.39000000e+00, 3.00000000e-01, 7.46594241e+00,\n        2.64802000e+01, 3.74750000e+04, 1.23703000e+02, 4.10000000e+00],\n       [2.71734400e+06, 2.39000000e+00, 1.70000000e+00, 4.39145647e+00,\n        2.40042100e+01, 8.95100000e+03, 1.25368500e+02, 1.89000000e+01],\n       [1.27317900e+08, 1.34000000e+00, 6.00000000e-02, 9.53660569e+00,\n        2.35000400e+01, 3.48000000e+04, 1.21965100e+02, 3.40000000e+00],\n       [1.59159660e+07, 2.51000000e+00, 1.00000000e-01, 1.47181043e+01,\n        2.62907800e+01, 1.87970000e+04, 1.28851700e+02, 2.59000000e+01],\n       [3.82444420e+07, 4.76000000e+00, 6.30000000e+00, 2.66308378e-01,\n        2.15925800e+01, 2.35800000e+03, 1.29934100e+02, 7.10000000e+01],\n       [2.14421500e+06, 1.50000000e+00, 6.00000000e-01, 3.34184866e+00,\n        2.64569300e+01, 2.09770000e+04, 1.29574600e+02, 1.05000000e+01],\n       [4.10938900e+06, 1.57000000e+00, 1.00000000e-01, 3.99672180e+00,\n        2.72011700e+01, 1.41580000e+04, 1.27503700e+02, 1.13000000e+01],\n       [1.97219400e+06, 3.34000000e+00, 2.36000000e+01, 8.61766942e-03,\n        2.19015700e+01, 2.04100000e+03, 1.31136100e+02, 1.14200000e+02],\n       [3.67278200e+06, 5.19000000e+00, 1.60000000e+00, 1.57352183e-01,\n        2.18953700e+01, 5.88000000e+02, 1.31255500e+02, 1.00900000e+02],\n       [3.21980200e+06, 1.42000000e+00, 1.00000000e-01, 4.49848339e+00,\n        2.68610200e+01, 2.32230000e+04, 1.30822600e+02, 8.20000000e+00],\n       [4.85079000e+05, 1.63000000e+00, 3.00000000e-01, 2.21680797e+01,\n        2.74340400e+01, 9.50010000e+04, 1.22370500e+02, 2.80000000e+00],\n       [1.99267980e+07, 4.79000000e+00, 2.00000000e-01, 9.94221476e-02,\n        2.14034700e+01, 1.52800000e+03, 1.32837100e+02, 6.67000000e+01],\n       [1.39046710e+07, 5.78000000e+00, 1.12000000e+01, 8.24698808e-02,\n        2.20346800e+01, 6.74000000e+02, 1.33939000e+02, 1.01100000e+02],\n       [2.71974190e+07, 2.05000000e+00, 5.00000000e-01, 7.75223395e+00,\n        2.47306900e+01, 1.99680000e+04, 1.23859300e+02, 8.00000000e+00],\n       [3.21026000e+05, 2.38000000e+00, 6.00000000e-02, 3.27772577e+00,\n        2.32199100e+01, 1.20290000e+04, 1.23322300e+02, 1.60000000e+01],\n       [1.42234030e+07, 6.82000000e+00, 1.00000000e+00, 4.10788666e-02,\n        2.17888100e+01, 1.60200000e+03, 1.28030800e+02, 1.48300000e+02],\n       [4.06392000e+05, 1.38000000e+00, 1.00000000e-01, 6.18277102e+00,\n        2.76836100e+01, 2.78720000e+04, 1.24157100e+02, 6.60000000e+00],\n       [3.41455200e+06, 4.94000000e+00, 7.00000000e-01, 6.13103977e-01,\n        2.26229500e+01, 3.35600000e+03, 1.29987500e+02, 1.03000000e+02],\n       [1.23801300e+06, 1.58000000e+00, 9.00000000e-01, 3.07876290e+00,\n        2.51566900e+01, 1.46150000e+04, 1.30878600e+02, 1.58000000e+01],\n       [1.14972821e+08, 2.35000000e+00, 3.00000000e-01, 4.26117187e+00,\n        2.74246800e+01, 1.58260000e+04, 1.22121600e+02, 1.79000000e+01],\n       [4.11116800e+06, 1.49000000e+00, 4.00000000e-01, 1.31332119e+00,\n        2.42369000e+01, 3.89000000e+03, 1.29942400e+02, 1.76000000e+01],\n       [2.62966600e+06, 2.37000000e+00, 6.00000000e-02, 3.75948682e+00,\n        2.48838500e+01, 7.56300000e+03, 1.29750400e+02, 3.48000000e+01],\n       [3.13505440e+07, 2.44000000e+00, 1.00000000e-01, 1.59408314e+00,\n        2.56318200e+01, 6.09100000e+03, 1.26528400e+02, 3.58000000e+01],\n       [2.29948670e+07, 5.54000000e+00, 1.14000000e+01, 1.04748301e-01,\n        2.19353600e+01, 8.64000000e+02, 1.35394900e+02, 1.14400000e+02],\n       [5.10300060e+07, 2.05000000e+00, 6.00000000e-01, 1.91053400e-01,\n        2.14493200e+01, 2.89100000e+03, 1.23142100e+02, 8.72000000e+01],\n       [2.63251830e+07, 2.90000000e+00, 4.00000000e-01, 1.05412983e-01,\n        2.07634400e+01, 1.86600000e+03, 1.25556100e+02, 5.07000000e+01],\n       [1.65198620e+07, 1.77000000e+00, 2.00000000e-01, 1.05330281e+01,\n        2.60154100e+01, 4.73880000e+04, 1.21695000e+02, 4.80000000e+00],\n       [4.28538000e+06, 2.12000000e+00, 1.00000000e-01, 8.00908440e+00,\n        2.77689300e+01, 3.21220000e+04, 1.18742100e+02, 6.40000000e+00],\n       [5.59452400e+06, 2.72000000e+00, 2.00000000e-01, 7.78151613e-01,\n        2.57729100e+01, 4.06000000e+03, 1.23479200e+02, 2.81000000e+01],\n       [1.50851300e+07, 7.59000000e+00, 8.00000000e-01, 6.34371088e-02,\n        2.12195800e+01, 8.43000000e+02, 1.35102100e+02, 1.41300000e+02],\n       [1.51115683e+08, 6.02000000e+00, 3.60000000e+00, 6.14689662e-01,\n        2.30332200e+01, 4.68400000e+03, 1.35492000e+02, 1.40900000e+02],\n       [4.77163300e+06, 1.96000000e+00, 1.00000000e-01, 1.05297688e+01,\n        2.69342400e+01, 6.52160000e+04, 1.26026600e+02, 3.60000000e+00],\n       [2.65228100e+06, 2.89000000e+00, 1.00000000e-01, 1.55720805e+01,\n        2.62410900e+01, 4.77990000e+04, 1.26887000e+02, 1.19000000e+01],\n       [1.63096985e+08, 3.58000000e+00, 1.00000000e-01, 9.35618056e-01,\n        2.22991400e+01, 4.18700000e+03, 1.26519600e+02, 9.55000000e+01],\n       [3.49867900e+06, 2.61000000e+00, 9.00000000e-01, 2.22379634e+00,\n        2.62695900e+01, 1.40330000e+04, 1.22682900e+02, 2.10000000e+01],\n       [6.54026700e+06, 4.07000000e+00, 9.00000000e-01, 5.30746337e-01,\n        2.50150600e+01, 1.98200000e+03, 1.20052400e+02, 6.97000000e+01],\n       [6.04713100e+06, 3.06000000e+00, 3.00000000e-01, 6.98581746e-01,\n        2.55422300e+01, 6.68400000e+03, 1.23615000e+02, 2.57000000e+01],\n       [2.86420480e+07, 2.58000000e+00, 4.00000000e-01, 1.45013444e+00,\n        2.47704100e+01, 9.24900000e+03, 1.19636800e+02, 2.32000000e+01],\n       [9.02971150e+07, 3.26000000e+00, 6.00000000e-02, 8.42120697e-01,\n        2.28726300e+01, 5.33200000e+03, 1.22345900e+02, 3.34000000e+01],\n       [3.85257520e+07, 1.33000000e+00, 1.00000000e-01, 8.27076715e+00,\n        2.66738000e+01, 1.99960000e+04, 1.29676500e+02, 6.70000000e+00],\n       [1.05774580e+07, 1.36000000e+00, 5.00000000e-01, 5.48692640e+00,\n        2.66844500e+01, 2.77470000e+04, 1.27263100e+02, 4.10000000e+00],\n       [1.38896200e+06, 2.20000000e+00, 6.00000000e-02, 4.87020615e+01,\n        2.81313800e+01, 1.26076000e+05, 1.26315300e+02, 9.50000000e+00],\n       [2.07416690e+07, 1.34000000e+00, 1.00000000e-01, 4.38344907e+00,\n        2.54106900e+01, 1.80320000e+04, 1.28755300e+02, 1.61000000e+01],\n       [1.43123163e+08, 1.49000000e+00, 1.00000000e+00, 1.19827176e+01,\n        2.60113100e+01, 2.25060000e+04, 1.28490300e+02, 1.35000000e+01],\n       [9.75031400e+06, 5.06000000e+00, 2.90000000e+00, 5.42444698e-02,\n        2.25545300e+01, 1.17300000e+03, 1.35100500e+02, 7.83000000e+01],\n       [1.22297030e+07, 5.11000000e+00, 8.00000000e-01, 4.61633711e-01,\n        2.19274300e+01, 2.16200000e+03, 1.30279500e+02, 7.58000000e+01],\n       [9.10953500e+06, 1.41000000e+00, 1.00000000e-01, 5.27122268e+00,\n        2.65149500e+01, 1.25220000e+04, 1.30375500e+02, 8.00000000e+00],\n       [5.52183800e+06, 5.13000000e+00, 1.60000000e+00, 1.18255775e-01,\n        2.25313900e+01, 1.28900000e+03, 1.34716000e+02, 1.79100000e+02],\n       [4.84964100e+06, 1.28000000e+00, 1.00000000e-01, 4.11444075e+00,\n        2.38399600e+01, 6.59910000e+04, 1.21173600e+02, 2.80000000e+00],\n       [5.39671000e+06, 1.31000000e+00, 6.00000000e-02, 6.90165446e+00,\n        2.69271700e+01, 2.46700000e+04, 1.29528000e+02, 8.80000000e+00],\n       [2.03059900e+06, 1.43000000e+00, 6.00000000e-02, 8.51182820e+00,\n        2.74398300e+01, 3.08160000e+04, 1.29923100e+02, 3.70000000e+00],\n       [9.13258900e+06, 7.06000000e+00, 6.00000000e-01, 6.82188892e-02,\n        2.19691700e+01, 6.15000000e+02, 1.31531800e+02, 1.68500000e+02],\n       [5.03488110e+07, 2.54000000e+00, 1.79000000e+01, 9.42796037e+00,\n        2.68553800e+01, 1.22630000e+04, 1.30994900e+02, 6.61000000e+01],\n       [4.58170160e+07, 1.42000000e+00, 4.00000000e-01, 7.29308876e+00,\n        2.74997500e+01, 3.46760000e+04, 1.22045300e+02, 5.00000000e+00],\n       [1.99495530e+07, 2.32000000e+00, 6.00000000e-02, 5.80791088e-01,\n        2.19667100e+01, 6.90700000e+03, 1.24861500e+02, 1.17000000e+01],\n       [3.44701380e+07, 4.79000000e+00, 1.00000000e+00, 3.82117945e-01,\n        2.24048400e+01, 3.24600000e+03, 1.29719900e+02, 8.47000000e+01],\n       [5.06657000e+05, 2.41000000e+00, 1.00000000e+00, 4.74113997e+00,\n        2.54988700e+01, 1.34700000e+04, 1.24635800e+02, 2.64000000e+01],\n       [1.15375000e+06, 3.70000000e+00, 2.59000000e+01, 9.49860795e-01,\n        2.31696900e+01, 5.88700000e+03, 1.31879300e+02, 1.12200000e+02],\n       [9.22633300e+06, 1.92000000e+00, 1.00000000e-01, 5.31568840e+00,\n        2.63762900e+01, 4.34210000e+04, 1.22947300e+02, 3.20000000e+00],\n       [7.64654200e+06, 1.47000000e+00, 4.00000000e-01, 5.33305762e+00,\n        2.62019500e+01, 5.50200000e+04, 1.19646500e+02, 4.70000000e+00],\n       [7.25407200e+06, 3.70000000e+00, 2.00000000e-01, 4.53167660e-01,\n        2.37796600e+01, 2.00100000e+03, 1.29965700e+02, 5.62000000e+01],\n       [4.28447440e+07, 5.54000000e+00, 5.80000000e+00, 1.54672995e-01,\n        2.24779200e+01, 2.03000000e+03, 1.30832800e+02, 7.24000000e+01],\n       [6.64532550e+07, 1.48000000e+00, 1.30000000e+00, 3.83510189e+00,\n        2.30080300e+01, 1.22160000e+04, 1.20496900e+02, 1.56000000e+01],\n       [6.05293700e+06, 4.88000000e+00, 3.20000000e+00, 2.51983337e-01,\n        2.18787500e+01, 1.21900000e+03, 1.31024800e+02, 9.64000000e+01],\n       [1.31537200e+06, 1.80000000e+00, 1.50000000e+00, 3.19577173e+01,\n        2.63966900e+01, 3.08750000e+04, 1.24993900e+02, 2.49000000e+01],\n       [1.04080910e+07, 2.04000000e+00, 6.00000000e-02, 2.44066948e+00,\n        2.51569900e+01, 9.93800000e+03, 1.28629100e+02, 1.94000000e+01],\n       [7.03443570e+07, 2.15000000e+00, 6.00000000e-02, 4.02190259e+00,\n        2.67037100e+01, 1.64540000e+04, 1.24067500e+02, 2.22000000e+01],\n       [3.10144270e+07, 6.34000000e+00, 6.40000000e+00, 1.00852839e-01,\n        2.23583300e+01, 1.43700000e+03, 1.34520400e+02, 8.93000000e+01],\n       [4.60284760e+07, 1.38000000e+00, 1.10000000e+00, 7.03235908e+00,\n        2.54237900e+01, 8.76200000e+03, 1.31496200e+02, 1.29000000e+01],\n       [6.16896200e+07, 1.87000000e+00, 2.00000000e-01, 8.52646682e+00,\n        2.73924900e+01, 3.77390000e+04, 1.24084500e+02, 5.60000000e+00],\n       [3.04473143e+08, 2.07000000e+00, 6.00000000e-01, 1.85459917e+01,\n        2.84569800e+01, 5.03840000e+04, 1.18477700e+02, 7.70000000e+00],\n       [3.35083200e+06, 2.11000000e+00, 5.00000000e-01, 2.48976355e+00,\n        2.63912300e+01, 1.53170000e+04, 1.24260400e+02, 1.30000000e+01],\n       [2.69527190e+07, 2.46000000e+00, 1.00000000e-01, 4.47666902e+00,\n        2.53205400e+01, 3.73300000e+03, 1.24346200e+02, 4.92000000e+01],\n       [8.65893420e+07, 1.86000000e+00, 4.00000000e-01, 1.47934658e+00,\n        2.09163000e+01, 4.08500000e+03, 1.21936700e+02, 2.62000000e+01],\n       [1.31145790e+07, 5.88000000e+00, 1.36000000e+01, 1.48981514e-01,\n        2.06832100e+01, 3.03900000e+03, 1.32449300e+02, 9.49000000e+01],\n       [1.34954620e+07, 3.85000000e+00, 1.51000000e+01, 6.54323190e-01,\n        2.20266000e+01, 1.28600000e+03, 1.31974500e+02, 9.83000000e+01]])\n\n\nX_gapminder = pd.read_csv(\"datasets/X_gapminder.csv\").values\nX_gapminder[:5]\n\narray([[3.48110590e+07, 2.73000000e+00, 1.00000000e-01, 3.32894466e+00,\n        2.45962000e+01, 1.23140000e+04, 1.29904900e+02, 2.95000000e+01],\n       [1.98422510e+07, 6.43000000e+00, 2.00000000e+00, 1.47435339e+00,\n        2.22508300e+01, 7.10300000e+03, 1.30124700e+02, 1.92000000e+02],\n       [4.03818600e+07, 2.24000000e+00, 5.00000000e-01, 4.78516998e+00,\n        2.75017000e+01, 1.46460000e+04, 1.18891500e+02, 1.54000000e+01],\n       [2.97502900e+06, 1.40000000e+00, 1.00000000e-01, 1.80410622e+00,\n        2.53554200e+01, 7.38300000e+03, 1.32810800e+02, 2.00000000e+01],\n       [2.13703480e+07, 1.96000000e+00, 1.00000000e-01, 1.80163133e+01,\n        2.75637300e+01, 4.13120000e+04, 1.17375500e+02, 5.20000000e+00]])\n\n\n\ny_gapminder = np.array([75.3, 58.3, 75.5, 72.5, 81.5, 80.4, 70.6, 72.2, 68.4, 75.3, 70.1,\n       79.4, 70.7, 63.2, 67.6, 70.9, 61.2, 73.9, 73.2, 59.4, 57.4, 66.2,\n       56.6, 80.7, 54.8, 78.9, 75.1, 62.6, 58.6, 79.7, 55.9, 76.5, 77.8,\n       78.7, 61. , 74. , 70.1, 74.1, 56.7, 60.4, 74. , 65.7, 79.4, 81. ,\n       57.5, 62.2, 72.1, 80. , 62.7, 79.5, 70.8, 58.3, 51.3, 63. , 61.7,\n       70.9, 73.8, 82. , 64.4, 69.5, 76.9, 79.4, 80.9, 81.4, 75.5, 82.6,\n       66.1, 61.5, 72.3, 77.6, 45.2, 61. , 72. , 80.7, 63.4, 51.4, 74.5,\n       78.2, 55.8, 81.4, 63.6, 72.1, 75.7, 69.6, 63.2, 73.3, 55. , 60.8,\n       68.6, 80.3, 80.2, 75.2, 59.7, 58. , 80.7, 74.6, 64.1, 77.1, 58.2,\n       73.6, 76.8, 69.4, 75.3, 79.2, 80.4, 73.4, 67.6, 62.2, 64.3, 76.4,\n       55.9, 80.9, 74.8, 78.5, 56.7, 55. , 81.1, 74.3, 67.4, 69.1, 46.1,\n       81.1, 81.9, 69.5, 59.7, 74.1, 60. , 71.3, 76.5, 75.1, 57.2, 68.2,\n       79.5, 78.2, 76. , 68.7, 75.4, 52. , 49. ])\n\n\ny_gapminder = pd.read_csv(\"datasets/y_gapminder.csv\").values\ny_gapminder[:5]\n\narray([[75.3],\n       [58.3],\n       [75.5],\n       [72.5],\n       [81.5]])\n\n\n\n# Setup the pipeline steps: steps\nsteps_gapminder = [('imputation', SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n         (\"scaler\", StandardScaler()),\n         (\"elasticnet\", ElasticNet())]\n\n# Create the pipeline: pipeline \npipeline_gapminder = Pipeline(steps_gapminder)\n\n# Specify the hyperparameter space\nparameters_gapminder = {\"elasticnet__l1_ratio\":np.linspace(0,1,30)}\n\n# Create train and test sets\nX_train_gapminder, X_test_gapminder, y_train_gapminder, y_test_gapminder = train_test_split(X_gapminder, \n                                                                                            y_gapminder, \n                                                                                            test_size=.4, \n                                                                                            random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv_gapminder = GridSearchCV(pipeline_gapminder, param_grid=parameters_gapminder, cv=3)\n\n# Fit to the training set\ngm_cv_gapminder.fit(X_train_gapminder, y_train_gapminder)\n\n# Compute and print the metrics\nr2_gapminder = gm_cv_gapminder.score(X_test_gapminder, y_test_gapminder)\nprint(\"Tuned ElasticNet Alpha: {}\".format(gm_cv_gapminder.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2_gapminder))\n\nTuned ElasticNet Alpha: {'elasticnet__l1_ratio': 1.0}\nTuned ElasticNet R squared: 0.8862016570888216"
  },
  {
    "objectID": "posts/2020-02-20-test.html",
    "href": "posts/2020-02-20-test.html",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "This notebook is a demonstration of some of capabilities of fastpages with notebooks.\nWith fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts!\n\n\nThe first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:\n# \"My Title\"\n> \"Awesome summary\"\n\n- toc: true- branch: master- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\n\nSetting toc: true will automatically generate a table of contents\nSetting badges: true will automatically include GitHub and Google Colab links to your notebook.\nSetting comments: true will enable commenting on your blog post, powered by utterances.\n\nThe title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README.\n\n\n\nA #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post.\nA #hide_input comment at the top of any code cell will only hide the input of that cell.\n\n\nThe comment #hide_input was used to hide the code that produced this.\n\n\nput a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\nput a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:\n\n\nCode\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\n\n\n\n\nCharts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook.\n\n\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(movies).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\n\n\n\n\nalt.Chart(movies).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=700,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\nYou can display tables per the usual way in your blog:\n\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\ndf = pd.read_json(movies)\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'Distributor', 'MPAA_Rating', 'IMDB_Rating', 'Rotten_Tomatoes_Rating']].head()\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide_Gross\n      Production_Budget\n      Distributor\n      MPAA_Rating\n      IMDB_Rating\n      Rotten_Tomatoes_Rating\n    \n  \n  \n    \n      0\n      The Land Girls\n      146083.0\n      8000000.0\n      Gramercy\n      R\n      6.1\n      NaN\n    \n    \n      1\n      First Love, Last Rites\n      10876.0\n      300000.0\n      Strand\n      R\n      6.9\n      NaN\n    \n    \n      2\n      I Married a Strange Person\n      203134.0\n      250000.0\n      Lionsgate\n      None\n      6.8\n      NaN\n    \n    \n      3\n      Let's Talk About Sex\n      373615.0\n      300000.0\n      Fine Line\n      None\n      NaN\n      13.0\n    \n    \n      4\n      Slam\n      1087521.0\n      1000000.0\n      Trimark\n      R\n      3.4\n      62.0\n    \n  \n\n\n\n\n\n\n\n\n\nYou can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax:\n![](my_icons/fastai_logo.png)\n\n\n\n\nRemote images can be included with the following markdown syntax:\n![](https://image.flaticon.com/icons/svg/36/36686.svg)\n\n\n\n\nAnimated Gifs work, too!\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)\n\n\n\n\nYou can include captions with markdown images like this:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")"
  },
  {
    "objectID": "posts/2020-02-20-test.html#github-flavored-emojis",
    "href": "posts/2020-02-20-test.html#github-flavored-emojis",
    "title": "Fastpages Notebook Blog Post",
    "section": "GitHub Flavored Emojis",
    "text": "GitHub Flavored Emojis\nTyping I give this post two :+1:! will render this:\nI give this post two :+1:!"
  },
  {
    "objectID": "posts/2020-02-20-test.html#tweetcards",
    "href": "posts/2020-02-20-test.html#tweetcards",
    "title": "Fastpages Notebook Blog Post",
    "section": "Tweetcards",
    "text": "Tweetcards\nTyping > twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20"
  },
  {
    "objectID": "posts/2020-02-20-test.html#youtube-videos",
    "href": "posts/2020-02-20-test.html#youtube-videos",
    "title": "Fastpages Notebook Blog Post",
    "section": "Youtube Videos",
    "text": "Youtube Videos\nTyping > youtube: https://youtu.be/XfoYk_Z5AkI will render this:"
  },
  {
    "objectID": "posts/2020-02-20-test.html#boxes-callouts",
    "href": "posts/2020-02-20-test.html#boxes-callouts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Boxes / Callouts",
    "text": "Boxes / Callouts\nTyping > Warning: There will be no second warning! will render this:\n\n\n\n\n\n\nWarning\n\n\n\nThere will be no second warning!\n\n\nTyping > Important: Pay attention! It's important. will render this:\n\n\n\n\n\n\nImportant\n\n\n\nPay attention! It’s important.\n\n\nTyping > Tip: This is my tip. will render this:\n\n\n\n\n\n\nTip\n\n\n\nThis is my tip.\n\n\nTyping > Note: Take note of this. will render this:\n\n\n\n\n\n\nNote\n\n\n\nTake note of this.\n\n\nTyping > Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs:\n\n\n\n\n\n\nNote\n\n\n\nA doc link to an example website: fast.ai should also work fine."
  },
  {
    "objectID": "posts/2020-02-20-test.html#footnotes",
    "href": "posts/2020-02-20-test.html#footnotes",
    "title": "Fastpages Notebook Blog Post",
    "section": "Footnotes",
    "text": "Footnotes\nYou can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this:\n{% raw %}For example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ 'This is the footnote.' | fndetail: 1 }}\n{{ 'This is the other footnote. You can even have a [link](www.github.com)!' | fndetail: 2 }}{% endraw %}\nFor example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ ‘This is the footnote.’ | fndetail: 1 }} {{ ‘This is the other footnote. You can even have a link!’ | fndetail: 2 }}"
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()\n\n%matplotlib inline"
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#optimal-parameters",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#optimal-parameters",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Optimal parameters",
    "text": "Optimal parameters\n\nOptimal parameters\n\nParameter values that bring the model in closest agreement with the data\n\n\n\nPackages to do statistical inference\n\n\n\n\n\n\n\n\npackage\n\n\n\n\n\nscipy.stats\n\n\n\nstatsmodels\n\n\n\nhacker stats with numpy\n\n\n\n\n\nHow often do we get no-hitters?\nThe number of games played between each no-hitter in the modern era (\\(1901-2015\\)) of Major League Baseball is stored in the array nohitter_times.\n\nnohitter_times = np.array([ 843, 1613, 1101,  215,  684,  814,  278,  324,  161,  219,  545,\n                           715,  966,  624,   29,  450,  107,   20,   91, 1325,  124, 1468,\n                           104, 1309,  429,   62, 1878, 1104,  123,  251,   93,  188,  983,\n                           166,   96,  702,   23,  524,   26,  299,   59,   39,   12,    2,\n                           308, 1114,  813,  887,  645, 2088,   42, 2090,   11,  886, 1665,\n                           1084, 2900, 2432,  750, 4021, 1070, 1765, 1322,   26,  548, 1525,\n                           77, 2181, 2752,  127, 2147,  211,   41, 1575,  151,  479,  697,\n                           557, 2267,  542,  392,   73,  603,  233,  255,  528,  397, 1529,\n                           1023, 1194,  462,  583,   37,  943,  996,  480, 1497,  717,  224,\n                           219, 1531,  498,   44,  288,  267,  600,   52,  269, 1086,  386,\n                           176, 2199,  216,   54,  675, 1243,  463,  650,  171,  327,  110,\n                           774,  509,    8,  197,  136,   12, 1124,   64,  380,  811,  232,\n                           192,  731,  715,  226,  605,  539, 1491,  323,  240,  179,  702,\n                           156,   82, 1397,  354,  778,  603, 1001,  385,  986,  203,  149,\n                           576,  445,  180, 1403,  252,  675, 1351, 2983, 1568,   45,  899,\n                           3260, 1025,   31,  100, 2055, 4043,   79,  238, 3931, 2351,  595,\n                           110,  215,    0,  563,  206,  660,  242,  577,  179,  157,  192,\n                           192, 1848,  792, 1693,   55,  388,  225, 1134, 1172, 1555,   31,\n                           1582, 1044,  378, 1687, 2915,  280,  765, 2819,  511, 1521,  745,\n                           2491,  580, 2072, 6450,  578,  745, 1075, 1103, 1549, 1520,  138,\n                           1202,  296,  277,  351,  391,  950,  459,   62, 1056, 1128,  139,\n                           420,   87,   71,  814,  603, 1349,  162, 1027,  783,  326,  101,\n                           876,  381,  905,  156,  419,  239,  119,  129,  467])\n\nIf we assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As we have seen, the Exponential distribution has a single parameter, which we will call τ, the typical interval time. The value of the parameter τ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters.\nWe will compute the value of this parameter from the data. Then, use np.random.exponential() to “repeat” the history of Major League Baseball by drawing inter-no-hitter times from an exponential distribution with the τ we found and plot the histogram as an approximation to the PDF.\n\n# Seed random number generator\nnp.random.seed(42)\n\n# Compute mean no-hitter time: tau\ntau = np.mean(nohitter_times)\n\n# Draw out of an exponential distribution with parameter tau: inter_nohitter_time\ninter_nohitter_time = np.random.exponential(tau, 100000)\n\n# Plot the PDF and label axes\n_ = plt.hist(inter_nohitter_time,\n             bins=50, density=True, histtype=\"step\")\n_ = plt.xlabel('Games between no-hitters')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see the typical shape of the Exponential distribution, going from a maximum at 0 and decaying to the right.\n\n\n\n\nDo the data follow our story?\nWe have modeled no-hitters using an Exponential distribution. Let’s create an ECDF of the real data. Overlay the theoretical CDF with the ECDF from the data. This helps us to verify that the Exponential distribution describes the observed data.\n\ndef ecdf(data):\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\n\n\n# Create an ECDF from real data: x, y\nx, y = ecdf(nohitter_times)\n\n# Create a CDF from theoretical samples: x_theor, y_theor\nx_theor, y_theor = ecdf(inter_nohitter_time)\n\n# Overlay the plots\nplt.plot(x_theor, y_theor)\nplt.plot(x, y, marker=\".\", linestyle=\"none\")\n\n# Margins and axis labels\nplt.margins(.02)\nplt.xlabel('Games between no-hitters')\nplt.ylabel('CDF')\n\n# Show the plot\nplt.show()\n\n\n\n\nIt looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was."
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#linear-regression-by-least-squares",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#linear-regression-by-least-squares",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Linear regression by least squares",
    "text": "Linear regression by least squares\n\nLeast squares\n\nThe process of nding the parameters for which the sum ofthe squares ofthe residuals is minimal\n\n\n\nEDA of literacy/fertility data\nwe will look at the correlation between female literacy and fertility (defined as the average number of children born per woman) throughout the world. For ease of analysis and interpretation, we will work with the illiteracy rate.\n\nilliteracy= np.array([ 9.5, 49.2,  1. , 11.2,  9.8, 60. , 50.2, 51.2,  0.6,  1. ,  8.5,\n                      6.1,  9.8,  1. , 42.2, 77.2, 18.7, 22.8,  8.5, 43.9,  1. ,  1. ,\n                      1.5, 10.8, 11.9,  3.4,  0.4,  3.1,  6.6, 33.7, 40.4,  2.3, 17.2,\n                      0.7, 36.1,  1. , 33.2, 55.9, 30.8, 87.4, 15.4, 54.6,  5.1,  1.1,\n                      10.2, 19.8,  0. , 40.7, 57.2, 59.9,  3.1, 55.7, 22.8, 10.9, 34.7,\n                      32.2, 43. ,  1.3,  1. ,  0.5, 78.4, 34.2, 84.9, 29.1, 31.3, 18.3,\n                      81.8, 39. , 11.2, 67. ,  4.1,  0.2, 78.1,  1. ,  7.1,  1. , 29. ,\n                      1.1, 11.7, 73.6, 33.9, 14. ,  0.3,  1. ,  0.8, 71.9, 40.1,  1. ,\n                      2.1,  3.8, 16.5,  4.1,  0.5, 44.4, 46.3, 18.7,  6.5, 36.8, 18.6,\n                      11.1, 22.1, 71.1,  1. ,  0. ,  0.9,  0.7, 45.5,  8.4,  0. ,  3.8,\n                      8.5,  2. ,  1. , 58.9,  0.3,  1. , 14. , 47. ,  4.1,  2.2,  7.2,\n                      0.3,  1.5, 50.5,  1.3,  0.6, 19.1,  6.9,  9.2,  2.2,  0.2, 12.3,\n                      4.9,  4.6,  0.3, 16.5, 65.7, 63.5, 16.8,  0.2,  1.8,  9.6, 15.2,\n                      14.4,  3.3, 10.6, 61.3, 10.9, 32.2,  9.3, 11.6, 20.7,  6.5,  6.7,\n                      3.5,  1. ,  1.6, 20.5,  1.5, 16.7,  2. ,  0.9])\n\n\nfertility = np.array([1.769, 2.682, 2.077, 2.132, 1.827, 3.872, 2.288, 5.173, 1.393,\n       1.262, 2.156, 3.026, 2.033, 1.324, 2.816, 5.211, 2.1  , 1.781,\n       1.822, 5.908, 1.881, 1.852, 1.39 , 2.281, 2.505, 1.224, 1.361,\n       1.468, 2.404, 5.52 , 4.058, 2.223, 4.859, 1.267, 2.342, 1.579,\n       6.254, 2.334, 3.961, 6.505, 2.53 , 2.823, 2.498, 2.248, 2.508,\n       3.04 , 1.854, 4.22 , 5.1  , 4.967, 1.325, 4.514, 3.173, 2.308,\n       4.62 , 4.541, 5.637, 1.926, 1.747, 2.294, 5.841, 5.455, 7.069,\n       2.859, 4.018, 2.513, 5.405, 5.737, 3.363, 4.89 , 1.385, 1.505,\n       6.081, 1.784, 1.378, 1.45 , 1.841, 1.37 , 2.612, 5.329, 5.33 ,\n       3.371, 1.281, 1.871, 2.153, 5.378, 4.45 , 1.46 , 1.436, 1.612,\n       3.19 , 2.752, 3.35 , 4.01 , 4.166, 2.642, 2.977, 3.415, 2.295,\n       3.019, 2.683, 5.165, 1.849, 1.836, 2.518, 2.43 , 4.528, 1.263,\n       1.885, 1.943, 1.899, 1.442, 1.953, 4.697, 1.582, 2.025, 1.841,\n       5.011, 1.212, 1.502, 2.516, 1.367, 2.089, 4.388, 1.854, 1.748,\n       2.978, 2.152, 2.362, 1.988, 1.426, 3.29 , 3.264, 1.436, 1.393,\n       2.822, 4.969, 5.659, 3.24 , 1.693, 1.647, 2.36 , 1.792, 3.45 ,\n       1.516, 2.233, 2.563, 5.283, 3.885, 0.966, 2.373, 2.663, 1.251,\n       2.052, 3.371, 2.093, 2.   , 3.883, 3.852, 3.718, 1.732, 3.928])\n\nIt is always a good idea to do some EDA ahead of our analysis. To this end, we will plot the fertility versus illiteracy and compute the Pearson correlation coefficient.\n\n\n\n\n\n\nNote\n\n\n\nThe Numpy array illiteracy has the illiteracy rate among females for most of the world’s nations.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe array fertility has the corresponding fertility data.\n\n\n\ndef pearson_r(x, y):\n    return np.corrcoef(x, y)[0,1]\n\n\n# Plot the illiteracy rate versus fertility\n_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n\n# Set the margins and label axes\nplt.margins(.02)\n_ = plt.xlabel('percent illiterate')\n_ = plt.ylabel('fertility')\n\n# Show the plot\nplt.show()\n\n# Show the Pearson correlation coefficient\npearson_r(illiteracy, fertility)\n\n\n\n\n0.8041324026815346\n\n\nWe can see the correlation between illiteracy and fertility by eye, and by the substantial Pearson correlation coefficient of 0.8. It is difficult to resolve in the scatter plot, but there are many points around near-zero illiteracy and about 1.8 children/woman.\n\n\nLinear regression\nWe will assume that fertility is a linear function of the female illiteracy rate. That is, \\(f=ai+b\\), where \\(a\\) is the slope and \\(b\\) is the intercept.\n\n\n\n\n\n\nNote\n\n\n\nWe can think of the intercept as the minimal fertility rate, probably somewhere between one and two.\n\n\nThe slope tells us how the fertility rate varies with illiteracy. We can find the best fit line using np.polyfit().\n\n# Plot the illiteracy rate versus fertility\n_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')\nplt.margins(0.02)\n_ = plt.xlabel('percent illiterate')\n_ = plt.ylabel('fertility')\n\n# Perform a linear regression using np.polyfit(): a, b\na, b = np.polyfit(illiteracy, fertility, deg=1)\n\n# Print the results to the screen\nprint('slope =', a, 'children per woman / percent illiterate')\nprint('intercept =', b, 'children per woman')\n\n# Make theoretical line to plot\nx = np.array([0, 100])\ny = a * x + b\n\n# Add regression line to your plot\n_ = plt.plot(x, y)\n\n# Draw the plot\nplt.show()\n\nslope = 0.04979854809063423 children per woman / percent illiterate\nintercept = 1.8880506106365562 children per woman\n\n\n\n\n\n\n\nHow is it optimal?\nThe function np.polyfit() that we used to get your regression parameters finds the optimal slope and intercept. It is optimizing the sum of the squares of the residuals, also known as RSS ( for residual sum of squares ).\nWe will plot the function that is being optimized, the RSS, versus the slope parameter \\(a\\). To do this, we will fix the intercept to be what we found in the optimization. Then, plot the RSS vs. the slope.\n\n# Specify slopes to consider: a_vals\na_vals = np.linspace(0,0.1, 200)\n\n# Initialize sum of square of residuals: rss\nrss = np.empty_like(a_vals)\n\n# Compute sum of square of residuals for each value of a_vals\nfor i, a in enumerate(a_vals):\n    rss[i] = np.sum((fertility - a*illiteracy - b)**2)\n\n# Plot the RSS\nplt.plot(a_vals, rss, '-')\nplt.xlabel('slope (children per woman / percent illiterate)')\nplt.ylabel('sum of square of residuals')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nthat the minimum on the plot, that is the value of the slope that gives the minimum sum of the square of the residuals, is the same value you got when performing the regression."
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#the-importance-of-eda-anscombes-quartet",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#the-importance-of-eda-anscombes-quartet",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "The importance of EDA: Anscombe’s quartet",
    "text": "The importance of EDA: Anscombe’s quartet\n\nLook before you leap!\n\nDo graphical EDA rst\n\n\n\nLinear regression on appropriate Anscombe data\nWe will perform a linear regression on the data set from Anscombe’s quartet that is most reasonably interpreted with linear regression.\n\nx = np.array([10.,  8., 13.,  9., 11., 14.,  6.,  4., 12.,  7.,  5.])\ny = np.array([ 8.04,  6.95,  7.58,  8.81,  8.33,  9.96,  7.24,  4.26, 10.84,\n        4.82,  5.68])\n\n\n# Perform linear regression: a, b\na, b = np.polyfit(x,y, deg=1)\n\n# Print the slope and intercept\nprint(a, b)\n\n# Generate theoretical x and y data: x_theor, y_theor\nx_theor = np.array([3, 15])\ny_theor = a * x_theor + b\n\n# Plot the Anscombe data and theoretical line\n_ = plt.plot(x, y, marker=\".\", linestyle=\"none\")\n_ = plt.plot(x_theor, y_theor)\n\n# Label the axes\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Show the plot\nplt.show()\n\n0.5000909090909091 3.000090909090908\n\n\n\n\n\n\n\nLinear regression on all Anscombe data\nNow, to verify that all four of the Anscombe data sets have the same slope and intercept from a linear regression, we will compute the slope and intercept for each set.\n\nanscombe_x = [np.array([10.,  8., 13.,  9., 11., 14.,  6.,  4., 12.,  7.,  5.]),\n np.array([10.,  8., 13.,  9., 11., 14.,  6.,  4., 12.,  7.,  5.]),\n np.array([10.,  8., 13.,  9., 11., 14.,  6.,  4., 12.,  7.,  5.]),\n np.array([ 8.,  8.,  8.,  8.,  8.,  8.,  8., 19.,  8.,  8.,  8.])]\n\n\nanscombe_y = [np.array([ 8.04,  6.95,  7.58,  8.81,  8.33,  9.96,  7.24,  4.26, 10.84,\n         4.82,  5.68]),\n np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.1 , 6.13, 3.1 , 9.13, 7.26, 4.74]),\n np.array([ 7.46,  6.77, 12.74,  7.11,  7.81,  8.84,  6.08,  5.39,  8.15,\n         6.42,  5.73]),\n np.array([ 6.58,  5.76,  7.71,  8.84,  8.47,  7.04,  5.25, 12.5 ,  5.56,\n         7.91,  6.89])]\n\nThe data are stored in lists;\n\n# Iterate through x,y pairs\nfor x, y in zip(anscombe_x, anscombe_y):\n    # Compute the slope and intercept: a, b\n    a, b = np.polyfit(x,y, deg=1)\n\n    # Print the result\n    print('slope:', a, 'intercept:', b)\n\nslope: 0.5000909090909091 intercept: 3.000090909090908\nslope: 0.5 intercept: 3.0009090909090905\nslope: 0.4997272727272729 intercept: 3.002454545454545\nslope: 0.4999090909090908 intercept: 3.001727272727274\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIndeed, they all have the same slope and intercept."
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#generating-bootstrap-replicates",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#generating-bootstrap-replicates",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Generating bootstrap replicates",
    "text": "Generating bootstrap replicates\n\nBootstrapping\n\nThe use of resampled data to perform statistical inference\n\n\n\nBootstrap sample\n\nA resampled array ofthe data\n\n\n\nBootstrap replicate\n\nA statistic computed from a resampled array\n\n\n\nGetting the terminology down\nIf we have a data set with \\(n\\) repeated measurements, a bootstrap sample is an array of length \\(n\\) that was drawn from the original data with replacement. bootstrap replicate is A single value of a statistic computed from a bootstrap sample.\n\n\nVisualizing bootstrap samples\nWe will generate bootstrap samples from the set of annual rainfall data measured at the Sheffield Weather Station in the UK from 1883 to 2015.\n\nrainfall = np.array([ 875.5,  648.2,  788.1,  940.3,  491.1,  743.5,  730.1,  686.5,\n        878.8,  865.6,  654.9,  831.5,  798.1,  681.8,  743.8,  689.1,\n        752.1,  837.2,  710.6,  749.2,  967.1,  701.2,  619. ,  747.6,\n        803.4,  645.6,  804.1,  787.4,  646.8,  997.1,  774. ,  734.5,\n        835. ,  840.7,  659.6,  828.3,  909.7,  856.9,  578.3,  904.2,\n        883.9,  740.1,  773.9,  741.4,  866.8,  871.1,  712.5,  919.2,\n        927.9,  809.4,  633.8,  626.8,  871.3,  774.3,  898.8,  789.6,\n        936.3,  765.4,  882.1,  681.1,  661.3,  847.9,  683.9,  985.7,\n        771.1,  736.6,  713.2,  774.5,  937.7,  694.5,  598.2,  983.8,\n        700.2,  901.3,  733.5,  964.4,  609.3, 1035.2,  718. ,  688.6,\n        736.8,  643.3, 1038.5,  969. ,  802.7,  876.6,  944.7,  786.6,\n        770.4,  808.6,  761.3,  774.2,  559.3,  674.2,  883.6,  823.9,\n        960.4,  877.8,  940.6,  831.8,  906.2,  866.5,  674.1,  998.1,\n        789.3,  915. ,  737.1,  763. ,  666.7,  824.5,  913.8,  905.1,\n        667.8,  747.4,  784.7,  925.4,  880.2, 1086.9,  764.4, 1050.1,\n        595.2,  855.2,  726.9,  785.2,  948.8,  970.6,  896. ,  618.4,\n        572.4, 1146.4,  728.2,  864.2,  793. ])\n\nThe data are stored in the NumPy array rainfall in units of millimeters (mm). By graphically displaying the bootstrap samples with an ECDF, we can get a feel for how bootstrap sampling allows probabilistic descriptions of data.\n\nfor _ in range(50):\n    # Generate bootstrap sample: bs_sample\n    bs_sample = np.random.choice(rainfall, size=len(rainfall))\n\n    # Compute and plot ECDF from bootstrap sample\n    x, y = ecdf(bs_sample)\n    _ = plt.plot(x, y, marker='.', linestyle='none',\n                 color='gray', alpha=0.1)\n\n# Compute and plot ECDF from original data\nx, y = ecdf(rainfall)\n_ = plt.plot(x, y, marker='.')\n\n# Make margins and label axes\nplt.margins(0.02)\n_ = plt.xlabel('yearly rainfall (mm)')\n_ = plt.ylabel('ECDF')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe bootstrap samples give an idea of how the distribution of rainfalls is spread."
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#bootstrap-confidence-intervals-1",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#bootstrap-confidence-intervals-1",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Bootstrap confidence intervals",
    "text": "Bootstrap confidence intervals\n\nCondence interval of a statistic\n\nIf we repeated measurements over and over again, \\(p\\%\\) of the observed values would lie within the \\(p\\%\\) condence interval.\n\n\n\nGenerating many bootstrap replicates\n\ndef bootstrap_replicate_1d(data, func):\n    \"\"\"Generate bootstrap replicate of 1D data.\"\"\"\n    return func(np.random.choice(data, size=len(data)))\n\nWe’ll write another function, draw_bs_reps(data, func, size=1), which generates many bootstrap replicates from the data set.\n\ndef draw_bs_reps(data, func, size=1):\n    \"\"\"Draw bootstrap replicates.\"\"\"\n\n    # Initialize array of replicates: bs_replicates\n    bs_replicates = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_replicates[i] = bootstrap_replicate_1d(data, func)\n\n    return bs_replicates\n\n\n\nBootstrap replicates of the mean and the SEM\nWe will compute a bootstrap estimate of the probability density function of the mean annual rainfall at the Sheffield Weather Station.\n\n\n\n\n\n\nNote\n\n\n\nwe are estimating the mean annual rainfall we would get if the Sheffield Weather Station could repeat all of the measurements from 1883 to 2015 over and over again. This is a probabilistic estimate of the mean.\n\n\nWe will plot the PDF as a histogram, and you will see that it is Normal.\nThe standard deviation of this distribution, called the standard error of the mean, or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)).\n\n# Take 10,000 bootstrap replicates of the mean: bs_replicates\nbs_replicates = draw_bs_reps(rainfall, np.mean, size=10000)\n\n# Compute and print SEM\nsem = np.std(rainfall) / np.sqrt(len(rainfall))\nprint(sem)\n\n# Compute and print standard deviation of bootstrap replicates\nbs_std = np.std(bs_replicates)\nprint(bs_std)\n\n# Make a histogram of the results\n_ = plt.hist(bs_replicates, bins=50, density=True)\n_ = plt.xlabel('mean annual rainfall (mm)')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n\n10.510549150506188\n10.358764199574097\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSEM we got from the known expression and the bootstrap replicates is the same and the distribution of the bootstrap replicates of the mean is Normal.\n\n\n\n\nConfidence intervals of rainfall data\nA confidence interval gives upper and lower bounds on the range of parameter values you might expect to get if we repeat our measurements. For named distributions, we can compute them analytically or look them up, but one of the many beautiful properties of the bootstrap method is that we can take percentiles of your bootstrap replicates to get your confidence interval. Conveniently, we can use the np.percentile() function.\n\nnp.percentile(bs_replicates, [2.5, 97.5])\n\narray([779.96900376, 820.62793233])\n\n\nit’s simple to get confidence intervals using bootstrap!\n\n\nBootstrap replicates of other statistics\nWe’ll generate bootstrap replicates for the variance of the annual rainfall at the Sheffield Weather Station and plot the histogram of the replicates.\n\n# Generate 10,000 bootstrap replicates of the variance: bs_replicates\nbs_replicates = draw_bs_reps(rainfall, np.var, size=10000)\n\n# Put the variance in units of square centimeters\nbs_replicates = bs_replicates/100\n\n# Make a histogram of the results\n_ = plt.hist(bs_replicates, density=True, bins=50)\n_ = plt.xlabel('variance of annual rainfall (sq. cm)')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is not normally distributed, as it has a longer tail to the right.\n\n\n\n\nConfidence interval on the rate of no-hitters\n\n# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates\nbs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000)\n\n# Compute the 95% confidence interval: conf_int\nconf_int = np.percentile(bs_replicates, [2.5, 97.5])\n\n# Print the confidence interval\nprint('95% confidence interval =', conf_int, 'games')\n\n# Plot the histogram of the replicates\n_ = plt.hist(bs_replicates, bins=50, density=True)\n_ = plt.xlabel(r'$\\tau$ (games)')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n\n95% confidence interval = [663.65229084 869.79741036] games\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis gives us an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games."
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#pairs-bootstrap",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#pairs-bootstrap",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Pairs bootstrap",
    "text": "Pairs bootstrap\n\nNonparametric inference\n\nMake no assumptions about the model or probability distribution underlying the data\n\n\n\nPairs bootstrap for linear regression\n\nResample data in pairs\nCompute slope and intercept from resampled data\nEach slope and intercept is a bootstrap replicate\nCompute condence intervals from percentiles of bootstrap replicates\n\n\n\nA function to do pairs bootstrap\npairs bootstrap involves resampling pairs of data. Each collection of pairs fit with a line, in this case using np.polyfit(). We do this again and again, getting bootstrap replicates of the parameter values. To have a useful tool for doing pairs bootstrap, we will write a function to perform pairs bootstrap on a set of x,y data.\n\ndef draw_bs_pairs_linreg(x, y, size=1):\n    \"\"\"Perform pairs bootstrap for linear regression.\"\"\"\n\n    # Set up array of indices to sample from: inds\n    inds = np.arange(len(x))\n\n    # Initialize replicates: bs_slope_reps, bs_intercept_reps\n    bs_slope_reps = np.empty(size)\n    bs_intercept_reps = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_inds = np.random.choice(inds, size=len(inds))\n        bs_x, bs_y = x[bs_inds], y[bs_inds]\n        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, deg=1)\n\n    return bs_slope_reps, bs_intercept_reps\n\n\n\nPairs bootstrap of literacy/fertility data\nUsing the function we just wrote, we’ll perform pairs bootstrap to plot a histogram describing the estimate of the slope from the illiteracy/fertility data. Also reporting the 95% confidence interval of the slope.\n\n# Generate replicates of slope and intercept using pairs bootstrap\nbs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000)\n\n# Compute and print 95% CI for slope\nprint(np.percentile(bs_slope_reps, [2.5, 97.5]))\n\n# Plot the histogram\n_ = plt.hist(bs_slope_reps, bins=50, density=True)\n_ = plt.xlabel('slope')\n_ = plt.ylabel('PDF')\nplt.show()\n\n[0.04389859 0.05528877]\n\n\n\n\n\n\n\nPlotting bootstrap regressions\nA nice way to visualize the variability we might expect in a linear regression is to plot the line we would get from each bootstrap replicate of the slope and intercept. We’ll do this for the first 100 of our bootstrap replicates of the slope and intercept\n\n# Generate array of x-values for bootstrap lines: x\nx = np.array([0,100])\n\n# Plot the bootstrap lines\nfor i in range(100):\n    _ = plt.plot(x, \n                 bs_slope_reps[i]*x + bs_intercept_reps[i],\n                 linewidth=0.5, alpha=0.2, color='red')\n\n# Plot the data\n_ = plt.plot(illiteracy, fertility, marker=\".\", linestyle=\"none\")\n\n# Label axes, set the margins, and show the plot\n_ = plt.xlabel('illiteracy')\n_ = plt.ylabel('fertility')\nplt.margins(0.02)\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#formulating-and-simulating-a-hypothesis",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#formulating-and-simulating-a-hypothesis",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Formulating and simulating a hypothesis",
    "text": "Formulating and simulating a hypothesis\n\nHypothesis testing\n\nAssessment of how reasonable the observed data are assuming a hypothesis is true\n\n\n\nNull hypothesis\n\nAnother name for the hypothesis you are testing\n\n\n\nPermutation\n\nRandom reordering of entries in an array\n\n\n\nGenerating a permutation sample\npermutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so we will write a function to generate a permutation sample from two data sets.\n\n\n\n\n\n\nNote\n\n\n\na permutation sample of two arrays having respectively n1 and n2 entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first n1 entries as the permutation sample of the first array and the last n2 entries as the permutation sample of the second array.\n\n\n\ndef permutation_sample(data1, data2):\n    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n\n    # Concatenate the data sets: data\n    data = np.concatenate((data1,data2))\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2\n\n\n\nVisualizing permutation sampling\nTo help see how permutation sampling works, we will generate permutation samples and look at them graphically.\nWe will use the Sheffield Weather Station data again, this time considering the monthly rainfall in June (a dry month) and November (a wet month). We expect these might be differently distributed, so we will take permutation samples to see how their ECDFs would look if they were identically distributed.\n\nrain_june = np.array([ 66.2,  39.7,  76.4,  26.5,  11.2,  61.8,   6.1,  48.4,  89.2,\n       104. ,  34. ,  60.6,  57.1,  79.1,  90.9,  32.3,  63.8,  78.2,\n        27.5,  43.4,  30.1,  17.3,  77.5,  44.9,  92.2,  39.6,  79.4,\n        66.1,  53.5,  98.5,  20.8,  55.5,  39.6,  56. ,  65.1,  14.8,\n        13.2,  88.1,   8.4,  32.1,  19.6,  40.4,   2.2,  77.5, 105.4,\n        77.2,  38. ,  27.1, 111.8,  17.2,  26.7,  23.3,  77.2,  87.2,\n        27.7,  50.6,  60.3,  15.1,   6. ,  29.4,  39.3,  56.3,  80.4,\n        85.3,  68.4,  72.5,  13.3,  28.4,  14.7,  37.4,  49.5,  57.2,\n        85.9,  82.1,  31.8, 126.6,  30.7,  41.4,  33.9,  13.5,  99.1,\n        70.2,  91.8,  61.3,  13.7,  54.9,  62.5,  24.2,  69.4,  83.1,\n        44. ,  48.5,  11.9,  16.6,  66.4,  90. ,  34.9, 132.8,  33.4,\n       225. ,   7.6,  40.9,  76.5,  48. , 140. ,  55.9,  54.1,  46.4,\n        68.6,  52.2, 108.3,  14.6,  11.3,  29.8, 130.9, 152.4,  61. ,\n        46.6,  43.9,  30.9, 111.1,  68.5,  42.2,   9.8, 285.6,  56.7,\n       168.2,  41.2,  47.8, 166.6,  37.8,  45.4,  43.2])\n\n\nrain_november = np.array([ 83.6,  30.9,  62.2,  37. ,  41. , 160.2,  18.2, 122.4,  71.3,\n        44.2,  49.1,  37.6, 114.5,  28.8,  82.5,  71.9,  50.7,  67.7,\n       112. ,  63.6,  42.8,  57.2,  99.1,  86.4,  84.4,  38.1,  17.7,\n       102.2, 101.3,  58. ,  82. , 101.4,  81.4, 100.1,  54.6,  39.6,\n        57.5,  29.2,  48.8,  37.3, 115.4,  55.6,  62. ,  95. ,  84.2,\n       118.1, 153.2,  83.4, 104.7,  59. ,  46.4,  50. , 147.6,  76.8,\n        59.9, 101.8, 136.6, 173. ,  92.5,  37. ,  59.8, 142.1,   9.9,\n       158.2,  72.6,  28. , 112.9, 119.3, 199.2,  50.7,  44. , 170.7,\n        67.2,  21.4,  61.3,  15.6, 106. , 116.2,  42.3,  38.5, 132.5,\n        40.8, 147.5,  93.9,  71.4,  87.3, 163.7, 141.4,  62.6,  84.9,\n        28.8, 121.1,  28.6,  32.4, 112. ,  50. ,  96.9,  81.8,  70.4,\n       117.5,  41.2, 124.9,  78.2,  93. ,  53.5,  50.5,  42.6,  47.9,\n        73.1, 129.1,  56.9, 103.3,  60.5, 134.3,  93.1,  49.5,  48.2,\n       167.9,  27. , 111.1,  55.4,  36.2,  57.4,  66.8,  58.3,  60. ,\n       161.6, 112.7,  37.4, 110.6,  56.6,  95.8, 126.8])\n\n\nfor _ in range(50):\n    # Generate permutation samples\n    perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)\n\n\n    # Compute ECDFs\n    x_1, y_1 = ecdf(perm_sample_1)\n    x_2, y_2 = ecdf(perm_sample_2)\n\n    # Plot ECDFs of permutation sample\n    _ = plt.plot(x_1, y_1, marker='.', linestyle='none',\n                 color='red', alpha=0.02)\n    _ = plt.plot(x_2, y_2, marker='.', linestyle='none',\n                 color='blue', alpha=0.02)\n\n# Create and plot ECDFs from original data\nx_1, y_1 = ecdf(rain_june)\nx_2, y_2 = ecdf(rain_november)\n_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')\n_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')\n\n# Label axes, set margin, and show plot\nplt.margins(0.02)\n_ = plt.xlabel('monthly rainfall (mm)')\n_ = plt.ylabel('ECDF')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed."
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#test-statistics-and-p-values",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#test-statistics-and-p-values",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Test statistics and p-values",
    "text": "Test statistics and p-values\n\nHypothesis testing\n\nAssessment of how reasonable the observed data are assuming a hypothesis is true\n\n\n\nTest statistic\n\nA single number that can be computed from observed data and from data you simulate under the null hypothesis\nIt serves as a basis of comparison between the two\n\n\n\np-value\n\nThe probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true\nNOT the probability that the null hypothesis is true\n\n\n\nStatistical signicance\n\nDetermined by the smallness of a p-value\n\n\n\nNull hypothesis signicance testing (NHST)\n\nAnother name for Hypothesis testing\n\n\n\nTest statistics\nWhen performing hypothesis tests, the choice of test statistic should be pertinent to the question you are seeking to answer in your hypothesis test.\n\n\n\n\n\n\nImportant\n\n\n\nThe most important thing to consider is: What are you asking?\n\n\n\n\np-value\nThe p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true.\n\n\nGenerating permutation replicates\na permutation replicate is a single value of a statistic computed from a permutation sample.\nAs the draw_bs_reps() function is useful for generating bootstrap replicates, it is useful to have a similar function, draw_perm_reps(), to generate permutation replicates.\n\ndef draw_perm_reps(data_1, data_2, func, size=1):\n    \"\"\"Generate multiple permutation replicates.\"\"\"\n\n    # Initialize array of replicates: perm_replicates\n    perm_replicates = np.empty(size)\n\n    for i in range(size):\n        # Generate permutation sample\n        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n\n        # Compute the test statistic\n        perm_replicates[i] = func(perm_sample_1,perm_sample_2)\n\n    return perm_replicates\n\n\n\nLook before you leap: EDA before hypothesis testing\nKleinteich and Gorb (Sci. Rep., 4, 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog’s tongue when it struck the target.\nFrog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. we will test the hypothesis that the two frogs have the same distribution of impact forces. it is important to do EDA first! Let’s make a bee swarm plot for the data.\nThey are stored in a Pandas data frame, frogs, where column ID is the identity of the frog and column impact_force is the impact force in Newtons (N).\n\nfrogs = pd.read_csv(\"datasets/frogs.csv\")\nfrogs.head()\n\n\n\n\n\n  \n    \n      \n      ID\n      impact_force\n    \n  \n  \n    \n      0\n      A\n      1.612\n    \n    \n      1\n      A\n      0.605\n    \n    \n      2\n      A\n      0.327\n    \n    \n      3\n      A\n      0.946\n    \n    \n      4\n      A\n      0.541\n    \n  \n\n\n\n\n\n# Make bee swarm plot\n_ = sns.swarmplot(data=frogs, x=\"ID\", y=\"impact_force\")\n\n# Label axes\n_ = plt.xlabel('frog')\n_ = plt.ylabel('impact force (N)')\n\n# Show the plot\nplt.show()\n\n\n\n\nEyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test.\n\n\nPermutation test on frog data\n\nfrogs[frogs.ID==\"A\"].impact_force.mean()\n\n0.70735\n\n\n\nfrogs[frogs.ID==\"B\"].impact_force.mean()\n\n0.4191000000000001\n\n\n\nfrogs[frogs.ID==\"A\"].impact_force.mean() - frogs[frogs.ID==\"B\"].impact_force.mean()\n\n0.28824999999999995\n\n\nThe average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance.\n\nforce_a = np.array([1.612, 0.605, 0.327, 0.946, 0.541, 1.539, 0.529, 0.628, 1.453,\n       0.297, 0.703, 0.269, 0.751, 0.245, 1.182, 0.515, 0.435, 0.383,\n       0.457, 0.73 ])\nforce_b = np.array([0.172, 0.142, 0.037, 0.453, 0.355, 0.022, 0.502, 0.273, 0.72 ,\n       0.582, 0.198, 0.198, 0.597, 0.516, 0.815, 0.402, 0.605, 0.711,\n       0.614, 0.468])\n\nWe will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis.\n\ndef diff_of_means(data_1, data_2):\n    \"\"\"Difference in means of two arrays.\"\"\"\n\n    # The difference of means of data_1, data_2: diff\n    diff = np.mean(data_1) - np.mean(data_2)\n\n    return diff\n\n\n\n# Compute difference of mean impact force from experiment: empirical_diff_means\nempirical_diff_means = diff_of_means(force_a, force_b)\n\nempirical_diff_means\n\n0.28825000000000006\n\n\n\n\n# Draw 10,000 permutation replicates: perm_replicates\nperm_replicates = draw_perm_reps(force_a, force_b,\n                                 diff_of_means, size=10000)\n\n# Compute p-value: p\np = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)\n\n# Print the result\nprint('p-value =', p)\n\np-value = 0.0058\n\n\nThe p-value tells us that there is about a 0.6% chance that we would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be “statistically significant,” but:\n\n\n\n\n\n\nWarning\n\n\n\nWarning! warning! warning! We have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be “statistically significant,” but they are definitely not the same!"
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#bootstrap-hypothesis-tests",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#bootstrap-hypothesis-tests",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Bootstrap hypothesis tests",
    "text": "Bootstrap hypothesis tests\n\nPipeline for hypothesis testing - Clearly state the null hypothesis - Define your test statistic - Generate many sets of simulated data assuming the null hypothesis is true - Compute the test statistic for each simulated data set - The p-value is the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data\n\n\nA one-sample bootstrap hypothesis test\nAnother juvenile frog was studied, Frog C, and we want to see if Frog B and Frog C have similar impact forces. Unfortunately, we do not have Frog C’s impact forces available, but we know they have a mean of 0.55 N. Because we don’t have the original data, we cannot do a permutation test, and we cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. We will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C.\nTo set up the bootstrap hypothesis test, we will take the mean as our test statistic. Our goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B’s impact forces is equal to that of Frog C is true. We will first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B’s distribution, such as the variance, unchanged.\n\n# Make an array of translated impact forces: translated_force_b\ntranslated_force_b = force_b - np.mean(force_b) + 0.55\n\n# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates\nbs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)\n\n# Compute fraction of replicates that are less than the observed Frog B force: p\np = np.sum(bs_replicates <= np.mean(force_b)) / 10000\n\n# Print the p-value\nprint('p = ', p)\n\np =  0.0046\n\n\nThe low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false.\n\n\nA two-sample bootstrap hypothesis test for difference of means\nWe now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test.\nTo do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed.\n\nforces_concat = np.array([1.612, 0.605, 0.327, 0.946, 0.541, 1.539, 0.529, 0.628, 1.453,\n       0.297, 0.703, 0.269, 0.751, 0.245, 1.182, 0.515, 0.435, 0.383,\n       0.457, 0.73 , 0.172, 0.142, 0.037, 0.453, 0.355, 0.022, 0.502,\n       0.273, 0.72 , 0.582, 0.198, 0.198, 0.597, 0.516, 0.815, 0.402,\n       0.605, 0.711, 0.614, 0.468])\n\n\nempirical_diff_means = 0.28825000000000006"
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#ab-testing",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#ab-testing",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "A/B testing",
    "text": "A/B testing\n\nA/B test\n\nUsed by organizations to see if a strategy change gives a better result\n\n\n\nNull hypothesis of an A/B test\n\nThe test statistic is impervious to the change\n\n\n\nThe vote for the Civil Rights Act in 1964\nThe Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding “present” and “abstain” votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote?\nTo answer this question, we will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. We will use the fraction of Democrats voting in favor as our test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That’s right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, we will permute the party labels of the House voters and then arbitrarily divide them into “Democrats” and “Republicans” and compute the fraction of Democrats voting yea.\n\n# Construct arrays of data: dems, reps\ndems = np.array([True] * 153 + [False] * 91)\nreps = np.array([True]*136 + [False]*35)\n\ndef frac_yea_dems(dems, reps):\n    \"\"\"Compute fraction of Democrat yea votes.\"\"\"\n    frac = np.sum(dems) / len(dems)\n    return frac\n\n# Acquire permutation samples: perm_replicates\nperm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000)\n\n# Compute and print p-value: p\np = np.sum(perm_replicates <= 153/244) / len(perm_replicates)\nprint('p-value =', p)\n\np-value = 0.0002\n\n\nThis small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias.\n\n\nA time-on-website analog\nIt turns out that we already did a hypothesis test analogous to an A/B test where we are interested in how much time is spent on the website before and after an ad campaign. The frog tongue force (a continuous quantity like time on the website) is an analog. “Before” = Frog A and “after” = Frog B. Let’s practice this again with something that actually is a before/after scenario.\nWe return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem we will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as our test statistic. The inter-no-hitter times for the respective eras are stored in the arrays nht_dead and nht_live, where “nht” is meant to stand for “no-hitter time.”\n\nnht_dead = np.array([  -1,  894,   10,  130,    1,  934,   29,    6,  485,  254,  372,\n         81,  191,  355,  180,  286,   47,  269,  361,  173,  246,  492,\n        462, 1319,   58,  297,   31, 2970,  640,  237,  434,  570,   77,\n        271,  563, 3365,   89,    0,  379,  221,  479,  367,  628,  843,\n       1613, 1101,  215,  684,  814,  278,  324,  161,  219,  545,  715,\n        966,  624,   29,  450,  107,   20,   91, 1325,  124, 1468,  104,\n       1309,  429,   62, 1878, 1104,  123,  251,   93,  188,  983,  166,\n         96,  702,   23,  524,   26,  299,   59,   39,   12,    2,  308,\n       1114,  813,  887])\n\n\nnht_live = np.array([ 645, 2088,   42, 2090,   11,  886, 1665, 1084, 2900, 2432,  750,\n       4021, 1070, 1765, 1322,   26,  548, 1525,   77, 2181, 2752,  127,\n       2147,  211,   41, 1575,  151,  479,  697,  557, 2267,  542,  392,\n         73,  603,  233,  255,  528,  397, 1529, 1023, 1194,  462,  583,\n         37,  943,  996,  480, 1497,  717,  224,  219, 1531,  498,   44,\n        288,  267,  600,   52,  269, 1086,  386,  176, 2199,  216,   54,\n        675, 1243,  463,  650,  171,  327,  110,  774,  509,    8,  197,\n        136,   12, 1124,   64,  380,  811,  232,  192,  731,  715,  226,\n        605,  539, 1491,  323,  240,  179,  702,  156,   82, 1397,  354,\n        778,  603, 1001,  385,  986,  203,  149,  576,  445,  180, 1403,\n        252,  675, 1351, 2983, 1568,   45,  899, 3260, 1025,   31,  100,\n       2055, 4043,   79,  238, 3931, 2351,  595,  110,  215,    0,  563,\n        206,  660,  242,  577,  179,  157,  192,  192, 1848,  792, 1693,\n         55,  388,  225, 1134, 1172, 1555,   31, 1582, 1044,  378, 1687,\n       2915,  280,  765, 2819,  511, 1521,  745, 2491,  580, 2072, 6450,\n        578,  745, 1075, 1103, 1549, 1520,  138, 1202,  296,  277,  351,\n        391,  950,  459,   62, 1056, 1128,  139,  420,   87,   71,  814,\n        603, 1349,  162, 1027,  783,  326,  101,  876,  381,  905,  156,\n        419,  239,  119,  129,  467])\n\n\n# Compute the observed difference in mean inter-no-hitter times: nht_diff_obs\nnht_diff_obs = diff_of_means(nht_dead, nht_live)\n\n# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates\nperm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000)\n\n\n# Compute and print the p-value: p\np = sum(perm_replicates <= nht_diff_obs)/len(perm_replicates)\nprint('p-val =', p)\n\np-val = 0.0002\n\n\nour p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance.\n\n\n\n\n\n\nWarning\n\n\n\nWatch out, though, we could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001.\n\n\n\nx_dead, y_dead = ecdf(nht_dead)\nx_live, y_live = ecdf(nht_live)\n\n_ = plt.plot(x_dead, y_dead)\n_ = plt.plot(x_live, y_live)\n_ = plt.xlabel(\"non hitter times\")\n_ = plt.legend([\"nht dead\", \"nht live\"])\n_ = plt.ylabel(\"CDF\")\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#test-of-correlation",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#test-of-correlation",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Test of correlation",
    "text": "Test of correlation\n\nHypothesis test of correlation\n\nPosit null hypothesis:the two variables are completely uncorrelated\nSimulate data assuming null hypothesis is true\nUse Pearson correlation, \\(\\rho\\), as test statistic\nCompute p-value as fraction of replicates that have ρ at least as large as observed.\n\n\n\nSimulating a null hypothesis concerning correlation\nThe observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. We will test this null hypothesis.\nTo do the test, we need to simulate the data assuming the null hypothesis is true. The best way to it is to Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data.\n\n\nHypothesis test on Pearson correlation\nThe observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. We will test this hypothesis. To do so, we’ll permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, we’ll compute the Pearson correlation coefficient and assess how many of the permutation replicates have a Pearson correlation coefficient greater than the observed one.\n\n# Compute observed correlation: r_obs\nr_obs = pearson_r(illiteracy, fertility)\n\n# Initialize permutation replicates: perm_replicates\nperm_replicates = np.empty(10000)\n\n# Draw replicates\nfor i in range(10000):\n    # Permute illiteracy measurments: illiteracy_permuted\n    illiteracy_permuted = np.random.permutation(illiteracy)\n\n    # Compute Pearson correlation\n    perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)\n\n# Compute p-value: p\np = np.sum(perm_replicates >= r_obs) / len(perm_replicates)\nprint('p-val =', p)\n\np-val = 0.0\n\n\nWe got a p-value of zero. In hacker statistics, this means that the p-value is very low, since we never got a single replicate in the 10,000 we took that had a Pearson correlation greater than the observed one. we could try increasing the number of replicates you take to continue to move the upper bound on the p-value lower and lower.\n\n\nDo neonicotinoid insecticides have unintended consequences?\nWe will investigate the effects of neonicotinoid insecticides on bee reproduction. These insecticides are very widely used in the United States to combat aphids and other pests that damage plants.\nIn a recent study, Straub, et al. (Proc. Roy. Soc. B, 2016) investigated the effects of neonicotinoids on the sperm of pollinating bees. In this and the next exercise, you will study how the pesticide treatment affected the count of live sperm per half milliliter of semen.\nFirst, we will do EDA, as usual. Plot ECDFs of the alive sperm count for untreated bees (stored in the Numpy array control) and bees treated with pesticide (stored in the Numpy array treated).\n\ncontrol = np.array([ 4.159234,  4.408002,  0.172812,  3.498278,  3.104912,  5.164174,\n        6.615262,  4.633066,  0.170408,  2.65    ,  0.0875  ,  1.997148,\n        6.92668 ,  4.574932,  3.896466,  5.209814,  3.70625 ,  0.      ,\n        4.62545 ,  3.01444 ,  0.732652,  0.4     ,  6.518382,  5.225   ,\n        6.218742,  6.840358,  1.211308,  0.368252,  3.59937 ,  4.212158,\n        6.052364,  2.115532,  6.60413 ,  5.26074 ,  6.05695 ,  6.481172,\n        3.171522,  3.057228,  0.218808,  5.215112,  4.465168,  2.28909 ,\n        3.732572,  2.17087 ,  1.834326,  6.074862,  5.841978,  8.524892,\n        4.698492,  2.965624,  2.324206,  3.409412,  4.830726,  0.1     ,\n        0.      ,  4.101432,  3.478162,  1.009688,  4.999296,  4.32196 ,\n        0.299592,  3.606032,  7.54026 ,  4.284024,  0.057494,  6.036668,\n        2.924084,  4.150144,  1.256926,  4.666502,  4.806594,  2.52478 ,\n        2.027654,  2.52283 ,  4.735598,  2.033236,  0.      ,  6.177294,\n        2.601834,  3.544408,  3.6045  ,  5.520346,  4.80698 ,  3.002478,\n        3.559816,  7.075844, 10.      ,  0.139772,  6.17171 ,  3.201232,\n        8.459546,  0.17857 ,  7.088276,  5.496662,  5.415086,  1.932282,\n        3.02838 ,  7.47996 ,  1.86259 ,  7.838498,  2.242718,  3.292958,\n        6.363644,  4.386898,  8.47533 ,  4.156304,  1.463956,  4.533628,\n        5.573922,  1.29454 ,  7.547504,  3.92466 ,  5.820258,  4.118522,\n        4.125   ,  2.286698,  0.591882,  1.273124,  0.      ,  0.      ,\n        0.      , 12.22502 ,  7.601604,  5.56798 ,  1.679914,  8.77096 ,\n        5.823942,  0.258374,  0.      ,  5.899236,  5.486354,  2.053148,\n        3.25541 ,  2.72564 ,  3.364066,  2.43427 ,  5.282548,  3.963666,\n        0.24851 ,  0.347916,  4.046862,  5.461436,  4.066104,  0.      ,\n        0.065   ])\n\n\ntreated = np.array([1.342686, 1.058476, 3.793784, 0.40428 , 4.528388, 2.142966,\n       3.937742, 0.1375  , 6.919164, 0.      , 3.597812, 5.196538,\n       2.78955 , 2.3229  , 1.090636, 5.323916, 1.021618, 0.931836,\n       2.78    , 0.412202, 1.180934, 2.8674  , 0.      , 0.064354,\n       3.008348, 0.876634, 0.      , 4.971712, 7.280658, 4.79732 ,\n       2.084956, 3.251514, 1.9405  , 1.566192, 0.58894 , 5.219658,\n       0.977976, 3.124584, 1.297564, 1.433328, 4.24337 , 0.880964,\n       2.376566, 3.763658, 1.918426, 3.74    , 3.841726, 4.69964 ,\n       4.386876, 0.      , 1.127432, 1.845452, 0.690314, 4.185602,\n       2.284732, 7.237594, 2.185148, 2.799124, 3.43218 , 0.63354 ,\n       1.142496, 0.586   , 2.372858, 1.80032 , 3.329306, 4.028804,\n       3.474156, 7.508752, 2.032824, 1.336556, 1.906496, 1.396046,\n       2.488104, 4.759114, 1.07853 , 3.19927 , 3.814252, 4.275962,\n       2.817056, 0.552198, 3.27194 , 5.11525 , 2.064628, 0.      ,\n       3.34101 , 6.177322, 0.      , 3.66415 , 2.352582, 1.531696])\n\n\n# Compute x,y values for ECDFs\nx_control, y_control = ecdf(control)\nx_treated, y_treated = ecdf(treated)\n\n# Plot the ECDFs\nplt.plot(x_control, y_control, marker='.', linestyle='none')\nplt.plot(x_treated, y_treated, marker='.', linestyle='none')\n\n# Set the margins\nplt.margins(0.02)\n\n# Add a legend\nplt.legend(('control', 'treated'), loc='lower right')\n\n# Label axes and show plot\nplt.xlabel('millions of alive sperm per mL')\nplt.ylabel('ECDF')\nplt.show()\n\n\n\n\nThe ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm. Let’s now do a hypothesis test.\n\n\nBootstrap hypothesis test on bee sperm counts\nWe will test the following hypothesis: On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. We will use the difference of means as the test statistic.\n\n# Compute the difference in mean sperm count: diff_means\ndiff_means = np.mean(control) - np.mean(treated)\n\n# Compute mean of pooled data: mean_count\nmean_count = np.mean(np.concatenate((control, treated)))\n\n# Generate shifted data sets\ncontrol_shifted = control - np.mean(control) + mean_count\ntreated_shifted = treated - np.mean(treated) + mean_count\n\n# Generate bootstrap replicates\nbs_reps_control = draw_bs_reps(control_shifted,\n                       np.mean, size=10000)\nbs_reps_treated = draw_bs_reps(treated_shifted,\n                       np.mean, size=10000)\n\n# Get replicates of difference of means: bs_replicates\nbs_replicates = bs_reps_control - bs_reps_treated\n\n# Compute and print p-value: p\np = np.sum(bs_replicates >= np.mean(control) - np.mean(treated)) \\\n            / len(bs_replicates)\nprint('p-value =', p)\n\np-value = 0.0\n\n\nThe p-value is small, most likely less than 0.0001, since we never saw a bootstrap replicated with a difference of means at least as extreme as what was observed."
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#variation-in-beak-shapes",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#variation-in-beak-shapes",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Variation in beak shapes",
    "text": "Variation in beak shapes\n\nEDA of beak length and depth\n\nbl_1975 = np.array([13.9 , 14.  , 12.9 , 13.5 , 12.9 , 14.6 , 13.  , 14.2 , 14.  ,\n       14.2 , 13.1 , 15.1 , 13.5 , 14.4 , 14.9 , 12.9 , 13.  , 14.9 ,\n       14.  , 13.8 , 13.  , 14.75, 13.7 , 13.8 , 14.  , 14.6 , 15.2 ,\n       13.5 , 15.1 , 15.  , 12.8 , 14.9 , 15.3 , 13.4 , 14.2 , 15.1 ,\n       15.1 , 14.  , 13.6 , 14.  , 14.  , 13.9 , 14.  , 14.9 , 15.6 ,\n       13.8 , 14.4 , 12.8 , 14.2 , 13.4 , 14.  , 14.8 , 14.2 , 13.5 ,\n       13.4 , 14.6 , 13.5 , 13.7 , 13.9 , 13.1 , 13.4 , 13.8 , 13.6 ,\n       14.  , 13.5 , 12.8 , 14.  , 13.4 , 14.9 , 15.54, 14.63, 14.73,\n       15.73, 14.83, 15.94, 15.14, 14.23, 14.15, 14.35, 14.95, 13.95,\n       14.05, 14.55, 14.05, 14.45, 15.05, 13.25])\n\n\nbl_2012 = np.array([14.3 , 12.5 , 13.7 , 13.8 , 12.  , 13.  , 13.  , 13.6 , 12.8 ,\n       13.6 , 12.95, 13.1 , 13.4 , 13.9 , 12.3 , 14.  , 12.5 , 12.3 ,\n       13.9 , 13.1 , 12.5 , 13.9 , 13.7 , 12.  , 14.4 , 13.5 , 13.8 ,\n       13.  , 14.9 , 12.5 , 12.3 , 12.8 , 13.4 , 13.8 , 13.5 , 13.5 ,\n       13.4 , 12.3 , 14.35, 13.2 , 13.8 , 14.6 , 14.3 , 13.8 , 13.6 ,\n       12.9 , 13.  , 13.5 , 13.2 , 13.7 , 13.1 , 13.2 , 12.6 , 13.  ,\n       13.9 , 13.2 , 15.  , 13.37, 11.4 , 13.8 , 13.  , 13.  , 13.1 ,\n       12.8 , 13.3 , 13.5 , 12.4 , 13.1 , 14.  , 13.5 , 11.8 , 13.7 ,\n       13.2 , 12.2 , 13.  , 13.1 , 14.7 , 13.7 , 13.5 , 13.3 , 14.1 ,\n       12.5 , 13.7 , 14.6 , 14.1 , 12.9 , 13.9 , 13.4 , 13.  , 12.7 ,\n       12.1 , 14.  , 14.9 , 13.9 , 12.9 , 14.6 , 14.  , 13.  , 12.7 ,\n       14.  , 14.1 , 14.1 , 13.  , 13.5 , 13.4 , 13.9 , 13.1 , 12.9 ,\n       14.  , 14.  , 14.1 , 14.7 , 13.4 , 13.8 , 13.4 , 13.8 , 12.4 ,\n       14.1 , 12.9 , 13.9 , 14.3 , 13.2 , 14.2 , 13.  , 14.6 , 13.1 ,\n       15.2 ])\n\nThe beak length data are stored as bl_1975 and bl_2012, again with units of millimeters (mm). We still have the beak depth data stored in bd_1975 and bd_2012. We will make scatter plots of beak depth (y-axis) versus beak length (x-axis) for the 1975 and 2012 specimens.\n\n# Make scatter plot of 1975 data\n_ = plt.plot(bl_1975, bd_1975, marker='.',\n             linestyle='None', alpha=0.5, color=\"blue\")\n\n# Make scatter plot of 2012 data\n_ = plt.plot(bl_2012, bd_2012, marker='.',\n            linestyle='None', alpha=0.5, color=\"red\")\n\n# Label axes and make legend\n_ = plt.xlabel('beak length (mm)')\n_ = plt.ylabel('beak depth (mm)')\n_ = plt.legend(('1975', '2012'), loc='upper left')\n\n# Show the plot\nplt.show()\n\n\n\n\nIn looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper.\n\n\nLinear regressions\nWe perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line.\n\n# Compute the linear regressions\nslope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975.values, deg=1)\nslope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012.values, deg=1)\n\n# Perform pairs bootstrap for the linear regressions\nbs_slope_reps_1975, bs_intercept_reps_1975 = \\\n        draw_bs_pairs_linreg(bl_1975, bd_1975.values, size=1000)\nbs_slope_reps_2012, bs_intercept_reps_2012 = \\\n        draw_bs_pairs_linreg(bl_2012, bd_2012.values, size=1000)\n\n# Compute confidence intervals of slopes\nslope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5])\nslope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5])\nintercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5])\n\nintercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5])\n\n\n# Print the results\nprint('1975: slope =', slope_1975,\n      'conf int =', slope_conf_int_1975)\nprint('1975: intercept =', intercept_1975,\n      'conf int =', intercept_conf_int_1975)\nprint('2012: slope =', slope_2012,\n      'conf int =', slope_conf_int_2012)\nprint('2012: intercept =', intercept_2012,\n      'conf int =', intercept_conf_int_2012)\n\n1975: slope = 0.46520516916059357 conf int = [0.3254032  0.58811989]\n1975: intercept = 2.3908752365842285 conf int = [0.67000784 4.40072329]\n2012: slope = 0.4626303588353126 conf int = [0.33989291 0.60535488]\n2012: intercept = 2.9772474982360184 conf int = [1.03792858 4.62029259]\n\n\nIt looks like they have the same slope, but different intercepts.\n\n\nDisplaying the linear regression results\n\n# Make scatter plot of 1975 data\n_ = plt.plot(bl_1975, bd_1975, marker='.',\n             linestyle='none', color='blue', alpha=0.5)\n\n# Make scatter plot of 2012 data\n_ = plt.plot(bl_2012, bd_2012, marker='.',\n             linestyle='none', color='red', alpha=0.5)\n\n# Label axes and make legend\n_ = plt.xlabel('beak length (mm)')\n_ = plt.ylabel('beak depth (mm)')\n_ = plt.legend(('1975', '2012'), loc='upper left')\n\n# Generate x-values for bootstrap lines: x\nx = np.array([10, 17])\n\n# Plot the bootstrap lines\nfor i in range(100):\n    plt.plot(x, x* bs_slope_reps_1975[i] + bs_intercept_reps_1975[i],\n             linewidth=0.5, alpha=0.2, color=\"blue\")\n    plt.plot(x, x*bs_slope_reps_2012[i] + bs_intercept_reps_2012[i],\n             linewidth=0.5, alpha=0.2, color=\"red\")\n\n# Draw the plot again\nplt.show()\n\n\n\n\n\n\nBeak length to depth ratio\nThe linear regressions showed interesting information about the beak geometry. The slope was the same in 1975 and 2012, suggesting that for every millimeter gained in beak length, the birds gained about half a millimeter in depth in both years. However, if we are interested in the shape of the beak, we want to compare the ratio of beak length to beak depth. Let’s make that comparison.\n\n# Compute length-to-depth ratios\nratio_1975 = bl_1975/bd_1975\nratio_2012 = bl_2012/bd_2012\n\n# Compute means\nmean_ratio_1975 = np.mean(ratio_1975)\nmean_ratio_2012 = np.mean(ratio_2012)\n\n# Generate bootstrap replicates of the means\nbs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, size=10000)\nbs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, size=10000)\n\n# Compute the 99% confidence intervals\nconf_int_1975 = np.percentile(bs_replicates_1975, [.5, 99.5])\nconf_int_2012 = np.percentile(bs_replicates_2012, [.5, 99.5])\n\n# Print the results\nprint('1975: mean ratio =', mean_ratio_1975,\n      'conf int =', conf_int_1975)\nprint('2012: mean ratio =', mean_ratio_2012,\n      'conf int =', conf_int_2012)\n\n1975: mean ratio = 1.5788823771858533 conf int = [1.55672661 1.60112763]\n2012: mean ratio = 1.4658342276847767 conf int = [1.44471693 1.48753163]\n\n\n\n_ = sns.pointplot([np.median(conf_int_1975), np.median(conf_int_2012)], [1975, 2012])\nplt.show()"
  },
  {
    "objectID": "posts/2020-07-08-statistical thinking in python (part 2).html#calculation-of-heritability",
    "href": "posts/2020-07-08-statistical thinking in python (part 2).html#calculation-of-heritability",
    "title": "Statistical Thinking in Python (Part 2)",
    "section": "Calculation of heritability",
    "text": "Calculation of heritability\n\nHeredity\n\nThe tendency for parental traits to be inherited by offspring\n\n\n\nEDA of heritability\nThe array bd_parent_scandens contains the average beak depth (in mm) of two parents of the species G. scandens\n\nbd_parent_scandens = np.array([ 8.3318,  8.4035,  8.5317,  8.7202,  8.7089,  8.7541,  8.773 ,\n        8.8107,  8.7919,  8.8069,  8.6523,  8.6146,  8.6938,  8.7127,\n        8.7466,  8.7504,  8.7805,  8.7428,  8.7164,  8.8032,  8.8258,\n        8.856 ,  8.9012,  8.9125,  8.8635,  8.8258,  8.8522,  8.8974,\n        8.9427,  8.9879,  8.9615,  8.9238,  8.9351,  9.0143,  9.0558,\n        9.0596,  8.9917,  8.905 ,  8.9314,  8.9465,  8.9879,  8.9804,\n        9.0219,  9.052 ,  9.0407,  9.0407,  8.9955,  8.9992,  8.9992,\n        9.0747,  9.0747,  9.5385,  9.4781,  9.4517,  9.3537,  9.2707,\n        9.1199,  9.1689,  9.1425,  9.135 ,  9.1011,  9.1727,  9.2217,\n        9.2255,  9.2821,  9.3235,  9.3198,  9.3198,  9.3198,  9.3273,\n        9.3725,  9.3989,  9.4253,  9.4593,  9.4442,  9.4291,  9.2632,\n        9.2293,  9.1878,  9.1425,  9.1275,  9.1802,  9.1765,  9.2481,\n        9.2481,  9.1991,  9.1689,  9.1765,  9.2406,  9.3198,  9.3235,\n        9.1991,  9.2971,  9.2443,  9.316 ,  9.2934,  9.3914,  9.3989,\n        9.5121,  9.6176,  9.5535,  9.4668,  9.3725,  9.3348,  9.3763,\n        9.3839,  9.4216,  9.4065,  9.3348,  9.4442,  9.4367,  9.5083,\n        9.448 ,  9.4781,  9.595 ,  9.6101,  9.5686,  9.6365,  9.7119,\n        9.8213,  9.825 ,  9.7609,  9.6516,  9.5988,  9.546 ,  9.6516,\n        9.7572,  9.8854, 10.0023,  9.3914])\n\nThe array bd_offspring_scandens contains the average beak depth of the offspring of the respective parents.\n\nbd_offspring_scandens = np.array([ 8.419 ,  9.2468,  8.1532,  8.0089,  8.2215,  8.3734,  8.5025,\n        8.6392,  8.7684,  8.8139,  8.7911,  8.9051,  8.9203,  8.8747,\n        8.943 ,  9.0038,  8.981 ,  9.0949,  9.2696,  9.1633,  9.1785,\n        9.1937,  9.2772,  9.0722,  8.9658,  8.9658,  8.5025,  8.4949,\n        8.4949,  8.5633,  8.6013,  8.6468,  8.1532,  8.3734,  8.662 ,\n        8.6924,  8.7456,  8.8367,  8.8595,  8.9658,  8.9582,  8.8671,\n        8.8671,  8.943 ,  9.0646,  9.1405,  9.2089,  9.2848,  9.3759,\n        9.4899,  9.4519,  8.1228,  8.2595,  8.3127,  8.4949,  8.6013,\n        8.4646,  8.5329,  8.7532,  8.8823,  9.0342,  8.6392,  8.6772,\n        8.6316,  8.7532,  8.8291,  8.8975,  8.9734,  9.0494,  9.1253,\n        9.1253,  9.1253,  9.1785,  9.2848,  9.4595,  9.3608,  9.2089,\n        9.2544,  9.3684,  9.3684,  9.2316,  9.1709,  9.2316,  9.0342,\n        8.8899,  8.8291,  8.981 ,  8.8975, 10.4089, 10.1886,  9.7633,\n        9.7329,  9.6114,  9.5051,  9.5127,  9.3684,  9.6266,  9.5354,\n       10.0215, 10.0215,  9.6266,  9.6038,  9.4063,  9.2316,  9.338 ,\n        9.262 ,  9.262 ,  9.4063,  9.4367,  9.0342,  8.943 ,  8.9203,\n        8.7835,  8.7835,  9.057 ,  8.9354,  8.8975,  8.8139,  8.8671,\n        9.0873,  9.2848,  9.2392,  9.2924,  9.4063,  9.3152,  9.4899,\n        9.5962,  9.6873,  9.5203,  9.6646])\n\nThe arrays bd_parent_fortis and bd_offspring_fortis contain the same information about measurements from G. fortis birds.\n\nbd_parent_fortis = np.array([10.1  ,  9.55 ,  9.4  , 10.25 , 10.125,  9.7  ,  9.05 ,  7.4  ,\n        9.   ,  8.65 ,  9.625,  9.9  ,  9.55 ,  9.05 ,  8.35 , 10.1  ,\n       10.1  ,  9.9  , 10.225, 10.   , 10.55 , 10.45 ,  9.2  , 10.2  ,\n        8.95 , 10.05 , 10.2  ,  9.5  ,  9.925,  9.95 , 10.05 ,  8.75 ,\n        9.2  , 10.15 ,  9.8  , 10.7  , 10.5  ,  9.55 , 10.55 , 10.475,\n        8.65 , 10.7  ,  9.1  ,  9.4  , 10.3  ,  9.65 ,  9.5  ,  9.7  ,\n       10.525,  9.95 , 10.1  ,  9.75 , 10.05 ,  9.9  , 10.   ,  9.1  ,\n        9.45 ,  9.25 ,  9.5  , 10.   , 10.525,  9.9  , 10.4  ,  8.95 ,\n        9.4  , 10.95 , 10.75 , 10.1  ,  8.05 ,  9.1  ,  9.55 ,  9.05 ,\n       10.2  , 10.   , 10.55 , 10.75 ,  8.175,  9.7  ,  8.8  , 10.75 ,\n        9.3  ,  9.7  ,  9.6  ,  9.75 ,  9.6  , 10.45 , 11.   , 10.85 ,\n       10.15 , 10.35 , 10.4  ,  9.95 ,  9.1  , 10.1  ,  9.85 ,  9.625,\n        9.475,  9.   ,  9.25 ,  9.1  ,  9.25 ,  9.2  ,  9.95 ,  8.65 ,\n        9.8  ,  9.4  ,  9.   ,  8.55 ,  8.75 ,  9.65 ,  8.95 ,  9.15 ,\n        9.85 , 10.225,  9.825, 10.   ,  9.425, 10.4  ,  9.875,  8.95 ,\n        8.9  ,  9.35 , 10.425, 10.   , 10.175,  9.875,  9.875,  9.15 ,\n        9.45 ,  9.025,  9.7  ,  9.7  , 10.05 , 10.3  ,  9.6  , 10.   ,\n        9.8  , 10.05 ,  8.75 , 10.55 ,  9.7  , 10.   ,  9.85 ,  9.8  ,\n        9.175,  9.65 ,  9.55 ,  9.9  , 11.55 , 11.3  , 10.4  , 10.8  ,\n        9.8  , 10.45 , 10.   , 10.75 ,  9.35 , 10.75 ,  9.175,  9.65 ,\n        8.8  , 10.55 , 10.675,  9.95 ,  9.55 ,  8.825,  9.7  ,  9.85 ,\n        9.8  ,  9.55 ,  9.275, 10.325,  9.15 ,  9.35 ,  9.15 ,  9.65 ,\n       10.575,  9.975,  9.55 ,  9.2  ,  9.925,  9.2  ,  9.3  ,  8.775,\n        9.325,  9.175,  9.325,  8.975,  9.7  ,  9.5  , 10.225, 10.025,\n        8.2  ,  8.2  ,  9.55 ,  9.05 ,  9.6  ,  9.6  , 10.15 ,  9.875,\n       10.485, 11.485, 10.985,  9.7  ,  9.65 ,  9.35 , 10.05 , 10.1  ,\n        9.9  ,  8.95 ,  9.3  ,  9.95 ,  9.45 ,  9.5  ,  8.45 ,  8.8  ,\n        8.525,  9.375, 10.2  ,  7.625,  8.375,  9.25 ,  9.4  , 10.55 ,\n        8.9  ,  8.8  ,  9.   ,  8.575,  8.575,  9.6  ,  9.375,  9.6  ,\n        9.95 ,  9.6  , 10.2  ,  9.85 ,  9.625,  9.025, 10.375, 10.25 ,\n        9.3  ,  9.5  ,  9.55 ,  8.55 ,  9.05 ,  9.9  ,  9.8  ,  9.75 ,\n       10.25 ,  9.1  ,  9.65 , 10.3  ,  8.9  ,  9.95 ,  9.5  ,  9.775,\n        9.425,  7.75 ,  7.55 ,  9.1  ,  9.6  ,  9.575,  8.95 ,  9.65 ,\n        9.65 ,  9.65 ,  9.525,  9.85 ,  9.05 ,  9.3  ,  8.9  ,  9.45 ,\n       10.   ,  9.85 ,  9.25 , 10.1  ,  9.125,  9.65 ,  9.1  ,  8.05 ,\n        7.4  ,  8.85 ,  9.075,  9.   ,  9.7  ,  8.7  ,  9.45 ,  9.7  ,\n        8.35 ,  8.85 ,  9.7  ,  9.45 , 10.3  , 10.   , 10.45 ,  9.45 ,\n        8.5  ,  8.3  , 10.   ,  9.225,  9.75 ,  9.15 ,  9.55 ,  9.   ,\n        9.275,  9.35 ,  8.95 ,  9.875,  8.45 ,  8.6  ,  9.7  ,  8.55 ,\n        9.05 ,  9.6  ,  8.65 ,  9.2  ,  8.95 ,  9.6  ,  9.15 ,  9.4  ,\n        8.95 ,  9.95 , 10.55 ,  9.7  ,  8.85 ,  8.8  , 10.   ,  9.05 ,\n        8.2  ,  8.1  ,  7.25 ,  8.3  ,  9.15 ,  8.6  ,  9.5  ,  8.05 ,\n        9.425,  9.3  ,  9.8  ,  9.3  ,  9.85 ,  9.5  ,  8.65 ,  9.825,\n        9.   , 10.45 ,  9.1  ,  9.55 ,  9.05 , 10.   ,  9.35 ,  8.375,\n        8.3  ,  8.8  , 10.1  ,  9.5  ,  9.75 , 10.1  ,  9.575,  9.425,\n        9.65 ,  8.725,  9.025,  8.5  ,  8.95 ,  9.3  ,  8.85 ,  8.95 ,\n        9.8  ,  9.5  ,  8.65 ,  9.1  ,  9.4  ,  8.475,  9.35 ,  7.95 ,\n        9.35 ,  8.575,  9.05 ,  8.175,  9.85 ,  7.85 ,  9.85 , 10.1  ,\n        9.35 ,  8.85 ,  8.75 ,  9.625,  9.25 ,  9.55 , 10.325,  8.55 ,\n        9.675,  9.15 ,  9.   ,  9.65 ,  8.6  ,  8.8  ,  9.   ,  9.95 ,\n        8.4  ,  9.35 , 10.3  ,  9.05 ,  9.975,  9.975,  8.65 ,  8.725,\n        8.2  ,  7.85 ,  8.775,  8.5  ,  9.4  ])\n\n\nbd_offspring_fortis = np.array([10.7 ,  9.78,  9.48,  9.6 , 10.27,  9.5 ,  9.  ,  7.46,  7.65,\n        8.63,  9.81,  9.4 ,  9.48,  8.75,  7.6 , 10.  , 10.09,  9.74,\n        9.64,  8.49, 10.15, 10.28,  9.2 , 10.01,  9.03,  9.94, 10.5 ,\n        9.7 , 10.02, 10.04,  9.43,  8.1 ,  9.5 ,  9.9 ,  9.48, 10.18,\n       10.16,  9.08, 10.39,  9.9 ,  8.4 , 10.6 ,  8.75,  9.46,  9.6 ,\n        9.6 ,  9.95, 10.05, 10.16, 10.1 ,  9.83,  9.46,  9.7 ,  9.82,\n       10.34,  8.02,  9.65,  9.87,  9.  , 11.14,  9.25,  8.14, 10.23,\n        8.7 ,  9.8 , 10.54, 11.19,  9.85,  8.1 ,  9.3 ,  9.34,  9.19,\n        9.52,  9.36,  8.8 ,  8.6 ,  8.  ,  8.5 ,  8.3 , 10.38,  8.54,\n        8.94, 10.  ,  9.76,  9.45,  9.89, 10.9 ,  9.91,  9.39,  9.86,\n        9.74,  9.9 ,  9.09,  9.69, 10.24,  8.9 ,  9.67,  8.93,  9.3 ,\n        8.67,  9.15,  9.23,  9.59,  9.03,  9.58,  8.97,  8.57,  8.47,\n        8.71,  9.21,  9.13,  8.5 ,  9.58,  9.21,  9.6 ,  9.32,  8.7 ,\n       10.46,  9.29,  9.24,  9.45,  9.35, 10.19,  9.91,  9.18,  9.89,\n        9.6 , 10.3 ,  9.45,  8.79,  9.2 ,  8.8 ,  9.69, 10.61,  9.6 ,\n        9.9 ,  9.26, 10.2 ,  8.79,  9.28,  8.83,  9.76, 10.2 ,  9.43,\n        9.4 ,  9.9 ,  9.5 ,  8.95,  9.98,  9.72,  9.86, 11.1 ,  9.14,\n       10.49,  9.75, 10.35,  9.73,  9.83,  8.69,  9.58,  8.42,  9.25,\n       10.12,  9.31,  9.99,  8.59,  8.74,  8.79,  9.6 ,  9.52,  8.93,\n       10.23,  9.35,  9.35,  9.09,  9.04,  9.75, 10.5 ,  9.09,  9.05,\n        9.54,  9.3 ,  9.06,  8.7 ,  9.32,  8.4 ,  8.67,  8.6 ,  9.53,\n        9.77,  9.65,  9.43,  8.35,  8.26,  9.5 ,  8.6 ,  9.57,  9.14,\n       10.79,  8.91,  9.93, 10.7 ,  9.3 ,  9.93,  9.51,  9.44, 10.05,\n       10.13,  9.24,  8.21,  8.9 ,  9.34,  8.77,  9.4 ,  8.82,  8.83,\n        8.6 ,  9.5 , 10.2 ,  8.09,  9.07,  9.29,  9.1 , 10.19,  9.25,\n        8.98,  9.02,  8.6 ,  8.25,  8.7 ,  9.9 ,  9.65,  9.45,  9.38,\n       10.4 ,  9.96,  9.46,  8.26, 10.05,  8.92,  9.5 ,  9.43,  8.97,\n        8.44,  8.92, 10.3 ,  8.4 ,  9.37,  9.91, 10.  ,  9.21,  9.95,\n        8.84,  9.82,  9.5 , 10.29,  8.4 ,  8.31,  9.29,  8.86,  9.4 ,\n        9.62,  8.62,  8.3 ,  9.8 ,  8.48,  9.61,  9.5 ,  9.37,  8.74,\n        9.31,  9.5 ,  9.49,  9.74,  9.2 ,  9.24,  9.7 ,  9.64,  9.2 ,\n        7.5 ,  7.5 ,  8.7 ,  8.31,  9.  ,  9.74,  9.31, 10.5 ,  9.3 ,\n        8.12,  9.34,  9.72,  9.  ,  9.65,  9.9 , 10.  , 10.1 ,  8.  ,\n        9.07,  9.75,  9.33,  8.11,  9.36,  9.74,  9.9 ,  9.23,  9.7 ,\n        8.2 ,  9.35,  9.49,  9.34,  8.87,  9.03,  9.07,  9.43,  8.2 ,\n        9.19,  9.  ,  9.2 ,  9.06,  9.81,  8.89,  9.4 , 10.45,  9.64,\n        9.03,  8.71,  9.91,  8.33,  8.2 ,  7.83,  7.14,  8.91,  9.18,\n        8.8 ,  9.9 ,  7.73,  9.25,  8.7 ,  9.5 ,  9.3 ,  9.05, 10.18,\n        8.85,  9.24,  9.15,  9.98,  8.77,  9.8 ,  8.65, 10.  ,  8.81,\n        8.01,  7.9 ,  9.41, 10.18,  9.55,  9.08,  8.4 ,  9.75,  8.9 ,\n        9.07,  9.35,  8.9 ,  8.19,  8.65,  9.19,  8.9 ,  9.28, 10.58,\n        9.  ,  9.4 ,  8.91,  9.93, 10.  ,  9.37,  7.4 ,  9.  ,  8.8 ,\n        9.18,  8.3 , 10.08,  7.9 ,  9.96, 10.4 ,  9.65,  8.8 ,  8.65,\n        9.7 ,  9.23,  9.43,  9.93,  8.47,  9.55,  9.28,  8.85,  8.9 ,\n        8.75,  8.63,  9.  ,  9.43,  8.28,  9.23, 10.4 ,  9.  ,  9.8 ,\n        9.77,  8.97,  8.37,  7.7 ,  7.9 ,  9.5 ,  8.2 ,  8.8 ])\n\nWe’ll make a scatter plot of the average offspring beak depth (y-axis) versus average parental beak depth (x-axis) for both species.\n\n# Make scatter plots\n_ = plt.plot(bd_parent_fortis, bd_offspring_fortis,\n             marker=\".\", linestyle=\"none\", color=\"blue\", alpha=.5)\n_ = plt.plot(bd_parent_scandens, bd_offspring_scandens,\n             marker=\".\", linestyle=\"none\", color=\"red\", alpha=.5)\n\n# Label axes\n_ = plt.xlabel('parental beak depth (mm)')\n_ = plt.ylabel('offspring beak depth (mm)')\n\n# Add legend\n_ = plt.legend(('G. fortis', 'G. scandens'), loc='lower right')\n\n# Show plot\nplt.show()\n\n\n\n\nIt appears as though there is a stronger correlation in G. fortis than in G. scandens. This suggests that beak depth is more strongly inherited in G. fortis. We’ll quantify this correlation\n\n\nCorrelation of offspring and parental data\nIn an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap.\n\ndef draw_bs_pairs(x, y, func, size=1):\n    \"\"\"Perform pairs bootstrap for a single statistic.\"\"\"\n\n    # Set up array of indices to sample from: inds\n    inds = np.arange(len(x))\n\n    # Initialize replicates: bs_replicates\n    bs_replicates = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_inds = np.random.choice(inds, len(inds))\n        bs_x, bs_y = x[bs_inds], y[bs_inds]\n        bs_replicates[i] = func(bs_x, bs_y)\n\n    return bs_replicates\n\n\n\nPearson correlation of offspring and parental data\nThe Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. We’ll compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens. And Do the same for G. fortis. Then, use the function draw_bs_pairs to compute a 95% confidence interval using pairs bootstrap.\n\n# Compute the Pearson correlation coefficients\nr_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens)\nr_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis)\n\n# Acquire 1000 bootstrap replicates of Pearson r\nbs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, size=1000)\n\nbs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, size=1000)\n\n\n# Compute 95% confidence intervals\nconf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5])\nconf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5])\n\n# Print results\nprint('G. scandens:', r_scandens, conf_int_scandens)\nprint('G. fortis:', r_fortis, conf_int_fortis)\n\nG. scandens: 0.41170636294012586 [0.28114522 0.55186026]\nG. fortis: 0.7283412395518486 [0.66716345 0.7791389 ]\n\n\nIt is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts.\n\n\nMeasuring heritability\n\n\n\n\n\n\nNote\n\n\n\nPearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets.\n\n\nThis is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone. We will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval.\n\n\n\n\n\n\nWarning\n\n\n\nStatistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient.\n\n\n\ndef heritability(parents, offspring):\n    \"\"\"Compute the heritability from parent and offspring samples.\"\"\"\n    covariance_matrix = np.cov(parents, offspring)\n    return covariance_matrix[0,1] / covariance_matrix[0,0]\n\n# Compute the heritability\nheritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens)\nheritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis)\n\n# Acquire 1000 bootstrap replicates of heritability\nreplicates_scandens = draw_bs_pairs(\n        bd_parent_scandens, bd_offspring_scandens, heritability, size=1000)\n        \nreplicates_fortis = draw_bs_pairs(\n        bd_parent_fortis, bd_offspring_fortis, heritability, size=1000)\n\n\n# Compute 95% confidence intervals\nconf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5])\nconf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5])\n\n# Print results\nprint('G. scandens:', heritability_scandens, conf_int_scandens)\nprint('G. fortis:', heritability_fortis, conf_int_fortis)\n\nG. scandens: 0.5485340868685983 [0.35159687 0.74984943]\nG. fortis: 0.7229051911438156 [0.64286124 0.78727894]\n\n\nHere again, we see that G. fortis has stronger heritability than G. scandens. This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization.\n\n\nIs beak depth heritable at all in G. scandens?\nThe heritability of beak depth in G. scandens seems low. It could be that this observed heritability was just achieved by chance and beak depth is actually not really heritable in the species. We will test that hypothesis here. To do this, We will do a pairs permutation test.\n\n# Initialize array of replicates: perm_replicates\nperm_replicates = np.empty(10000)\n\n# Draw replicates\nfor i in range(10000):\n    # Permute parent beak depths\n    bd_parent_permuted = np.random.permutation(bd_parent_scandens)\n    perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens)\n\n\n# Compute p-value: p\np = np.sum(perm_replicates >= heritability_scandens) / len(perm_replicates)\n\n# Print the p-value\nprint('p-val =', p)\n\np-val = 0.0\n\n\nWe get a p-value of zero, which means that none of the 10,000 permutation pairs replicates we drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens, just not as much as in G. fortis. If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance.\n\n_ = plt.hist(perm_replicates, bins = int(np.sqrt(len(perm_replicates))))\n_ = plt.ylabel(\"counts\")\n_ = plt.xlabel(\"Heritability replicates\")\nplt.show()"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#libraries-setup",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#libraries-setup",
    "title": "Hyperparameter Tuning with Python",
    "section": "Libraries setup",
    "text": "Libraries setup\n\n%matplotlib inline\nplt.style.use(\"ggplot\")\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_colwidth\", None)"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-analysis-question",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-analysis-question",
    "title": "Hyperparameter Tuning with Python",
    "section": "1.1 Data Analysis Question",
    "text": "1.1 Data Analysis Question\nWe will be performing hyperparameter tuning techniques to the most accurate model in an effort to achieve optimal predictions."
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#metric-for-success",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#metric-for-success",
    "title": "Hyperparameter Tuning with Python",
    "section": "1.2 Metric For Success",
    "text": "1.2 Metric For Success\nThis will be a regression task, We will use the regression metrics to determine how the model works:\n\n\\(R^2\\) Score\nMean Absolute Error\nResidual Sum of Squares"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#the-context",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#the-context",
    "title": "Hyperparameter Tuning with Python",
    "section": "1.3 The Context",
    "text": "1.3 The Context\nBuild a solution the would make optimal predictions of rental prices for the city of Amsterdam."
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#experimental-design",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#experimental-design",
    "title": "Hyperparameter Tuning with Python",
    "section": "1.4 Experimental Design",
    "text": "1.4 Experimental Design\n\nLoading the dataset\nExploring the dataset\nData manipulation, data cleaning and visualization\nData modeling\nHyperparameter tuning\nConclusion and recommendation"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-relevance",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-relevance",
    "title": "Hyperparameter Tuning with Python",
    "section": "1.5 Data Relevance",
    "text": "1.5 Data Relevance\nThe Data Provided in relevant to the research question"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-cleaning-data-preparation",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-cleaning-data-preparation",
    "title": "Hyperparameter Tuning with Python",
    "section": "2. Data Cleaning & Data Preparation",
    "text": "2. Data Cleaning & Data Preparation"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#loading-and-preview-datasets",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#loading-and-preview-datasets",
    "title": "Hyperparameter Tuning with Python",
    "section": "2.1 Loading and Preview Datasets",
    "text": "2.1 Loading and Preview Datasets\n\nrentals = pd.read_csv(\"datasets/raw/listing_summary.csv\")\nrentals.head()\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      host_id\n      host_name\n      neighbourhood_group\n      neighbourhood\n      latitude\n      longitude\n      room_type\n      price\n      minimum_nights\n      number_of_reviews\n      last_review\n      reviews_per_month\n      calculated_host_listings_count\n      availability_365\n    \n  \n  \n    \n      0\n      2818\n      Quiet Garden View Room & Super Fast WiFi\n      3159\n      Daniel\n      NaN\n      Oostelijk Havengebied - Indische Buurt\n      52.36575\n      4.94142\n      Private room\n      59\n      3\n      278\n      2020-02-14\n      2.06\n      1\n      169\n    \n    \n      1\n      20168\n      Studio with private bathroom in the centre 1\n      59484\n      Alexander\n      NaN\n      Centrum-Oost\n      52.36509\n      4.89354\n      Private room\n      100\n      1\n      340\n      2020-04-09\n      2.76\n      2\n      106\n    \n    \n      2\n      25428\n      Lovely apt in City Centre (w.lift) near Jordaan\n      56142\n      Joan\n      NaN\n      Centrum-West\n      52.37297\n      4.88339\n      Entire home/apt\n      125\n      14\n      5\n      2020-02-09\n      0.18\n      1\n      132\n    \n    \n      3\n      27886\n      Romantic, stylish B&B houseboat in canal district\n      97647\n      Flip\n      NaN\n      Centrum-West\n      52.38761\n      4.89188\n      Private room\n      155\n      2\n      217\n      2020-03-02\n      2.15\n      1\n      172\n    \n    \n      4\n      28871\n      Comfortable double room\n      124245\n      Edwin\n      NaN\n      Centrum-West\n      52.36719\n      4.89092\n      Private room\n      75\n      2\n      332\n      2020-03-16\n      2.82\n      3\n      210\n    \n  \n\n\n\n\n\nDataset Glossary\nBefore starting the analysis, let’s load the glossary to understand the column descriptions\n\nwith open(\"datasets/raw/Glossary - Sheet1 (1).csv\", encoding='utf8') as file:\n    print(file.read())\n\nroom_id: A unique number identifying an Airbnb listing.\nhost_id: A unique number identifying an Airbnb host.\nneighborhood: A subregion of the city or search area for which the survey is carried out. For some cities there is no neighbourhood information.\n\"room_type: One of “Entire home/apt”, “Private room”, or “Shared room”.\"\nhost_response_rate: The rate at which the particular host responds to the customers.\n\"price: The price (in $US) for a night stay. In early surveys, there may be some values that were recorded by month.\"\naccomodates: The number of guests a listing can accommodate.\nbathrooms: The number of bathrooms a listing offers.\nbedrooms: The number of bedrooms a listing offers.\nbeds: The number of beds a listing offers.\n\"minimum_nights: The minimum stay for a visit, as posted by the host.\"\n\"maximum nights: The maximum stay for a visit, as posted by the host.\"\noverall_satisfaction: The average rating (out of five) that the listing has received from those visitors who left a review.\n\"number_of_reviews: The number of reviews that a listing has received. Airbnb has said that 70% of visits end up with a review, so the number of reviews can be used to estimate the number of visits. Note that such an estimate will not be reliable for an individual listing (especially as reviews occasionally vanish from the site), but over a city as a whole it should be a useful metric of traffic.\"\nreviews_per_month: The number of reviews that a listing has received per month.\nhost_listings_count: The number of listings for a particular host.\navailability_365: The number of days for which a particular host is available in a year."
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-exploration",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#data-exploration",
    "title": "Hyperparameter Tuning with Python",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe name column contains the basic information about the Airbnb room, since this column won’t be of any help in our analysis or modeling, we will drop the column\n\nrentals.drop(columns=['name'], inplace=True)\nrentals.shape\n\n(19362, 15)\n\n\nThe dataset has over 19362 rentals with 15 rentals information. Each row represents an Airbnb listing\n\nrentals.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 19362 entries, 0 to 19361\nData columns (total 15 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   id                              19362 non-null  int64  \n 1   host_id                         19362 non-null  int64  \n 2   host_name                       19358 non-null  object \n 3   neighbourhood_group             0 non-null      float64\n 4   neighbourhood                   19362 non-null  object \n 5   latitude                        19362 non-null  float64\n 6   longitude                       19362 non-null  float64\n 7   room_type                       19362 non-null  object \n 8   price                           19362 non-null  int64  \n 9   minimum_nights                  19362 non-null  int64  \n 10  number_of_reviews               19362 non-null  int64  \n 11  last_review                     17078 non-null  object \n 12  reviews_per_month               17078 non-null  float64\n 13  calculated_host_listings_count  19362 non-null  int64  \n 14  availability_365                19362 non-null  int64  \ndtypes: float64(4), int64(7), object(4)\nmemory usage: 2.2+ MB\n\n\nneighbourhood_group 19362 missing values, that means every row in this column is a missing value. last_review and reviews_per_month have the same number of missing values."
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#handling-duplicates-and-missing-values",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#handling-duplicates-and-missing-values",
    "title": "Hyperparameter Tuning with Python",
    "section": "Handling Duplicates and Missing Values",
    "text": "Handling Duplicates and Missing Values\nWe will start by - droping any duplicates if any, - remove the neighbourhood_group - drop records with missing values for last_review and reviews_per_month\n\nrentals.drop_duplicates(inplace=True)\nrentals.shape\n\n(19362, 15)\n\n\nThere were no duplicates in the dataset. Let’s continue by checking missing values and dropping them\n\nrentals.isnull().sum()\n\nid                                    0\nhost_id                               0\nhost_name                             4\nneighbourhood_group               19362\nneighbourhood                         0\nlatitude                              0\nlongitude                             0\nroom_type                             0\nprice                                 0\nminimum_nights                        0\nnumber_of_reviews                     0\nlast_review                        2284\nreviews_per_month                  2284\ncalculated_host_listings_count        0\navailability_365                      0\ndtype: int64\n\n\n\nrentals.drop(columns=['neighbourhood_group'], inplace=True)\nrentals.dropna(inplace=True)\nrentals.isnull().sum()\n\nid                                0\nhost_id                           0\nhost_name                         0\nneighbourhood                     0\nlatitude                          0\nlongitude                         0\nroom_type                         0\nprice                             0\nminimum_nights                    0\nnumber_of_reviews                 0\nlast_review                       0\nreviews_per_month                 0\ncalculated_host_listings_count    0\navailability_365                  0\ndtype: int64\n\n\n\nrentals.shape\n\n(17075, 14)\n\n\nAfter removing the missing values, the dataset now has 17075 Airbnb listings"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#checking-for-anomalies",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#checking-for-anomalies",
    "title": "Hyperparameter Tuning with Python",
    "section": "Checking for anomalies",
    "text": "Checking for anomalies\nTo ensure there are no anomalies in the dataset, we will check the number of unique items in every column\n\ncols = rentals.columns.to_list()\n\nfor col in cols:\n    print(f\"{col:>34} No. of Unique Items:{rentals[col].nunique():>8}\")\n\n                                id No. of Unique Items:   17075\n                           host_id No. of Unique Items:   15182\n                         host_name No. of Unique Items:    5382\n                     neighbourhood No. of Unique Items:      22\n                          latitude No. of Unique Items:    5720\n                         longitude No. of Unique Items:    9193\n                         room_type No. of Unique Items:       4\n                             price No. of Unique Items:     422\n                    minimum_nights No. of Unique Items:      60\n                 number_of_reviews No. of Unique Items:     422\n                       last_review No. of Unique Items:    1590\n                 reviews_per_month No. of Unique Items:     703\n    calculated_host_listings_count No. of Unique Items:      23\n                  availability_365 No. of Unique Items:     366"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#outliers-visualization",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#outliers-visualization",
    "title": "Hyperparameter Tuning with Python",
    "section": "Outliers visualization",
    "text": "Outliers visualization\n\nplt.figure(figsize=(20,10))\nrentals.boxplot()\nplt.show()"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#records-with-outliers",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#records-with-outliers",
    "title": "Hyperparameter Tuning with Python",
    "section": "Records with outliers",
    "text": "Records with outliers\n\nq1 = rentals.quantile(.25)\nq3 = rentals.quantile(.75)\niqr = q3 - q1\noutliers_df = rentals[\n    ((rentals < (q1 - 1.5*iqr)) | (rentals > (q3 + 1.5*iqr))).any(axis=1)\n]\noutliers_df.shape\n\n(8544, 14)\n\n\nThere are Over 8500 outliers in the dataset\n\noutliers_df.sample(5)\n\n\n\n\n\n  \n    \n      \n      id\n      host_id\n      host_name\n      neighbourhood\n      latitude\n      longitude\n      room_type\n      price\n      minimum_nights\n      number_of_reviews\n      last_review\n      reviews_per_month\n      calculated_host_listings_count\n      availability_365\n    \n  \n  \n    \n      12117\n      22620913\n      2888051\n      Lokke\n      Noord-West\n      52.41524\n      4.89218\n      Entire home/apt\n      80\n      4\n      2\n      2018-05-06\n      0.08\n      1\n      0\n    \n    \n      3511\n      7166128\n      16634603\n      Hermen\n      Centrum-Oost\n      52.36991\n      4.92621\n      Entire home/apt\n      110\n      5\n      7\n      2018-08-19\n      0.12\n      1\n      0\n    \n    \n      15707\n      32722735\n      246045083\n      Babette\n      De Baarsjes - Oud-West\n      52.36514\n      4.86184\n      Entire home/apt\n      120\n      2\n      8\n      2019-06-29\n      0.64\n      1\n      0\n    \n    \n      13425\n      26037649\n      193680275\n      Merle\n      Westerpark\n      52.38374\n      4.87314\n      Entire home/apt\n      125\n      5\n      5\n      2018-11-18\n      0.24\n      1\n      0\n    \n    \n      730\n      1277443\n      6952882\n      Jason & Lotte\n      Bos en Lommer\n      52.38129\n      4.85746\n      Entire home/apt\n      98\n      5\n      4\n      2014-07-07\n      0.06\n      1\n      0\n    \n  \n\n\n\n\nLet’s check the percentage of outliers to the original dataset\n\nprint(f\"Percentage of outliers: {outliers_df.shape[0]/rentals.shape[0]*100:.2f}\")\n\nPercentage of outliers: 50.04\n\n\nIt would be tricky to drop the records with outliers since that will reduce our dataset by half so we will leave them there. However, we will drop the host_id variable later on, right before modeling."
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#univariate-data-analysis",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#univariate-data-analysis",
    "title": "Hyperparameter Tuning with Python",
    "section": "3.1 Univariate Data Analysis",
    "text": "3.1 Univariate Data Analysis\n\nTop 10 Most common hosts\n\nplt.figure(figsize=(20,12))\nrentals.host_name.value_counts()[:10].plot(kind='bar')\nplt.title(\"Top 10 Hosts\")\nplt.xlabel(\"Host Name\")\nplt.ylabel(\"Number of Listings\")\nplt.show()\n\n\n\n\nMartijn is the host name with most listings in the Airbnb rentals listing.\n\n\nthe top 10 most common neighbourhoods\n\nplt.figure(figsize = (20, 12))\nrentals.neighbourhood.value_counts()[:10].sort_values().plot(kind = 'barh')\nplt.xticks(ha = \"right\")\nplt.title(\"Top 10 Neighbourhoods\")\nplt.xlabel(\"Number of Listings\")\nplt.ylabel(\"Neighbourhood\")\nplt.show()\n\n\n\n\nDe Baarsjes - Oud-West Had the most number of rental listings\n\n\nthe most common room types\n\nrentals.room_type.value_counts()\n\nEntire home/apt    13308\nPrivate room        3497\nHotel room           232\nShared room           38\nName: room_type, dtype: int64\n\n\n\nplt.figure(figsize=(8,8))\nrentals.room_type.value_counts().plot(kind='pie', autopct=\"%0.1f%%\", labels=rentals.room_type.value_counts().index)\nplt.legend()\nplt.title(\"the most common room types\")\nplt.show()\n\n\n\n\n\n\ndistribution of price\n\nplt.figure(figsize = (14,8))\nsns.distplot(rentals.price)\nplt.title(\"Distribution of $price$\")\nplt.show()\n\n\n\n\nThe distribution of price is skewed to the right and is not continous.\n\n\nthe top 10 most common minimum number of nights to spend\n\nplt.figure(figsize=(20,10))\nrentals.minimum_nights.value_counts()[:10].sort_values().plot(kind='barh')\nplt.title(\"the top 10 most common minimum number of nights to spend\")\nplt.xlabel(\"Number of listings\")\nplt.ylabel(\"Minimum Nights\")\nplt.show()\n\n\n\n\nThe most common minimum number of nights to spend is 2."
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#bivariate-analysis",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#bivariate-analysis",
    "title": "Hyperparameter Tuning with Python",
    "section": "3.2 Bivariate Analysis",
    "text": "3.2 Bivariate Analysis\n\nprice by room type\n\nrentals.hist('price', by='room_type', figsize=(20,12))\nplt.show()\n\n\n\n\n\n\naverage price by neighbourhood\n\nrentals.groupby('neighbourhood')['price'].mean().sort_values(ascending=False)\n\nneighbourhood\nCentrum-West                              203.653397\nCentrum-Oost                              190.534247\nZuid                                      178.813243\nOud-Noord                                 168.763566\nDe Pijp - Rivierenbuurt                   166.601787\nIJburg - Zeeburgereiland                  157.723077\nWesterpark                                151.241140\nDe Baarsjes - Oud-West                    149.376228\nOud-Oost                                  144.606957\nWatergraafsmeer                           141.199125\nBuitenveldert - Zuidas                    137.690355\nOostelijk Havengebied - Indische Buurt    135.101163\nNoord-Oost                                130.495833\nNoord-West                                126.318885\nBos en Lommer                             122.525304\nDe Aker - Nieuw Sloten                    121.268908\nSlotervaart                               119.791549\nGeuzenveld - Slotermeer                   113.937143\nOsdorp                                    103.418182\nGaasperdam - Driemond                      96.317757\nBijlmer-Centrum                            91.434343\nBijlmer-Oost                               89.696629\nName: price, dtype: float64\n\n\n\nplt.figure(figsize=(20, 10))\nrentals.groupby('neighbourhood')['price'].mean().sort_values(ascending=False)[:10].sort_values().plot(kind='barh')\nplt.xlabel(\"Average Neighbourhood Price\")\nplt.title(\"average price by neighbourhood\")\nplt.show()\n\n\n\n\nBased on neighbour hood, Centrum West Has the highest average weight of listings.\n\n\naverage price by minimum_nights\n\nrentals.groupby('minimum_nights')['price'].mean().sort_values(ascending=False)[:15]\n\nminimum_nights\n365     3000.000000\n200      999.000000\n99       999.000000\n52       429.000000\n30       331.551020\n222      300.000000\n27       275.000000\n48       250.000000\n31       243.750000\n150      203.000000\n21       201.761905\n15       197.888889\n1000     185.000000\n240      180.000000\n28       175.900000\nName: price, dtype: float64\n\n\n\nplt.figure(figsize=(20, 10))\nrentals.groupby('minimum_nights')['price'].mean().sort_values(ascending=False)[:10].sort_values().plot(kind='barh')\nplt.xlabel(\"Average minimum nights Price\")\nplt.title(\"average price by minimum nights\")\nplt.show()\n\n\n\n\nBy average, those who spend 365 nights spend most price"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#feature-engineering",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#feature-engineering",
    "title": "Hyperparameter Tuning with Python",
    "section": "3.3 Feature Engineering",
    "text": "3.3 Feature Engineering\n\nrentals.head()\n\n\n\n\n\n  \n    \n      \n      id\n      host_id\n      host_name\n      neighbourhood\n      latitude\n      longitude\n      room_type\n      price\n      minimum_nights\n      number_of_reviews\n      last_review\n      reviews_per_month\n      calculated_host_listings_count\n      availability_365\n    \n  \n  \n    \n      0\n      2818\n      3159\n      Daniel\n      Oostelijk Havengebied - Indische Buurt\n      52.36575\n      4.94142\n      Private room\n      59\n      3\n      278\n      2020-02-14\n      2.06\n      1\n      169\n    \n    \n      1\n      20168\n      59484\n      Alexander\n      Centrum-Oost\n      52.36509\n      4.89354\n      Private room\n      100\n      1\n      340\n      2020-04-09\n      2.76\n      2\n      106\n    \n    \n      2\n      25428\n      56142\n      Joan\n      Centrum-West\n      52.37297\n      4.88339\n      Entire home/apt\n      125\n      14\n      5\n      2020-02-09\n      0.18\n      1\n      132\n    \n    \n      3\n      27886\n      97647\n      Flip\n      Centrum-West\n      52.38761\n      4.89188\n      Private room\n      155\n      2\n      217\n      2020-03-02\n      2.15\n      1\n      172\n    \n    \n      4\n      28871\n      124245\n      Edwin\n      Centrum-West\n      52.36719\n      4.89092\n      Private room\n      75\n      2\n      332\n      2020-03-16\n      2.82\n      3\n      210\n    \n  \n\n\n\n\n\nrentals['room_type_encoded'] = LabelEncoder().fit_transform(rentals.room_type)\nrentals.head()\n\n\n\n\n\n  \n    \n      \n      id\n      host_id\n      host_name\n      neighbourhood\n      latitude\n      longitude\n      room_type\n      price\n      minimum_nights\n      number_of_reviews\n      last_review\n      reviews_per_month\n      calculated_host_listings_count\n      availability_365\n      room_type_encoded\n    \n  \n  \n    \n      0\n      2818\n      3159\n      Daniel\n      Oostelijk Havengebied - Indische Buurt\n      52.36575\n      4.94142\n      Private room\n      59\n      3\n      278\n      2020-02-14\n      2.06\n      1\n      169\n      2\n    \n    \n      1\n      20168\n      59484\n      Alexander\n      Centrum-Oost\n      52.36509\n      4.89354\n      Private room\n      100\n      1\n      340\n      2020-04-09\n      2.76\n      2\n      106\n      2\n    \n    \n      2\n      25428\n      56142\n      Joan\n      Centrum-West\n      52.37297\n      4.88339\n      Entire home/apt\n      125\n      14\n      5\n      2020-02-09\n      0.18\n      1\n      132\n      0\n    \n    \n      3\n      27886\n      97647\n      Flip\n      Centrum-West\n      52.38761\n      4.89188\n      Private room\n      155\n      2\n      217\n      2020-03-02\n      2.15\n      1\n      172\n      2\n    \n    \n      4\n      28871\n      124245\n      Edwin\n      Centrum-West\n      52.36719\n      4.89092\n      Private room\n      75\n      2\n      332\n      2020-03-16\n      2.82\n      3\n      210\n      2\n    \n  \n\n\n\n\n\nrentals.neighbourhood.nunique()\n\n22\n\n\n\nrentals_2 = rentals.drop(columns=['id', 'host_id', 'host_name', 'neighbourhood', 'last_review', 'room_type'])\nrentals_2.head()\n\n\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      price\n      minimum_nights\n      number_of_reviews\n      reviews_per_month\n      calculated_host_listings_count\n      availability_365\n      room_type_encoded\n    \n  \n  \n    \n      0\n      52.36575\n      4.94142\n      59\n      3\n      278\n      2.06\n      1\n      169\n      2\n    \n    \n      1\n      52.36509\n      4.89354\n      100\n      1\n      340\n      2.76\n      2\n      106\n      2\n    \n    \n      2\n      52.37297\n      4.88339\n      125\n      14\n      5\n      0.18\n      1\n      132\n      0\n    \n    \n      3\n      52.38761\n      4.89188\n      155\n      2\n      217\n      2.15\n      1\n      172\n      2\n    \n    \n      4\n      52.36719\n      4.89092\n      75\n      2\n      332\n      2.82\n      3\n      210\n      2"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#normal-modeling",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#normal-modeling",
    "title": "Hyperparameter Tuning with Python",
    "section": "4.1 Normal Modeling",
    "text": "4.1 Normal Modeling\n\ndt = DecisionTreeRegressor()\nrf = RandomForestRegressor()\n\n\ndt.fit(X_train, y_train)\ndt_pred = dt.predict(X_test)\nprint(f\"DT RMSE: {np.sqrt(mean_squared_error(y_test, dt_pred)):.2f}\")\nprint(f\"DT R2: {r2_score(y_test, dt_pred):.2f}\")\n\nDT RMSE: 249.36\nDT R2: -5.03\n\n\n\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nprint(f\"rf RMSE: {np.sqrt(mean_squared_error(y_test, rf_pred)):.2f}\")\nprint(f\"rf R2: {r2_score(y_test, rf_pred):.2f}\")\n\nrf RMSE: 111.97\nrf R2: -0.22\n\n\n\ny.sample(int(y.shape[0]*.1)).mean()\n\n154.93731693028704\n\n\nBoth models are performing well, we will use Random Forest Regressor because it is the best perfroming model producing the best fit."
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#modeling-with-grid-search",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#modeling-with-grid-search",
    "title": "Hyperparameter Tuning with Python",
    "section": "4.2 Modeling with Grid Search",
    "text": "4.2 Modeling with Grid Search\n\nrf.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'criterion': 'mse',\n 'max_depth': None,\n 'max_features': 'auto',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_impurity_split': None,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\nparam_grid = {\n    'bootstrap': [True, False],\n    'n_estimators': [100,110,120,130,140],\n    'max_depth': [10, 20, 30, 40, 50]\n}\ngrid_search = GridSearchCV(rf, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid_result = grid_search.fit(X_train, y_train)\nprint(f\"Best Score: {grid_result.best_score_}\\nBest Parameters: {grid_result.best_params_}\")\n\nBest Score: -0.060206490328136075\nBest Parameters: {'bootstrap': True, 'max_depth': 40, 'n_estimators': 140}\n\n\nWe will use the generated parameters to train our model\n\nrf_gs = RandomForestRegressor(bootstrap=True, max_depth=40, n_estimators=140, n_jobs=-1)\nrf_gs.fit(X_train, y_train)\nrf_gs_pred = rf_gs.predict(X_test)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, rf_gs_pred)):.2f}\\nR2 Score: {r2_score(y_test, rf_gs_pred):2f}\")\n\nRMSE: 112.40\nR2 Score: -0.225879"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#modeling-with-random-search",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#modeling-with-random-search",
    "title": "Hyperparameter Tuning with Python",
    "section": "4.3 Modeling with Random Search",
    "text": "4.3 Modeling with Random Search\n\nrandom_search = RandomizedSearchCV(rf, param_distributions=param_grid, cv=5)\nrandom_result = random_search.fit(X_train, y_train)\nprint(f\"Best Score: {random_result.best_score_}\\nBest Parameters: {random_result.best_params_}\")\n\nBest Score: -0.11714015166923782\nBest Parameters: {'n_estimators': 120, 'max_depth': 40, 'bootstrap': True}\n\n\n\nrf_rs = RandomForestRegressor(bootstrap=True, max_depth=40, n_estimators=140, n_jobs=-1)\nrf_rs.fit(X_train, y_train)\nrf_rs_pred = rf_rs.predict(X_test)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, rf_rs_pred)):.2f}\\nR2 Score: {r2_score(y_test, rf_rs_pred):2f}\")\n\nRMSE: 108.37\nR2 Score: -0.139507"
  },
  {
    "objectID": "posts/2021-09-21-hyperparameter_tuning_with_python.html#modeling-with-bayesian-optimization",
    "href": "posts/2021-09-21-hyperparameter_tuning_with_python.html#modeling-with-bayesian-optimization",
    "title": "Hyperparameter Tuning with Python",
    "section": "4.4 Modeling with Bayesian Optimization",
    "text": "4.4 Modeling with Bayesian Optimization\n\ndef objective(space):\n    model = RandomForestRegressor(\n        n_estimators=space['n_estimators'], \n        bootstrap=space['bootstrap'], \n        max_depth=space['max_depth']\n    )\n    accuracy = cross_val_score(model, X_train, y_train, cv=5).mean()\n    return {'loss': -accuracy, 'status': STATUS_OK }\nparam_grid = {\n    'bootstrap': hp.choice('bootstrap',  [True, False]),\n    'n_estimators': hp.choice('n_estimators', [100,110,120,130,140]),\n    'max_depth': hp.quniform('max_depth', 10, 50, 10)\n}\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= param_grid,\n            algo= tpe.suggest,\n            max_evals = 100,\n            trials= trials)\nbest\n\n100%|███████████████████████████████████████████| 100/100 [1:20:39<00:00, 48.39s/trial, best loss: 0.05966699243049871]\n\n\n{'bootstrap': 0, 'max_depth': 10.0, 'n_estimators': 3}\n\n\n\nbtsp = {0:True, 1:False}\nn_est = {0:100,1:110,2:120,3:130,4:140}\nrf_bo = RandomForestRegressor(\n    bootstrap=btsp[best['bootstrap']],\n    max_depth = best['max_depth'],\n    n_estimators=n_est[best['n_estimators']]\n).fit(X_train, y_train)\nrf_bo_pred = rf_bo.predict(X_test)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, rf_bo_pred)):.2f}\\nR2 Score: {r2_score(y_test, rf_bo_pred):2f}\")\n\nRMSE: 107.42\nR2 Score: -0.119587"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html",
    "href": "posts/2020-10-19-cluster analysis in python.html",
    "title": "Cluster Analysis in Python",
    "section": "",
    "text": "You have probably come across Google News, which automatically groups similar news articles under a topic. Have you ever wondered what process runs in the background to arrive at these groups? We will be exploring unsupervised learning through clustering using the SciPy library in Python. We will cover pre-processing of data and application of hierarchical and k-means clustering. We will explore player statistics from a popular football video game, FIFA 18. We will be able to quickly apply various clustering algorithms on data, visualize the clusters formed and analyze results."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#unsupervised-learning-basics",
    "href": "posts/2020-10-19-cluster analysis in python.html#unsupervised-learning-basics",
    "title": "Cluster Analysis in Python",
    "section": "Unsupervised learning: basics",
    "text": "Unsupervised learning: basics\n\nEveryday example: Google news\n\nHow does Google News classify articles?\nUnsupervised Learning Algorithm: Clustering\nMatch frequent terms in articles to find similarity\n\n\n\nWhat is unsupervised learning?\n\nA group of machine learning algorithms that find patterns in data\nData for algorithms has not been labeled, classified or characterized\nThe objective of the algorithm is to interpret any structure in the data\nCommon unsupervised learning algorithms: clustering, neural networks, anomaly detection\n\n\n\nWhat is clustering?\n\nThe process of grouping items with similar characteristics\nItems in groups similar to each other than in other groups\nExample: distance between points on a 2D plane\n\n\n\nPlotting data for clustering - Pokemon sightings\n\n\nx_coordinates = [80, 93, 86, 98, 86, 9, 15, 3, 10, 20, 44, 56, 49, 62, 44]\ny_coordinates = [87, 96, 95, 92, 92, 57, 49, 47, 59, 55, 25, 2, 10, 24, 10]\n\n_ = sns.scatterplot(x_coordinates, y_coordinates)\nplt.show()\n\n\n\n\nVisualizing helps in determining how many clusters are in the data.\n\nUnsupervised learning in real world\nSegmentation of learners at DataCamp based on courses they complete. The training data has no labels. As the training data has no labels, an unsupervised algorithm needs to be used to understand patterns in the data.\n\n\nPokémon sightings\nThere have been reports of sightings of rare, legendary Pokémon. We have been asked to investigate! We will plot the coordinates of sightings to find out where the Pokémon might be. The X and Y coordinates of the points are stored in list x_p and y_p, respectively\n\nx_p = [9, 6, 2, 3, 1, 7, 1, 6, 1, 7, 23, 26, 25, 23, 21, 23, 23, 20, 30, 23]\ny_p = [8, 4, 10, 6, 0, 4, 10, 10, 6, 1, 29, 25, 30, 29, 29, 30, 25, 27, 26, 30]\n\n_ = sns.scatterplot(x_p, y_p)\nplt.show()\n\n\n\n\nNotice the areas where the sightings are dense. This indicates that there is not one, but two legendary Pokémon out there!"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#basics-of-cluster-analysis",
    "href": "posts/2020-10-19-cluster analysis in python.html#basics-of-cluster-analysis",
    "title": "Cluster Analysis in Python",
    "section": "Basics of cluster analysis",
    "text": "Basics of cluster analysis\n\nWhat is a cluster?\n\nA group of items with similar characteristics\nGoogle News: articles where similar words andword associations appear together\nCustomer Segments\n\n\n\nClustering algorithms\n\nHierarchical clustering\nK means clustering\nOther clustering algorithms: DBSCAN, Gaussian Methods\n\n\n\nHierarchical clustering in SciPy\n\n\nx_coordinates = [80.1, 93.1, 86.6, 98.5, 86.4, 9.5, 15.2, 3.4, 10.4, 20.3, 44.2, 56.8, 49.2, 62.5, 44.0]\ny_coordinates = [87.2, 96.1, 95.6, 92.4, 92.4, 57.7, 49.4, 47.3, 59.1, 55.5, 25.6, 2.1, 10.9, 24.1, 10.3]\ndf_c = pd.DataFrame({'x_cood':x_coordinates, 'y_cood':y_coordinates})\ndf_c.head()\n\n\n\n\n\n  \n    \n      \n      x_cood\n      y_cood\n    \n  \n  \n    \n      0\n      80.1\n      87.2\n    \n    \n      1\n      93.1\n      96.1\n    \n    \n      2\n      86.6\n      95.6\n    \n    \n      3\n      98.5\n      92.4\n    \n    \n      4\n      86.4\n      92.4\n    \n  \n\n\n\n\n\nZ_c = linkage(df_c, method=\"ward\")\ndf_c['cluster_labels'] = fcluster(Z_c, 3, criterion=\"maxclust\")\n_ = sns.scatterplot(data=df_c, x=\"x_cood\", y=\"y_cood\", hue=\"cluster_labels\", palette=\"RdGy\")\nplt.show()\n\n\n\n\n\nK-means clustering in SciPy\n\n\ndf_c = pd.DataFrame({'x_cood':x_coordinates, 'y_cood':y_coordinates})\ncentroids_c, _ = kmeans(df_c, 3)\ndf_c[\"cluster_labels\"], _ = vq(df_c, centroids_c)\n_ = sns.scatterplot(data=df_c, x=\"x_cood\", y=\"y_cood\", hue=\"cluster_labels\", palette=\"RdGy\")\nplt.show()\n\n\n\n\n\nPokémon sightings: hierarchical clustering\nWe are going to continue the investigation into the sightings of legendary Pokémon. In the scatter plot we identified two areas where Pokémon sightings were dense. This means that the points seem to separate into two clusters. We will form two clusters of the sightings using hierarchical clustering.\n\ndf_p = pd.DataFrame({'x':x_p, 'y':y_p})\ndf_p.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      9\n      8\n    \n    \n      1\n      6\n      4\n    \n    \n      2\n      2\n      10\n    \n    \n      3\n      3\n      6\n    \n    \n      4\n      1\n      0\n    \n  \n\n\n\n\n‘x’ and ‘y’ are columns of X and Y coordinates of the locations of sightings, stored in a Pandas data frame,\n\n# Use the linkage() function to compute distance\nZ_p = linkage(df_p, 'ward')\n\n# Generate cluster labels for each data point with two clusters\ndf_p['cluster_labels'] = fcluster(Z_p, 2, criterion='maxclust')\n\n# Plot the points with seaborn\nsns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster_labels\", data=df_p)\nplt.show()\n\n\n\n\nthe resulting plot has an extra cluster labelled 0 in the legend.\n\n\nPokémon sightings: k-means clustering\nWe are going to continue the investigation into the sightings of legendary Pokémon. We will use the same example of Pokémon sightings. We will form clusters of the sightings using k-means clustering.\nx and y are columns of X and Y coordinates of the locations of sightings, stored in a Pandas data frame\n\ndf_p.dtypes\n\nx                 int64\ny                 int64\ncluster_labels    int32\ndtype: object\n\n\n\ndf_p = df_p.apply(lambda x: x.astype(\"float\"))\n\n\n# Compute cluster centers\ncentroids_p, _ = kmeans(df_p, 2)\n\n# Assign cluster labels to each data point\ndf_p['cluster_labels'], _ = vq(df_p, centroids_p)\n\n# Plot the points with seaborn\nsns.scatterplot(x=\"x\", y=\"y\", hue=\"cluster_labels\", data=df_p)\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#data-preparation-for-cluster-analysis",
    "href": "posts/2020-10-19-cluster analysis in python.html#data-preparation-for-cluster-analysis",
    "title": "Cluster Analysis in Python",
    "section": "Data preparation for cluster analysis",
    "text": "Data preparation for cluster analysis\n\nWhy do we need to prepare data for clustering?\n\nVariables have incomparable units (product dimensions in cm, price in $)\nVariables with same units have vastly different scales and variances (expenditures on cereals, travel)\nData in raw form may lead to bias in clustering\nClusters may be heavily dependent on one variable\nSolution: normalization of individual variables\n\n\n\nNormalization of data\n\nNormalization: process of rescaling data to a standard deviation of 1\n\n\nx_new = x / std_dev(x)\n\ndata = [5, 1, 3, 3, 2, 3, 3, 8, 1, 2, 2, 3, 5]\nscaled_data = whiten(data)\nscaled_data\n\narray([2.72733941, 0.54546788, 1.63640365, 1.63640365, 1.09093577,\n       1.63640365, 1.63640365, 4.36374306, 0.54546788, 1.09093577,\n       1.09093577, 1.63640365, 2.72733941])\n\n\n\nIllustration: normalization of data\n\n\n_ = sns.lineplot(x=range(len(data)), y=data, label=\"original\")\n_ = sns.lineplot(x=range(len(data)), y=scaled_data, label='scaled')\nplt.show()\n\n\n\n\n\nNormalize basic list data\nlet us try to normalize some data. goals_for is a list of goals scored by a football team in their last ten matches. Let us standardize the data using the whiten() function.\n\ngoals_for = [4,3,2,3,1,1,2,0,1,4]\n\n# Use the whiten() function to standardize the data\nscaled_goals_for = whiten(goals_for)\nscaled_goals_for\n\narray([3.07692308, 2.30769231, 1.53846154, 2.30769231, 0.76923077,\n       0.76923077, 1.53846154, 0.        , 0.76923077, 3.07692308])\n\n\nthe scaled values have less variations in them.\n\n\nVisualize normalized data\nAfter normalizing the data, we can compare the scaled data to the original data to see the difference.\n\n_ = sns.lineplot(x=range(len(goals_for)), y=goals_for, label=\"original\")\n_ = sns.lineplot(x=range(len(goals_for)), y=scaled_goals_for, label=\"scaled\")\nplt.show()\n\n\n\n\nscaled values have lower variations in them.\n\n\nNormalization of small numbers\n\n# Prepare data\nrate_cuts = [0.0025, 0.001, -0.0005, -0.001, -0.0005, 0.0025, -0.001, -0.0015, -0.001, 0.0005]\n\n# Use the whiten() function to standardize the data\nscaled_rate_cuts = whiten(rate_cuts)\n\n# Plot original data\nplt.plot(rate_cuts, label='original')\n\n# Plot scaled data\nplt.plot(scaled_rate_cuts, label='scaled')\n\nplt.legend()\nplt.show()\n\n\n\n\nthe original data are negligible as compared to the scaled data\n\n\nFIFA 18: Normalize data\nFIFA 18 is a football video game that was released in 2017 for PC and consoles. The dataset that we are about to work on contains data on the 1000 top individual players in the game. We will explore various features of the data as we move ahead.\n\nfifa = pd.read_csv(\"datasets/fifa.csv\")\nfifa.head()\n\n\n\n\n\n  \n    \n      \n      ID\n      name\n      full_name\n      club\n      club_logo\n      special\n      age\n      league\n      birth_date\n      height_cm\n      ...\n      prefers_cb\n      prefers_lb\n      prefers_lwb\n      prefers_ls\n      prefers_lf\n      prefers_lam\n      prefers_lcm\n      prefers_ldm\n      prefers_lcb\n      prefers_gk\n    \n  \n  \n    \n      0\n      20801\n      Cristiano Ronaldo\n      C. Ronaldo dos Santos Aveiro\n      Real Madrid CF\n      https://cdn.sofifa.org/18/teams/243.png\n      2228\n      32\n      Spanish Primera División\n      1985-02-05\n      185.0\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      1\n      158023\n      L. Messi\n      Lionel Messi\n      FC Barcelona\n      https://cdn.sofifa.org/18/teams/241.png\n      2158\n      30\n      Spanish Primera División\n      1987-06-24\n      170.0\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      190871\n      Neymar\n      Neymar da Silva Santos Jr.\n      Paris Saint-Germain\n      https://cdn.sofifa.org/18/teams/73.png\n      2100\n      25\n      French Ligue 1\n      1992-02-05\n      175.0\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      3\n      176580\n      L. Suárez\n      Luis Suárez\n      FC Barcelona\n      https://cdn.sofifa.org/18/teams/241.png\n      2291\n      30\n      Spanish Primera División\n      1987-01-24\n      182.0\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      4\n      167495\n      M. Neuer\n      Manuel Neuer\n      FC Bayern Munich\n      https://cdn.sofifa.org/18/teams/21.png\n      1493\n      31\n      German Bundesliga\n      1986-03-27\n      193.0\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n    \n  \n\n5 rows × 185 columns\n\n\n\nWe will work with two columns, eur_wage, the wage of a player in Euros and eur_value, their current transfer market value.\n\n# Scale wage and value\nfifa['scaled_wage'] = whiten(fifa['eur_wage'])\nfifa['scaled_value'] = whiten(fifa['eur_value'])\n\n# Plot the two columns in a scatter plot\nfifa.plot(x=\"scaled_wage\", y=\"scaled_value\", kind='scatter')\nplt.show()\n\n\n\n\n\n# Check mean and standard deviation of scaled values\nfifa[['scaled_wage', 'scaled_value']].describe()\n\n\n\n\n\n  \n    \n      \n      scaled_wage\n      scaled_value\n    \n  \n  \n    \n      count\n      1000.000000\n      1000.000000\n    \n    \n      mean\n      1.119812\n      1.306272\n    \n    \n      std\n      1.000500\n      1.000500\n    \n    \n      min\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.467717\n      0.730412\n    \n    \n      50%\n      0.854794\n      1.022576\n    \n    \n      75%\n      1.407184\n      1.542995\n    \n    \n      max\n      9.112425\n      8.984064\n    \n  \n\n\n\n\nthe scaled values have a standard deviation of 1."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#basics-of-hierarchical-clustering",
    "href": "posts/2020-10-19-cluster analysis in python.html#basics-of-hierarchical-clustering",
    "title": "Cluster Analysis in Python",
    "section": "Basics of hierarchical clustering",
    "text": "Basics of hierarchical clustering\n\nCreating a distance matrix using linkage\n\nscipy.cluster.hierarchy.linkage(observations,\n                                method='single',\n                                metric='euclidean',\n                                optimal_ordering=False\n)\n\nmethod: how to calculate the proximity of clusters\nmetric: distance metric\noptimal_ordering: order data points\n\n\nWhich method should use?\n\nsingle: based on two closest objects\ncomplete: based on two farthest objects\naverage: based on the arithmetic mean of all objects\ncentroid: based on the geometric mean of all objects\nmedian: based on the median of all objects\nward: based on the sum of squares\n\n\n\nCreate cluster labels with fcluster\n\nscipy.cluster.hierarchy.fcluster(distance_matrix,\n                                 num_clusters,\n                                 criterion\n)\n\ndistance_matrix: output of linkage() method\nnum_clusters: number of clusters\ncriterion: how to decide thresholds to form clusters\n\n\nFinal thoughts on selecting a method\n\nNo one right method for all\nNeed to carefully understand the distribution of data\n\n\n\nHierarchical clustering: ward method\nIt is time for Comic-Con! Comic-Con is an annual comic-based convention held in major cities in the world. We have the data of last year’s footfall, the number of people at the convention ground at a given time. We would like to decide the location of the stall to maximize sales. Using the ward method, we’ll apply hierarchical clustering to find the two points of attraction in the area.\n\ncomic_con = pd.read_csv(\"datasets/comic_con.csv\")\ncomic_con.head()\n\n\n\n\n\n  \n    \n      \n      x_coordinate\n      y_coordinate\n      x_scaled\n      y_scaled\n    \n  \n  \n    \n      0\n      17\n      4\n      0.509349\n      0.090010\n    \n    \n      1\n      20\n      6\n      0.599234\n      0.135015\n    \n    \n      2\n      35\n      0\n      1.048660\n      0.000000\n    \n    \n      3\n      14\n      0\n      0.419464\n      0.000000\n    \n    \n      4\n      37\n      4\n      1.108583\n      0.090010\n    \n  \n\n\n\n\n\n# Use the linkage() function\ndistance_matrix_cc = linkage(comic_con[['x_scaled', 'y_scaled']], method = \"ward\", metric = 'euclidean')\n\n# Assign cluster labels\ncomic_con['cluster_labels'] = fcluster(distance_matrix_cc, 2, criterion='maxclust')\n\n# Plot clusters\nsns.scatterplot(x='x_scaled', y='y_scaled', \n                hue='cluster_labels', data = comic_con)\nplt.show()\n\n\n\n\nthe two clusters correspond to the points of attractions in the figure towards the bottom (a stage) and the top right (an interesting stall).\n\n\nHierarchical clustering: single method\nLet us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.\n\n# Use the linkage() function\ndistance_matrix_cc = linkage(comic_con[['x_scaled', 'y_scaled']], method = \"single\", metric = \"euclidean\")\n\n# Assign cluster labels\ncomic_con['cluster_labels'] = fcluster(distance_matrix_cc, 2, criterion=\"maxclust\")\n\n# Plot clusters\nsns.scatterplot(x='x_scaled', y='y_scaled', \n                hue='cluster_labels', data = comic_con)\nplt.show()\n\n\n\n\nthe clusters formed are not different from the ones created using the ward method.\n\n\nHierarchical clustering: complete method\nFor the third and final time, let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.\n\n# Use the linkage() function\ndistance_matrix_cc = linkage(comic_con[['x_scaled', 'y_scaled']], method=\"complete\")\n\n# Assign cluster labels\ncomic_con['cluster_labels'] = fcluster(distance_matrix_cc, 2, criterion=\"maxclust\")\n\n# Plot clusters\nsns.scatterplot(x='x_scaled', y='y_scaled', \n                hue='cluster_labels', data = comic_con)\nplt.show()\n\n\n\n\nCoincidentally, the clusters formed are not different from the ward or single methods."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#visualize-clusters",
    "href": "posts/2020-10-19-cluster analysis in python.html#visualize-clusters",
    "title": "Cluster Analysis in Python",
    "section": "Visualize clusters",
    "text": "Visualize clusters\n\nWhy visualize clusters?\n\nTry to make sense of the clusters formed\nAn additional step in validation of clusters\nSpot trends in data\n\n\n\nAn introduction to seaborn\n\nseaborn: a Python data visualization library based on matplotlib\nHas better, easily modiable aesthetics than matplotlib!\nContains functions that make data visualization tasks easy in the context of data analytics\nUse case for clustering: hue parameter for plots\n\n\n\nVisualize clusters with matplotlib\n\n# Plot a scatter plot\ncomic_con.plot.scatter(x='x_scaled',\n                       y='y_scaled',\n                       c='cluster_labels')\nplt.show()\n\n\n\n\n\n\nVisualize clusters with seaborn\n\n# Plot a scatter plot using seaborn\nsns.scatterplot(x='x_scaled', \n                y='y_scaled', \n                hue='cluster_labels', \n                data = comic_con)\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#how-many-clusters",
    "href": "posts/2020-10-19-cluster analysis in python.html#how-many-clusters",
    "title": "Cluster Analysis in Python",
    "section": "How many clusters?",
    "text": "How many clusters?\n\nIntroduction to dendrograms\n\nStrategy till now - decide clusters on visual inspection\nDendrograms help in showing progressions as clusters are merged\nA dendrogram is a branching diagram that demonstrates how each cluster is composed by branching out into its child nodes\n\n\n\nCreate a dendrogram\nDendrograms are branching diagrams that show the merging of clusters as we move through the distance matrix. Let us use the Comic Con footfall data to create a dendrogram.\n\n# Create a dendrogram\ndn_cc = dendrogram(distance_matrix_cc)\n\n# Display the dendogram\nplt.show()\n\n\n\n\nthe top two clusters are farthest away from each other."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#limitations-of-hierarchical-clustering",
    "href": "posts/2020-10-19-cluster analysis in python.html#limitations-of-hierarchical-clustering",
    "title": "Cluster Analysis in Python",
    "section": "Limitations of hierarchical clustering",
    "text": "Limitations of hierarchical clustering\n\nMeasuring speed in hierarchical clustering\n\ntimeit module\nMeasure the speed of .linkage() method\nUse randomly generated points\nRun various iterations to extrapolate\n\n\n\npoints_s = 100\ndf_s = pd.DataFrame({'x':np.random.sample(points_s),\n                    'y':np.random.sample(points_s)})\n%timeit linkage(df_s[['x', 'y']], method='ward', metric='euclidean')\n\n2 ms ± 312 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nComparison of runtime of linkage method\n\nIncreasing runtime with data points\nQuadratic increase of runtime\nNot feasible for large datasets\n\n\n\n%timeit linkage(comic_con[['x_scaled', 'y_scaled']], method=\"complete\", metric='euclidean')\n\n1.47 ms ± 70 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\nFIFA 18: exploring defenders\nIn the FIFA 18 dataset, various attributes of players are present. Two such attributes are:\n\nsliding tackle: a number between 0-99 which signifies how accurate a player is able to perform sliding tackles\naggression: a number between 0-99 which signifies the commitment and will of a player\n\nThese are typically high in defense-minded players. We will perform clustering based on these attributes in the data.\n\nfifa[['sliding_tackle', 'aggression']].head()\n\n\n\n\n\n  \n    \n      \n      sliding_tackle\n      aggression\n    \n  \n  \n    \n      0\n      23\n      63\n    \n    \n      1\n      26\n      48\n    \n    \n      2\n      33\n      56\n    \n    \n      3\n      38\n      78\n    \n    \n      4\n      11\n      29\n    \n  \n\n\n\n\n\nfifa['scaled_sliding_tackle'] = whiten(fifa.sliding_tackle)\nfifa['scaled_aggression'] = whiten(fifa.aggression)\n\n\n# Fit the data into a hierarchical clustering algorithm\ndistance_matrix_f = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')\n# Assign cluster labels to each row of data\nfifa['cluster_labels'] = fcluster(distance_matrix_f, 3, criterion='maxclust')\n# Display cluster centers of each cluster\nfifa[['scaled_sliding_tackle', 'scaled_aggression', 'cluster_labels']].groupby('cluster_labels').mean()\n\n\n\n\n\n  \n    \n      \n      scaled_sliding_tackle\n      scaled_aggression\n    \n    \n      cluster_labels\n      \n      \n    \n  \n  \n    \n      1\n      2.837810\n      4.280968\n    \n    \n      2\n      0.579966\n      1.766698\n    \n    \n      3\n      1.166930\n      3.415214\n    \n  \n\n\n\n\n\n# Create a scatter plot through seaborn\nsns.scatterplot(x=\"scaled_sliding_tackle\", y=\"scaled_aggression\", hue=\"cluster_labels\", data=fifa)\nplt.show()"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#basics-of-k-means-clustering",
    "href": "posts/2020-10-19-cluster analysis in python.html#basics-of-k-means-clustering",
    "title": "Cluster Analysis in Python",
    "section": "Basics of k-means clustering",
    "text": "Basics of k-means clustering\n\nWhy k-means clustering?\n\nA critical drawback of hierarchical clustering: runtime\nK means runs signicantly faster on large datasets\n\n\n\nStep 1: Generate cluster centers\n\nkmeans(obs, k_or_guess, iter, thresh, check_finite)\n\nobs: standardized observations\nk_or_guess: number of clusters\niter: number of iterations (default: 20)\nthres: threshold (default: 1e-05)\ncheck_finite: whether to check if observations contain only finite numbers (default: True)\nReturns two objects:\n\ncluster centers, distortion\n\n\n\nStep 2: Generate cluster labels\n\nvq(obs, code_book, check_finite=True)\n\nobs: standardized observations\ncode_book: cluster centers\ncheck_finite: whether to check if observations contain only finite numbers (default: True)\nReturns two objects:\n\na list of cluster labels,\na list of distortions\n\n\n\nA note on distortions\n\nkmeans returns a single value of distortions\nvq returns a list of distortions.\n\n\n\nRunning k-means\n\n\nK-means clustering\nLet us use the Comic Con dataset and check how k-means clustering works on it.\nthe two steps of k-means clustering:\n\nDefine cluster centers through kmeans() function. It has two required arguments: observations and number of clusters.\nAssign cluster labels through the vq() function. It has two required arguments: observations and cluster centers.\n\n\n# Generate cluster centers\ncluster_centers_cc, distortion_cc = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n\n# Assign cluster labels\ncomic_con['cluster_labels'], distortion_list_cc = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers_cc)\n\n# Plot clusters\nsns.scatterplot(x='x_scaled', y='y_scaled', \n                hue='cluster_labels', data = comic_con)\nplt.show()\n\n\n\n\n\n\nRuntime of k-means clustering\n\n%timeit kmeans(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 3)\n\n31.9 ms ± 3.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nIt took only about 5 seconds to run hierarchical clustering on this data, but only 50 milliseconds to run k-means clustering."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#how-many-clusters-1",
    "href": "posts/2020-10-19-cluster analysis in python.html#how-many-clusters-1",
    "title": "Cluster Analysis in Python",
    "section": "How many clusters?",
    "text": "How many clusters?\n\nHow to find the right k?\n\nNo absolute method to find right number of clusters (k) in k-means clustering\nElbow method\n\n\n\nDistortions revisited\n\nDistortion: sum of squared distances of points from cluster centers\nDecreases with an increasing number ofclusters\nBecomes zero when the number of clusters equals the number of points\nElbow plot: line plot between cluster centers and distortion\n\n\n\nElbow method\n\nElbow plot: plot of the number of clusters and distortion\nElbow plot helps indicate number of clusters present in data\n\n\n\nFinal thoughts on using the elbow method\n\nOnly gives an indication of optimal k (numbers of clusters)\nDoes not always pinpoint how many k (numbers of clusters)\nOther methods: average silhouette and gap statistic\n\n\n\nElbow method on distinct clusters\nLet us use the comic con data set to see how the elbow plot looks on a data set with distinct, well-defined clusters.\n\ndistortions_cc = []\nnum_clusters_cc = range(1, 7)\n\n# Create a list of distortions from the kmeans function\nfor i in num_clusters_cc:\n    cluster_centers_cc, distortion_cc = kmeans(comic_con[['x_scaled', 'y_scaled']], i)\n    distortions_cc.append(distortion_cc)\n\n# Create a data frame with two lists - num_clusters, distortions\nelbow_plot_cc = pd.DataFrame({'num_clusters': num_clusters_cc, 'distortions': distortions_cc})\n\n# Creat a line plot of num_clusters and distortions\nsns.lineplot(x=\"num_clusters\", y=\"distortions\", data = elbow_plot_cc)\nplt.xticks(num_clusters_cc)\nplt.show()\n\n\n\n\n\nuniform_data = pd.read_csv(\"datasets/uniform_data.csv\")\ndistortions_u = []\nnum_clusters_u = range(2, 7)\n\n# Create a list of distortions from the kmeans function\nfor i in num_clusters_u:\n    cluster_centers_u, distortion_u = kmeans(uniform_data[['x_scaled', 'y_scaled']], i)\n    distortions_u.append(distortion_u)\n\n# Create a data frame with two lists - number of clusters and distortions\nelbow_plot_u = pd.DataFrame({'num_clusters': num_clusters_u, 'distortions': distortions_u})\n\n# Creat a line plot of num_clusters and distortions\nsns.lineplot(x=\"num_clusters\", y=\"distortions\", data=elbow_plot_u)\nplt.xticks(num_clusters_u)\nplt.show()\n\n\n\n\nThere is no well defined elbow in this plot!"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#limitations-of-k-means-clustering",
    "href": "posts/2020-10-19-cluster analysis in python.html#limitations-of-k-means-clustering",
    "title": "Cluster Analysis in Python",
    "section": "Limitations of k-means clustering",
    "text": "Limitations of k-means clustering\n\nHow to find the right K (number of clusters)?\nImpact of seeds\nBiased towards equal sized clusters\n\n\nFinal thoughts\n\nEach technique has its pros and cons\nConsider your data size and patterns before deciding on algorithm\nClustering is exploratory phase of analysis\n\n\n\nImpact of seeds on distinct clusters\nwhether seeds impact the clusters in the Comic Con data, where the clusters are well-defined.\n\n# Initialize seed\nnp.random.seed(0)\n\n# Run kmeans clustering\ncluster_centers_cc, distortion_cc = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\ncomic_con['cluster_labels'], distortion_list_cc = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers_cc)\n\n# Plot the scatterplot\nsns.scatterplot(x='x_scaled', y='y_scaled', \n                hue='cluster_labels', data = comic_con)\nplt.show()\n\n\n\n\n\n# Initialize seed\nnp.random.seed([1,2,1000])\n\n# Run kmeans clustering\ncluster_centers_cc, distortion_cc = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\ncomic_con['cluster_labels'], distortion_list_cc = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers_cc)\n\n# Plot the scatterplot\nsns.scatterplot(x='x_scaled', y='y_scaled', \n                hue='cluster_labels', data = comic_con)\nplt.show()\n\n\n\n\n\n\nUniform clustering patterns\nlet us look at the bias in k-means clustering towards the formation of uniform clusters. Let us use a mouse-like dataset for our next exercise. A mouse-like dataset is a group of points that resemble the head of a mouse: it has three clusters of points arranged in circles, one each for the face and two ears of a mouse.\n\nmouse = pd.read_csv(\"datasets/mouse.csv\")\n# Generate cluster centers\ncluster_centers_m, distortion_m = kmeans(mouse[['x_scaled', 'y_scaled']], 3)\n\n# Assign cluster labels\nmouse['cluster_labels'], distortion_list_m = vq(mouse[['x_scaled', 'y_scaled']], cluster_centers_m)\n\n# Plot clusters\nsns.scatterplot(x='x_scaled', y='y_scaled', \n                hue='cluster_labels', data = mouse)\nplt.show()\n\n\n\n\nkmeans is unable to capture the three visible clusters clearly, and the two clusters towards the top have taken in some points along the boundary. This happens due to the underlying assumption in kmeans algorithm to minimize distortions which leads to clusters that are similar in terms of area.\n\n\nFIFA 18: defenders revisited\nIn the FIFA 18 dataset, various attributes of players are present. Two such attributes are:\n\ndefending: a number which signifies the defending attributes of a player\nphysical: a number which signifies the physical attributes of a player\n\nThese are typically defense-minded players. We will perform clustering based on these attributes in the data.\n\nfifa = pd.read_csv(\"datasets/fifa2.csv\")\n\n# Set up a random seed in numpy\nnp.random.seed([1000, 2000])\n\n# Fit the data into a k-means algorithm\ncluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)\n# Assign cluster labels\nfifa['cluster_labels'],_ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)\n# Display cluster centers \nfifa[['scaled_def', 'scaled_phy', 'cluster_labels']].groupby('cluster_labels').mean()\n\n\n\n\n\n  \n    \n      \n      scaled_def\n      scaled_phy\n    \n    \n      cluster_labels\n      \n      \n    \n  \n  \n    \n      0\n      3.743692\n      8.867419\n    \n    \n      1\n      1.865936\n      7.082691\n    \n    \n      2\n      2.096297\n      8.944870\n    \n  \n\n\n\n\n\n# Create a scatter plot through seaborn\nsns.scatterplot(x=\"scaled_def\", y=\"scaled_phy\", hue=\"cluster_labels\", data=fifa)\nplt.show()\n\n\n\n\nthe seed has an impact on clustering as the data is uniformly distributed."
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#dominant-colors-in-images",
    "href": "posts/2020-10-19-cluster analysis in python.html#dominant-colors-in-images",
    "title": "Cluster Analysis in Python",
    "section": "Dominant colors in images",
    "text": "Dominant colors in images\n\nAll images consist of pixelsEach pixel has three values: Red, Green and Blue\nPixel color: combination of these RGB values\nPerform k-means on standardized RGB values to find cluster centers\nUses: Identifying features in satellite images\n\n\nTools to find dominant colors\n\nConvert image to pixels: matplotlib.image.imread\nDisplay colors of cluster centers: matplotlib.pyplot.imshow \n\n\n\nimage = img.imread(\"datasets/sea.jpg\")\nimage.shape\n\n(390, 632, 3)\n\n\n\nimage[0][:1]\n\narray([[255, 255, 255]], dtype=uint8)\n\n\n\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x2088a4426a0>\n\n\n\n\n\n\nr = []\ng = []\nb = []\nfor row in image:\n    for pixel in row:\n        temp_r, temp_g, temp_b = pixel\n        r.append(temp_r)\n        g.append(temp_g)\n        b.append(temp_b)\n\n\nData frame with RGB values\n\n\npixels = pd.DataFrame({'red':r, 'green':g, 'blue':b})\npixels.head()\n\n\n\n\n\n  \n    \n      \n      red\n      green\n      blue\n    \n  \n  \n    \n      0\n      255\n      255\n      255\n    \n    \n      1\n      255\n      255\n      255\n    \n    \n      2\n      255\n      255\n      255\n    \n    \n      3\n      255\n      255\n      255\n    \n    \n      4\n      255\n      255\n      255\n    \n  \n\n\n\n\n\nCreate an elbow plot\n\n\npixels[['scaled_red', 'scaled_blue', 'scaled_green']] = pixels[['red', 'blue', 'green']].apply(lambda x: x/np.std(x)*255)\ndistortions_i = []\nnum_clusters_i = []\n\nfor i in num_clusters_i:\n    cluster_centers_i, distortion_i = kmeans(pixels[['scaled_red', 'scaled_blue', 'scaled_green']], i)\n    distortions_i.append(distortion_i)\n\nelbow_plot_i = pd.DataFrame({'num_clusters':num_clusters_i, 'distortions':distortions_i})\n_ = sns.lineplot(data=elbow_plot_i, x=\"num_clusters\", y='distortions')\nplt.xticks(num_clusters_i)\nplt.show()\n\n\n\n\n\nExtract RGB values from image\nThere are broadly three steps to find the dominant colors in an image:\n\nExtract RGB values into three lists.\nPerform k-means clustering on scaled RGB values.\nDisplay the colors of cluster centers.\n\n\nbatman_df = pd.read_csv(\"datasets/batman.csv\")\nbatman_df.head()\n\n\n\n\n\n  \n    \n      \n      red\n      blue\n      green\n      scaled_red\n      scaled_blue\n      scaled_green\n    \n  \n  \n    \n      0\n      10\n      15\n      9\n      0.134338\n      0.179734\n      0.126269\n    \n    \n      1\n      14\n      49\n      36\n      0.188074\n      0.587133\n      0.505076\n    \n    \n      2\n      55\n      125\n      103\n      0.738862\n      1.497787\n      1.445077\n    \n    \n      3\n      35\n      129\n      98\n      0.470185\n      1.545716\n      1.374928\n    \n    \n      4\n      38\n      134\n      101\n      0.510486\n      1.605628\n      1.417017\n    \n  \n\n\n\n\n\ndistortions = []\nnum_clusters = range(1, 7)\n\n# Create a list of distortions from the kmeans function\nfor i in num_clusters:\n    cluster_centers, distortion = kmeans(batman_df[['scaled_red', 'scaled_blue', 'scaled_green']], i)\n    distortions.append(distortion)\n\n# Create a data frame with two lists, num_clusters and distortions\nelbow_plot = pd.DataFrame({'num_clusters':num_clusters, 'distortions':distortions})\n\n# Create a line plot of num_clusters and distortions\nsns.lineplot(x=\"num_clusters\", y=\"distortions\", data = elbow_plot)\nplt.xticks(num_clusters)\nplt.show()\n\n\n\n\nthere are three distinct colors present in the image, which is supported by the elbow plot.\n\n\nDisplay dominant colors\nTo display the dominant colors, we will convert the colors of the cluster centers to their raw values and then converted them to the range of 0-1, using the following formula:\nconverted_pixel = standardized_pixel * pixel_std / 255\n\n# Get standard deviations of each color\nr_std, g_std, b_std = batman_df[['red', 'green', 'blue']].std()\n\ncolors = []\nfor cluster_center in cluster_centers:\n    scaled_r, scaled_g, scaled_b = cluster_center\n    # Convert each standardized value to scaled value\n    colors.append((\n        scaled_r * r_std / 255,\n        scaled_g * g_std / 255,\n        scaled_b * b_std / 255\n    ))\n\n# Display colors of cluster centers\nplt.imshow([colors])\nplt.show()\n\n\n\n\n\n\nDocument clustering\n\nconcepts\n\nClean data before processing\nDetermine the importance of the terms in a document (in TF-IDF matrix)\nCluster the TF-IDF matrix4. Find top terms, documents in each cluste\n\n\n\nClean and tokenize data\n\nConvert text into smaller parts called tokens, clean data for processing\n\n\n\nTF-IDF (Term Frequency - Inverse DocumentFrequency)\n\nA weighted measure: evaluate how important a word is to a document in a collection\n\n\n\nClustering with sparse matrix\n\nkmeans() in SciPy does not support sparse matrices\nUse .todense() to convert to a matrix\n\n\ncluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n\nTop terms per cluster\n\nCluster centers: lists with a size equal to the number of terms\nEach value in the cluster center is its importance\nCreate a dictionary and print top terms\n\n\n\nMore considerations\n\nWork with hyperlinks, emoticons etc.\nNormalize words (run, ran, running -> run)\n.todense() may not work with large datasets\n\n\n\ndef remove_noise(text, stop_words = []):    \n    tokens = word_tokenize(text)    \n    cleaned_tokens = []    \n    for token in tokens:        \n        token = re.sub('[^A-Za-z0-9]+', '', token)        \n        if len(token) > 1 and token.lower() not in stop_words:            \n            # Get lowercase            \n            cleaned_tokens.append(token.lower())    \n    return cleaned_tokens\nremove_noise(\"It is lovely weather we are having. I hope the weather continues.\")\n\n['it',\n 'is',\n 'lovely',\n 'weather',\n 'we',\n 'are',\n 'having',\n 'hope',\n 'the',\n 'weather',\n 'continues']\n\n\n\n\nTF-IDF of movie plots\nLet us use the plots of randomly selected movies to perform document clustering on. Before performing clustering on documents, they need to be cleaned of any unwanted noise (such as special characters and stop words) and converted into a sparse matrix through TF-IDF of the documents.\n\nstop_words_2 = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt']\n\n\nremove_noise(\"It is lovely weather we are having. I hope the weather continues.\", stop_words=stop_words_2)\n\n['lovely', 'weather', 'hope', 'weather', 'continues']\n\n\n\ndef remove_noise(text, stop_words = stop_words_2):    \n    tokens = word_tokenize(text)    \n    cleaned_tokens = []    \n    for token in tokens:        \n        token = re.sub('[^A-Za-z0-9]+', '', token)        \n        if len(token) > 1 and token.lower() not in stop_words:            \n            # Get lowercase            \n            cleaned_tokens.append(token.lower())    \n    return cleaned_tokens\n\n\nplots = pd.read_csv(\"datasets/plots.csv\")['0'].to_list()\nplots[0][:10]\n\n'Cable Hogu'\n\n\n\n# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(min_df=.1, max_df=.75, max_features=50, tokenizer=remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(plots)\n\n\n\nTop terms in movie clusters\n\nnum_clusters = 2\n\n# Generate cluster centers through the kmeans function\ncluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    # Sort the terms and print top 3 terms\n    center_terms = dict(zip(terms, cluster_centers[i]))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms[:3])\n\n['police', 'man', 'one']\n['father', 'back', 'one']"
  },
  {
    "objectID": "posts/2020-10-19-cluster analysis in python.html#clustering-with-multiple-features",
    "href": "posts/2020-10-19-cluster analysis in python.html#clustering-with-multiple-features",
    "title": "Cluster Analysis in Python",
    "section": "Clustering with multiple features",
    "text": "Clustering with multiple features\n\nFeature reduction\n\nFactoranalysis\nMultidimensional scaling\n\n\n\nClustering with many features\nReduce features using a technique like Factor Analysis. explore steps to reduce the number of features.\n\n\nBasic checks on clusters\nIn the FIFA 18 dataset, we have concentrated on defenders in previous exercises. Let us try to focus on attacking attributes of a player. Pace (pac), Dribbling (dri) and Shooting (sho) are features that are present in attack minded players.\n\nfifa = pd.read_csv(\"datasets/fifa3.csv\")\n# Print the size of the clusters\nfifa.groupby(\"cluster_labels\")['ID'].count()\n\ncluster_labels\n0     83\n1    107\n2     60\nName: ID, dtype: int64\n\n\n\n# Print the mean value of wages in each cluster\nfifa.groupby([\"cluster_labels\"])['eur_wage'].mean()\n\ncluster_labels\n0    132108.433735\n1    130308.411215\n2    117583.333333\nName: eur_wage, dtype: float64\n\n\nthe cluster sizes are not very different, and there are no significant differences that can be seen in the wages. Further analysis is required to validate these clusters.\n\n\nFIFA 18: what makes a complete player?\nThe overall level of a player in FIFA 18 is defined by six characteristics: pace (pac), shooting (sho), passing (pas), dribbling (dri), defending (def), physical (phy).\nHere is a sample card: \n\nfeatures= ['pac', 'sho', 'pas', 'dri', 'def', 'phy']\nscaled_features = ['scaled_pac',\n 'scaled_sho',\n 'scaled_pas',\n 'scaled_dri',\n 'scaled_def',\n 'scaled_phy']\nfifa[scaled_features] = fifa[features].apply(lambda x: whiten(x))\n# Create centroids with kmeans for 2 clusters\ncluster_centers,_ = kmeans(fifa[scaled_features], 2)\n# Assign cluster labels and print cluster centers\nfifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)\nfifa.groupby(\"cluster_labels\")[scaled_features].mean()\n\n\n\n\n\n  \n    \n      \n      scaled_pac\n      scaled_sho\n      scaled_pas\n      scaled_dri\n      scaled_def\n      scaled_phy\n    \n    \n      cluster_labels\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      6.617743\n      3.885153\n      7.353643\n      7.148098\n      3.862353\n      9.009407\n    \n    \n      1\n      7.762181\n      5.610629\n      8.620873\n      8.968266\n      2.262328\n      8.009867\n    \n  \n\n\n\n\n\n# Plot cluster centers to visualize clusters\nfifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind=\"bar\")\nplt.show()\n\n\n\n\n\n# Get the name column of first 5 players in each cluster\nfor cluster in fifa['cluster_labels'].unique():\n    print(cluster, fifa[fifa['cluster_labels'] == cluster]['name'].values[:5])\n\n1 ['Cristiano Ronaldo' 'L. Messi' 'Neymar' 'L. Suárez' 'M. Neuer']\n0 ['Sergio Ramos' 'G. Chiellini' 'L. Bonucci' 'J. Boateng' 'D. Godín']\n\n\nthe top players in each cluster are representative of the overall characteristics of the cluster - one of the clusters primarily represents attackers, whereas the other represents defenders. Surprisingly, a top goalkeeper Manuel Neuer is seen in the attackers group, but he is known for going out of the box and participating in open play, which are reflected in his FIFA 18 attributes."
  },
  {
    "objectID": "posts/2020-08-30-weather visualization.html",
    "href": "posts/2020-08-30-weather visualization.html",
    "title": "Weather Visualization",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\n\n\naustin_weather = pd.read_csv('../data/austin_weather.csv')\nseattle_weather = pd.read_csv('../data/seattle_weather.csv')\n\n\naustin_weather.head()\n\n\n\n\n\n  \n    \n      \n      DATE\n      STATION\n      NAME\n      MLY-CLDD-BASE45\n      MLY-CLDD-BASE50\n      MLY-CLDD-BASE55\n      MLY-CLDD-BASE57\n      MLY-CLDD-BASE60\n      MLY-CLDD-BASE70\n      MLY-CLDD-BASE72\n      ...\n      MLY-TMIN-AVGNDS-LSTH070\n      MLY-TMIN-NORMAL\n      MLY-TMIN-PRBOCC-LSTH016\n      MLY-TMIN-PRBOCC-LSTH020\n      MLY-TMIN-PRBOCC-LSTH024\n      MLY-TMIN-PRBOCC-LSTH028\n      MLY-TMIN-PRBOCC-LSTH032\n      MLY-TMIN-PRBOCC-LSTH036\n      MLY-TMIN-STDDEV\n      MONTH\n    \n  \n  \n    \n      0\n      1\n      USW00013904\n      AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US\n      190\n      103\n      50\n      35\n      18\n      1\n      -7777\n      ...\n      310\n      36.3\n      298\n      570\n      839\n      967\n      997\n      1000\n      2.9\n      Jan\n    \n    \n      1\n      2\n      USW00013904\n      AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US\n      228\n      132\n      68\n      49\n      29\n      3\n      1\n      ...\n      280\n      39.4\n      103\n      327\n      614\n      867\n      973\n      999\n      3.2\n      Feb\n    \n    \n      2\n      3\n      USW00013904\n      AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US\n      446\n      306\n      185\n      146\n      98\n      13\n      6\n      ...\n      308\n      46.6\n      10\n      73\n      242\n      494\n      761\n      928\n      3.6\n      Mar\n    \n    \n      3\n      4\n      USW00013904\n      AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US\n      668\n      519\n      373\n      318\n      240\n      53\n      32\n      ...\n      287\n      54.7\n      0\n      0\n      0\n      48\n      189\n      453\n      4.1\n      Apr\n    \n    \n      4\n      5\n      USW00013904\n      AUSTIN BERGSTROM INTERNATIONAL AIRPORT, TX US\n      936\n      781\n      626\n      564\n      471\n      181\n      134\n      ...\n      250\n      63.7\n      0\n      0\n      0\n      0\n      0\n      0\n      2.5\n      May\n    \n  \n\n5 rows × 68 columns\n\n\n\n\nseattle_weather.head()\n\n\n\n\n\n  \n    \n      \n      DATE\n      STATION\n      NAME\n      MLY-CLDD-BASE45\n      MLY-CLDD-BASE50\n      MLY-CLDD-BASE55\n      MLY-CLDD-BASE57\n      MLY-CLDD-BASE60\n      MLY-CLDD-BASE70\n      MLY-CLDD-BASE72\n      ...\n      MLY-TMIN-AVGNDS-LSTH070\n      MLY-TMIN-NORMAL\n      MLY-TMIN-PRBOCC-LSTH016\n      MLY-TMIN-PRBOCC-LSTH020\n      MLY-TMIN-PRBOCC-LSTH024\n      MLY-TMIN-PRBOCC-LSTH028\n      MLY-TMIN-PRBOCC-LSTH032\n      MLY-TMIN-PRBOCC-LSTH036\n      MLY-TMIN-STDDEV\n      MONTH\n    \n  \n  \n    \n      0\n      1\n      USW00094290\n      SEATTLE SAND PT WSFO, WA US\n      27.0\n      3.0\n      -7777.0\n      -7777.0\n      0.0\n      0.0\n      0.0\n      ...\n      310.0\n      37.0\n      64.0\n      129.0\n      317.0\n      709.0\n      959.0\n      1000.0\n      2.3\n      Jan\n    \n    \n      1\n      2\n      USW00094290\n      SEATTLE SAND PT WSFO, WA US\n      31.0\n      3.0\n      -7777.0\n      -7777.0\n      0.0\n      0.0\n      0.0\n      ...\n      280.0\n      36.9\n      15.0\n      76.0\n      273.0\n      616.0\n      917.0\n      1000.0\n      2.6\n      Feb\n    \n    \n      2\n      3\n      USW00094290\n      SEATTLE SAND PT WSFO, WA US\n      81.0\n      16.0\n      2.0\n      -7777.0\n      -7777.0\n      0.0\n      0.0\n      ...\n      310.0\n      39.3\n      0.0\n      20.0\n      41.0\n      152.0\n      670.0\n      986.0\n      1.8\n      Mar\n    \n    \n      3\n      4\n      USW00094290\n      SEATTLE SAND PT WSFO, WA US\n      169.0\n      58.0\n      12.0\n      6.0\n      1.0\n      0.0\n      0.0\n      ...\n      300.0\n      42.5\n      0.0\n      0.0\n      0.0\n      0.0\n      114.0\n      711.0\n      1.5\n      Apr\n    \n    \n      4\n      5\n      USW00094290\n      SEATTLE SAND PT WSFO, WA US\n      343.0\n      193.0\n      78.0\n      49.0\n      21.0\n      -7777.0\n      -7777.0\n      ...\n      310.0\n      47.8\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      58.0\n      1.9\n      May\n    \n  \n\n5 rows × 81 columns\n\n\n\n\nWeather Patterns in Seattle and Austin\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(14,6))\n\n# Plot MLY-PRCP-NORMAL from seattle_weather against the MONTH\nax.plot(seattle_weather[\"MONTH\"], seattle_weather['MLY-PRCP-NORMAL'], c='blue', marker='o', linestyle='dashdot')\n\n# Plot MLY-PRCP-NORMAL from austin_weather against MONTH\nax.plot(austin_weather['MONTH'], austin_weather['MLY-PRCP-NORMAL'], c='red', marker='v', linestyle='--')\n\nax.set_xlabel('Time (months)')\nax.set_ylabel('Precipitation (inches)')\nax.set_title('Weather patterns in Seattle and Austin')\n\nplt.show()\n\n\n\n\n\n\nMonthly Precipitation and Temperature in Seattle and Austin\n\nfig,ax=plt.subplots(2,2, sharey=True, figsize=(15,4))\nax[0,0].plot(seattle_weather['MONTH'], seattle_weather['MLY-PRCP-NORMAL'])\nax[0,1].plot(seattle_weather['MONTH'], seattle_weather['MLY-TAVG-NORMAL'])\nax[1,0].plot(austin_weather['MONTH'], austin_weather['MLY-PRCP-NORMAL'])\nax[1,1].plot(austin_weather['MONTH'], austin_weather['MLY-TAVG-NORMAL'])\nplt.show()\n\n\n\n\n\n\nPrecipitation data in Seattle and Austin\n\n# Create a figure and an array of axes: 2 rows, 1 column with shared y axis\nfig, ax = plt.subplots(2, 1, sharey=True)\n\n# Plot Seattle precipitation data in the top axes\nax[0].plot(seattle_weather['MONTH'], seattle_weather['MLY-PRCP-NORMAL'], color = 'b')\nax[0].plot(seattle_weather['MONTH'], seattle_weather['MLY-PRCP-25PCTL'], color = 'b', linestyle = '--')\nax[0].plot(seattle_weather['MONTH'], seattle_weather['MLY-PRCP-75PCTL'], color = 'b', linestyle = '--')\n\n# Plot Austin precipitation data in the bottom axes\nax[1].plot(austin_weather['MONTH'], austin_weather['MLY-PRCP-NORMAL'], color = 'r')\nax[1].plot(austin_weather['MONTH'], austin_weather['MLY-PRCP-25PCTL'], color = 'r', linestyle = '--')\nax[1].plot(austin_weather['MONTH'], austin_weather['MLY-PRCP-75PCTL'], color = 'r', linestyle = '--')\n\nplt.show()\n\n\n\n\n\n\nAdding error-bars\n\nplt.style.use('seaborn')\nfig, ax = plt.subplots(figsize=(14,6))\nax.errorbar(seattle_weather['MONTH'], seattle_weather['MLY-TAVG-NORMAL'],yerr=seattle_weather['MLY-TAVG-STDDEV'])\nax.errorbar(austin_weather['MONTH'], austin_weather['MLY-TAVG-NORMAL'],yerr=austin_weather['MLY-TAVG-STDDEV']) \nax.set_ylabel('Temperature (Fahrenheit)')\nplt.show()"
  }
]